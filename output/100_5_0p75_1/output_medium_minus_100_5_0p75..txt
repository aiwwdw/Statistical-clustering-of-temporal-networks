nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  1%|          | 1/100 [18:22<30:18:17, 1102.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  2%|▏         | 2/100 [31:31<24:59:16, 917.92s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  3%|▎         | 3/100 [49:39<26:50:11, 995.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  4%|▍         | 4/100 [1:05:34<26:07:07, 979.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  5%|▌         | 5/100 [1:20:04<24:48:26, 940.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  6%|▌         | 6/100 [1:32:15<22:41:44, 869.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  7%|▋         | 7/100 [1:48:38<23:24:49, 906.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  8%|▊         | 8/100 [2:06:42<24:36:24, 962.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  9%|▉         | 9/100 [2:23:15<24:34:24, 972.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 10%|█         | 10/100 [2:41:25<25:13:07, 1008.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 11%|█         | 11/100 [2:56:55<24:20:34, 984.66s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 12%|█▏        | 12/100 [3:14:47<24:42:47, 1011.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 13%|█▎        | 13/100 [3:25:19<21:39:47, 896.41s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 14%|█▍        | 14/100 [3:43:08<22:39:34, 948.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 15%|█▌        | 15/100 [4:00:14<22:56:31, 971.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 16%|█▌        | 16/100 [4:16:20<22:38:14, 970.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 17%|█▋        | 17/100 [4:33:19<22:42:05, 984.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 18%|█▊        | 18/100 [4:49:59<22:32:03, 989.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 19%|█▉        | 19/100 [5:04:48<21:34:49, 959.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 20%|██        | 20/100 [5:20:25<21:10:15, 952.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 21%|██        | 21/100 [5:33:53<19:57:05, 909.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 22%|██▏       | 22/100 [5:46:11<18:35:05, 857.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 23%|██▎       | 23/100 [6:02:27<19:06:26, 893.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 24%|██▍       | 24/100 [6:20:40<20:07:12, 953.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 25%|██▌       | 25/100 [6:37:35<20:14:33, 971.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 26%|██▌       | 26/100 [6:53:28<19:51:30, 966.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 27%|██▋       | 27/100 [7:11:23<20:15:19, 998.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 28%|██▊       | 28/100 [7:26:49<19:32:24, 977.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 29%|██▉       | 29/100 [7:43:51<19:32:03, 990.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 30%|███       | 30/100 [7:55:39<17:36:31, 905.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 31%|███       | 31/100 [8:10:56<17:25:28, 909.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 32%|███▏      | 32/100 [8:27:09<17:31:59, 928.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 33%|███▎      | 33/100 [8:42:34<17:15:33, 927.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 34%|███▍      | 34/100 [9:00:27<17:48:01, 970.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11296.490089460178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23621.619140625
inf tensor(23621.6191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11617.26953125
tensor(23621.6191, grad_fn=<NegBackward0>) tensor(11617.2695, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11616.6162109375
tensor(11617.2695, grad_fn=<NegBackward0>) tensor(11616.6162, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11616.29296875
tensor(11616.6162, grad_fn=<NegBackward0>) tensor(11616.2930, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11615.642578125
tensor(11616.2930, grad_fn=<NegBackward0>) tensor(11615.6426, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11614.7626953125
tensor(11615.6426, grad_fn=<NegBackward0>) tensor(11614.7627, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11614.591796875
tensor(11614.7627, grad_fn=<NegBackward0>) tensor(11614.5918, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11614.134765625
tensor(11614.5918, grad_fn=<NegBackward0>) tensor(11614.1348, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11613.08203125
tensor(11614.1348, grad_fn=<NegBackward0>) tensor(11613.0820, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11612.216796875
tensor(11613.0820, grad_fn=<NegBackward0>) tensor(11612.2168, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11429.064453125
tensor(11612.2168, grad_fn=<NegBackward0>) tensor(11429.0645, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11380.072265625
tensor(11429.0645, grad_fn=<NegBackward0>) tensor(11380.0723, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11377.8154296875
tensor(11380.0723, grad_fn=<NegBackward0>) tensor(11377.8154, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11377.263671875
tensor(11377.8154, grad_fn=<NegBackward0>) tensor(11377.2637, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11377.1533203125
tensor(11377.2637, grad_fn=<NegBackward0>) tensor(11377.1533, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11377.0556640625
tensor(11377.1533, grad_fn=<NegBackward0>) tensor(11377.0557, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11377.021484375
tensor(11377.0557, grad_fn=<NegBackward0>) tensor(11377.0215, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11376.9892578125
tensor(11377.0215, grad_fn=<NegBackward0>) tensor(11376.9893, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11376.970703125
tensor(11376.9893, grad_fn=<NegBackward0>) tensor(11376.9707, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11376.955078125
tensor(11376.9707, grad_fn=<NegBackward0>) tensor(11376.9551, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11375.1962890625
tensor(11376.9551, grad_fn=<NegBackward0>) tensor(11375.1963, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11373.349609375
tensor(11375.1963, grad_fn=<NegBackward0>) tensor(11373.3496, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11373.3232421875
tensor(11373.3496, grad_fn=<NegBackward0>) tensor(11373.3232, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11373.3154296875
tensor(11373.3232, grad_fn=<NegBackward0>) tensor(11373.3154, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11373.3095703125
tensor(11373.3154, grad_fn=<NegBackward0>) tensor(11373.3096, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11373.3076171875
tensor(11373.3096, grad_fn=<NegBackward0>) tensor(11373.3076, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11373.3037109375
tensor(11373.3076, grad_fn=<NegBackward0>) tensor(11373.3037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11373.30078125
tensor(11373.3037, grad_fn=<NegBackward0>) tensor(11373.3008, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11373.298828125
tensor(11373.3008, grad_fn=<NegBackward0>) tensor(11373.2988, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11373.2939453125
tensor(11373.2988, grad_fn=<NegBackward0>) tensor(11373.2939, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11373.291015625
tensor(11373.2939, grad_fn=<NegBackward0>) tensor(11373.2910, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11373.28515625
tensor(11373.2910, grad_fn=<NegBackward0>) tensor(11373.2852, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11373.267578125
tensor(11373.2852, grad_fn=<NegBackward0>) tensor(11373.2676, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11373.2646484375
tensor(11373.2676, grad_fn=<NegBackward0>) tensor(11373.2646, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11373.2626953125
tensor(11373.2646, grad_fn=<NegBackward0>) tensor(11373.2627, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11373.2578125
tensor(11373.2627, grad_fn=<NegBackward0>) tensor(11373.2578, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11370.4560546875
tensor(11373.2578, grad_fn=<NegBackward0>) tensor(11370.4561, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11370.3916015625
tensor(11370.4561, grad_fn=<NegBackward0>) tensor(11370.3916, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11370.3916015625
tensor(11370.3916, grad_fn=<NegBackward0>) tensor(11370.3916, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11370.3896484375
tensor(11370.3916, grad_fn=<NegBackward0>) tensor(11370.3896, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11370.3896484375
tensor(11370.3896, grad_fn=<NegBackward0>) tensor(11370.3896, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11370.388671875
tensor(11370.3896, grad_fn=<NegBackward0>) tensor(11370.3887, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11370.3876953125
tensor(11370.3887, grad_fn=<NegBackward0>) tensor(11370.3877, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11370.3876953125
tensor(11370.3877, grad_fn=<NegBackward0>) tensor(11370.3877, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11370.38671875
tensor(11370.3877, grad_fn=<NegBackward0>) tensor(11370.3867, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11370.3857421875
tensor(11370.3867, grad_fn=<NegBackward0>) tensor(11370.3857, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11370.3857421875
tensor(11370.3857, grad_fn=<NegBackward0>) tensor(11370.3857, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11370.3916015625
tensor(11370.3857, grad_fn=<NegBackward0>) tensor(11370.3916, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11370.384765625
tensor(11370.3857, grad_fn=<NegBackward0>) tensor(11370.3848, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11370.384765625
tensor(11370.3848, grad_fn=<NegBackward0>) tensor(11370.3848, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11370.3828125
tensor(11370.3848, grad_fn=<NegBackward0>) tensor(11370.3828, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11370.3837890625
tensor(11370.3828, grad_fn=<NegBackward0>) tensor(11370.3838, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11370.3837890625
tensor(11370.3828, grad_fn=<NegBackward0>) tensor(11370.3838, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11370.3818359375
tensor(11370.3828, grad_fn=<NegBackward0>) tensor(11370.3818, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11370.380859375
tensor(11370.3818, grad_fn=<NegBackward0>) tensor(11370.3809, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11370.3798828125
tensor(11370.3809, grad_fn=<NegBackward0>) tensor(11370.3799, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11370.376953125
tensor(11370.3799, grad_fn=<NegBackward0>) tensor(11370.3770, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11370.3779296875
tensor(11370.3770, grad_fn=<NegBackward0>) tensor(11370.3779, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11370.3759765625
tensor(11370.3770, grad_fn=<NegBackward0>) tensor(11370.3760, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11370.3388671875
tensor(11370.3760, grad_fn=<NegBackward0>) tensor(11370.3389, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11370.341796875
tensor(11370.3389, grad_fn=<NegBackward0>) tensor(11370.3418, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11370.3310546875
tensor(11370.3389, grad_fn=<NegBackward0>) tensor(11370.3311, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11370.3154296875
tensor(11370.3311, grad_fn=<NegBackward0>) tensor(11370.3154, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11319.3515625
tensor(11370.3154, grad_fn=<NegBackward0>) tensor(11319.3516, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11307.3173828125
tensor(11319.3516, grad_fn=<NegBackward0>) tensor(11307.3174, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11307.185546875
tensor(11307.3174, grad_fn=<NegBackward0>) tensor(11307.1855, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11307.1591796875
tensor(11307.1855, grad_fn=<NegBackward0>) tensor(11307.1592, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11307.1552734375
tensor(11307.1592, grad_fn=<NegBackward0>) tensor(11307.1553, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11307.154296875
tensor(11307.1553, grad_fn=<NegBackward0>) tensor(11307.1543, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11307.1494140625
tensor(11307.1543, grad_fn=<NegBackward0>) tensor(11307.1494, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11301.2734375
tensor(11307.1494, grad_fn=<NegBackward0>) tensor(11301.2734, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11301.2431640625
tensor(11301.2734, grad_fn=<NegBackward0>) tensor(11301.2432, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11301.236328125
tensor(11301.2432, grad_fn=<NegBackward0>) tensor(11301.2363, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11301.2353515625
tensor(11301.2363, grad_fn=<NegBackward0>) tensor(11301.2354, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11295.2333984375
tensor(11301.2354, grad_fn=<NegBackward0>) tensor(11295.2334, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11295.23046875
tensor(11295.2334, grad_fn=<NegBackward0>) tensor(11295.2305, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11286.5419921875
tensor(11295.2305, grad_fn=<NegBackward0>) tensor(11286.5420, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11286.529296875
tensor(11286.5420, grad_fn=<NegBackward0>) tensor(11286.5293, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11286.5283203125
tensor(11286.5293, grad_fn=<NegBackward0>) tensor(11286.5283, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11286.5673828125
tensor(11286.5283, grad_fn=<NegBackward0>) tensor(11286.5674, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11286.5302734375
tensor(11286.5283, grad_fn=<NegBackward0>) tensor(11286.5303, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11286.5224609375
tensor(11286.5283, grad_fn=<NegBackward0>) tensor(11286.5225, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11286.521484375
tensor(11286.5225, grad_fn=<NegBackward0>) tensor(11286.5215, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11286.515625
tensor(11286.5215, grad_fn=<NegBackward0>) tensor(11286.5156, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11286.51171875
tensor(11286.5156, grad_fn=<NegBackward0>) tensor(11286.5117, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11286.5107421875
tensor(11286.5117, grad_fn=<NegBackward0>) tensor(11286.5107, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11286.51171875
tensor(11286.5107, grad_fn=<NegBackward0>) tensor(11286.5117, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11286.5107421875
tensor(11286.5107, grad_fn=<NegBackward0>) tensor(11286.5107, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11286.51171875
tensor(11286.5107, grad_fn=<NegBackward0>) tensor(11286.5117, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11286.4990234375
tensor(11286.5107, grad_fn=<NegBackward0>) tensor(11286.4990, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11286.4697265625
tensor(11286.4990, grad_fn=<NegBackward0>) tensor(11286.4697, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11286.4697265625
tensor(11286.4697, grad_fn=<NegBackward0>) tensor(11286.4697, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11286.46875
tensor(11286.4697, grad_fn=<NegBackward0>) tensor(11286.4688, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11286.4697265625
tensor(11286.4688, grad_fn=<NegBackward0>) tensor(11286.4697, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11286.4658203125
tensor(11286.4688, grad_fn=<NegBackward0>) tensor(11286.4658, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11286.4677734375
tensor(11286.4658, grad_fn=<NegBackward0>) tensor(11286.4678, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11286.4677734375
tensor(11286.4658, grad_fn=<NegBackward0>) tensor(11286.4678, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11286.4658203125
tensor(11286.4658, grad_fn=<NegBackward0>) tensor(11286.4658, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11286.4677734375
tensor(11286.4658, grad_fn=<NegBackward0>) tensor(11286.4678, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11286.4931640625
tensor(11286.4658, grad_fn=<NegBackward0>) tensor(11286.4932, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7724, 0.2276],
        [0.2482, 0.7518]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4733, 0.5267], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3066, 0.1068],
         [0.5559, 0.2072]],

        [[0.6467, 0.0978],
         [0.6584, 0.6513]],

        [[0.6937, 0.1007],
         [0.6545, 0.5007]],

        [[0.6960, 0.1000],
         [0.6543, 0.7226]],

        [[0.5424, 0.1015],
         [0.7166, 0.6976]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.9761606886326935
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22235.677734375
inf tensor(22235.6777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11691.390625
tensor(22235.6777, grad_fn=<NegBackward0>) tensor(11691.3906, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11618.2060546875
tensor(11691.3906, grad_fn=<NegBackward0>) tensor(11618.2061, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11616.443359375
tensor(11618.2061, grad_fn=<NegBackward0>) tensor(11616.4434, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11615.369140625
tensor(11616.4434, grad_fn=<NegBackward0>) tensor(11615.3691, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11614.982421875
tensor(11615.3691, grad_fn=<NegBackward0>) tensor(11614.9824, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11614.705078125
tensor(11614.9824, grad_fn=<NegBackward0>) tensor(11614.7051, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11614.2255859375
tensor(11614.7051, grad_fn=<NegBackward0>) tensor(11614.2256, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11614.0419921875
tensor(11614.2256, grad_fn=<NegBackward0>) tensor(11614.0420, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11613.8779296875
tensor(11614.0420, grad_fn=<NegBackward0>) tensor(11613.8779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11613.7392578125
tensor(11613.8779, grad_fn=<NegBackward0>) tensor(11613.7393, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11613.6162109375
tensor(11613.7393, grad_fn=<NegBackward0>) tensor(11613.6162, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11613.4990234375
tensor(11613.6162, grad_fn=<NegBackward0>) tensor(11613.4990, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11613.3837890625
tensor(11613.4990, grad_fn=<NegBackward0>) tensor(11613.3838, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11613.287109375
tensor(11613.3838, grad_fn=<NegBackward0>) tensor(11613.2871, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11613.1962890625
tensor(11613.2871, grad_fn=<NegBackward0>) tensor(11613.1963, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11613.0927734375
tensor(11613.1963, grad_fn=<NegBackward0>) tensor(11613.0928, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11612.9091796875
tensor(11613.0928, grad_fn=<NegBackward0>) tensor(11612.9092, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11611.390625
tensor(11612.9092, grad_fn=<NegBackward0>) tensor(11611.3906, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11611.251953125
tensor(11611.3906, grad_fn=<NegBackward0>) tensor(11611.2520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11611.2099609375
tensor(11611.2520, grad_fn=<NegBackward0>) tensor(11611.2100, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11611.1435546875
tensor(11611.2100, grad_fn=<NegBackward0>) tensor(11611.1436, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11611.060546875
tensor(11611.1436, grad_fn=<NegBackward0>) tensor(11611.0605, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11611.0322265625
tensor(11611.0605, grad_fn=<NegBackward0>) tensor(11611.0322, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11611.0205078125
tensor(11611.0322, grad_fn=<NegBackward0>) tensor(11611.0205, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11611.0068359375
tensor(11611.0205, grad_fn=<NegBackward0>) tensor(11611.0068, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11610.98046875
tensor(11611.0068, grad_fn=<NegBackward0>) tensor(11610.9805, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11610.9111328125
tensor(11610.9805, grad_fn=<NegBackward0>) tensor(11610.9111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11580.1904296875
tensor(11610.9111, grad_fn=<NegBackward0>) tensor(11580.1904, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11497.025390625
tensor(11580.1904, grad_fn=<NegBackward0>) tensor(11497.0254, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11496.556640625
tensor(11497.0254, grad_fn=<NegBackward0>) tensor(11496.5566, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11487.3349609375
tensor(11496.5566, grad_fn=<NegBackward0>) tensor(11487.3350, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11487.2822265625
tensor(11487.3350, grad_fn=<NegBackward0>) tensor(11487.2822, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11484.2314453125
tensor(11487.2822, grad_fn=<NegBackward0>) tensor(11484.2314, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11473.69140625
tensor(11484.2314, grad_fn=<NegBackward0>) tensor(11473.6914, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11465.3388671875
tensor(11473.6914, grad_fn=<NegBackward0>) tensor(11465.3389, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11465.322265625
tensor(11465.3389, grad_fn=<NegBackward0>) tensor(11465.3223, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11465.2783203125
tensor(11465.3223, grad_fn=<NegBackward0>) tensor(11465.2783, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11465.1484375
tensor(11465.2783, grad_fn=<NegBackward0>) tensor(11465.1484, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11465.125
tensor(11465.1484, grad_fn=<NegBackward0>) tensor(11465.1250, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11465.109375
tensor(11465.1250, grad_fn=<NegBackward0>) tensor(11465.1094, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11464.9228515625
tensor(11465.1094, grad_fn=<NegBackward0>) tensor(11464.9229, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11464.919921875
tensor(11464.9229, grad_fn=<NegBackward0>) tensor(11464.9199, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11464.921875
tensor(11464.9199, grad_fn=<NegBackward0>) tensor(11464.9219, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11464.9189453125
tensor(11464.9199, grad_fn=<NegBackward0>) tensor(11464.9189, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11464.9375
tensor(11464.9189, grad_fn=<NegBackward0>) tensor(11464.9375, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11464.9169921875
tensor(11464.9189, grad_fn=<NegBackward0>) tensor(11464.9170, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11464.91796875
tensor(11464.9170, grad_fn=<NegBackward0>) tensor(11464.9180, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11464.8505859375
tensor(11464.9170, grad_fn=<NegBackward0>) tensor(11464.8506, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11464.88671875
tensor(11464.8506, grad_fn=<NegBackward0>) tensor(11464.8867, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11464.84765625
tensor(11464.8506, grad_fn=<NegBackward0>) tensor(11464.8477, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11464.845703125
tensor(11464.8477, grad_fn=<NegBackward0>) tensor(11464.8457, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11464.83984375
tensor(11464.8457, grad_fn=<NegBackward0>) tensor(11464.8398, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11464.7646484375
tensor(11464.8398, grad_fn=<NegBackward0>) tensor(11464.7646, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11464.75
tensor(11464.7646, grad_fn=<NegBackward0>) tensor(11464.7500, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11449.21875
tensor(11464.7500, grad_fn=<NegBackward0>) tensor(11449.2188, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11424.3564453125
tensor(11449.2188, grad_fn=<NegBackward0>) tensor(11424.3564, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11400.158203125
tensor(11424.3564, grad_fn=<NegBackward0>) tensor(11400.1582, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11378.396484375
tensor(11400.1582, grad_fn=<NegBackward0>) tensor(11378.3965, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11364.779296875
tensor(11378.3965, grad_fn=<NegBackward0>) tensor(11364.7793, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11357.9482421875
tensor(11364.7793, grad_fn=<NegBackward0>) tensor(11357.9482, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11357.94921875
tensor(11357.9482, grad_fn=<NegBackward0>) tensor(11357.9492, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11357.9443359375
tensor(11357.9482, grad_fn=<NegBackward0>) tensor(11357.9443, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11357.9443359375
tensor(11357.9443, grad_fn=<NegBackward0>) tensor(11357.9443, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11357.9423828125
tensor(11357.9443, grad_fn=<NegBackward0>) tensor(11357.9424, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11351.8623046875
tensor(11357.9424, grad_fn=<NegBackward0>) tensor(11351.8623, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11344.4169921875
tensor(11351.8623, grad_fn=<NegBackward0>) tensor(11344.4170, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11344.4013671875
tensor(11344.4170, grad_fn=<NegBackward0>) tensor(11344.4014, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11338.4619140625
tensor(11344.4014, grad_fn=<NegBackward0>) tensor(11338.4619, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11331.84375
tensor(11338.4619, grad_fn=<NegBackward0>) tensor(11331.8438, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11331.818359375
tensor(11331.8438, grad_fn=<NegBackward0>) tensor(11331.8184, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11331.818359375
tensor(11331.8184, grad_fn=<NegBackward0>) tensor(11331.8184, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11331.8251953125
tensor(11331.8184, grad_fn=<NegBackward0>) tensor(11331.8252, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11331.818359375
tensor(11331.8184, grad_fn=<NegBackward0>) tensor(11331.8184, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11329.7265625
tensor(11331.8184, grad_fn=<NegBackward0>) tensor(11329.7266, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11329.6259765625
tensor(11329.7266, grad_fn=<NegBackward0>) tensor(11329.6260, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11322.5234375
tensor(11329.6260, grad_fn=<NegBackward0>) tensor(11322.5234, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11322.51953125
tensor(11322.5234, grad_fn=<NegBackward0>) tensor(11322.5195, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11322.5185546875
tensor(11322.5195, grad_fn=<NegBackward0>) tensor(11322.5186, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11319.9248046875
tensor(11322.5186, grad_fn=<NegBackward0>) tensor(11319.9248, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11319.9228515625
tensor(11319.9248, grad_fn=<NegBackward0>) tensor(11319.9229, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11319.923828125
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9238, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11319.9208984375
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9209, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11314.154296875
tensor(11319.9209, grad_fn=<NegBackward0>) tensor(11314.1543, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11314.12890625
tensor(11314.1543, grad_fn=<NegBackward0>) tensor(11314.1289, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11314.1123046875
tensor(11314.1289, grad_fn=<NegBackward0>) tensor(11314.1123, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11314.087890625
tensor(11314.1123, grad_fn=<NegBackward0>) tensor(11314.0879, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11314.0986328125
tensor(11314.0879, grad_fn=<NegBackward0>) tensor(11314.0986, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11314.0888671875
tensor(11314.0879, grad_fn=<NegBackward0>) tensor(11314.0889, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11314.083984375
tensor(11314.0879, grad_fn=<NegBackward0>) tensor(11314.0840, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11314.083984375
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0840, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11314.1103515625
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.1104, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11314.083984375
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0840, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11314.1005859375
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.1006, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11314.0908203125
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0908, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11314.0986328125
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0986, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11314.099609375
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0996, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11314.083984375
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0840, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11314.0908203125
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0908, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11314.0810546875
tensor(11314.0840, grad_fn=<NegBackward0>) tensor(11314.0811, grad_fn=<NegBackward0>)
pi: tensor([[0.7582, 0.2418],
        [0.2553, 0.7447]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5128, 0.4872], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3001, 0.1116],
         [0.6213, 0.2090]],

        [[0.7239, 0.0978],
         [0.6906, 0.7085]],

        [[0.6743, 0.1007],
         [0.6650, 0.5613]],

        [[0.7028, 0.0998],
         [0.6816, 0.5169]],

        [[0.6174, 0.1014],
         [0.6687, 0.5087]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369480537608971
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368975622965784
Average Adjusted Rand Index: 0.9393891719157426
[0.9760961002009502, 0.9368975622965784] [0.9761606886326935, 0.9393891719157426] [11286.466796875, 11314.080078125]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11026.867268347329
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20848.083984375
inf tensor(20848.0840, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11302.0869140625
tensor(20848.0840, grad_fn=<NegBackward0>) tensor(11302.0869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11298.1650390625
tensor(11302.0869, grad_fn=<NegBackward0>) tensor(11298.1650, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11295.9677734375
tensor(11298.1650, grad_fn=<NegBackward0>) tensor(11295.9678, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11291.7744140625
tensor(11295.9678, grad_fn=<NegBackward0>) tensor(11291.7744, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11289.9365234375
tensor(11291.7744, grad_fn=<NegBackward0>) tensor(11289.9365, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11289.6943359375
tensor(11289.9365, grad_fn=<NegBackward0>) tensor(11289.6943, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11289.6201171875
tensor(11289.6943, grad_fn=<NegBackward0>) tensor(11289.6201, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11289.576171875
tensor(11289.6201, grad_fn=<NegBackward0>) tensor(11289.5762, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11289.5478515625
tensor(11289.5762, grad_fn=<NegBackward0>) tensor(11289.5479, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11289.5263671875
tensor(11289.5479, grad_fn=<NegBackward0>) tensor(11289.5264, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11289.51171875
tensor(11289.5264, grad_fn=<NegBackward0>) tensor(11289.5117, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11289.4990234375
tensor(11289.5117, grad_fn=<NegBackward0>) tensor(11289.4990, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11289.48828125
tensor(11289.4990, grad_fn=<NegBackward0>) tensor(11289.4883, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11289.4794921875
tensor(11289.4883, grad_fn=<NegBackward0>) tensor(11289.4795, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11289.474609375
tensor(11289.4795, grad_fn=<NegBackward0>) tensor(11289.4746, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11289.4677734375
tensor(11289.4746, grad_fn=<NegBackward0>) tensor(11289.4678, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11289.4638671875
tensor(11289.4678, grad_fn=<NegBackward0>) tensor(11289.4639, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11289.4580078125
tensor(11289.4639, grad_fn=<NegBackward0>) tensor(11289.4580, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11289.4560546875
tensor(11289.4580, grad_fn=<NegBackward0>) tensor(11289.4561, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11289.4521484375
tensor(11289.4561, grad_fn=<NegBackward0>) tensor(11289.4521, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11289.44921875
tensor(11289.4521, grad_fn=<NegBackward0>) tensor(11289.4492, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11289.447265625
tensor(11289.4492, grad_fn=<NegBackward0>) tensor(11289.4473, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11289.4443359375
tensor(11289.4473, grad_fn=<NegBackward0>) tensor(11289.4443, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11289.4423828125
tensor(11289.4443, grad_fn=<NegBackward0>) tensor(11289.4424, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11289.4423828125
tensor(11289.4424, grad_fn=<NegBackward0>) tensor(11289.4424, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11289.4404296875
tensor(11289.4424, grad_fn=<NegBackward0>) tensor(11289.4404, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11289.4365234375
tensor(11289.4404, grad_fn=<NegBackward0>) tensor(11289.4365, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11289.4375
tensor(11289.4365, grad_fn=<NegBackward0>) tensor(11289.4375, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11289.435546875
tensor(11289.4365, grad_fn=<NegBackward0>) tensor(11289.4355, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11289.4345703125
tensor(11289.4355, grad_fn=<NegBackward0>) tensor(11289.4346, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11289.4326171875
tensor(11289.4346, grad_fn=<NegBackward0>) tensor(11289.4326, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11289.431640625
tensor(11289.4326, grad_fn=<NegBackward0>) tensor(11289.4316, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11289.431640625
tensor(11289.4316, grad_fn=<NegBackward0>) tensor(11289.4316, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11289.4306640625
tensor(11289.4316, grad_fn=<NegBackward0>) tensor(11289.4307, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11289.4306640625
tensor(11289.4307, grad_fn=<NegBackward0>) tensor(11289.4307, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11289.4287109375
tensor(11289.4307, grad_fn=<NegBackward0>) tensor(11289.4287, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11289.431640625
tensor(11289.4287, grad_fn=<NegBackward0>) tensor(11289.4316, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11289.4296875
tensor(11289.4287, grad_fn=<NegBackward0>) tensor(11289.4297, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11289.4287109375
tensor(11289.4287, grad_fn=<NegBackward0>) tensor(11289.4287, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11289.4287109375
tensor(11289.4287, grad_fn=<NegBackward0>) tensor(11289.4287, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11289.427734375
tensor(11289.4287, grad_fn=<NegBackward0>) tensor(11289.4277, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11289.427734375
tensor(11289.4277, grad_fn=<NegBackward0>) tensor(11289.4277, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11289.4267578125
tensor(11289.4277, grad_fn=<NegBackward0>) tensor(11289.4268, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11289.4267578125
tensor(11289.4268, grad_fn=<NegBackward0>) tensor(11289.4268, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11289.4267578125
tensor(11289.4268, grad_fn=<NegBackward0>) tensor(11289.4268, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11289.4267578125
tensor(11289.4268, grad_fn=<NegBackward0>) tensor(11289.4268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11289.42578125
tensor(11289.4268, grad_fn=<NegBackward0>) tensor(11289.4258, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11289.4248046875
tensor(11289.4258, grad_fn=<NegBackward0>) tensor(11289.4248, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11289.4248046875
tensor(11289.4248, grad_fn=<NegBackward0>) tensor(11289.4248, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11289.4267578125
tensor(11289.4248, grad_fn=<NegBackward0>) tensor(11289.4268, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11289.423828125
tensor(11289.4248, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11289.4228515625
tensor(11289.4238, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11289.423828125
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11289.423828125
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11289.423828125
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11289.423828125
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -11289.4228515625
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11289.423828125
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11289.4228515625
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11289.421875
tensor(11289.4229, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11289.423828125
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11289.423828125
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4238, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11289.421875
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11289.4228515625
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11289.4228515625
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11289.4228515625
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4229, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11289.421875
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11289.421875
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11289.421875
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11289.4208984375
tensor(11289.4219, grad_fn=<NegBackward0>) tensor(11289.4209, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11289.4208984375
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4209, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11289.4208984375
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4209, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11289.4208984375
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4209, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11289.421875
tensor(11289.4209, grad_fn=<NegBackward0>) tensor(11289.4219, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[8.9163e-01, 1.0837e-01],
        [9.9990e-01, 9.5913e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8278, 0.1722], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1580, 0.2107],
         [0.5963, 0.2854]],

        [[0.6363, 0.2232],
         [0.5983, 0.7248]],

        [[0.6488, 0.2037],
         [0.5095, 0.6514]],

        [[0.5982, 0.1706],
         [0.6185, 0.7088]],

        [[0.6654, 0.2443],
         [0.5123, 0.6578]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.02104340201665936
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: -0.027635546198997343
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0034364818206869723
Global Adjusted Rand Index: 0.014404427931290575
Average Adjusted Rand Index: 0.000257756416558687
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23407.884765625
inf tensor(23407.8848, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11296.439453125
tensor(23407.8848, grad_fn=<NegBackward0>) tensor(11296.4395, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11292.8359375
tensor(11296.4395, grad_fn=<NegBackward0>) tensor(11292.8359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11291.5556640625
tensor(11292.8359, grad_fn=<NegBackward0>) tensor(11291.5557, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11291.2275390625
tensor(11291.5557, grad_fn=<NegBackward0>) tensor(11291.2275, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11291.0927734375
tensor(11291.2275, grad_fn=<NegBackward0>) tensor(11291.0928, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11291.02734375
tensor(11291.0928, grad_fn=<NegBackward0>) tensor(11291.0273, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11290.9853515625
tensor(11291.0273, grad_fn=<NegBackward0>) tensor(11290.9854, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11290.951171875
tensor(11290.9854, grad_fn=<NegBackward0>) tensor(11290.9512, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11290.912109375
tensor(11290.9512, grad_fn=<NegBackward0>) tensor(11290.9121, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11290.8583984375
tensor(11290.9121, grad_fn=<NegBackward0>) tensor(11290.8584, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11290.7607421875
tensor(11290.8584, grad_fn=<NegBackward0>) tensor(11290.7607, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11290.6103515625
tensor(11290.7607, grad_fn=<NegBackward0>) tensor(11290.6104, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11290.435546875
tensor(11290.6104, grad_fn=<NegBackward0>) tensor(11290.4355, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11290.2880859375
tensor(11290.4355, grad_fn=<NegBackward0>) tensor(11290.2881, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11290.197265625
tensor(11290.2881, grad_fn=<NegBackward0>) tensor(11290.1973, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11290.1552734375
tensor(11290.1973, grad_fn=<NegBackward0>) tensor(11290.1553, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11290.1337890625
tensor(11290.1553, grad_fn=<NegBackward0>) tensor(11290.1338, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11290.125
tensor(11290.1338, grad_fn=<NegBackward0>) tensor(11290.1250, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11290.12109375
tensor(11290.1250, grad_fn=<NegBackward0>) tensor(11290.1211, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11290.1181640625
tensor(11290.1211, grad_fn=<NegBackward0>) tensor(11290.1182, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11290.1162109375
tensor(11290.1182, grad_fn=<NegBackward0>) tensor(11290.1162, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11290.1142578125
tensor(11290.1162, grad_fn=<NegBackward0>) tensor(11290.1143, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11290.11328125
tensor(11290.1143, grad_fn=<NegBackward0>) tensor(11290.1133, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11290.1123046875
tensor(11290.1133, grad_fn=<NegBackward0>) tensor(11290.1123, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11290.1103515625
tensor(11290.1123, grad_fn=<NegBackward0>) tensor(11290.1104, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11290.1103515625
tensor(11290.1104, grad_fn=<NegBackward0>) tensor(11290.1104, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11290.109375
tensor(11290.1104, grad_fn=<NegBackward0>) tensor(11290.1094, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11290.1083984375
tensor(11290.1094, grad_fn=<NegBackward0>) tensor(11290.1084, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11290.1083984375
tensor(11290.1084, grad_fn=<NegBackward0>) tensor(11290.1084, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11290.1083984375
tensor(11290.1084, grad_fn=<NegBackward0>) tensor(11290.1084, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11290.107421875
tensor(11290.1084, grad_fn=<NegBackward0>) tensor(11290.1074, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11290.1064453125
tensor(11290.1074, grad_fn=<NegBackward0>) tensor(11290.1064, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11290.10546875
tensor(11290.1064, grad_fn=<NegBackward0>) tensor(11290.1055, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11290.1064453125
tensor(11290.1055, grad_fn=<NegBackward0>) tensor(11290.1064, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11290.1044921875
tensor(11290.1055, grad_fn=<NegBackward0>) tensor(11290.1045, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11290.1044921875
tensor(11290.1045, grad_fn=<NegBackward0>) tensor(11290.1045, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11290.10546875
tensor(11290.1045, grad_fn=<NegBackward0>) tensor(11290.1055, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11290.103515625
tensor(11290.1045, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11290.103515625
tensor(11290.1035, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11290.103515625
tensor(11290.1035, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11290.103515625
tensor(11290.1035, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11290.103515625
tensor(11290.1035, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11290.1025390625
tensor(11290.1035, grad_fn=<NegBackward0>) tensor(11290.1025, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11290.1025390625
tensor(11290.1025, grad_fn=<NegBackward0>) tensor(11290.1025, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11290.1015625
tensor(11290.1025, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11290.1064453125
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1064, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11290.103515625
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1035, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11290.1044921875
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1045, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11290.1025390625
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1025, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -11290.1015625
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11290.1015625
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11290.1005859375
tensor(11290.1016, grad_fn=<NegBackward0>) tensor(11290.1006, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11290.1005859375
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1006, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11290.1015625
tensor(11290.1006, grad_fn=<NegBackward0>) tensor(11290.1016, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[3.5421e-04, 9.9965e-01],
        [6.2698e-02, 9.3730e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0808, 0.9192], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3221, 0.2202],
         [0.5508, 0.1700]],

        [[0.6306, 0.0913],
         [0.6303, 0.6742]],

        [[0.6762, 0.1800],
         [0.5910, 0.6323]],

        [[0.6662, 0.0740],
         [0.6220, 0.5409]],

        [[0.7150, 0.2640],
         [0.5976, 0.6892]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.03757255222298627
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.008096346523631212
Global Adjusted Rand Index: 0.006440342816818952
Average Adjusted Rand Index: 0.00744702327970741
[0.014404427931290575, 0.006440342816818952] [0.000257756416558687, 0.00744702327970741] [11289.421875, 11290.1015625]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11151.264544570342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22786.3984375
inf tensor(22786.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11452.6494140625
tensor(22786.3984, grad_fn=<NegBackward0>) tensor(11452.6494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11448.4248046875
tensor(11452.6494, grad_fn=<NegBackward0>) tensor(11448.4248, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11445.892578125
tensor(11448.4248, grad_fn=<NegBackward0>) tensor(11445.8926, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11425.6640625
tensor(11445.8926, grad_fn=<NegBackward0>) tensor(11425.6641, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11284.125
tensor(11425.6641, grad_fn=<NegBackward0>) tensor(11284.1250, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11210.8759765625
tensor(11284.1250, grad_fn=<NegBackward0>) tensor(11210.8760, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11185.1572265625
tensor(11210.8760, grad_fn=<NegBackward0>) tensor(11185.1572, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11184.5166015625
tensor(11185.1572, grad_fn=<NegBackward0>) tensor(11184.5166, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11184.2373046875
tensor(11184.5166, grad_fn=<NegBackward0>) tensor(11184.2373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11182.8720703125
tensor(11184.2373, grad_fn=<NegBackward0>) tensor(11182.8721, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11182.4296875
tensor(11182.8721, grad_fn=<NegBackward0>) tensor(11182.4297, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11174.4599609375
tensor(11182.4297, grad_fn=<NegBackward0>) tensor(11174.4600, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11173.384765625
tensor(11174.4600, grad_fn=<NegBackward0>) tensor(11173.3848, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11172.701171875
tensor(11173.3848, grad_fn=<NegBackward0>) tensor(11172.7012, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11172.6533203125
tensor(11172.7012, grad_fn=<NegBackward0>) tensor(11172.6533, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11172.6171875
tensor(11172.6533, grad_fn=<NegBackward0>) tensor(11172.6172, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11172.591796875
tensor(11172.6172, grad_fn=<NegBackward0>) tensor(11172.5918, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11172.5703125
tensor(11172.5918, grad_fn=<NegBackward0>) tensor(11172.5703, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11168.2099609375
tensor(11172.5703, grad_fn=<NegBackward0>) tensor(11168.2100, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11168.189453125
tensor(11168.2100, grad_fn=<NegBackward0>) tensor(11168.1895, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11168.18359375
tensor(11168.1895, grad_fn=<NegBackward0>) tensor(11168.1836, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11168.16796875
tensor(11168.1836, grad_fn=<NegBackward0>) tensor(11168.1680, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11168.1591796875
tensor(11168.1680, grad_fn=<NegBackward0>) tensor(11168.1592, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11168.1533203125
tensor(11168.1592, grad_fn=<NegBackward0>) tensor(11168.1533, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11168.1435546875
tensor(11168.1533, grad_fn=<NegBackward0>) tensor(11168.1436, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11168.13671875
tensor(11168.1436, grad_fn=<NegBackward0>) tensor(11168.1367, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11168.130859375
tensor(11168.1367, grad_fn=<NegBackward0>) tensor(11168.1309, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11168.1259765625
tensor(11168.1309, grad_fn=<NegBackward0>) tensor(11168.1260, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11168.1220703125
tensor(11168.1260, grad_fn=<NegBackward0>) tensor(11168.1221, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11168.1171875
tensor(11168.1221, grad_fn=<NegBackward0>) tensor(11168.1172, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11168.11328125
tensor(11168.1172, grad_fn=<NegBackward0>) tensor(11168.1133, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11168.109375
tensor(11168.1133, grad_fn=<NegBackward0>) tensor(11168.1094, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11168.1064453125
tensor(11168.1094, grad_fn=<NegBackward0>) tensor(11168.1064, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11168.0986328125
tensor(11168.1064, grad_fn=<NegBackward0>) tensor(11168.0986, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11168.08203125
tensor(11168.0986, grad_fn=<NegBackward0>) tensor(11168.0820, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11167.6865234375
tensor(11168.0820, grad_fn=<NegBackward0>) tensor(11167.6865, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11167.439453125
tensor(11167.6865, grad_fn=<NegBackward0>) tensor(11167.4395, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11167.4345703125
tensor(11167.4395, grad_fn=<NegBackward0>) tensor(11167.4346, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11167.4287109375
tensor(11167.4346, grad_fn=<NegBackward0>) tensor(11167.4287, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11167.4189453125
tensor(11167.4287, grad_fn=<NegBackward0>) tensor(11167.4189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11167.4150390625
tensor(11167.4189, grad_fn=<NegBackward0>) tensor(11167.4150, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11167.4091796875
tensor(11167.4150, grad_fn=<NegBackward0>) tensor(11167.4092, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11167.400390625
tensor(11167.4092, grad_fn=<NegBackward0>) tensor(11167.4004, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11167.3857421875
tensor(11167.4004, grad_fn=<NegBackward0>) tensor(11167.3857, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11167.36328125
tensor(11167.3857, grad_fn=<NegBackward0>) tensor(11167.3633, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11167.322265625
tensor(11167.3633, grad_fn=<NegBackward0>) tensor(11167.3223, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11167.2314453125
tensor(11167.3223, grad_fn=<NegBackward0>) tensor(11167.2314, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11166.9130859375
tensor(11167.2314, grad_fn=<NegBackward0>) tensor(11166.9131, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11121.3984375
tensor(11166.9131, grad_fn=<NegBackward0>) tensor(11121.3984, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11114.806640625
tensor(11121.3984, grad_fn=<NegBackward0>) tensor(11114.8066, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11114.677734375
tensor(11114.8066, grad_fn=<NegBackward0>) tensor(11114.6777, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11114.66796875
tensor(11114.6777, grad_fn=<NegBackward0>) tensor(11114.6680, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11114.662109375
tensor(11114.6680, grad_fn=<NegBackward0>) tensor(11114.6621, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11114.306640625
tensor(11114.6621, grad_fn=<NegBackward0>) tensor(11114.3066, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11114.302734375
tensor(11114.3066, grad_fn=<NegBackward0>) tensor(11114.3027, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11114.302734375
tensor(11114.3027, grad_fn=<NegBackward0>) tensor(11114.3027, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11114.30078125
tensor(11114.3027, grad_fn=<NegBackward0>) tensor(11114.3008, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11114.30078125
tensor(11114.3008, grad_fn=<NegBackward0>) tensor(11114.3008, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11114.2998046875
tensor(11114.3008, grad_fn=<NegBackward0>) tensor(11114.2998, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11114.2998046875
tensor(11114.2998, grad_fn=<NegBackward0>) tensor(11114.2998, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11114.298828125
tensor(11114.2998, grad_fn=<NegBackward0>) tensor(11114.2988, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11114.296875
tensor(11114.2988, grad_fn=<NegBackward0>) tensor(11114.2969, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11114.2177734375
tensor(11114.2969, grad_fn=<NegBackward0>) tensor(11114.2178, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11114.2158203125
tensor(11114.2178, grad_fn=<NegBackward0>) tensor(11114.2158, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11114.21484375
tensor(11114.2158, grad_fn=<NegBackward0>) tensor(11114.2148, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11114.21484375
tensor(11114.2148, grad_fn=<NegBackward0>) tensor(11114.2148, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11114.2138671875
tensor(11114.2148, grad_fn=<NegBackward0>) tensor(11114.2139, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11114.2158203125
tensor(11114.2139, grad_fn=<NegBackward0>) tensor(11114.2158, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11114.212890625
tensor(11114.2139, grad_fn=<NegBackward0>) tensor(11114.2129, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11114.2138671875
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2139, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11114.212890625
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2129, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11114.212890625
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2129, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11114.2138671875
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2139, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11114.21484375
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2148, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11114.21875
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2188, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11114.2119140625
tensor(11114.2129, grad_fn=<NegBackward0>) tensor(11114.2119, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11114.2109375
tensor(11114.2119, grad_fn=<NegBackward0>) tensor(11114.2109, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11114.208984375
tensor(11114.2109, grad_fn=<NegBackward0>) tensor(11114.2090, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11114.2099609375
tensor(11114.2090, grad_fn=<NegBackward0>) tensor(11114.2100, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11114.2080078125
tensor(11114.2090, grad_fn=<NegBackward0>) tensor(11114.2080, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11114.2080078125
tensor(11114.2080, grad_fn=<NegBackward0>) tensor(11114.2080, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11114.205078125
tensor(11114.2080, grad_fn=<NegBackward0>) tensor(11114.2051, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11114.2138671875
tensor(11114.2051, grad_fn=<NegBackward0>) tensor(11114.2139, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11114.1884765625
tensor(11114.2051, grad_fn=<NegBackward0>) tensor(11114.1885, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11114.1787109375
tensor(11114.1885, grad_fn=<NegBackward0>) tensor(11114.1787, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11114.1689453125
tensor(11114.1787, grad_fn=<NegBackward0>) tensor(11114.1689, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11114.1689453125
tensor(11114.1689, grad_fn=<NegBackward0>) tensor(11114.1689, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11114.16796875
tensor(11114.1689, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11114.16796875
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11114.16796875
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11114.16796875
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11114.1689453125
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1689, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11114.16796875
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11114.1787109375
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1787, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11114.1669921875
tensor(11114.1680, grad_fn=<NegBackward0>) tensor(11114.1670, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11114.283203125
tensor(11114.1670, grad_fn=<NegBackward0>) tensor(11114.2832, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11114.1669921875
tensor(11114.1670, grad_fn=<NegBackward0>) tensor(11114.1670, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11114.16796875
tensor(11114.1670, grad_fn=<NegBackward0>) tensor(11114.1680, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11114.24609375
tensor(11114.1670, grad_fn=<NegBackward0>) tensor(11114.2461, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7997, 0.2003],
        [0.2652, 0.7348]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4231, 0.5769], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.0980],
         [0.6288, 0.1975]],

        [[0.6900, 0.0972],
         [0.7045, 0.7105]],

        [[0.5697, 0.0972],
         [0.6605, 0.5452]],

        [[0.6328, 0.1015],
         [0.5098, 0.6409]],

        [[0.6169, 0.0949],
         [0.5216, 0.6424]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7720646154931399
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9137633297841882
Average Adjusted Rand Index: 0.9148964095069626
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23880.841796875
inf tensor(23880.8418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11448.6474609375
tensor(23880.8418, grad_fn=<NegBackward0>) tensor(11448.6475, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11440.1376953125
tensor(11448.6475, grad_fn=<NegBackward0>) tensor(11440.1377, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11438.8603515625
tensor(11440.1377, grad_fn=<NegBackward0>) tensor(11438.8604, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11438.5498046875
tensor(11438.8604, grad_fn=<NegBackward0>) tensor(11438.5498, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11431.3935546875
tensor(11438.5498, grad_fn=<NegBackward0>) tensor(11431.3936, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11286.5947265625
tensor(11431.3936, grad_fn=<NegBackward0>) tensor(11286.5947, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11233.7626953125
tensor(11286.5947, grad_fn=<NegBackward0>) tensor(11233.7627, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11207.9482421875
tensor(11233.7627, grad_fn=<NegBackward0>) tensor(11207.9482, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11202.1748046875
tensor(11207.9482, grad_fn=<NegBackward0>) tensor(11202.1748, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11195.00390625
tensor(11202.1748, grad_fn=<NegBackward0>) tensor(11195.0039, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11194.8505859375
tensor(11195.0039, grad_fn=<NegBackward0>) tensor(11194.8506, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11194.8369140625
tensor(11194.8506, grad_fn=<NegBackward0>) tensor(11194.8369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11194.8310546875
tensor(11194.8369, grad_fn=<NegBackward0>) tensor(11194.8311, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11194.82421875
tensor(11194.8311, grad_fn=<NegBackward0>) tensor(11194.8242, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11194.81640625
tensor(11194.8242, grad_fn=<NegBackward0>) tensor(11194.8164, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11194.7783203125
tensor(11194.8164, grad_fn=<NegBackward0>) tensor(11194.7783, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11194.7734375
tensor(11194.7783, grad_fn=<NegBackward0>) tensor(11194.7734, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11193.34765625
tensor(11194.7734, grad_fn=<NegBackward0>) tensor(11193.3477, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11193.2998046875
tensor(11193.3477, grad_fn=<NegBackward0>) tensor(11193.2998, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11193.2470703125
tensor(11193.2998, grad_fn=<NegBackward0>) tensor(11193.2471, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11193.2451171875
tensor(11193.2471, grad_fn=<NegBackward0>) tensor(11193.2451, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11193.244140625
tensor(11193.2451, grad_fn=<NegBackward0>) tensor(11193.2441, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11193.2421875
tensor(11193.2441, grad_fn=<NegBackward0>) tensor(11193.2422, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11193.2431640625
tensor(11193.2422, grad_fn=<NegBackward0>) tensor(11193.2432, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11193.2392578125
tensor(11193.2422, grad_fn=<NegBackward0>) tensor(11193.2393, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11193.11328125
tensor(11193.2393, grad_fn=<NegBackward0>) tensor(11193.1133, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11193.111328125
tensor(11193.1133, grad_fn=<NegBackward0>) tensor(11193.1113, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11193.109375
tensor(11193.1113, grad_fn=<NegBackward0>) tensor(11193.1094, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11192.23046875
tensor(11193.1094, grad_fn=<NegBackward0>) tensor(11192.2305, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11192.228515625
tensor(11192.2305, grad_fn=<NegBackward0>) tensor(11192.2285, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11192.224609375
tensor(11192.2285, grad_fn=<NegBackward0>) tensor(11192.2246, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11192.2236328125
tensor(11192.2246, grad_fn=<NegBackward0>) tensor(11192.2236, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11192.22265625
tensor(11192.2236, grad_fn=<NegBackward0>) tensor(11192.2227, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11192.220703125
tensor(11192.2227, grad_fn=<NegBackward0>) tensor(11192.2207, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11192.22265625
tensor(11192.2207, grad_fn=<NegBackward0>) tensor(11192.2227, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11192.2216796875
tensor(11192.2207, grad_fn=<NegBackward0>) tensor(11192.2217, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11192.076171875
tensor(11192.2207, grad_fn=<NegBackward0>) tensor(11192.0762, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11192.0654296875
tensor(11192.0762, grad_fn=<NegBackward0>) tensor(11192.0654, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11192.0634765625
tensor(11192.0654, grad_fn=<NegBackward0>) tensor(11192.0635, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11192.0615234375
tensor(11192.0635, grad_fn=<NegBackward0>) tensor(11192.0615, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11192.0439453125
tensor(11192.0615, grad_fn=<NegBackward0>) tensor(11192.0439, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11192.0400390625
tensor(11192.0439, grad_fn=<NegBackward0>) tensor(11192.0400, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11190.6083984375
tensor(11192.0400, grad_fn=<NegBackward0>) tensor(11190.6084, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11190.4521484375
tensor(11190.6084, grad_fn=<NegBackward0>) tensor(11190.4521, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11190.4501953125
tensor(11190.4521, grad_fn=<NegBackward0>) tensor(11190.4502, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11190.44921875
tensor(11190.4502, grad_fn=<NegBackward0>) tensor(11190.4492, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11190.4501953125
tensor(11190.4492, grad_fn=<NegBackward0>) tensor(11190.4502, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11190.44921875
tensor(11190.4492, grad_fn=<NegBackward0>) tensor(11190.4492, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11190.4482421875
tensor(11190.4492, grad_fn=<NegBackward0>) tensor(11190.4482, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11190.44921875
tensor(11190.4482, grad_fn=<NegBackward0>) tensor(11190.4492, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11190.4482421875
tensor(11190.4482, grad_fn=<NegBackward0>) tensor(11190.4482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11190.4482421875
tensor(11190.4482, grad_fn=<NegBackward0>) tensor(11190.4482, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11190.44140625
tensor(11190.4482, grad_fn=<NegBackward0>) tensor(11190.4414, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11190.44140625
tensor(11190.4414, grad_fn=<NegBackward0>) tensor(11190.4414, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11190.4423828125
tensor(11190.4414, grad_fn=<NegBackward0>) tensor(11190.4424, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11190.4384765625
tensor(11190.4414, grad_fn=<NegBackward0>) tensor(11190.4385, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11190.43359375
tensor(11190.4385, grad_fn=<NegBackward0>) tensor(11190.4336, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11190.4375
tensor(11190.4336, grad_fn=<NegBackward0>) tensor(11190.4375, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11190.435546875
tensor(11190.4336, grad_fn=<NegBackward0>) tensor(11190.4355, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11190.43359375
tensor(11190.4336, grad_fn=<NegBackward0>) tensor(11190.4336, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11190.4326171875
tensor(11190.4336, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11190.44140625
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4414, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11190.43359375
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4336, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11190.443359375
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4434, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11190.4326171875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11190.4365234375
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4365, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11190.43359375
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4336, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11190.4296875
tensor(11190.4326, grad_fn=<NegBackward0>) tensor(11190.4297, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11190.4326171875
tensor(11190.4297, grad_fn=<NegBackward0>) tensor(11190.4326, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11188.8828125
tensor(11190.4297, grad_fn=<NegBackward0>) tensor(11188.8828, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11188.8759765625
tensor(11188.8828, grad_fn=<NegBackward0>) tensor(11188.8760, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11188.8759765625
tensor(11188.8760, grad_fn=<NegBackward0>) tensor(11188.8760, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11188.8759765625
tensor(11188.8760, grad_fn=<NegBackward0>) tensor(11188.8760, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11188.8759765625
tensor(11188.8760, grad_fn=<NegBackward0>) tensor(11188.8760, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11188.875
tensor(11188.8760, grad_fn=<NegBackward0>) tensor(11188.8750, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11188.8759765625
tensor(11188.8750, grad_fn=<NegBackward0>) tensor(11188.8760, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11188.875
tensor(11188.8750, grad_fn=<NegBackward0>) tensor(11188.8750, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11188.908203125
tensor(11188.8750, grad_fn=<NegBackward0>) tensor(11188.9082, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11188.8642578125
tensor(11188.8750, grad_fn=<NegBackward0>) tensor(11188.8643, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11188.8681640625
tensor(11188.8643, grad_fn=<NegBackward0>) tensor(11188.8682, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11188.86328125
tensor(11188.8643, grad_fn=<NegBackward0>) tensor(11188.8633, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11188.8642578125
tensor(11188.8633, grad_fn=<NegBackward0>) tensor(11188.8643, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11188.865234375
tensor(11188.8633, grad_fn=<NegBackward0>) tensor(11188.8652, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11188.8037109375
tensor(11188.8633, grad_fn=<NegBackward0>) tensor(11188.8037, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11188.888671875
tensor(11188.8037, grad_fn=<NegBackward0>) tensor(11188.8887, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11186.267578125
tensor(11188.8037, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11186.349609375
tensor(11186.2676, grad_fn=<NegBackward0>) tensor(11186.3496, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11186.267578125
tensor(11186.2676, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11186.267578125
tensor(11186.2676, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11186.267578125
tensor(11186.2676, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11186.2666015625
tensor(11186.2676, grad_fn=<NegBackward0>) tensor(11186.2666, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11186.267578125
tensor(11186.2666, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11186.267578125
tensor(11186.2666, grad_fn=<NegBackward0>) tensor(11186.2676, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11185.2880859375
tensor(11186.2666, grad_fn=<NegBackward0>) tensor(11185.2881, grad_fn=<NegBackward0>)
pi: tensor([[0.2684, 0.7316],
        [0.8180, 0.1820]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5535, 0.4465], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2326, 0.0978],
         [0.5290, 0.2687]],

        [[0.7145, 0.0992],
         [0.7298, 0.5866]],

        [[0.6931, 0.0973],
         [0.6219, 0.5937]],

        [[0.6111, 0.0962],
         [0.6728, 0.5308]],

        [[0.5261, 0.0945],
         [0.5053, 0.6394]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 13
Adjusted Rand Index: 0.5431645430176624
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.057651762802340724
Average Adjusted Rand Index: 0.7604293989588824
[0.9137633297841882, 0.057651762802340724] [0.9148964095069626, 0.7604293989588824] [11114.185546875, 11185.255859375]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11264.301674469172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21521.068359375
inf tensor(21521.0684, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11595.8427734375
tensor(21521.0684, grad_fn=<NegBackward0>) tensor(11595.8428, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11582.1181640625
tensor(11595.8428, grad_fn=<NegBackward0>) tensor(11582.1182, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11574.33203125
tensor(11582.1182, grad_fn=<NegBackward0>) tensor(11574.3320, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11524.65625
tensor(11574.3320, grad_fn=<NegBackward0>) tensor(11524.6562, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11350.6318359375
tensor(11524.6562, grad_fn=<NegBackward0>) tensor(11350.6318, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11290.4833984375
tensor(11350.6318, grad_fn=<NegBackward0>) tensor(11290.4834, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11277.169921875
tensor(11290.4834, grad_fn=<NegBackward0>) tensor(11277.1699, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11275.0947265625
tensor(11277.1699, grad_fn=<NegBackward0>) tensor(11275.0947, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11270.7880859375
tensor(11275.0947, grad_fn=<NegBackward0>) tensor(11270.7881, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11270.5849609375
tensor(11270.7881, grad_fn=<NegBackward0>) tensor(11270.5850, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11263.3544921875
tensor(11270.5850, grad_fn=<NegBackward0>) tensor(11263.3545, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11252.125
tensor(11263.3545, grad_fn=<NegBackward0>) tensor(11252.1250, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11252.021484375
tensor(11252.1250, grad_fn=<NegBackward0>) tensor(11252.0215, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11250.3349609375
tensor(11252.0215, grad_fn=<NegBackward0>) tensor(11250.3350, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11250.3017578125
tensor(11250.3350, grad_fn=<NegBackward0>) tensor(11250.3018, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11250.27734375
tensor(11250.3018, grad_fn=<NegBackward0>) tensor(11250.2773, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11250.2568359375
tensor(11250.2773, grad_fn=<NegBackward0>) tensor(11250.2568, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11250.2177734375
tensor(11250.2568, grad_fn=<NegBackward0>) tensor(11250.2178, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11250.130859375
tensor(11250.2178, grad_fn=<NegBackward0>) tensor(11250.1309, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11250.1171875
tensor(11250.1309, grad_fn=<NegBackward0>) tensor(11250.1172, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11250.107421875
tensor(11250.1172, grad_fn=<NegBackward0>) tensor(11250.1074, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11250.0986328125
tensor(11250.1074, grad_fn=<NegBackward0>) tensor(11250.0986, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11250.0439453125
tensor(11250.0986, grad_fn=<NegBackward0>) tensor(11250.0439, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11250.0361328125
tensor(11250.0439, grad_fn=<NegBackward0>) tensor(11250.0361, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11250.0322265625
tensor(11250.0361, grad_fn=<NegBackward0>) tensor(11250.0322, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11250.029296875
tensor(11250.0322, grad_fn=<NegBackward0>) tensor(11250.0293, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11250.0263671875
tensor(11250.0293, grad_fn=<NegBackward0>) tensor(11250.0264, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11250.021484375
tensor(11250.0264, grad_fn=<NegBackward0>) tensor(11250.0215, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11250.0185546875
tensor(11250.0215, grad_fn=<NegBackward0>) tensor(11250.0186, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11250.0146484375
tensor(11250.0186, grad_fn=<NegBackward0>) tensor(11250.0146, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11250.0068359375
tensor(11250.0146, grad_fn=<NegBackward0>) tensor(11250.0068, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11249.7861328125
tensor(11250.0068, grad_fn=<NegBackward0>) tensor(11249.7861, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11249.7685546875
tensor(11249.7861, grad_fn=<NegBackward0>) tensor(11249.7686, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11249.7666015625
tensor(11249.7686, grad_fn=<NegBackward0>) tensor(11249.7666, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11249.763671875
tensor(11249.7666, grad_fn=<NegBackward0>) tensor(11249.7637, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11249.7685546875
tensor(11249.7637, grad_fn=<NegBackward0>) tensor(11249.7686, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11249.76171875
tensor(11249.7637, grad_fn=<NegBackward0>) tensor(11249.7617, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11249.759765625
tensor(11249.7617, grad_fn=<NegBackward0>) tensor(11249.7598, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11249.7578125
tensor(11249.7598, grad_fn=<NegBackward0>) tensor(11249.7578, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11249.7578125
tensor(11249.7578, grad_fn=<NegBackward0>) tensor(11249.7578, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11249.7587890625
tensor(11249.7578, grad_fn=<NegBackward0>) tensor(11249.7588, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11249.7705078125
tensor(11249.7578, grad_fn=<NegBackward0>) tensor(11249.7705, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11249.755859375
tensor(11249.7578, grad_fn=<NegBackward0>) tensor(11249.7559, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11249.75390625
tensor(11249.7559, grad_fn=<NegBackward0>) tensor(11249.7539, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11249.75390625
tensor(11249.7539, grad_fn=<NegBackward0>) tensor(11249.7539, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11249.7529296875
tensor(11249.7539, grad_fn=<NegBackward0>) tensor(11249.7529, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11249.751953125
tensor(11249.7529, grad_fn=<NegBackward0>) tensor(11249.7520, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11249.7568359375
tensor(11249.7520, grad_fn=<NegBackward0>) tensor(11249.7568, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11249.7490234375
tensor(11249.7520, grad_fn=<NegBackward0>) tensor(11249.7490, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11249.75
tensor(11249.7490, grad_fn=<NegBackward0>) tensor(11249.7500, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11249.7490234375
tensor(11249.7490, grad_fn=<NegBackward0>) tensor(11249.7490, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11249.7490234375
tensor(11249.7490, grad_fn=<NegBackward0>) tensor(11249.7490, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11249.748046875
tensor(11249.7490, grad_fn=<NegBackward0>) tensor(11249.7480, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11249.748046875
tensor(11249.7480, grad_fn=<NegBackward0>) tensor(11249.7480, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11249.748046875
tensor(11249.7480, grad_fn=<NegBackward0>) tensor(11249.7480, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11249.75390625
tensor(11249.7480, grad_fn=<NegBackward0>) tensor(11249.7539, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11249.748046875
tensor(11249.7480, grad_fn=<NegBackward0>) tensor(11249.7480, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11249.74609375
tensor(11249.7480, grad_fn=<NegBackward0>) tensor(11249.7461, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11249.74609375
tensor(11249.7461, grad_fn=<NegBackward0>) tensor(11249.7461, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11249.7451171875
tensor(11249.7461, grad_fn=<NegBackward0>) tensor(11249.7451, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11249.7470703125
tensor(11249.7451, grad_fn=<NegBackward0>) tensor(11249.7471, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11249.7470703125
tensor(11249.7451, grad_fn=<NegBackward0>) tensor(11249.7471, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11249.7451171875
tensor(11249.7451, grad_fn=<NegBackward0>) tensor(11249.7451, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11249.7451171875
tensor(11249.7451, grad_fn=<NegBackward0>) tensor(11249.7451, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11249.6767578125
tensor(11249.7451, grad_fn=<NegBackward0>) tensor(11249.6768, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11249.6630859375
tensor(11249.6768, grad_fn=<NegBackward0>) tensor(11249.6631, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11249.66015625
tensor(11249.6631, grad_fn=<NegBackward0>) tensor(11249.6602, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11249.6650390625
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6650, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11249.6611328125
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6611, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11249.66015625
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6602, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11249.6611328125
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6611, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11249.6611328125
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6611, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11249.6640625
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6641, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11249.662109375
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6621, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11249.6611328125
tensor(11249.6602, grad_fn=<NegBackward0>) tensor(11249.6611, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7553, 0.2447],
        [0.2351, 0.7649]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5647, 0.4353], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.0996],
         [0.6396, 0.1978]],

        [[0.5337, 0.1051],
         [0.5014, 0.6724]],

        [[0.5561, 0.1084],
         [0.6260, 0.5251]],

        [[0.7083, 0.0968],
         [0.7218, 0.6127]],

        [[0.7153, 0.0944],
         [0.5463, 0.6301]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.960320594001476
Average Adjusted Rand Index: 0.9603213980845977
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22375.791015625
inf tensor(22375.7910, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11591.76171875
tensor(22375.7910, grad_fn=<NegBackward0>) tensor(11591.7617, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11573.0888671875
tensor(11591.7617, grad_fn=<NegBackward0>) tensor(11573.0889, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11569.39453125
tensor(11573.0889, grad_fn=<NegBackward0>) tensor(11569.3945, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11568.916015625
tensor(11569.3945, grad_fn=<NegBackward0>) tensor(11568.9160, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11568.4521484375
tensor(11568.9160, grad_fn=<NegBackward0>) tensor(11568.4521, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11567.94921875
tensor(11568.4521, grad_fn=<NegBackward0>) tensor(11567.9492, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11567.2001953125
tensor(11567.9492, grad_fn=<NegBackward0>) tensor(11567.2002, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11560.8408203125
tensor(11567.2002, grad_fn=<NegBackward0>) tensor(11560.8408, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11559.7958984375
tensor(11560.8408, grad_fn=<NegBackward0>) tensor(11559.7959, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11506.642578125
tensor(11559.7959, grad_fn=<NegBackward0>) tensor(11506.6426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11407.8740234375
tensor(11506.6426, grad_fn=<NegBackward0>) tensor(11407.8740, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11392.041015625
tensor(11407.8740, grad_fn=<NegBackward0>) tensor(11392.0410, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11391.7470703125
tensor(11392.0410, grad_fn=<NegBackward0>) tensor(11391.7471, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11388.47265625
tensor(11391.7471, grad_fn=<NegBackward0>) tensor(11388.4727, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11384.0966796875
tensor(11388.4727, grad_fn=<NegBackward0>) tensor(11384.0967, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11380.525390625
tensor(11384.0967, grad_fn=<NegBackward0>) tensor(11380.5254, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11377.826171875
tensor(11380.5254, grad_fn=<NegBackward0>) tensor(11377.8262, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11377.4248046875
tensor(11377.8262, grad_fn=<NegBackward0>) tensor(11377.4248, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11377.4033203125
tensor(11377.4248, grad_fn=<NegBackward0>) tensor(11377.4033, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11371.6826171875
tensor(11377.4033, grad_fn=<NegBackward0>) tensor(11371.6826, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11371.6708984375
tensor(11371.6826, grad_fn=<NegBackward0>) tensor(11371.6709, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11371.2509765625
tensor(11371.6709, grad_fn=<NegBackward0>) tensor(11371.2510, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11370.6142578125
tensor(11371.2510, grad_fn=<NegBackward0>) tensor(11370.6143, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11370.529296875
tensor(11370.6143, grad_fn=<NegBackward0>) tensor(11370.5293, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11370.5234375
tensor(11370.5293, grad_fn=<NegBackward0>) tensor(11370.5234, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11370.521484375
tensor(11370.5234, grad_fn=<NegBackward0>) tensor(11370.5215, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11370.5146484375
tensor(11370.5215, grad_fn=<NegBackward0>) tensor(11370.5146, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11369.970703125
tensor(11370.5146, grad_fn=<NegBackward0>) tensor(11369.9707, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11369.9677734375
tensor(11369.9707, grad_fn=<NegBackward0>) tensor(11369.9678, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11369.96484375
tensor(11369.9678, grad_fn=<NegBackward0>) tensor(11369.9648, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11369.9609375
tensor(11369.9648, grad_fn=<NegBackward0>) tensor(11369.9609, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11369.9609375
tensor(11369.9609, grad_fn=<NegBackward0>) tensor(11369.9609, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11369.958984375
tensor(11369.9609, grad_fn=<NegBackward0>) tensor(11369.9590, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11369.9609375
tensor(11369.9590, grad_fn=<NegBackward0>) tensor(11369.9609, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11369.9580078125
tensor(11369.9590, grad_fn=<NegBackward0>) tensor(11369.9580, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11369.9609375
tensor(11369.9580, grad_fn=<NegBackward0>) tensor(11369.9609, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11369.9541015625
tensor(11369.9580, grad_fn=<NegBackward0>) tensor(11369.9541, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11369.9521484375
tensor(11369.9541, grad_fn=<NegBackward0>) tensor(11369.9521, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11369.9443359375
tensor(11369.9521, grad_fn=<NegBackward0>) tensor(11369.9443, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11369.9443359375
tensor(11369.9443, grad_fn=<NegBackward0>) tensor(11369.9443, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11369.9404296875
tensor(11369.9443, grad_fn=<NegBackward0>) tensor(11369.9404, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11369.9404296875
tensor(11369.9404, grad_fn=<NegBackward0>) tensor(11369.9404, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11369.947265625
tensor(11369.9404, grad_fn=<NegBackward0>) tensor(11369.9473, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11369.939453125
tensor(11369.9404, grad_fn=<NegBackward0>) tensor(11369.9395, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11369.939453125
tensor(11369.9395, grad_fn=<NegBackward0>) tensor(11369.9395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11369.9384765625
tensor(11369.9395, grad_fn=<NegBackward0>) tensor(11369.9385, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11369.939453125
tensor(11369.9385, grad_fn=<NegBackward0>) tensor(11369.9395, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11369.939453125
tensor(11369.9385, grad_fn=<NegBackward0>) tensor(11369.9395, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11369.9384765625
tensor(11369.9385, grad_fn=<NegBackward0>) tensor(11369.9385, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11369.9384765625
tensor(11369.9385, grad_fn=<NegBackward0>) tensor(11369.9385, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11369.9375
tensor(11369.9385, grad_fn=<NegBackward0>) tensor(11369.9375, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11369.9453125
tensor(11369.9375, grad_fn=<NegBackward0>) tensor(11369.9453, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11369.947265625
tensor(11369.9375, grad_fn=<NegBackward0>) tensor(11369.9473, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11369.9267578125
tensor(11369.9375, grad_fn=<NegBackward0>) tensor(11369.9268, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11369.9287109375
tensor(11369.9268, grad_fn=<NegBackward0>) tensor(11369.9287, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11369.9267578125
tensor(11369.9268, grad_fn=<NegBackward0>) tensor(11369.9268, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11369.927734375
tensor(11369.9268, grad_fn=<NegBackward0>) tensor(11369.9277, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11369.9208984375
tensor(11369.9268, grad_fn=<NegBackward0>) tensor(11369.9209, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11369.9228515625
tensor(11369.9209, grad_fn=<NegBackward0>) tensor(11369.9229, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11369.9287109375
tensor(11369.9209, grad_fn=<NegBackward0>) tensor(11369.9287, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11369.9189453125
tensor(11369.9209, grad_fn=<NegBackward0>) tensor(11369.9189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11369.919921875
tensor(11369.9189, grad_fn=<NegBackward0>) tensor(11369.9199, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11369.919921875
tensor(11369.9189, grad_fn=<NegBackward0>) tensor(11369.9199, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11369.4873046875
tensor(11369.9189, grad_fn=<NegBackward0>) tensor(11369.4873, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11369.4697265625
tensor(11369.4873, grad_fn=<NegBackward0>) tensor(11369.4697, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11369.46875
tensor(11369.4697, grad_fn=<NegBackward0>) tensor(11369.4688, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11369.46875
tensor(11369.4688, grad_fn=<NegBackward0>) tensor(11369.4688, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11368.8740234375
tensor(11369.4688, grad_fn=<NegBackward0>) tensor(11368.8740, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11368.8994140625
tensor(11368.8740, grad_fn=<NegBackward0>) tensor(11368.8994, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11368.849609375
tensor(11368.8740, grad_fn=<NegBackward0>) tensor(11368.8496, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11368.6025390625
tensor(11368.8496, grad_fn=<NegBackward0>) tensor(11368.6025, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11368.52734375
tensor(11368.6025, grad_fn=<NegBackward0>) tensor(11368.5273, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11368.5283203125
tensor(11368.5273, grad_fn=<NegBackward0>) tensor(11368.5283, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11368.52734375
tensor(11368.5273, grad_fn=<NegBackward0>) tensor(11368.5273, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11368.5283203125
tensor(11368.5273, grad_fn=<NegBackward0>) tensor(11368.5283, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11368.466796875
tensor(11368.5273, grad_fn=<NegBackward0>) tensor(11368.4668, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11368.4677734375
tensor(11368.4668, grad_fn=<NegBackward0>) tensor(11368.4678, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11368.5205078125
tensor(11368.4668, grad_fn=<NegBackward0>) tensor(11368.5205, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11368.46484375
tensor(11368.4668, grad_fn=<NegBackward0>) tensor(11368.4648, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11368.4541015625
tensor(11368.4648, grad_fn=<NegBackward0>) tensor(11368.4541, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11368.4482421875
tensor(11368.4541, grad_fn=<NegBackward0>) tensor(11368.4482, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11368.44921875
tensor(11368.4482, grad_fn=<NegBackward0>) tensor(11368.4492, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11368.4482421875
tensor(11368.4482, grad_fn=<NegBackward0>) tensor(11368.4482, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11368.4541015625
tensor(11368.4482, grad_fn=<NegBackward0>) tensor(11368.4541, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11368.443359375
tensor(11368.4482, grad_fn=<NegBackward0>) tensor(11368.4434, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11368.5732421875
tensor(11368.4434, grad_fn=<NegBackward0>) tensor(11368.5732, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11368.439453125
tensor(11368.4434, grad_fn=<NegBackward0>) tensor(11368.4395, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11367.94140625
tensor(11368.4395, grad_fn=<NegBackward0>) tensor(11367.9414, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11360.92578125
tensor(11367.9414, grad_fn=<NegBackward0>) tensor(11360.9258, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11360.92578125
tensor(11360.9258, grad_fn=<NegBackward0>) tensor(11360.9258, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11359.9453125
tensor(11360.9258, grad_fn=<NegBackward0>) tensor(11359.9453, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11359.9443359375
tensor(11359.9453, grad_fn=<NegBackward0>) tensor(11359.9443, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11359.9443359375
tensor(11359.9443, grad_fn=<NegBackward0>) tensor(11359.9443, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11359.953125
tensor(11359.9443, grad_fn=<NegBackward0>) tensor(11359.9531, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11359.943359375
tensor(11359.9443, grad_fn=<NegBackward0>) tensor(11359.9434, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11360.1904296875
tensor(11359.9434, grad_fn=<NegBackward0>) tensor(11360.1904, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11355.7451171875
tensor(11359.9434, grad_fn=<NegBackward0>) tensor(11355.7451, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11352.5302734375
tensor(11355.7451, grad_fn=<NegBackward0>) tensor(11352.5303, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11352.3046875
tensor(11352.5303, grad_fn=<NegBackward0>) tensor(11352.3047, grad_fn=<NegBackward0>)
pi: tensor([[0.4833, 0.5167],
        [0.4841, 0.5159]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5855, 0.4145], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2821, 0.0965],
         [0.6875, 0.2286]],

        [[0.5167, 0.1018],
         [0.7114, 0.5399]],

        [[0.6468, 0.1062],
         [0.7195, 0.5061]],

        [[0.5899, 0.0944],
         [0.5307, 0.5921]],

        [[0.6809, 0.0940],
         [0.6000, 0.5801]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.33043525191222567
Average Adjusted Rand Index: 0.8469476019813884
[0.960320594001476, 0.33043525191222567] [0.9603213980845977, 0.8469476019813884] [11249.6611328125, 11352.27734375]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11075.959692879685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21441.23046875
inf tensor(21441.2305, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11246.109375
tensor(21441.2305, grad_fn=<NegBackward0>) tensor(11246.1094, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11245.4560546875
tensor(11246.1094, grad_fn=<NegBackward0>) tensor(11245.4561, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11243.521484375
tensor(11245.4561, grad_fn=<NegBackward0>) tensor(11243.5215, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11243.1640625
tensor(11243.5215, grad_fn=<NegBackward0>) tensor(11243.1641, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11241.544921875
tensor(11243.1641, grad_fn=<NegBackward0>) tensor(11241.5449, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11184.71484375
tensor(11241.5449, grad_fn=<NegBackward0>) tensor(11184.7148, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11184.521484375
tensor(11184.7148, grad_fn=<NegBackward0>) tensor(11184.5215, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11184.4248046875
tensor(11184.5215, grad_fn=<NegBackward0>) tensor(11184.4248, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11184.3056640625
tensor(11184.4248, grad_fn=<NegBackward0>) tensor(11184.3057, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11184.1298828125
tensor(11184.3057, grad_fn=<NegBackward0>) tensor(11184.1299, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11183.6513671875
tensor(11184.1299, grad_fn=<NegBackward0>) tensor(11183.6514, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11168.1064453125
tensor(11183.6514, grad_fn=<NegBackward0>) tensor(11168.1064, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11044.16796875
tensor(11168.1064, grad_fn=<NegBackward0>) tensor(11044.1680, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11038.83203125
tensor(11044.1680, grad_fn=<NegBackward0>) tensor(11038.8320, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11038.353515625
tensor(11038.8320, grad_fn=<NegBackward0>) tensor(11038.3535, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11038.2138671875
tensor(11038.3535, grad_fn=<NegBackward0>) tensor(11038.2139, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11038.115234375
tensor(11038.2139, grad_fn=<NegBackward0>) tensor(11038.1152, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11038.07421875
tensor(11038.1152, grad_fn=<NegBackward0>) tensor(11038.0742, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11038.0546875
tensor(11038.0742, grad_fn=<NegBackward0>) tensor(11038.0547, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11038.0380859375
tensor(11038.0547, grad_fn=<NegBackward0>) tensor(11038.0381, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11038.0244140625
tensor(11038.0381, grad_fn=<NegBackward0>) tensor(11038.0244, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11038.015625
tensor(11038.0244, grad_fn=<NegBackward0>) tensor(11038.0156, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11038.0087890625
tensor(11038.0156, grad_fn=<NegBackward0>) tensor(11038.0088, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11038.001953125
tensor(11038.0088, grad_fn=<NegBackward0>) tensor(11038.0020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11037.9931640625
tensor(11038.0020, grad_fn=<NegBackward0>) tensor(11037.9932, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11037.984375
tensor(11037.9932, grad_fn=<NegBackward0>) tensor(11037.9844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11037.939453125
tensor(11037.9844, grad_fn=<NegBackward0>) tensor(11037.9395, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11037.8466796875
tensor(11037.9395, grad_fn=<NegBackward0>) tensor(11037.8467, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11037.83984375
tensor(11037.8467, grad_fn=<NegBackward0>) tensor(11037.8398, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11037.8349609375
tensor(11037.8398, grad_fn=<NegBackward0>) tensor(11037.8350, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11037.8251953125
tensor(11037.8350, grad_fn=<NegBackward0>) tensor(11037.8252, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11037.806640625
tensor(11037.8252, grad_fn=<NegBackward0>) tensor(11037.8066, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11037.7998046875
tensor(11037.8066, grad_fn=<NegBackward0>) tensor(11037.7998, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11037.796875
tensor(11037.7998, grad_fn=<NegBackward0>) tensor(11037.7969, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11037.794921875
tensor(11037.7969, grad_fn=<NegBackward0>) tensor(11037.7949, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11037.7939453125
tensor(11037.7949, grad_fn=<NegBackward0>) tensor(11037.7939, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11037.79296875
tensor(11037.7939, grad_fn=<NegBackward0>) tensor(11037.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11037.7919921875
tensor(11037.7930, grad_fn=<NegBackward0>) tensor(11037.7920, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11037.7900390625
tensor(11037.7920, grad_fn=<NegBackward0>) tensor(11037.7900, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11037.771484375
tensor(11037.7900, grad_fn=<NegBackward0>) tensor(11037.7715, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11037.7470703125
tensor(11037.7715, grad_fn=<NegBackward0>) tensor(11037.7471, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11037.74609375
tensor(11037.7471, grad_fn=<NegBackward0>) tensor(11037.7461, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11037.74609375
tensor(11037.7461, grad_fn=<NegBackward0>) tensor(11037.7461, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11037.7451171875
tensor(11037.7461, grad_fn=<NegBackward0>) tensor(11037.7451, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11037.7568359375
tensor(11037.7451, grad_fn=<NegBackward0>) tensor(11037.7568, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11037.7451171875
tensor(11037.7451, grad_fn=<NegBackward0>) tensor(11037.7451, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11037.7431640625
tensor(11037.7451, grad_fn=<NegBackward0>) tensor(11037.7432, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11037.744140625
tensor(11037.7432, grad_fn=<NegBackward0>) tensor(11037.7441, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11037.7431640625
tensor(11037.7432, grad_fn=<NegBackward0>) tensor(11037.7432, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11037.7392578125
tensor(11037.7432, grad_fn=<NegBackward0>) tensor(11037.7393, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11037.7353515625
tensor(11037.7393, grad_fn=<NegBackward0>) tensor(11037.7354, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11037.732421875
tensor(11037.7354, grad_fn=<NegBackward0>) tensor(11037.7324, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11037.7333984375
tensor(11037.7324, grad_fn=<NegBackward0>) tensor(11037.7334, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11037.73046875
tensor(11037.7324, grad_fn=<NegBackward0>) tensor(11037.7305, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11037.728515625
tensor(11037.7305, grad_fn=<NegBackward0>) tensor(11037.7285, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11037.728515625
tensor(11037.7285, grad_fn=<NegBackward0>) tensor(11037.7285, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11037.7275390625
tensor(11037.7285, grad_fn=<NegBackward0>) tensor(11037.7275, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11037.7275390625
tensor(11037.7275, grad_fn=<NegBackward0>) tensor(11037.7275, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11037.7265625
tensor(11037.7275, grad_fn=<NegBackward0>) tensor(11037.7266, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11037.7265625
tensor(11037.7266, grad_fn=<NegBackward0>) tensor(11037.7266, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11037.724609375
tensor(11037.7266, grad_fn=<NegBackward0>) tensor(11037.7246, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11037.7255859375
tensor(11037.7246, grad_fn=<NegBackward0>) tensor(11037.7256, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11037.7412109375
tensor(11037.7246, grad_fn=<NegBackward0>) tensor(11037.7412, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11037.72265625
tensor(11037.7246, grad_fn=<NegBackward0>) tensor(11037.7227, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11037.724609375
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7246, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11037.72265625
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7227, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11037.724609375
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7246, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11037.7451171875
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7451, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11037.7236328125
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7236, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11037.736328125
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7363, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11037.7216796875
tensor(11037.7227, grad_fn=<NegBackward0>) tensor(11037.7217, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11037.7255859375
tensor(11037.7217, grad_fn=<NegBackward0>) tensor(11037.7256, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11037.7216796875
tensor(11037.7217, grad_fn=<NegBackward0>) tensor(11037.7217, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11037.7314453125
tensor(11037.7217, grad_fn=<NegBackward0>) tensor(11037.7314, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11037.720703125
tensor(11037.7217, grad_fn=<NegBackward0>) tensor(11037.7207, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11037.4921875
tensor(11037.7207, grad_fn=<NegBackward0>) tensor(11037.4922, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11037.443359375
tensor(11037.4922, grad_fn=<NegBackward0>) tensor(11037.4434, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11037.4375
tensor(11037.4434, grad_fn=<NegBackward0>) tensor(11037.4375, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11037.482421875
tensor(11037.4375, grad_fn=<NegBackward0>) tensor(11037.4824, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11037.4375
tensor(11037.4375, grad_fn=<NegBackward0>) tensor(11037.4375, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11037.4365234375
tensor(11037.4375, grad_fn=<NegBackward0>) tensor(11037.4365, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11037.5
tensor(11037.4365, grad_fn=<NegBackward0>) tensor(11037.5000, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11037.435546875
tensor(11037.4365, grad_fn=<NegBackward0>) tensor(11037.4355, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11037.5205078125
tensor(11037.4355, grad_fn=<NegBackward0>) tensor(11037.5205, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11037.4345703125
tensor(11037.4355, grad_fn=<NegBackward0>) tensor(11037.4346, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11037.4384765625
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4385, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11037.4345703125
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4346, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11037.4345703125
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4346, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11037.4345703125
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4346, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11037.4404296875
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4404, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11037.431640625
tensor(11037.4346, grad_fn=<NegBackward0>) tensor(11037.4316, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11037.453125
tensor(11037.4316, grad_fn=<NegBackward0>) tensor(11037.4531, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11037.232421875
tensor(11037.4316, grad_fn=<NegBackward0>) tensor(11037.2324, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11036.7861328125
tensor(11037.2324, grad_fn=<NegBackward0>) tensor(11036.7861, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11036.6123046875
tensor(11036.7861, grad_fn=<NegBackward0>) tensor(11036.6123, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11036.61328125
tensor(11036.6123, grad_fn=<NegBackward0>) tensor(11036.6133, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11036.611328125
tensor(11036.6123, grad_fn=<NegBackward0>) tensor(11036.6113, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11036.611328125
tensor(11036.6113, grad_fn=<NegBackward0>) tensor(11036.6113, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11036.6201171875
tensor(11036.6113, grad_fn=<NegBackward0>) tensor(11036.6201, grad_fn=<NegBackward0>)
1
pi: tensor([[0.8212, 0.1788],
        [0.3121, 0.6879]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.0935],
         [0.5095, 0.2944]],

        [[0.5805, 0.1037],
         [0.7046, 0.6590]],

        [[0.6050, 0.1100],
         [0.5542, 0.5655]],

        [[0.5225, 0.0976],
         [0.6961, 0.6850]],

        [[0.6688, 0.1020],
         [0.5094, 0.6884]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 3
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8822045595164437
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7717037988036487
Global Adjusted Rand Index: 0.8683189864502563
Average Adjusted Rand Index: 0.8680583735978031
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22775.6796875
inf tensor(22775.6797, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11246.595703125
tensor(22775.6797, grad_fn=<NegBackward0>) tensor(11246.5957, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11245.3583984375
tensor(11246.5957, grad_fn=<NegBackward0>) tensor(11245.3584, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11243.4189453125
tensor(11245.3584, grad_fn=<NegBackward0>) tensor(11243.4189, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11242.6884765625
tensor(11243.4189, grad_fn=<NegBackward0>) tensor(11242.6885, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11187.5771484375
tensor(11242.6885, grad_fn=<NegBackward0>) tensor(11187.5771, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11184.4404296875
tensor(11187.5771, grad_fn=<NegBackward0>) tensor(11184.4404, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11184.208984375
tensor(11184.4404, grad_fn=<NegBackward0>) tensor(11184.2090, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11183.6630859375
tensor(11184.2090, grad_fn=<NegBackward0>) tensor(11183.6631, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11164.62890625
tensor(11183.6631, grad_fn=<NegBackward0>) tensor(11164.6289, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11046.2353515625
tensor(11164.6289, grad_fn=<NegBackward0>) tensor(11046.2354, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11038.3115234375
tensor(11046.2354, grad_fn=<NegBackward0>) tensor(11038.3115, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11037.93359375
tensor(11038.3115, grad_fn=<NegBackward0>) tensor(11037.9336, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11037.41015625
tensor(11037.9336, grad_fn=<NegBackward0>) tensor(11037.4102, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11037.3359375
tensor(11037.4102, grad_fn=<NegBackward0>) tensor(11037.3359, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11037.294921875
tensor(11037.3359, grad_fn=<NegBackward0>) tensor(11037.2949, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11037.2607421875
tensor(11037.2949, grad_fn=<NegBackward0>) tensor(11037.2607, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11037.2294921875
tensor(11037.2607, grad_fn=<NegBackward0>) tensor(11037.2295, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11037.2158203125
tensor(11037.2295, grad_fn=<NegBackward0>) tensor(11037.2158, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11037.2001953125
tensor(11037.2158, grad_fn=<NegBackward0>) tensor(11037.2002, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11037.173828125
tensor(11037.2002, grad_fn=<NegBackward0>) tensor(11037.1738, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11037.1640625
tensor(11037.1738, grad_fn=<NegBackward0>) tensor(11037.1641, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11037.1572265625
tensor(11037.1641, grad_fn=<NegBackward0>) tensor(11037.1572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11037.1513671875
tensor(11037.1572, grad_fn=<NegBackward0>) tensor(11037.1514, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11037.14453125
tensor(11037.1514, grad_fn=<NegBackward0>) tensor(11037.1445, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11037.1171875
tensor(11037.1445, grad_fn=<NegBackward0>) tensor(11037.1172, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11037.1103515625
tensor(11037.1172, grad_fn=<NegBackward0>) tensor(11037.1104, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11037.1083984375
tensor(11037.1104, grad_fn=<NegBackward0>) tensor(11037.1084, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11037.1044921875
tensor(11037.1084, grad_fn=<NegBackward0>) tensor(11037.1045, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11037.103515625
tensor(11037.1045, grad_fn=<NegBackward0>) tensor(11037.1035, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11037.1005859375
tensor(11037.1035, grad_fn=<NegBackward0>) tensor(11037.1006, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11037.099609375
tensor(11037.1006, grad_fn=<NegBackward0>) tensor(11037.0996, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11037.103515625
tensor(11037.0996, grad_fn=<NegBackward0>) tensor(11037.1035, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11037.095703125
tensor(11037.0996, grad_fn=<NegBackward0>) tensor(11037.0957, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11037.0947265625
tensor(11037.0957, grad_fn=<NegBackward0>) tensor(11037.0947, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11037.09375
tensor(11037.0947, grad_fn=<NegBackward0>) tensor(11037.0938, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11037.08984375
tensor(11037.0938, grad_fn=<NegBackward0>) tensor(11037.0898, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11037.0830078125
tensor(11037.0898, grad_fn=<NegBackward0>) tensor(11037.0830, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11037.056640625
tensor(11037.0830, grad_fn=<NegBackward0>) tensor(11037.0566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11037.0556640625
tensor(11037.0566, grad_fn=<NegBackward0>) tensor(11037.0557, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11037.0556640625
tensor(11037.0557, grad_fn=<NegBackward0>) tensor(11037.0557, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11037.052734375
tensor(11037.0557, grad_fn=<NegBackward0>) tensor(11037.0527, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11037.05078125
tensor(11037.0527, grad_fn=<NegBackward0>) tensor(11037.0508, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11036.7734375
tensor(11037.0508, grad_fn=<NegBackward0>) tensor(11036.7734, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11036.765625
tensor(11036.7734, grad_fn=<NegBackward0>) tensor(11036.7656, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11036.7490234375
tensor(11036.7656, grad_fn=<NegBackward0>) tensor(11036.7490, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11036.744140625
tensor(11036.7490, grad_fn=<NegBackward0>) tensor(11036.7441, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11036.7294921875
tensor(11036.7441, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11036.7314453125
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7314, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11036.7333984375
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7334, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11036.7294921875
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11036.7294921875
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11036.7294921875
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11036.7294921875
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7295, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11036.7275390625
tensor(11036.7295, grad_fn=<NegBackward0>) tensor(11036.7275, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11036.73046875
tensor(11036.7275, grad_fn=<NegBackward0>) tensor(11036.7305, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11036.736328125
tensor(11036.7275, grad_fn=<NegBackward0>) tensor(11036.7363, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11036.73046875
tensor(11036.7275, grad_fn=<NegBackward0>) tensor(11036.7305, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11036.7353515625
tensor(11036.7275, grad_fn=<NegBackward0>) tensor(11036.7354, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11036.728515625
tensor(11036.7275, grad_fn=<NegBackward0>) tensor(11036.7285, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.8222, 0.1778],
        [0.3077, 0.6923]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4808, 0.5192], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.0936],
         [0.5191, 0.2936]],

        [[0.5397, 0.1039],
         [0.5106, 0.5701]],

        [[0.6580, 0.1100],
         [0.5274, 0.6742]],

        [[0.5718, 0.0975],
         [0.5055, 0.6916]],

        [[0.6688, 0.1020],
         [0.6673, 0.7205]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080123577576726
time is 3
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8444975548124031
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7717037988036487
Global Adjusted Rand Index: 0.8386877478720036
Average Adjusted Rand Index: 0.8379717404141251
[0.8683189864502563, 0.8386877478720036] [0.8680583735978031, 0.8379717404141251] [11036.6103515625, 11036.728515625]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11338.365068936346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23625.318359375
inf tensor(23625.3184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11660.8916015625
tensor(23625.3184, grad_fn=<NegBackward0>) tensor(11660.8916, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11594.4765625
tensor(11660.8916, grad_fn=<NegBackward0>) tensor(11594.4766, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11589.4423828125
tensor(11594.4766, grad_fn=<NegBackward0>) tensor(11589.4424, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11390.43359375
tensor(11589.4424, grad_fn=<NegBackward0>) tensor(11390.4336, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11328.7490234375
tensor(11390.4336, grad_fn=<NegBackward0>) tensor(11328.7490, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11328.203125
tensor(11328.7490, grad_fn=<NegBackward0>) tensor(11328.2031, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11327.91015625
tensor(11328.2031, grad_fn=<NegBackward0>) tensor(11327.9102, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11326.0205078125
tensor(11327.9102, grad_fn=<NegBackward0>) tensor(11326.0205, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11325.8544921875
tensor(11326.0205, grad_fn=<NegBackward0>) tensor(11325.8545, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11325.8203125
tensor(11325.8545, grad_fn=<NegBackward0>) tensor(11325.8203, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11325.7919921875
tensor(11325.8203, grad_fn=<NegBackward0>) tensor(11325.7920, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11325.76953125
tensor(11325.7920, grad_fn=<NegBackward0>) tensor(11325.7695, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11325.7509765625
tensor(11325.7695, grad_fn=<NegBackward0>) tensor(11325.7510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11325.7265625
tensor(11325.7510, grad_fn=<NegBackward0>) tensor(11325.7266, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11325.7138671875
tensor(11325.7266, grad_fn=<NegBackward0>) tensor(11325.7139, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11325.6865234375
tensor(11325.7139, grad_fn=<NegBackward0>) tensor(11325.6865, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11319.8505859375
tensor(11325.6865, grad_fn=<NegBackward0>) tensor(11319.8506, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11319.8447265625
tensor(11319.8506, grad_fn=<NegBackward0>) tensor(11319.8447, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11319.8408203125
tensor(11319.8447, grad_fn=<NegBackward0>) tensor(11319.8408, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11319.837890625
tensor(11319.8408, grad_fn=<NegBackward0>) tensor(11319.8379, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11319.83203125
tensor(11319.8379, grad_fn=<NegBackward0>) tensor(11319.8320, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11319.8310546875
tensor(11319.8320, grad_fn=<NegBackward0>) tensor(11319.8311, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11319.8271484375
tensor(11319.8311, grad_fn=<NegBackward0>) tensor(11319.8271, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11319.82421875
tensor(11319.8271, grad_fn=<NegBackward0>) tensor(11319.8242, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11319.8232421875
tensor(11319.8242, grad_fn=<NegBackward0>) tensor(11319.8232, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11319.822265625
tensor(11319.8232, grad_fn=<NegBackward0>) tensor(11319.8223, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11319.8212890625
tensor(11319.8223, grad_fn=<NegBackward0>) tensor(11319.8213, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11319.822265625
tensor(11319.8213, grad_fn=<NegBackward0>) tensor(11319.8223, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11319.8193359375
tensor(11319.8213, grad_fn=<NegBackward0>) tensor(11319.8193, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11319.81640625
tensor(11319.8193, grad_fn=<NegBackward0>) tensor(11319.8164, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11319.8154296875
tensor(11319.8164, grad_fn=<NegBackward0>) tensor(11319.8154, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11319.814453125
tensor(11319.8154, grad_fn=<NegBackward0>) tensor(11319.8145, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11319.814453125
tensor(11319.8145, grad_fn=<NegBackward0>) tensor(11319.8145, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11319.8115234375
tensor(11319.8145, grad_fn=<NegBackward0>) tensor(11319.8115, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11319.8115234375
tensor(11319.8115, grad_fn=<NegBackward0>) tensor(11319.8115, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11319.8125
tensor(11319.8115, grad_fn=<NegBackward0>) tensor(11319.8125, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11319.8095703125
tensor(11319.8115, grad_fn=<NegBackward0>) tensor(11319.8096, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11319.806640625
tensor(11319.8096, grad_fn=<NegBackward0>) tensor(11319.8066, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11319.794921875
tensor(11319.8066, grad_fn=<NegBackward0>) tensor(11319.7949, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11319.794921875
tensor(11319.7949, grad_fn=<NegBackward0>) tensor(11319.7949, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11319.79296875
tensor(11319.7949, grad_fn=<NegBackward0>) tensor(11319.7930, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11319.79296875
tensor(11319.7930, grad_fn=<NegBackward0>) tensor(11319.7930, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11319.7958984375
tensor(11319.7930, grad_fn=<NegBackward0>) tensor(11319.7959, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11319.791015625
tensor(11319.7930, grad_fn=<NegBackward0>) tensor(11319.7910, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11319.7919921875
tensor(11319.7910, grad_fn=<NegBackward0>) tensor(11319.7920, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11319.791015625
tensor(11319.7910, grad_fn=<NegBackward0>) tensor(11319.7910, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11319.791015625
tensor(11319.7910, grad_fn=<NegBackward0>) tensor(11319.7910, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11319.7900390625
tensor(11319.7910, grad_fn=<NegBackward0>) tensor(11319.7900, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11319.7900390625
tensor(11319.7900, grad_fn=<NegBackward0>) tensor(11319.7900, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11319.7890625
tensor(11319.7900, grad_fn=<NegBackward0>) tensor(11319.7891, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11319.787109375
tensor(11319.7891, grad_fn=<NegBackward0>) tensor(11319.7871, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11319.787109375
tensor(11319.7871, grad_fn=<NegBackward0>) tensor(11319.7871, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11319.7890625
tensor(11319.7871, grad_fn=<NegBackward0>) tensor(11319.7891, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11319.7890625
tensor(11319.7871, grad_fn=<NegBackward0>) tensor(11319.7891, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11319.7861328125
tensor(11319.7871, grad_fn=<NegBackward0>) tensor(11319.7861, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11319.787109375
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7871, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11319.7880859375
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7881, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11319.787109375
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7871, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11319.7861328125
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7861, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11319.787109375
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7871, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11319.78125
tensor(11319.7861, grad_fn=<NegBackward0>) tensor(11319.7812, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11319.779296875
tensor(11319.7812, grad_fn=<NegBackward0>) tensor(11319.7793, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11319.779296875
tensor(11319.7793, grad_fn=<NegBackward0>) tensor(11319.7793, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11319.779296875
tensor(11319.7793, grad_fn=<NegBackward0>) tensor(11319.7793, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11319.7568359375
tensor(11319.7793, grad_fn=<NegBackward0>) tensor(11319.7568, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11319.75390625
tensor(11319.7568, grad_fn=<NegBackward0>) tensor(11319.7539, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11319.7568359375
tensor(11319.7539, grad_fn=<NegBackward0>) tensor(11319.7568, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11319.7548828125
tensor(11319.7539, grad_fn=<NegBackward0>) tensor(11319.7549, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11319.751953125
tensor(11319.7539, grad_fn=<NegBackward0>) tensor(11319.7520, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11319.751953125
tensor(11319.7520, grad_fn=<NegBackward0>) tensor(11319.7520, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11319.748046875
tensor(11319.7520, grad_fn=<NegBackward0>) tensor(11319.7480, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11319.744140625
tensor(11319.7480, grad_fn=<NegBackward0>) tensor(11319.7441, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11319.744140625
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7441, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11319.744140625
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7441, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11319.7705078125
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7705, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11319.74609375
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7461, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11319.7626953125
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7627, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11319.798828125
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7988, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11319.75390625
tensor(11319.7441, grad_fn=<NegBackward0>) tensor(11319.7539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7749, 0.2251],
        [0.2555, 0.7445]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5392, 0.4608], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3047, 0.1025],
         [0.7208, 0.2026]],

        [[0.7097, 0.1001],
         [0.6461, 0.5942]],

        [[0.6261, 0.1074],
         [0.6830, 0.6154]],

        [[0.5339, 0.0955],
         [0.5627, 0.5881]],

        [[0.6126, 0.1012],
         [0.6324, 0.5580]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9137634579875737
Average Adjusted Rand Index: 0.9136123807895586
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22438.03125
inf tensor(22438.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11665.34765625
tensor(22438.0312, grad_fn=<NegBackward0>) tensor(11665.3477, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11659.6240234375
tensor(11665.3477, grad_fn=<NegBackward0>) tensor(11659.6240, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11590.8681640625
tensor(11659.6240, grad_fn=<NegBackward0>) tensor(11590.8682, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11491.7197265625
tensor(11590.8682, grad_fn=<NegBackward0>) tensor(11491.7197, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11334.2900390625
tensor(11491.7197, grad_fn=<NegBackward0>) tensor(11334.2900, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11331.099609375
tensor(11334.2900, grad_fn=<NegBackward0>) tensor(11331.0996, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11321.0146484375
tensor(11331.0996, grad_fn=<NegBackward0>) tensor(11321.0146, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11320.5791015625
tensor(11321.0146, grad_fn=<NegBackward0>) tensor(11320.5791, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11320.4912109375
tensor(11320.5791, grad_fn=<NegBackward0>) tensor(11320.4912, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11320.4404296875
tensor(11320.4912, grad_fn=<NegBackward0>) tensor(11320.4404, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11320.3984375
tensor(11320.4404, grad_fn=<NegBackward0>) tensor(11320.3984, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11320.337890625
tensor(11320.3984, grad_fn=<NegBackward0>) tensor(11320.3379, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11320.291015625
tensor(11320.3379, grad_fn=<NegBackward0>) tensor(11320.2910, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11319.953125
tensor(11320.2910, grad_fn=<NegBackward0>) tensor(11319.9531, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11319.943359375
tensor(11319.9531, grad_fn=<NegBackward0>) tensor(11319.9434, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11319.9287109375
tensor(11319.9434, grad_fn=<NegBackward0>) tensor(11319.9287, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11319.9111328125
tensor(11319.9287, grad_fn=<NegBackward0>) tensor(11319.9111, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11319.9033203125
tensor(11319.9111, grad_fn=<NegBackward0>) tensor(11319.9033, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11319.896484375
tensor(11319.9033, grad_fn=<NegBackward0>) tensor(11319.8965, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11319.8916015625
tensor(11319.8965, grad_fn=<NegBackward0>) tensor(11319.8916, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11319.88671875
tensor(11319.8916, grad_fn=<NegBackward0>) tensor(11319.8867, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11319.8837890625
tensor(11319.8867, grad_fn=<NegBackward0>) tensor(11319.8838, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11319.87890625
tensor(11319.8838, grad_fn=<NegBackward0>) tensor(11319.8789, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11319.8740234375
tensor(11319.8789, grad_fn=<NegBackward0>) tensor(11319.8740, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11319.869140625
tensor(11319.8740, grad_fn=<NegBackward0>) tensor(11319.8691, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11319.8681640625
tensor(11319.8691, grad_fn=<NegBackward0>) tensor(11319.8682, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11319.87109375
tensor(11319.8682, grad_fn=<NegBackward0>) tensor(11319.8711, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11319.841796875
tensor(11319.8682, grad_fn=<NegBackward0>) tensor(11319.8418, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11319.810546875
tensor(11319.8418, grad_fn=<NegBackward0>) tensor(11319.8105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11319.8076171875
tensor(11319.8105, grad_fn=<NegBackward0>) tensor(11319.8076, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11319.810546875
tensor(11319.8076, grad_fn=<NegBackward0>) tensor(11319.8105, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11319.8046875
tensor(11319.8076, grad_fn=<NegBackward0>) tensor(11319.8047, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11319.80078125
tensor(11319.8047, grad_fn=<NegBackward0>) tensor(11319.8008, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11319.765625
tensor(11319.8008, grad_fn=<NegBackward0>) tensor(11319.7656, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11319.6826171875
tensor(11319.7656, grad_fn=<NegBackward0>) tensor(11319.6826, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11319.6796875
tensor(11319.6826, grad_fn=<NegBackward0>) tensor(11319.6797, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11319.6787109375
tensor(11319.6797, grad_fn=<NegBackward0>) tensor(11319.6787, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11319.685546875
tensor(11319.6787, grad_fn=<NegBackward0>) tensor(11319.6855, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11319.677734375
tensor(11319.6787, grad_fn=<NegBackward0>) tensor(11319.6777, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11319.6767578125
tensor(11319.6777, grad_fn=<NegBackward0>) tensor(11319.6768, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11319.67578125
tensor(11319.6768, grad_fn=<NegBackward0>) tensor(11319.6758, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11319.67578125
tensor(11319.6758, grad_fn=<NegBackward0>) tensor(11319.6758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11319.6748046875
tensor(11319.6758, grad_fn=<NegBackward0>) tensor(11319.6748, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11319.67578125
tensor(11319.6748, grad_fn=<NegBackward0>) tensor(11319.6758, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11319.671875
tensor(11319.6748, grad_fn=<NegBackward0>) tensor(11319.6719, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11319.66796875
tensor(11319.6719, grad_fn=<NegBackward0>) tensor(11319.6680, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11319.66796875
tensor(11319.6680, grad_fn=<NegBackward0>) tensor(11319.6680, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11319.666015625
tensor(11319.6680, grad_fn=<NegBackward0>) tensor(11319.6660, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11319.6669921875
tensor(11319.6660, grad_fn=<NegBackward0>) tensor(11319.6670, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11319.6650390625
tensor(11319.6660, grad_fn=<NegBackward0>) tensor(11319.6650, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11319.666015625
tensor(11319.6650, grad_fn=<NegBackward0>) tensor(11319.6660, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11319.666015625
tensor(11319.6650, grad_fn=<NegBackward0>) tensor(11319.6660, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11319.666015625
tensor(11319.6650, grad_fn=<NegBackward0>) tensor(11319.6660, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11319.666015625
tensor(11319.6650, grad_fn=<NegBackward0>) tensor(11319.6660, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -11319.6669921875
tensor(11319.6650, grad_fn=<NegBackward0>) tensor(11319.6670, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.7730, 0.2270],
        [0.2568, 0.7432]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5372, 0.4628], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3050, 0.1025],
         [0.6559, 0.2027]],

        [[0.5508, 0.1001],
         [0.6335, 0.5469]],

        [[0.7084, 0.1074],
         [0.5587, 0.5202]],

        [[0.6541, 0.0955],
         [0.5159, 0.7266]],

        [[0.7179, 0.1012],
         [0.6132, 0.6361]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9137634579875737
Average Adjusted Rand Index: 0.9136123807895586
[0.9137634579875737, 0.9137634579875737] [0.9136123807895586, 0.9136123807895586] [11319.75390625, 11319.6669921875]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11335.986975058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22922.96875
inf tensor(22922.9688, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11613.69921875
tensor(22922.9688, grad_fn=<NegBackward0>) tensor(11613.6992, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11612.1435546875
tensor(11613.6992, grad_fn=<NegBackward0>) tensor(11612.1436, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11609.126953125
tensor(11612.1436, grad_fn=<NegBackward0>) tensor(11609.1270, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11605.6435546875
tensor(11609.1270, grad_fn=<NegBackward0>) tensor(11605.6436, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11604.0380859375
tensor(11605.6436, grad_fn=<NegBackward0>) tensor(11604.0381, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11598.25
tensor(11604.0381, grad_fn=<NegBackward0>) tensor(11598.2500, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11496.2177734375
tensor(11598.2500, grad_fn=<NegBackward0>) tensor(11496.2178, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11404.513671875
tensor(11496.2178, grad_fn=<NegBackward0>) tensor(11404.5137, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11377.1494140625
tensor(11404.5137, grad_fn=<NegBackward0>) tensor(11377.1494, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11373.998046875
tensor(11377.1494, grad_fn=<NegBackward0>) tensor(11373.9980, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11373.625
tensor(11373.9980, grad_fn=<NegBackward0>) tensor(11373.6250, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11365.1728515625
tensor(11373.6250, grad_fn=<NegBackward0>) tensor(11365.1729, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11365.0419921875
tensor(11365.1729, grad_fn=<NegBackward0>) tensor(11365.0420, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11364.96484375
tensor(11365.0420, grad_fn=<NegBackward0>) tensor(11364.9648, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11364.90625
tensor(11364.9648, grad_fn=<NegBackward0>) tensor(11364.9062, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11364.849609375
tensor(11364.9062, grad_fn=<NegBackward0>) tensor(11364.8496, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11364.4375
tensor(11364.8496, grad_fn=<NegBackward0>) tensor(11364.4375, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11364.376953125
tensor(11364.4375, grad_fn=<NegBackward0>) tensor(11364.3770, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11364.3408203125
tensor(11364.3770, grad_fn=<NegBackward0>) tensor(11364.3408, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11364.3154296875
tensor(11364.3408, grad_fn=<NegBackward0>) tensor(11364.3154, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11364.3017578125
tensor(11364.3154, grad_fn=<NegBackward0>) tensor(11364.3018, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11364.2890625
tensor(11364.3018, grad_fn=<NegBackward0>) tensor(11364.2891, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11364.27734375
tensor(11364.2891, grad_fn=<NegBackward0>) tensor(11364.2773, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11364.265625
tensor(11364.2773, grad_fn=<NegBackward0>) tensor(11364.2656, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11364.2294921875
tensor(11364.2656, grad_fn=<NegBackward0>) tensor(11364.2295, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11364.1875
tensor(11364.2295, grad_fn=<NegBackward0>) tensor(11364.1875, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11364.1787109375
tensor(11364.1875, grad_fn=<NegBackward0>) tensor(11364.1787, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11364.1728515625
tensor(11364.1787, grad_fn=<NegBackward0>) tensor(11364.1729, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11364.16796875
tensor(11364.1729, grad_fn=<NegBackward0>) tensor(11364.1680, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11364.1630859375
tensor(11364.1680, grad_fn=<NegBackward0>) tensor(11364.1631, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11364.158203125
tensor(11364.1631, grad_fn=<NegBackward0>) tensor(11364.1582, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11364.15625
tensor(11364.1582, grad_fn=<NegBackward0>) tensor(11364.1562, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11364.1513671875
tensor(11364.1562, grad_fn=<NegBackward0>) tensor(11364.1514, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11362.84375
tensor(11364.1514, grad_fn=<NegBackward0>) tensor(11362.8438, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11362.69140625
tensor(11362.8438, grad_fn=<NegBackward0>) tensor(11362.6914, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11362.6904296875
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6904, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11362.6865234375
tensor(11362.6904, grad_fn=<NegBackward0>) tensor(11362.6865, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11362.6865234375
tensor(11362.6865, grad_fn=<NegBackward0>) tensor(11362.6865, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11362.68359375
tensor(11362.6865, grad_fn=<NegBackward0>) tensor(11362.6836, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11362.681640625
tensor(11362.6836, grad_fn=<NegBackward0>) tensor(11362.6816, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11362.6787109375
tensor(11362.6816, grad_fn=<NegBackward0>) tensor(11362.6787, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11362.677734375
tensor(11362.6787, grad_fn=<NegBackward0>) tensor(11362.6777, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11362.6767578125
tensor(11362.6777, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11362.67578125
tensor(11362.6768, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11362.6748046875
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11362.673828125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6738, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11362.671875
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6719, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11362.6708984375
tensor(11362.6719, grad_fn=<NegBackward0>) tensor(11362.6709, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11362.6650390625
tensor(11362.6709, grad_fn=<NegBackward0>) tensor(11362.6650, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11362.658203125
tensor(11362.6650, grad_fn=<NegBackward0>) tensor(11362.6582, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11362.658203125
tensor(11362.6582, grad_fn=<NegBackward0>) tensor(11362.6582, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11362.6572265625
tensor(11362.6582, grad_fn=<NegBackward0>) tensor(11362.6572, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11362.6572265625
tensor(11362.6572, grad_fn=<NegBackward0>) tensor(11362.6572, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11362.65625
tensor(11362.6572, grad_fn=<NegBackward0>) tensor(11362.6562, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11362.6552734375
tensor(11362.6562, grad_fn=<NegBackward0>) tensor(11362.6553, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11362.6552734375
tensor(11362.6553, grad_fn=<NegBackward0>) tensor(11362.6553, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11362.6533203125
tensor(11362.6553, grad_fn=<NegBackward0>) tensor(11362.6533, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11362.65234375
tensor(11362.6533, grad_fn=<NegBackward0>) tensor(11362.6523, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11362.654296875
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6543, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11362.66015625
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6602, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11362.65234375
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6523, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11362.65234375
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6523, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11362.65234375
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6523, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11362.650390625
tensor(11362.6523, grad_fn=<NegBackward0>) tensor(11362.6504, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11362.6494140625
tensor(11362.6504, grad_fn=<NegBackward0>) tensor(11362.6494, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11362.6494140625
tensor(11362.6494, grad_fn=<NegBackward0>) tensor(11362.6494, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11362.6474609375
tensor(11362.6494, grad_fn=<NegBackward0>) tensor(11362.6475, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11362.6474609375
tensor(11362.6475, grad_fn=<NegBackward0>) tensor(11362.6475, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11362.6396484375
tensor(11362.6475, grad_fn=<NegBackward0>) tensor(11362.6396, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11362.6376953125
tensor(11362.6396, grad_fn=<NegBackward0>) tensor(11362.6377, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11362.6376953125
tensor(11362.6377, grad_fn=<NegBackward0>) tensor(11362.6377, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11362.638671875
tensor(11362.6377, grad_fn=<NegBackward0>) tensor(11362.6387, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11362.634765625
tensor(11362.6377, grad_fn=<NegBackward0>) tensor(11362.6348, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11362.6357421875
tensor(11362.6348, grad_fn=<NegBackward0>) tensor(11362.6357, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11362.6337890625
tensor(11362.6348, grad_fn=<NegBackward0>) tensor(11362.6338, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11362.6318359375
tensor(11362.6338, grad_fn=<NegBackward0>) tensor(11362.6318, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11362.630859375
tensor(11362.6318, grad_fn=<NegBackward0>) tensor(11362.6309, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11362.6259765625
tensor(11362.6309, grad_fn=<NegBackward0>) tensor(11362.6260, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11362.619140625
tensor(11362.6260, grad_fn=<NegBackward0>) tensor(11362.6191, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11362.587890625
tensor(11362.6191, grad_fn=<NegBackward0>) tensor(11362.5879, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11362.55859375
tensor(11362.5879, grad_fn=<NegBackward0>) tensor(11362.5586, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11362.5400390625
tensor(11362.5586, grad_fn=<NegBackward0>) tensor(11362.5400, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11362.4951171875
tensor(11362.5400, grad_fn=<NegBackward0>) tensor(11362.4951, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11362.4775390625
tensor(11362.4951, grad_fn=<NegBackward0>) tensor(11362.4775, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11362.462890625
tensor(11362.4775, grad_fn=<NegBackward0>) tensor(11362.4629, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11362.4375
tensor(11362.4629, grad_fn=<NegBackward0>) tensor(11362.4375, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11362.4365234375
tensor(11362.4375, grad_fn=<NegBackward0>) tensor(11362.4365, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11362.4287109375
tensor(11362.4365, grad_fn=<NegBackward0>) tensor(11362.4287, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11362.427734375
tensor(11362.4287, grad_fn=<NegBackward0>) tensor(11362.4277, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11362.4248046875
tensor(11362.4277, grad_fn=<NegBackward0>) tensor(11362.4248, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11362.431640625
tensor(11362.4248, grad_fn=<NegBackward0>) tensor(11362.4316, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11362.408203125
tensor(11362.4248, grad_fn=<NegBackward0>) tensor(11362.4082, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11362.408203125
tensor(11362.4082, grad_fn=<NegBackward0>) tensor(11362.4082, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11362.3740234375
tensor(11362.4082, grad_fn=<NegBackward0>) tensor(11362.3740, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11362.3251953125
tensor(11362.3740, grad_fn=<NegBackward0>) tensor(11362.3252, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11314.7978515625
tensor(11362.3252, grad_fn=<NegBackward0>) tensor(11314.7979, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11314.6806640625
tensor(11314.7979, grad_fn=<NegBackward0>) tensor(11314.6807, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11314.6298828125
tensor(11314.6807, grad_fn=<NegBackward0>) tensor(11314.6299, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11306.75
tensor(11314.6299, grad_fn=<NegBackward0>) tensor(11306.7500, grad_fn=<NegBackward0>)
pi: tensor([[0.7539, 0.2461],
        [0.2613, 0.7387]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4417, 0.5583], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3128, 0.1071],
         [0.6908, 0.2037]],

        [[0.5705, 0.0987],
         [0.6889, 0.7050]],

        [[0.6477, 0.1084],
         [0.6046, 0.6305]],

        [[0.6629, 0.1046],
         [0.5113, 0.7133]],

        [[0.6292, 0.0979],
         [0.6637, 0.5472]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9061162604323688
Average Adjusted Rand Index: 0.9065785537225516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20481.58984375
inf tensor(20481.5898, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11613.6494140625
tensor(20481.5898, grad_fn=<NegBackward0>) tensor(11613.6494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11610.73046875
tensor(11613.6494, grad_fn=<NegBackward0>) tensor(11610.7305, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11604.5439453125
tensor(11610.7305, grad_fn=<NegBackward0>) tensor(11604.5439, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11601.0224609375
tensor(11604.5439, grad_fn=<NegBackward0>) tensor(11601.0225, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11383.2919921875
tensor(11601.0225, grad_fn=<NegBackward0>) tensor(11383.2920, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11365.4150390625
tensor(11383.2920, grad_fn=<NegBackward0>) tensor(11365.4150, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11364.97265625
tensor(11365.4150, grad_fn=<NegBackward0>) tensor(11364.9727, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11364.3740234375
tensor(11364.9727, grad_fn=<NegBackward0>) tensor(11364.3740, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11362.9169921875
tensor(11364.3740, grad_fn=<NegBackward0>) tensor(11362.9170, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11362.7705078125
tensor(11362.9170, grad_fn=<NegBackward0>) tensor(11362.7705, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11362.7373046875
tensor(11362.7705, grad_fn=<NegBackward0>) tensor(11362.7373, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11362.712890625
tensor(11362.7373, grad_fn=<NegBackward0>) tensor(11362.7129, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11362.64453125
tensor(11362.7129, grad_fn=<NegBackward0>) tensor(11362.6445, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11362.6201171875
tensor(11362.6445, grad_fn=<NegBackward0>) tensor(11362.6201, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11362.607421875
tensor(11362.6201, grad_fn=<NegBackward0>) tensor(11362.6074, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11362.5888671875
tensor(11362.6074, grad_fn=<NegBackward0>) tensor(11362.5889, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11362.572265625
tensor(11362.5889, grad_fn=<NegBackward0>) tensor(11362.5723, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11362.5625
tensor(11362.5723, grad_fn=<NegBackward0>) tensor(11362.5625, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11362.5517578125
tensor(11362.5625, grad_fn=<NegBackward0>) tensor(11362.5518, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11362.54296875
tensor(11362.5518, grad_fn=<NegBackward0>) tensor(11362.5430, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11362.5322265625
tensor(11362.5430, grad_fn=<NegBackward0>) tensor(11362.5322, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11362.5224609375
tensor(11362.5322, grad_fn=<NegBackward0>) tensor(11362.5225, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11362.509765625
tensor(11362.5225, grad_fn=<NegBackward0>) tensor(11362.5098, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11362.494140625
tensor(11362.5098, grad_fn=<NegBackward0>) tensor(11362.4941, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11362.470703125
tensor(11362.4941, grad_fn=<NegBackward0>) tensor(11362.4707, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11362.4384765625
tensor(11362.4707, grad_fn=<NegBackward0>) tensor(11362.4385, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11362.392578125
tensor(11362.4385, grad_fn=<NegBackward0>) tensor(11362.3926, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11362.337890625
tensor(11362.3926, grad_fn=<NegBackward0>) tensor(11362.3379, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11362.294921875
tensor(11362.3379, grad_fn=<NegBackward0>) tensor(11362.2949, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11362.26171875
tensor(11362.2949, grad_fn=<NegBackward0>) tensor(11362.2617, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11362.224609375
tensor(11362.2617, grad_fn=<NegBackward0>) tensor(11362.2246, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11362.1650390625
tensor(11362.2246, grad_fn=<NegBackward0>) tensor(11362.1650, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11307.3642578125
tensor(11362.1650, grad_fn=<NegBackward0>) tensor(11307.3643, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11306.7783203125
tensor(11307.3643, grad_fn=<NegBackward0>) tensor(11306.7783, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11306.6953125
tensor(11306.7783, grad_fn=<NegBackward0>) tensor(11306.6953, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11306.681640625
tensor(11306.6953, grad_fn=<NegBackward0>) tensor(11306.6816, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11306.6865234375
tensor(11306.6816, grad_fn=<NegBackward0>) tensor(11306.6865, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11306.6689453125
tensor(11306.6816, grad_fn=<NegBackward0>) tensor(11306.6689, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11306.66015625
tensor(11306.6689, grad_fn=<NegBackward0>) tensor(11306.6602, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11306.6279296875
tensor(11306.6602, grad_fn=<NegBackward0>) tensor(11306.6279, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11306.619140625
tensor(11306.6279, grad_fn=<NegBackward0>) tensor(11306.6191, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11306.6142578125
tensor(11306.6191, grad_fn=<NegBackward0>) tensor(11306.6143, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11306.6103515625
tensor(11306.6143, grad_fn=<NegBackward0>) tensor(11306.6104, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11306.609375
tensor(11306.6104, grad_fn=<NegBackward0>) tensor(11306.6094, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11306.6005859375
tensor(11306.6094, grad_fn=<NegBackward0>) tensor(11306.6006, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11306.5791015625
tensor(11306.6006, grad_fn=<NegBackward0>) tensor(11306.5791, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11306.578125
tensor(11306.5791, grad_fn=<NegBackward0>) tensor(11306.5781, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11306.578125
tensor(11306.5781, grad_fn=<NegBackward0>) tensor(11306.5781, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11306.5810546875
tensor(11306.5781, grad_fn=<NegBackward0>) tensor(11306.5811, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11306.5771484375
tensor(11306.5781, grad_fn=<NegBackward0>) tensor(11306.5771, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11306.5771484375
tensor(11306.5771, grad_fn=<NegBackward0>) tensor(11306.5771, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11306.5751953125
tensor(11306.5771, grad_fn=<NegBackward0>) tensor(11306.5752, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11306.5751953125
tensor(11306.5752, grad_fn=<NegBackward0>) tensor(11306.5752, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11306.576171875
tensor(11306.5752, grad_fn=<NegBackward0>) tensor(11306.5762, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11306.57421875
tensor(11306.5752, grad_fn=<NegBackward0>) tensor(11306.5742, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11306.576171875
tensor(11306.5742, grad_fn=<NegBackward0>) tensor(11306.5762, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11306.57421875
tensor(11306.5742, grad_fn=<NegBackward0>) tensor(11306.5742, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11306.5732421875
tensor(11306.5742, grad_fn=<NegBackward0>) tensor(11306.5732, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11306.572265625
tensor(11306.5732, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11306.572265625
tensor(11306.5723, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11306.5712890625
tensor(11306.5723, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11306.583984375
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5840, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11306.572265625
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11306.572265625
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11306.572265625
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11306.5712890625
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11306.5712890625
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11306.5703125
tensor(11306.5713, grad_fn=<NegBackward0>) tensor(11306.5703, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11306.5712890625
tensor(11306.5703, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11306.5703125
tensor(11306.5703, grad_fn=<NegBackward0>) tensor(11306.5703, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11306.572265625
tensor(11306.5703, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11306.5693359375
tensor(11306.5703, grad_fn=<NegBackward0>) tensor(11306.5693, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11306.572265625
tensor(11306.5693, grad_fn=<NegBackward0>) tensor(11306.5723, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11306.5712890625
tensor(11306.5693, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11306.5712890625
tensor(11306.5693, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11306.5712890625
tensor(11306.5693, grad_fn=<NegBackward0>) tensor(11306.5713, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11306.5703125
tensor(11306.5693, grad_fn=<NegBackward0>) tensor(11306.5703, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7534, 0.2466],
        [0.2616, 0.7384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4417, 0.5583], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3127, 0.1072],
         [0.5464, 0.2037]],

        [[0.5696, 0.0987],
         [0.6840, 0.7024]],

        [[0.6782, 0.1084],
         [0.6627, 0.6043]],

        [[0.7108, 0.1045],
         [0.6239, 0.6915]],

        [[0.6345, 0.0979],
         [0.7112, 0.5451]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9061162604323688
Average Adjusted Rand Index: 0.9065785537225516
[0.9061162604323688, 0.9061162604323688] [0.9065785537225516, 0.9065785537225516] [11306.662109375, 11306.5703125]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11461.800636335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20748.98828125
inf tensor(20748.9883, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11759.0712890625
tensor(20748.9883, grad_fn=<NegBackward0>) tensor(11759.0713, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11753.51171875
tensor(11759.0713, grad_fn=<NegBackward0>) tensor(11753.5117, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11723.9580078125
tensor(11753.5117, grad_fn=<NegBackward0>) tensor(11723.9580, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11568.0908203125
tensor(11723.9580, grad_fn=<NegBackward0>) tensor(11568.0908, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11461.0625
tensor(11568.0908, grad_fn=<NegBackward0>) tensor(11461.0625, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11453.408203125
tensor(11461.0625, grad_fn=<NegBackward0>) tensor(11453.4082, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11452.412109375
tensor(11453.4082, grad_fn=<NegBackward0>) tensor(11452.4121, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11452.0693359375
tensor(11452.4121, grad_fn=<NegBackward0>) tensor(11452.0693, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11442.7822265625
tensor(11452.0693, grad_fn=<NegBackward0>) tensor(11442.7822, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11441.5849609375
tensor(11442.7822, grad_fn=<NegBackward0>) tensor(11441.5850, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11441.373046875
tensor(11441.5850, grad_fn=<NegBackward0>) tensor(11441.3730, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11441.337890625
tensor(11441.3730, grad_fn=<NegBackward0>) tensor(11441.3379, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11441.3134765625
tensor(11441.3379, grad_fn=<NegBackward0>) tensor(11441.3135, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11438.673828125
tensor(11441.3135, grad_fn=<NegBackward0>) tensor(11438.6738, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11438.6533203125
tensor(11438.6738, grad_fn=<NegBackward0>) tensor(11438.6533, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11438.640625
tensor(11438.6533, grad_fn=<NegBackward0>) tensor(11438.6406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11438.6318359375
tensor(11438.6406, grad_fn=<NegBackward0>) tensor(11438.6318, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11438.6259765625
tensor(11438.6318, grad_fn=<NegBackward0>) tensor(11438.6260, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11438.619140625
tensor(11438.6260, grad_fn=<NegBackward0>) tensor(11438.6191, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11438.61328125
tensor(11438.6191, grad_fn=<NegBackward0>) tensor(11438.6133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11438.580078125
tensor(11438.6133, grad_fn=<NegBackward0>) tensor(11438.5801, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11438.515625
tensor(11438.5801, grad_fn=<NegBackward0>) tensor(11438.5156, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11438.51171875
tensor(11438.5156, grad_fn=<NegBackward0>) tensor(11438.5117, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11438.5107421875
tensor(11438.5117, grad_fn=<NegBackward0>) tensor(11438.5107, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11438.5068359375
tensor(11438.5107, grad_fn=<NegBackward0>) tensor(11438.5068, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11438.5029296875
tensor(11438.5068, grad_fn=<NegBackward0>) tensor(11438.5029, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11438.3779296875
tensor(11438.5029, grad_fn=<NegBackward0>) tensor(11438.3779, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11435.3828125
tensor(11438.3779, grad_fn=<NegBackward0>) tensor(11435.3828, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11435.3818359375
tensor(11435.3828, grad_fn=<NegBackward0>) tensor(11435.3818, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11435.37890625
tensor(11435.3818, grad_fn=<NegBackward0>) tensor(11435.3789, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11435.3720703125
tensor(11435.3789, grad_fn=<NegBackward0>) tensor(11435.3721, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11435.36328125
tensor(11435.3721, grad_fn=<NegBackward0>) tensor(11435.3633, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11435.3623046875
tensor(11435.3633, grad_fn=<NegBackward0>) tensor(11435.3623, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11435.3603515625
tensor(11435.3623, grad_fn=<NegBackward0>) tensor(11435.3604, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11435.3603515625
tensor(11435.3604, grad_fn=<NegBackward0>) tensor(11435.3604, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11435.359375
tensor(11435.3604, grad_fn=<NegBackward0>) tensor(11435.3594, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11435.357421875
tensor(11435.3594, grad_fn=<NegBackward0>) tensor(11435.3574, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11435.353515625
tensor(11435.3574, grad_fn=<NegBackward0>) tensor(11435.3535, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11435.357421875
tensor(11435.3535, grad_fn=<NegBackward0>) tensor(11435.3574, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11435.353515625
tensor(11435.3535, grad_fn=<NegBackward0>) tensor(11435.3535, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11435.3505859375
tensor(11435.3535, grad_fn=<NegBackward0>) tensor(11435.3506, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11435.3505859375
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3506, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11435.353515625
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3535, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11435.3515625
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3516, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11435.353515625
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3535, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11435.3505859375
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3506, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11435.3486328125
tensor(11435.3506, grad_fn=<NegBackward0>) tensor(11435.3486, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11435.3486328125
tensor(11435.3486, grad_fn=<NegBackward0>) tensor(11435.3486, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11435.34765625
tensor(11435.3486, grad_fn=<NegBackward0>) tensor(11435.3477, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11435.345703125
tensor(11435.3477, grad_fn=<NegBackward0>) tensor(11435.3457, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11435.341796875
tensor(11435.3457, grad_fn=<NegBackward0>) tensor(11435.3418, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11435.341796875
tensor(11435.3418, grad_fn=<NegBackward0>) tensor(11435.3418, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11435.341796875
tensor(11435.3418, grad_fn=<NegBackward0>) tensor(11435.3418, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11435.33984375
tensor(11435.3418, grad_fn=<NegBackward0>) tensor(11435.3398, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11433.6650390625
tensor(11435.3398, grad_fn=<NegBackward0>) tensor(11433.6650, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11433.6396484375
tensor(11433.6650, grad_fn=<NegBackward0>) tensor(11433.6396, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11433.6435546875
tensor(11433.6396, grad_fn=<NegBackward0>) tensor(11433.6436, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11433.638671875
tensor(11433.6396, grad_fn=<NegBackward0>) tensor(11433.6387, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11433.63671875
tensor(11433.6387, grad_fn=<NegBackward0>) tensor(11433.6367, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11433.486328125
tensor(11433.6367, grad_fn=<NegBackward0>) tensor(11433.4863, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11433.4892578125
tensor(11433.4863, grad_fn=<NegBackward0>) tensor(11433.4893, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11433.4853515625
tensor(11433.4863, grad_fn=<NegBackward0>) tensor(11433.4854, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11433.484375
tensor(11433.4854, grad_fn=<NegBackward0>) tensor(11433.4844, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11433.4892578125
tensor(11433.4844, grad_fn=<NegBackward0>) tensor(11433.4893, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11433.4833984375
tensor(11433.4844, grad_fn=<NegBackward0>) tensor(11433.4834, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11433.4814453125
tensor(11433.4834, grad_fn=<NegBackward0>) tensor(11433.4814, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11433.4833984375
tensor(11433.4814, grad_fn=<NegBackward0>) tensor(11433.4834, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11433.4794921875
tensor(11433.4814, grad_fn=<NegBackward0>) tensor(11433.4795, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11433.4794921875
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.4795, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11433.58203125
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.5820, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11433.4794921875
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.4795, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11433.4853515625
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.4854, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11433.48046875
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.4805, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11433.478515625
tensor(11433.4795, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11433.498046875
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4980, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11433.4833984375
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4834, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11433.48046875
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4805, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11433.4892578125
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4893, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11433.4970703125
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4971, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11433.478515625
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4785, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11433.4599609375
tensor(11433.4785, grad_fn=<NegBackward0>) tensor(11433.4600, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11433.4580078125
tensor(11433.4600, grad_fn=<NegBackward0>) tensor(11433.4580, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11433.4501953125
tensor(11433.4580, grad_fn=<NegBackward0>) tensor(11433.4502, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11433.4482421875
tensor(11433.4502, grad_fn=<NegBackward0>) tensor(11433.4482, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11433.41796875
tensor(11433.4482, grad_fn=<NegBackward0>) tensor(11433.4180, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11433.41796875
tensor(11433.4180, grad_fn=<NegBackward0>) tensor(11433.4180, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11433.4169921875
tensor(11433.4180, grad_fn=<NegBackward0>) tensor(11433.4170, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11433.416015625
tensor(11433.4170, grad_fn=<NegBackward0>) tensor(11433.4160, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11433.6181640625
tensor(11433.4160, grad_fn=<NegBackward0>) tensor(11433.6182, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11433.3984375
tensor(11433.4160, grad_fn=<NegBackward0>) tensor(11433.3984, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11433.3994140625
tensor(11433.3984, grad_fn=<NegBackward0>) tensor(11433.3994, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11433.5634765625
tensor(11433.3984, grad_fn=<NegBackward0>) tensor(11433.5635, grad_fn=<NegBackward0>)
2
pi: tensor([[0.8008, 0.1992],
        [0.2622, 0.7378]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5812, 0.4188], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2934, 0.0917],
         [0.7138, 0.1975]],

        [[0.7100, 0.0948],
         [0.5597, 0.6960]],

        [[0.6930, 0.1105],
         [0.5611, 0.6509]],

        [[0.5723, 0.1149],
         [0.6465, 0.6420]],

        [[0.5687, 0.1066],
         [0.7123, 0.6798]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.9368830730314118
Average Adjusted Rand Index: 0.9385719462186468
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20721.921875
inf tensor(20721.9219, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11757.9912109375
tensor(20721.9219, grad_fn=<NegBackward0>) tensor(11757.9912, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11748.759765625
tensor(11757.9912, grad_fn=<NegBackward0>) tensor(11748.7598, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11726.466796875
tensor(11748.7598, grad_fn=<NegBackward0>) tensor(11726.4668, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11578.01171875
tensor(11726.4668, grad_fn=<NegBackward0>) tensor(11578.0117, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11466.0234375
tensor(11578.0117, grad_fn=<NegBackward0>) tensor(11466.0234, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11454.640625
tensor(11466.0234, grad_fn=<NegBackward0>) tensor(11454.6406, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11454.1796875
tensor(11454.6406, grad_fn=<NegBackward0>) tensor(11454.1797, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11454.0361328125
tensor(11454.1797, grad_fn=<NegBackward0>) tensor(11454.0361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11453.966796875
tensor(11454.0361, grad_fn=<NegBackward0>) tensor(11453.9668, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11453.9111328125
tensor(11453.9668, grad_fn=<NegBackward0>) tensor(11453.9111, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11450.935546875
tensor(11453.9111, grad_fn=<NegBackward0>) tensor(11450.9355, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11447.6767578125
tensor(11450.9355, grad_fn=<NegBackward0>) tensor(11447.6768, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11444.751953125
tensor(11447.6768, grad_fn=<NegBackward0>) tensor(11444.7520, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11444.7236328125
tensor(11444.7520, grad_fn=<NegBackward0>) tensor(11444.7236, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11444.677734375
tensor(11444.7236, grad_fn=<NegBackward0>) tensor(11444.6777, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11444.6611328125
tensor(11444.6777, grad_fn=<NegBackward0>) tensor(11444.6611, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11444.6533203125
tensor(11444.6611, grad_fn=<NegBackward0>) tensor(11444.6533, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11444.6337890625
tensor(11444.6533, grad_fn=<NegBackward0>) tensor(11444.6338, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11443.275390625
tensor(11444.6338, grad_fn=<NegBackward0>) tensor(11443.2754, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11443.12109375
tensor(11443.2754, grad_fn=<NegBackward0>) tensor(11443.1211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11443.1025390625
tensor(11443.1211, grad_fn=<NegBackward0>) tensor(11443.1025, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11443.0888671875
tensor(11443.1025, grad_fn=<NegBackward0>) tensor(11443.0889, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11443.0517578125
tensor(11443.0889, grad_fn=<NegBackward0>) tensor(11443.0518, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11443.046875
tensor(11443.0518, grad_fn=<NegBackward0>) tensor(11443.0469, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11435.328125
tensor(11443.0469, grad_fn=<NegBackward0>) tensor(11435.3281, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11435.28515625
tensor(11435.3281, grad_fn=<NegBackward0>) tensor(11435.2852, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11435.283203125
tensor(11435.2852, grad_fn=<NegBackward0>) tensor(11435.2832, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11435.28125
tensor(11435.2832, grad_fn=<NegBackward0>) tensor(11435.2812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11435.2802734375
tensor(11435.2812, grad_fn=<NegBackward0>) tensor(11435.2803, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11435.2763671875
tensor(11435.2803, grad_fn=<NegBackward0>) tensor(11435.2764, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11435.275390625
tensor(11435.2764, grad_fn=<NegBackward0>) tensor(11435.2754, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11435.26171875
tensor(11435.2754, grad_fn=<NegBackward0>) tensor(11435.2617, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11435.24609375
tensor(11435.2617, grad_fn=<NegBackward0>) tensor(11435.2461, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11435.2421875
tensor(11435.2461, grad_fn=<NegBackward0>) tensor(11435.2422, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11435.25
tensor(11435.2422, grad_fn=<NegBackward0>) tensor(11435.2500, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11435.240234375
tensor(11435.2422, grad_fn=<NegBackward0>) tensor(11435.2402, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11435.2392578125
tensor(11435.2402, grad_fn=<NegBackward0>) tensor(11435.2393, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11435.2373046875
tensor(11435.2393, grad_fn=<NegBackward0>) tensor(11435.2373, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11434.220703125
tensor(11435.2373, grad_fn=<NegBackward0>) tensor(11434.2207, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11433.462890625
tensor(11434.2207, grad_fn=<NegBackward0>) tensor(11433.4629, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11433.4619140625
tensor(11433.4629, grad_fn=<NegBackward0>) tensor(11433.4619, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11433.451171875
tensor(11433.4619, grad_fn=<NegBackward0>) tensor(11433.4512, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11433.337890625
tensor(11433.4512, grad_fn=<NegBackward0>) tensor(11433.3379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11433.3349609375
tensor(11433.3379, grad_fn=<NegBackward0>) tensor(11433.3350, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11433.3359375
tensor(11433.3350, grad_fn=<NegBackward0>) tensor(11433.3359, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11433.333984375
tensor(11433.3350, grad_fn=<NegBackward0>) tensor(11433.3340, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11433.3359375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3359, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11433.333984375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3340, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11433.3349609375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3350, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11433.333984375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3340, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11433.333984375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3340, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11433.3359375
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3359, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11433.33203125
tensor(11433.3340, grad_fn=<NegBackward0>) tensor(11433.3320, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11433.3330078125
tensor(11433.3320, grad_fn=<NegBackward0>) tensor(11433.3330, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11433.33203125
tensor(11433.3320, grad_fn=<NegBackward0>) tensor(11433.3320, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11433.3310546875
tensor(11433.3320, grad_fn=<NegBackward0>) tensor(11433.3311, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11433.3466796875
tensor(11433.3311, grad_fn=<NegBackward0>) tensor(11433.3467, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11433.328125
tensor(11433.3311, grad_fn=<NegBackward0>) tensor(11433.3281, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11433.326171875
tensor(11433.3281, grad_fn=<NegBackward0>) tensor(11433.3262, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11433.3203125
tensor(11433.3262, grad_fn=<NegBackward0>) tensor(11433.3203, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11433.3203125
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.3203, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11433.32421875
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.3242, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11433.328125
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.3281, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11433.4091796875
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.4092, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11433.3203125
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.3203, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11433.3173828125
tensor(11433.3203, grad_fn=<NegBackward0>) tensor(11433.3174, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11433.3193359375
tensor(11433.3174, grad_fn=<NegBackward0>) tensor(11433.3193, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11433.3173828125
tensor(11433.3174, grad_fn=<NegBackward0>) tensor(11433.3174, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11433.322265625
tensor(11433.3174, grad_fn=<NegBackward0>) tensor(11433.3223, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11433.318359375
tensor(11433.3174, grad_fn=<NegBackward0>) tensor(11433.3184, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11433.310546875
tensor(11433.3174, grad_fn=<NegBackward0>) tensor(11433.3105, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11433.3125
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3125, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11433.341796875
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3418, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11433.310546875
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3105, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11433.314453125
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3145, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11433.310546875
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3105, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11433.314453125
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3145, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11433.3115234375
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3115, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11433.37890625
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3789, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11433.310546875
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3105, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11433.310546875
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3105, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11433.3115234375
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3115, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11433.3095703125
tensor(11433.3105, grad_fn=<NegBackward0>) tensor(11433.3096, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11433.31640625
tensor(11433.3096, grad_fn=<NegBackward0>) tensor(11433.3164, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11433.3095703125
tensor(11433.3096, grad_fn=<NegBackward0>) tensor(11433.3096, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11433.3095703125
tensor(11433.3096, grad_fn=<NegBackward0>) tensor(11433.3096, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11433.306640625
tensor(11433.3096, grad_fn=<NegBackward0>) tensor(11433.3066, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11433.3330078125
tensor(11433.3066, grad_fn=<NegBackward0>) tensor(11433.3330, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11433.3056640625
tensor(11433.3066, grad_fn=<NegBackward0>) tensor(11433.3057, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11433.4208984375
tensor(11433.3057, grad_fn=<NegBackward0>) tensor(11433.4209, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11433.2783203125
tensor(11433.3057, grad_fn=<NegBackward0>) tensor(11433.2783, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11433.2763671875
tensor(11433.2783, grad_fn=<NegBackward0>) tensor(11433.2764, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11433.2783203125
tensor(11433.2764, grad_fn=<NegBackward0>) tensor(11433.2783, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11433.27734375
tensor(11433.2764, grad_fn=<NegBackward0>) tensor(11433.2773, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11433.2861328125
tensor(11433.2764, grad_fn=<NegBackward0>) tensor(11433.2861, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11433.27734375
tensor(11433.2764, grad_fn=<NegBackward0>) tensor(11433.2773, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11433.298828125
tensor(11433.2764, grad_fn=<NegBackward0>) tensor(11433.2988, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7992, 0.2008],
        [0.2591, 0.7409]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5818, 0.4182], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2948, 0.0917],
         [0.6252, 0.1969]],

        [[0.6305, 0.0948],
         [0.6739, 0.6877]],

        [[0.6022, 0.1114],
         [0.6967, 0.6445]],

        [[0.6421, 0.1156],
         [0.6765, 0.5559]],

        [[0.5591, 0.1070],
         [0.5359, 0.6004]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.9368830730314118
Average Adjusted Rand Index: 0.9385719462186468
[0.9368830730314118, 0.9368830730314118] [0.9385719462186468, 0.9385719462186468] [11433.41015625, 11433.298828125]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11143.271236841018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23692.646484375
inf tensor(23692.6465, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11392.830078125
tensor(23692.6465, grad_fn=<NegBackward0>) tensor(11392.8301, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11392.380859375
tensor(11392.8301, grad_fn=<NegBackward0>) tensor(11392.3809, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11391.8857421875
tensor(11392.3809, grad_fn=<NegBackward0>) tensor(11391.8857, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11391.423828125
tensor(11391.8857, grad_fn=<NegBackward0>) tensor(11391.4238, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11391.0869140625
tensor(11391.4238, grad_fn=<NegBackward0>) tensor(11391.0869, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11390.9423828125
tensor(11391.0869, grad_fn=<NegBackward0>) tensor(11390.9424, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11390.8671875
tensor(11390.9424, grad_fn=<NegBackward0>) tensor(11390.8672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11390.814453125
tensor(11390.8672, grad_fn=<NegBackward0>) tensor(11390.8145, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11390.7587890625
tensor(11390.8145, grad_fn=<NegBackward0>) tensor(11390.7588, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11390.7333984375
tensor(11390.7588, grad_fn=<NegBackward0>) tensor(11390.7334, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11390.71484375
tensor(11390.7334, grad_fn=<NegBackward0>) tensor(11390.7148, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11390.69921875
tensor(11390.7148, grad_fn=<NegBackward0>) tensor(11390.6992, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11390.6865234375
tensor(11390.6992, grad_fn=<NegBackward0>) tensor(11390.6865, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11390.6708984375
tensor(11390.6865, grad_fn=<NegBackward0>) tensor(11390.6709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11390.6572265625
tensor(11390.6709, grad_fn=<NegBackward0>) tensor(11390.6572, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11390.6416015625
tensor(11390.6572, grad_fn=<NegBackward0>) tensor(11390.6416, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11390.62109375
tensor(11390.6416, grad_fn=<NegBackward0>) tensor(11390.6211, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11390.578125
tensor(11390.6211, grad_fn=<NegBackward0>) tensor(11390.5781, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11390.3798828125
tensor(11390.5781, grad_fn=<NegBackward0>) tensor(11390.3799, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11389.947265625
tensor(11390.3799, grad_fn=<NegBackward0>) tensor(11389.9473, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11389.6376953125
tensor(11389.9473, grad_fn=<NegBackward0>) tensor(11389.6377, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11389.236328125
tensor(11389.6377, grad_fn=<NegBackward0>) tensor(11389.2363, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11388.6201171875
tensor(11389.2363, grad_fn=<NegBackward0>) tensor(11388.6201, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11387.8701171875
tensor(11388.6201, grad_fn=<NegBackward0>) tensor(11387.8701, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11387.767578125
tensor(11387.8701, grad_fn=<NegBackward0>) tensor(11387.7676, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11387.74609375
tensor(11387.7676, grad_fn=<NegBackward0>) tensor(11387.7461, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11387.73828125
tensor(11387.7461, grad_fn=<NegBackward0>) tensor(11387.7383, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11387.732421875
tensor(11387.7383, grad_fn=<NegBackward0>) tensor(11387.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11387.728515625
tensor(11387.7324, grad_fn=<NegBackward0>) tensor(11387.7285, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11387.724609375
tensor(11387.7285, grad_fn=<NegBackward0>) tensor(11387.7246, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11387.72265625
tensor(11387.7246, grad_fn=<NegBackward0>) tensor(11387.7227, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11387.7216796875
tensor(11387.7227, grad_fn=<NegBackward0>) tensor(11387.7217, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11387.7197265625
tensor(11387.7217, grad_fn=<NegBackward0>) tensor(11387.7197, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11387.7177734375
tensor(11387.7197, grad_fn=<NegBackward0>) tensor(11387.7178, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11387.71484375
tensor(11387.7178, grad_fn=<NegBackward0>) tensor(11387.7148, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11387.7158203125
tensor(11387.7148, grad_fn=<NegBackward0>) tensor(11387.7158, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11387.712890625
tensor(11387.7148, grad_fn=<NegBackward0>) tensor(11387.7129, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11387.712890625
tensor(11387.7129, grad_fn=<NegBackward0>) tensor(11387.7129, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11387.7119140625
tensor(11387.7129, grad_fn=<NegBackward0>) tensor(11387.7119, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11387.7109375
tensor(11387.7119, grad_fn=<NegBackward0>) tensor(11387.7109, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11387.712890625
tensor(11387.7109, grad_fn=<NegBackward0>) tensor(11387.7129, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11387.7109375
tensor(11387.7109, grad_fn=<NegBackward0>) tensor(11387.7109, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11387.7099609375
tensor(11387.7109, grad_fn=<NegBackward0>) tensor(11387.7100, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11387.7099609375
tensor(11387.7100, grad_fn=<NegBackward0>) tensor(11387.7100, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11387.7109375
tensor(11387.7100, grad_fn=<NegBackward0>) tensor(11387.7109, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11387.708984375
tensor(11387.7100, grad_fn=<NegBackward0>) tensor(11387.7090, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11387.7080078125
tensor(11387.7090, grad_fn=<NegBackward0>) tensor(11387.7080, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11387.70703125
tensor(11387.7080, grad_fn=<NegBackward0>) tensor(11387.7070, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11387.708984375
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7090, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11387.7080078125
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7080, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11387.70703125
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7070, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11387.7080078125
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7080, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11387.7119140625
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7119, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11387.70703125
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7070, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11387.7060546875
tensor(11387.7070, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11387.70703125
tensor(11387.7061, grad_fn=<NegBackward0>) tensor(11387.7070, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11387.712890625
tensor(11387.7061, grad_fn=<NegBackward0>) tensor(11387.7129, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11387.705078125
tensor(11387.7061, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11387.705078125
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11387.705078125
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11387.708984375
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7090, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11387.7060546875
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11387.7060546875
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11387.705078125
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11387.7060546875
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11387.705078125
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11387.705078125
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11387.7119140625
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7119, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11387.7041015625
tensor(11387.7051, grad_fn=<NegBackward0>) tensor(11387.7041, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11387.7060546875
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11387.7041015625
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7041, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11387.705078125
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11387.7060546875
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11387.7060546875
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11387.703125
tensor(11387.7041, grad_fn=<NegBackward0>) tensor(11387.7031, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11387.705078125
tensor(11387.7031, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11387.705078125
tensor(11387.7031, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11387.7060546875
tensor(11387.7031, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11387.7060546875
tensor(11387.7031, grad_fn=<NegBackward0>) tensor(11387.7061, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11387.705078125
tensor(11387.7031, grad_fn=<NegBackward0>) tensor(11387.7051, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[3.5147e-01, 6.4853e-01],
        [1.9111e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1835, 0.8165], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1257, 0.1395],
         [0.6297, 0.1762]],

        [[0.5173, 0.1110],
         [0.7027, 0.7074]],

        [[0.5964, 0.2682],
         [0.6028, 0.5864]],

        [[0.6764, 0.2322],
         [0.5814, 0.5706]],

        [[0.6308, 0.1853],
         [0.6806, 0.5487]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01357925919519104
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.003348257218003585
Average Adjusted Rand Index: -0.00045830801606543505
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24367.107421875
inf tensor(24367.1074, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11390.994140625
tensor(24367.1074, grad_fn=<NegBackward0>) tensor(11390.9941, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11388.837890625
tensor(11390.9941, grad_fn=<NegBackward0>) tensor(11388.8379, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11388.28515625
tensor(11388.8379, grad_fn=<NegBackward0>) tensor(11388.2852, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11387.5205078125
tensor(11388.2852, grad_fn=<NegBackward0>) tensor(11387.5205, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11384.7275390625
tensor(11387.5205, grad_fn=<NegBackward0>) tensor(11384.7275, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11381.623046875
tensor(11384.7275, grad_fn=<NegBackward0>) tensor(11381.6230, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11217.57421875
tensor(11381.6230, grad_fn=<NegBackward0>) tensor(11217.5742, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11205.828125
tensor(11217.5742, grad_fn=<NegBackward0>) tensor(11205.8281, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11193.126953125
tensor(11205.8281, grad_fn=<NegBackward0>) tensor(11193.1270, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11191.8603515625
tensor(11193.1270, grad_fn=<NegBackward0>) tensor(11191.8604, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11189.984375
tensor(11191.8604, grad_fn=<NegBackward0>) tensor(11189.9844, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11189.9169921875
tensor(11189.9844, grad_fn=<NegBackward0>) tensor(11189.9170, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11189.8701171875
tensor(11189.9170, grad_fn=<NegBackward0>) tensor(11189.8701, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11189.826171875
tensor(11189.8701, grad_fn=<NegBackward0>) tensor(11189.8262, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11189.8046875
tensor(11189.8262, grad_fn=<NegBackward0>) tensor(11189.8047, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11189.787109375
tensor(11189.8047, grad_fn=<NegBackward0>) tensor(11189.7871, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11189.7724609375
tensor(11189.7871, grad_fn=<NegBackward0>) tensor(11189.7725, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11189.7626953125
tensor(11189.7725, grad_fn=<NegBackward0>) tensor(11189.7627, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11189.7529296875
tensor(11189.7627, grad_fn=<NegBackward0>) tensor(11189.7529, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11189.7451171875
tensor(11189.7529, grad_fn=<NegBackward0>) tensor(11189.7451, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11189.7412109375
tensor(11189.7451, grad_fn=<NegBackward0>) tensor(11189.7412, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11189.732421875
tensor(11189.7412, grad_fn=<NegBackward0>) tensor(11189.7324, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11189.71484375
tensor(11189.7324, grad_fn=<NegBackward0>) tensor(11189.7148, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11189.58984375
tensor(11189.7148, grad_fn=<NegBackward0>) tensor(11189.5898, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11189.583984375
tensor(11189.5898, grad_fn=<NegBackward0>) tensor(11189.5840, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11189.580078125
tensor(11189.5840, grad_fn=<NegBackward0>) tensor(11189.5801, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11189.576171875
tensor(11189.5801, grad_fn=<NegBackward0>) tensor(11189.5762, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11189.5751953125
tensor(11189.5762, grad_fn=<NegBackward0>) tensor(11189.5752, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11189.5732421875
tensor(11189.5752, grad_fn=<NegBackward0>) tensor(11189.5732, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11189.5693359375
tensor(11189.5732, grad_fn=<NegBackward0>) tensor(11189.5693, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11189.5693359375
tensor(11189.5693, grad_fn=<NegBackward0>) tensor(11189.5693, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11189.5654296875
tensor(11189.5693, grad_fn=<NegBackward0>) tensor(11189.5654, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11189.55859375
tensor(11189.5654, grad_fn=<NegBackward0>) tensor(11189.5586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11189.5576171875
tensor(11189.5586, grad_fn=<NegBackward0>) tensor(11189.5576, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11189.5556640625
tensor(11189.5576, grad_fn=<NegBackward0>) tensor(11189.5557, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11189.5556640625
tensor(11189.5557, grad_fn=<NegBackward0>) tensor(11189.5557, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11189.5546875
tensor(11189.5557, grad_fn=<NegBackward0>) tensor(11189.5547, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11189.552734375
tensor(11189.5547, grad_fn=<NegBackward0>) tensor(11189.5527, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11189.5517578125
tensor(11189.5527, grad_fn=<NegBackward0>) tensor(11189.5518, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11189.5517578125
tensor(11189.5518, grad_fn=<NegBackward0>) tensor(11189.5518, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11189.5498046875
tensor(11189.5518, grad_fn=<NegBackward0>) tensor(11189.5498, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11189.5498046875
tensor(11189.5498, grad_fn=<NegBackward0>) tensor(11189.5498, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11189.5498046875
tensor(11189.5498, grad_fn=<NegBackward0>) tensor(11189.5498, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11189.544921875
tensor(11189.5498, grad_fn=<NegBackward0>) tensor(11189.5449, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11189.54296875
tensor(11189.5449, grad_fn=<NegBackward0>) tensor(11189.5430, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11189.546875
tensor(11189.5430, grad_fn=<NegBackward0>) tensor(11189.5469, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11189.5341796875
tensor(11189.5430, grad_fn=<NegBackward0>) tensor(11189.5342, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11189.5078125
tensor(11189.5342, grad_fn=<NegBackward0>) tensor(11189.5078, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11188.951171875
tensor(11189.5078, grad_fn=<NegBackward0>) tensor(11188.9512, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11188.6455078125
tensor(11188.9512, grad_fn=<NegBackward0>) tensor(11188.6455, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11188.4189453125
tensor(11188.6455, grad_fn=<NegBackward0>) tensor(11188.4189, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11181.9765625
tensor(11188.4189, grad_fn=<NegBackward0>) tensor(11181.9766, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11179.486328125
tensor(11181.9766, grad_fn=<NegBackward0>) tensor(11179.4863, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11179.330078125
tensor(11179.4863, grad_fn=<NegBackward0>) tensor(11179.3301, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11179.2587890625
tensor(11179.3301, grad_fn=<NegBackward0>) tensor(11179.2588, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11179.2568359375
tensor(11179.2588, grad_fn=<NegBackward0>) tensor(11179.2568, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11179.255859375
tensor(11179.2568, grad_fn=<NegBackward0>) tensor(11179.2559, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11179.25390625
tensor(11179.2559, grad_fn=<NegBackward0>) tensor(11179.2539, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11179.255859375
tensor(11179.2539, grad_fn=<NegBackward0>) tensor(11179.2559, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11179.251953125
tensor(11179.2539, grad_fn=<NegBackward0>) tensor(11179.2520, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11179.2548828125
tensor(11179.2520, grad_fn=<NegBackward0>) tensor(11179.2549, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11179.2509765625
tensor(11179.2520, grad_fn=<NegBackward0>) tensor(11179.2510, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11179.2490234375
tensor(11179.2510, grad_fn=<NegBackward0>) tensor(11179.2490, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11179.2470703125
tensor(11179.2490, grad_fn=<NegBackward0>) tensor(11179.2471, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11179.24609375
tensor(11179.2471, grad_fn=<NegBackward0>) tensor(11179.2461, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11179.2451171875
tensor(11179.2461, grad_fn=<NegBackward0>) tensor(11179.2451, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11179.2451171875
tensor(11179.2451, grad_fn=<NegBackward0>) tensor(11179.2451, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11179.2451171875
tensor(11179.2451, grad_fn=<NegBackward0>) tensor(11179.2451, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11179.24609375
tensor(11179.2451, grad_fn=<NegBackward0>) tensor(11179.2461, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11179.2451171875
tensor(11179.2451, grad_fn=<NegBackward0>) tensor(11179.2451, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11179.244140625
tensor(11179.2451, grad_fn=<NegBackward0>) tensor(11179.2441, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11179.2431640625
tensor(11179.2441, grad_fn=<NegBackward0>) tensor(11179.2432, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11179.2275390625
tensor(11179.2432, grad_fn=<NegBackward0>) tensor(11179.2275, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11179.228515625
tensor(11179.2275, grad_fn=<NegBackward0>) tensor(11179.2285, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11179.2216796875
tensor(11179.2275, grad_fn=<NegBackward0>) tensor(11179.2217, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11179.18359375
tensor(11179.2217, grad_fn=<NegBackward0>) tensor(11179.1836, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11179.18359375
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1836, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11179.216796875
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.2168, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11179.18359375
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1836, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11179.21484375
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.2148, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11179.1845703125
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1846, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11179.1845703125
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1846, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11179.18359375
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1836, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11179.189453125
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1895, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11179.18359375
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1836, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11179.2001953125
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.2002, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11179.181640625
tensor(11179.1836, grad_fn=<NegBackward0>) tensor(11179.1816, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11179.1806640625
tensor(11179.1816, grad_fn=<NegBackward0>) tensor(11179.1807, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11179.1806640625
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.1807, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11179.2060546875
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.2061, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11179.1806640625
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.1807, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11179.1806640625
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.1807, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11179.1806640625
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.1807, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11179.1796875
tensor(11179.1807, grad_fn=<NegBackward0>) tensor(11179.1797, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11179.173828125
tensor(11179.1797, grad_fn=<NegBackward0>) tensor(11179.1738, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11179.189453125
tensor(11179.1738, grad_fn=<NegBackward0>) tensor(11179.1895, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11179.17578125
tensor(11179.1738, grad_fn=<NegBackward0>) tensor(11179.1758, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11179.1748046875
tensor(11179.1738, grad_fn=<NegBackward0>) tensor(11179.1748, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11179.173828125
tensor(11179.1738, grad_fn=<NegBackward0>) tensor(11179.1738, grad_fn=<NegBackward0>)
pi: tensor([[0.6946, 0.3054],
        [0.3115, 0.6885]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1878, 0.8122], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3094, 0.0968],
         [0.5509, 0.1970]],

        [[0.6328, 0.1109],
         [0.6825, 0.6694]],

        [[0.6533, 0.1009],
         [0.6199, 0.7222]],

        [[0.5818, 0.0996],
         [0.6305, 0.7215]],

        [[0.6635, 0.1008],
         [0.6241, 0.6432]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.04637005375299527
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.5290007089118618
Average Adjusted Rand Index: 0.7619180385187047
[-0.003348257218003585, 0.5290007089118618] [-0.00045830801606543505, 0.7619180385187047] [11387.705078125, 11179.1748046875]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11114.699742540199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22558.826171875
inf tensor(22558.8262, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11451.2705078125
tensor(22558.8262, grad_fn=<NegBackward0>) tensor(11451.2705, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11439.82421875
tensor(11451.2705, grad_fn=<NegBackward0>) tensor(11439.8242, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11370.8115234375
tensor(11439.8242, grad_fn=<NegBackward0>) tensor(11370.8115, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11329.69140625
tensor(11370.8115, grad_fn=<NegBackward0>) tensor(11329.6914, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11221.2978515625
tensor(11329.6914, grad_fn=<NegBackward0>) tensor(11221.2979, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11100.7158203125
tensor(11221.2979, grad_fn=<NegBackward0>) tensor(11100.7158, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11099.0390625
tensor(11100.7158, grad_fn=<NegBackward0>) tensor(11099.0391, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11098.603515625
tensor(11099.0391, grad_fn=<NegBackward0>) tensor(11098.6035, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11098.3974609375
tensor(11098.6035, grad_fn=<NegBackward0>) tensor(11098.3975, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11098.263671875
tensor(11098.3975, grad_fn=<NegBackward0>) tensor(11098.2637, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11095.09375
tensor(11098.2637, grad_fn=<NegBackward0>) tensor(11095.0938, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11095.013671875
tensor(11095.0938, grad_fn=<NegBackward0>) tensor(11095.0137, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11094.919921875
tensor(11095.0137, grad_fn=<NegBackward0>) tensor(11094.9199, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11093.3701171875
tensor(11094.9199, grad_fn=<NegBackward0>) tensor(11093.3701, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11093.326171875
tensor(11093.3701, grad_fn=<NegBackward0>) tensor(11093.3262, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11093.2939453125
tensor(11093.3262, grad_fn=<NegBackward0>) tensor(11093.2939, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11093.2705078125
tensor(11093.2939, grad_fn=<NegBackward0>) tensor(11093.2705, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11093.251953125
tensor(11093.2705, grad_fn=<NegBackward0>) tensor(11093.2520, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11093.23828125
tensor(11093.2520, grad_fn=<NegBackward0>) tensor(11093.2383, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11093.2294921875
tensor(11093.2383, grad_fn=<NegBackward0>) tensor(11093.2295, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11093.21875
tensor(11093.2295, grad_fn=<NegBackward0>) tensor(11093.2188, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11093.2119140625
tensor(11093.2188, grad_fn=<NegBackward0>) tensor(11093.2119, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11093.2041015625
tensor(11093.2119, grad_fn=<NegBackward0>) tensor(11093.2041, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11093.1962890625
tensor(11093.2041, grad_fn=<NegBackward0>) tensor(11093.1963, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11093.1875
tensor(11093.1963, grad_fn=<NegBackward0>) tensor(11093.1875, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11093.1806640625
tensor(11093.1875, grad_fn=<NegBackward0>) tensor(11093.1807, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11093.1748046875
tensor(11093.1807, grad_fn=<NegBackward0>) tensor(11093.1748, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11093.169921875
tensor(11093.1748, grad_fn=<NegBackward0>) tensor(11093.1699, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11093.162109375
tensor(11093.1699, grad_fn=<NegBackward0>) tensor(11093.1621, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11093.154296875
tensor(11093.1621, grad_fn=<NegBackward0>) tensor(11093.1543, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11093.1494140625
tensor(11093.1543, grad_fn=<NegBackward0>) tensor(11093.1494, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11093.1376953125
tensor(11093.1494, grad_fn=<NegBackward0>) tensor(11093.1377, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11093.0634765625
tensor(11093.1377, grad_fn=<NegBackward0>) tensor(11093.0635, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11093.05859375
tensor(11093.0635, grad_fn=<NegBackward0>) tensor(11093.0586, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11093.056640625
tensor(11093.0586, grad_fn=<NegBackward0>) tensor(11093.0566, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11093.0546875
tensor(11093.0566, grad_fn=<NegBackward0>) tensor(11093.0547, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11093.0615234375
tensor(11093.0547, grad_fn=<NegBackward0>) tensor(11093.0615, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11093.05078125
tensor(11093.0547, grad_fn=<NegBackward0>) tensor(11093.0508, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11093.0498046875
tensor(11093.0508, grad_fn=<NegBackward0>) tensor(11093.0498, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11093.046875
tensor(11093.0498, grad_fn=<NegBackward0>) tensor(11093.0469, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11093.0556640625
tensor(11093.0469, grad_fn=<NegBackward0>) tensor(11093.0557, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11093.0419921875
tensor(11093.0469, grad_fn=<NegBackward0>) tensor(11093.0420, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11093.041015625
tensor(11093.0420, grad_fn=<NegBackward0>) tensor(11093.0410, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11093.0390625
tensor(11093.0410, grad_fn=<NegBackward0>) tensor(11093.0391, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11093.0400390625
tensor(11093.0391, grad_fn=<NegBackward0>) tensor(11093.0400, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11093.0390625
tensor(11093.0391, grad_fn=<NegBackward0>) tensor(11093.0391, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11093.037109375
tensor(11093.0391, grad_fn=<NegBackward0>) tensor(11093.0371, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11093.03515625
tensor(11093.0371, grad_fn=<NegBackward0>) tensor(11093.0352, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11093.0361328125
tensor(11093.0352, grad_fn=<NegBackward0>) tensor(11093.0361, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11093.0302734375
tensor(11093.0352, grad_fn=<NegBackward0>) tensor(11093.0303, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11093.0283203125
tensor(11093.0303, grad_fn=<NegBackward0>) tensor(11093.0283, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11093.0263671875
tensor(11093.0283, grad_fn=<NegBackward0>) tensor(11093.0264, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11093.0185546875
tensor(11093.0264, grad_fn=<NegBackward0>) tensor(11093.0186, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11093.017578125
tensor(11093.0186, grad_fn=<NegBackward0>) tensor(11093.0176, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11093.017578125
tensor(11093.0176, grad_fn=<NegBackward0>) tensor(11093.0176, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11093.01953125
tensor(11093.0176, grad_fn=<NegBackward0>) tensor(11093.0195, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11093.017578125
tensor(11093.0176, grad_fn=<NegBackward0>) tensor(11093.0176, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11093.0166015625
tensor(11093.0176, grad_fn=<NegBackward0>) tensor(11093.0166, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11093.0185546875
tensor(11093.0166, grad_fn=<NegBackward0>) tensor(11093.0186, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11093.017578125
tensor(11093.0166, grad_fn=<NegBackward0>) tensor(11093.0176, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11093.01171875
tensor(11093.0166, grad_fn=<NegBackward0>) tensor(11093.0117, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11093.013671875
tensor(11093.0117, grad_fn=<NegBackward0>) tensor(11093.0137, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11093.01171875
tensor(11093.0117, grad_fn=<NegBackward0>) tensor(11093.0117, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11093.009765625
tensor(11093.0117, grad_fn=<NegBackward0>) tensor(11093.0098, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11093.0078125
tensor(11093.0098, grad_fn=<NegBackward0>) tensor(11093.0078, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11093.005859375
tensor(11093.0078, grad_fn=<NegBackward0>) tensor(11093.0059, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11093.005859375
tensor(11093.0059, grad_fn=<NegBackward0>) tensor(11093.0059, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11093.00390625
tensor(11093.0059, grad_fn=<NegBackward0>) tensor(11093.0039, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11093.00390625
tensor(11093.0039, grad_fn=<NegBackward0>) tensor(11093.0039, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11093.005859375
tensor(11093.0039, grad_fn=<NegBackward0>) tensor(11093.0059, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11093.0078125
tensor(11093.0039, grad_fn=<NegBackward0>) tensor(11093.0078, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11093.00390625
tensor(11093.0039, grad_fn=<NegBackward0>) tensor(11093.0039, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11093.0
tensor(11093.0039, grad_fn=<NegBackward0>) tensor(11093., grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11092.876953125
tensor(11093., grad_fn=<NegBackward0>) tensor(11092.8770, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11092.8759765625
tensor(11092.8770, grad_fn=<NegBackward0>) tensor(11092.8760, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11092.87109375
tensor(11092.8760, grad_fn=<NegBackward0>) tensor(11092.8711, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11092.86328125
tensor(11092.8711, grad_fn=<NegBackward0>) tensor(11092.8633, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11092.8828125
tensor(11092.8633, grad_fn=<NegBackward0>) tensor(11092.8828, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11092.86328125
tensor(11092.8633, grad_fn=<NegBackward0>) tensor(11092.8633, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11092.89453125
tensor(11092.8633, grad_fn=<NegBackward0>) tensor(11092.8945, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11092.8623046875
tensor(11092.8633, grad_fn=<NegBackward0>) tensor(11092.8623, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11092.861328125
tensor(11092.8623, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11092.861328125
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11092.861328125
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11092.8623046875
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8623, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11092.8623046875
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8623, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11092.8671875
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8672, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11092.861328125
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11092.9384765625
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.9385, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11092.8603515625
tensor(11092.8613, grad_fn=<NegBackward0>) tensor(11092.8604, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11092.8623046875
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8623, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11092.861328125
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11092.8603515625
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8604, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11092.861328125
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11092.8603515625
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8604, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11092.861328125
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11092.8642578125
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8643, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11092.966796875
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.9668, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11092.861328125
tensor(11092.8604, grad_fn=<NegBackward0>) tensor(11092.8613, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7204, 0.2796],
        [0.2624, 0.7376]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5793, 0.4207], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3001, 0.1030],
         [0.5707, 0.1997]],

        [[0.6259, 0.0900],
         [0.6613, 0.6642]],

        [[0.5618, 0.0971],
         [0.5684, 0.6467]],

        [[0.7189, 0.0937],
         [0.5302, 0.5380]],

        [[0.7254, 0.0915],
         [0.6120, 0.7075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207884124763394
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368977953316787
Average Adjusted Rand Index: 0.9368011953537072
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20226.037109375
inf tensor(20226.0371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11440.6201171875
tensor(20226.0371, grad_fn=<NegBackward0>) tensor(11440.6201, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11361.3115234375
tensor(11440.6201, grad_fn=<NegBackward0>) tensor(11361.3115, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11340.03515625
tensor(11361.3115, grad_fn=<NegBackward0>) tensor(11340.0352, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11257.0712890625
tensor(11340.0352, grad_fn=<NegBackward0>) tensor(11257.0713, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11112.5712890625
tensor(11257.0713, grad_fn=<NegBackward0>) tensor(11112.5713, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11103.634765625
tensor(11112.5713, grad_fn=<NegBackward0>) tensor(11103.6348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11100.390625
tensor(11103.6348, grad_fn=<NegBackward0>) tensor(11100.3906, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11094.5244140625
tensor(11100.3906, grad_fn=<NegBackward0>) tensor(11094.5244, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11094.125
tensor(11094.5244, grad_fn=<NegBackward0>) tensor(11094.1250, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11094.05859375
tensor(11094.1250, grad_fn=<NegBackward0>) tensor(11094.0586, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11094.017578125
tensor(11094.0586, grad_fn=<NegBackward0>) tensor(11094.0176, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11093.982421875
tensor(11094.0176, grad_fn=<NegBackward0>) tensor(11093.9824, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11093.9287109375
tensor(11093.9824, grad_fn=<NegBackward0>) tensor(11093.9287, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11093.892578125
tensor(11093.9287, grad_fn=<NegBackward0>) tensor(11093.8926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11093.876953125
tensor(11093.8926, grad_fn=<NegBackward0>) tensor(11093.8770, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11093.8662109375
tensor(11093.8770, grad_fn=<NegBackward0>) tensor(11093.8662, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11093.859375
tensor(11093.8662, grad_fn=<NegBackward0>) tensor(11093.8594, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11093.849609375
tensor(11093.8594, grad_fn=<NegBackward0>) tensor(11093.8496, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11093.8427734375
tensor(11093.8496, grad_fn=<NegBackward0>) tensor(11093.8428, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11093.8349609375
tensor(11093.8428, grad_fn=<NegBackward0>) tensor(11093.8350, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11093.8291015625
tensor(11093.8350, grad_fn=<NegBackward0>) tensor(11093.8291, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11093.8251953125
tensor(11093.8291, grad_fn=<NegBackward0>) tensor(11093.8252, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11093.8193359375
tensor(11093.8252, grad_fn=<NegBackward0>) tensor(11093.8193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11093.6123046875
tensor(11093.8193, grad_fn=<NegBackward0>) tensor(11093.6123, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11093.5615234375
tensor(11093.6123, grad_fn=<NegBackward0>) tensor(11093.5615, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11093.5595703125
tensor(11093.5615, grad_fn=<NegBackward0>) tensor(11093.5596, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11093.5576171875
tensor(11093.5596, grad_fn=<NegBackward0>) tensor(11093.5576, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11093.5537109375
tensor(11093.5576, grad_fn=<NegBackward0>) tensor(11093.5537, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11093.5537109375
tensor(11093.5537, grad_fn=<NegBackward0>) tensor(11093.5537, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11093.55078125
tensor(11093.5537, grad_fn=<NegBackward0>) tensor(11093.5508, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11093.5498046875
tensor(11093.5508, grad_fn=<NegBackward0>) tensor(11093.5498, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11093.548828125
tensor(11093.5498, grad_fn=<NegBackward0>) tensor(11093.5488, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11093.546875
tensor(11093.5488, grad_fn=<NegBackward0>) tensor(11093.5469, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11093.544921875
tensor(11093.5469, grad_fn=<NegBackward0>) tensor(11093.5449, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11093.552734375
tensor(11093.5449, grad_fn=<NegBackward0>) tensor(11093.5527, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11093.5419921875
tensor(11093.5449, grad_fn=<NegBackward0>) tensor(11093.5420, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11093.541015625
tensor(11093.5420, grad_fn=<NegBackward0>) tensor(11093.5410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11093.541015625
tensor(11093.5410, grad_fn=<NegBackward0>) tensor(11093.5410, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11093.5400390625
tensor(11093.5410, grad_fn=<NegBackward0>) tensor(11093.5400, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11093.5400390625
tensor(11093.5400, grad_fn=<NegBackward0>) tensor(11093.5400, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11093.5400390625
tensor(11093.5400, grad_fn=<NegBackward0>) tensor(11093.5400, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11093.5400390625
tensor(11093.5400, grad_fn=<NegBackward0>) tensor(11093.5400, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11093.537109375
tensor(11093.5400, grad_fn=<NegBackward0>) tensor(11093.5371, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11093.5380859375
tensor(11093.5371, grad_fn=<NegBackward0>) tensor(11093.5381, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11093.537109375
tensor(11093.5371, grad_fn=<NegBackward0>) tensor(11093.5371, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11093.546875
tensor(11093.5371, grad_fn=<NegBackward0>) tensor(11093.5469, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11093.5341796875
tensor(11093.5371, grad_fn=<NegBackward0>) tensor(11093.5342, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11093.53515625
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5352, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11093.53515625
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5352, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11093.53515625
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5352, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11093.5400390625
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5400, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11093.5341796875
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5342, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11093.5322265625
tensor(11093.5342, grad_fn=<NegBackward0>) tensor(11093.5322, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11093.5361328125
tensor(11093.5322, grad_fn=<NegBackward0>) tensor(11093.5361, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11093.53515625
tensor(11093.5322, grad_fn=<NegBackward0>) tensor(11093.5352, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11093.533203125
tensor(11093.5322, grad_fn=<NegBackward0>) tensor(11093.5332, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11093.5263671875
tensor(11093.5322, grad_fn=<NegBackward0>) tensor(11093.5264, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11093.52734375
tensor(11093.5264, grad_fn=<NegBackward0>) tensor(11093.5273, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11093.5244140625
tensor(11093.5264, grad_fn=<NegBackward0>) tensor(11093.5244, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11093.369140625
tensor(11093.5244, grad_fn=<NegBackward0>) tensor(11093.3691, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11093.3671875
tensor(11093.3691, grad_fn=<NegBackward0>) tensor(11093.3672, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11093.3662109375
tensor(11093.3672, grad_fn=<NegBackward0>) tensor(11093.3662, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11093.365234375
tensor(11093.3662, grad_fn=<NegBackward0>) tensor(11093.3652, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11093.3671875
tensor(11093.3652, grad_fn=<NegBackward0>) tensor(11093.3672, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11093.365234375
tensor(11093.3652, grad_fn=<NegBackward0>) tensor(11093.3652, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11093.3671875
tensor(11093.3652, grad_fn=<NegBackward0>) tensor(11093.3672, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11093.3642578125
tensor(11093.3652, grad_fn=<NegBackward0>) tensor(11093.3643, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11093.2666015625
tensor(11093.3643, grad_fn=<NegBackward0>) tensor(11093.2666, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11093.2783203125
tensor(11093.2666, grad_fn=<NegBackward0>) tensor(11093.2783, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11093.1435546875
tensor(11093.2666, grad_fn=<NegBackward0>) tensor(11093.1436, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11093.1435546875
tensor(11093.1436, grad_fn=<NegBackward0>) tensor(11093.1436, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11093.146484375
tensor(11093.1436, grad_fn=<NegBackward0>) tensor(11093.1465, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11093.142578125
tensor(11093.1436, grad_fn=<NegBackward0>) tensor(11093.1426, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11093.14453125
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1445, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11093.1435546875
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1436, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11093.1484375
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1484, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11093.142578125
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1426, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11093.1884765625
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1885, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11093.1416015625
tensor(11093.1426, grad_fn=<NegBackward0>) tensor(11093.1416, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11093.142578125
tensor(11093.1416, grad_fn=<NegBackward0>) tensor(11093.1426, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11093.142578125
tensor(11093.1416, grad_fn=<NegBackward0>) tensor(11093.1426, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11093.1328125
tensor(11093.1416, grad_fn=<NegBackward0>) tensor(11093.1328, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11093.130859375
tensor(11093.1328, grad_fn=<NegBackward0>) tensor(11093.1309, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11093.1298828125
tensor(11093.1309, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11093.38671875
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.3867, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11093.1298828125
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11093.130859375
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1309, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11093.1298828125
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11093.1337890625
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1338, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11093.130859375
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1309, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11093.1298828125
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11093.1298828125
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11093.1298828125
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11093.12890625
tensor(11093.1299, grad_fn=<NegBackward0>) tensor(11093.1289, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11093.1455078125
tensor(11093.1289, grad_fn=<NegBackward0>) tensor(11093.1455, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11093.1328125
tensor(11093.1289, grad_fn=<NegBackward0>) tensor(11093.1328, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11093.1298828125
tensor(11093.1289, grad_fn=<NegBackward0>) tensor(11093.1299, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11093.12890625
tensor(11093.1289, grad_fn=<NegBackward0>) tensor(11093.1289, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11093.1123046875
tensor(11093.1289, grad_fn=<NegBackward0>) tensor(11093.1123, grad_fn=<NegBackward0>)
pi: tensor([[0.7214, 0.2786],
        [0.2616, 0.7384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5847, 0.4153], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.1025],
         [0.7241, 0.2000]],

        [[0.6130, 0.0899],
         [0.5554, 0.5008]],

        [[0.7117, 0.0970],
         [0.6998, 0.6504]],

        [[0.6022, 0.0937],
         [0.5070, 0.6148]],

        [[0.6495, 0.0914],
         [0.5449, 0.6272]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9291543100011382
Average Adjusted Rand Index: 0.9291199743773794
[0.9368977953316787, 0.9291543100011382] [0.9368011953537072, 0.9291199743773794] [11092.8603515625, 11093.1123046875]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11120.324906730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21418.521484375
inf tensor(21418.5215, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11331.4501953125
tensor(21418.5215, grad_fn=<NegBackward0>) tensor(11331.4502, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11330.751953125
tensor(11331.4502, grad_fn=<NegBackward0>) tensor(11330.7520, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11330.0205078125
tensor(11330.7520, grad_fn=<NegBackward0>) tensor(11330.0205, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11327.833984375
tensor(11330.0205, grad_fn=<NegBackward0>) tensor(11327.8340, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11326.9833984375
tensor(11327.8340, grad_fn=<NegBackward0>) tensor(11326.9834, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11326.458984375
tensor(11326.9834, grad_fn=<NegBackward0>) tensor(11326.4590, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11326.0888671875
tensor(11326.4590, grad_fn=<NegBackward0>) tensor(11326.0889, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11325.7265625
tensor(11326.0889, grad_fn=<NegBackward0>) tensor(11325.7266, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11325.4091796875
tensor(11325.7266, grad_fn=<NegBackward0>) tensor(11325.4092, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11324.8515625
tensor(11325.4092, grad_fn=<NegBackward0>) tensor(11324.8516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11290.5341796875
tensor(11324.8516, grad_fn=<NegBackward0>) tensor(11290.5342, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11120.8251953125
tensor(11290.5342, grad_fn=<NegBackward0>) tensor(11120.8252, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11111.8994140625
tensor(11120.8252, grad_fn=<NegBackward0>) tensor(11111.8994, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11110.490234375
tensor(11111.8994, grad_fn=<NegBackward0>) tensor(11110.4902, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11110.3046875
tensor(11110.4902, grad_fn=<NegBackward0>) tensor(11110.3047, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11109.4609375
tensor(11110.3047, grad_fn=<NegBackward0>) tensor(11109.4609, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11108.9736328125
tensor(11109.4609, grad_fn=<NegBackward0>) tensor(11108.9736, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11108.884765625
tensor(11108.9736, grad_fn=<NegBackward0>) tensor(11108.8848, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11108.3916015625
tensor(11108.8848, grad_fn=<NegBackward0>) tensor(11108.3916, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11107.3564453125
tensor(11108.3916, grad_fn=<NegBackward0>) tensor(11107.3564, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11107.2451171875
tensor(11107.3564, grad_fn=<NegBackward0>) tensor(11107.2451, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11107.232421875
tensor(11107.2451, grad_fn=<NegBackward0>) tensor(11107.2324, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11107.2216796875
tensor(11107.2324, grad_fn=<NegBackward0>) tensor(11107.2217, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11107.212890625
tensor(11107.2217, grad_fn=<NegBackward0>) tensor(11107.2129, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11107.2001953125
tensor(11107.2129, grad_fn=<NegBackward0>) tensor(11107.2002, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11107.18359375
tensor(11107.2002, grad_fn=<NegBackward0>) tensor(11107.1836, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11107.0869140625
tensor(11107.1836, grad_fn=<NegBackward0>) tensor(11107.0869, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11106.740234375
tensor(11107.0869, grad_fn=<NegBackward0>) tensor(11106.7402, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11106.7353515625
tensor(11106.7402, grad_fn=<NegBackward0>) tensor(11106.7354, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11106.7236328125
tensor(11106.7354, grad_fn=<NegBackward0>) tensor(11106.7236, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11101.0771484375
tensor(11106.7236, grad_fn=<NegBackward0>) tensor(11101.0771, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11101.0732421875
tensor(11101.0771, grad_fn=<NegBackward0>) tensor(11101.0732, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11101.068359375
tensor(11101.0732, grad_fn=<NegBackward0>) tensor(11101.0684, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11101.060546875
tensor(11101.0684, grad_fn=<NegBackward0>) tensor(11101.0605, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11101.0419921875
tensor(11101.0605, grad_fn=<NegBackward0>) tensor(11101.0420, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11101.0244140625
tensor(11101.0420, grad_fn=<NegBackward0>) tensor(11101.0244, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11101.0205078125
tensor(11101.0244, grad_fn=<NegBackward0>) tensor(11101.0205, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11101.0166015625
tensor(11101.0205, grad_fn=<NegBackward0>) tensor(11101.0166, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11101.02734375
tensor(11101.0166, grad_fn=<NegBackward0>) tensor(11101.0273, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11101.015625
tensor(11101.0166, grad_fn=<NegBackward0>) tensor(11101.0156, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11101.015625
tensor(11101.0156, grad_fn=<NegBackward0>) tensor(11101.0156, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11101.01171875
tensor(11101.0156, grad_fn=<NegBackward0>) tensor(11101.0117, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11101.009765625
tensor(11101.0117, grad_fn=<NegBackward0>) tensor(11101.0098, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11101.009765625
tensor(11101.0098, grad_fn=<NegBackward0>) tensor(11101.0098, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11101.0078125
tensor(11101.0098, grad_fn=<NegBackward0>) tensor(11101.0078, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11101.0302734375
tensor(11101.0078, grad_fn=<NegBackward0>) tensor(11101.0303, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11100.990234375
tensor(11101.0078, grad_fn=<NegBackward0>) tensor(11100.9902, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11100.990234375
tensor(11100.9902, grad_fn=<NegBackward0>) tensor(11100.9902, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11100.9892578125
tensor(11100.9902, grad_fn=<NegBackward0>) tensor(11100.9893, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11100.98828125
tensor(11100.9893, grad_fn=<NegBackward0>) tensor(11100.9883, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11100.9892578125
tensor(11100.9883, grad_fn=<NegBackward0>) tensor(11100.9893, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11100.994140625
tensor(11100.9883, grad_fn=<NegBackward0>) tensor(11100.9941, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11100.9912109375
tensor(11100.9883, grad_fn=<NegBackward0>) tensor(11100.9912, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11100.9873046875
tensor(11100.9883, grad_fn=<NegBackward0>) tensor(11100.9873, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11100.98828125
tensor(11100.9873, grad_fn=<NegBackward0>) tensor(11100.9883, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11100.9931640625
tensor(11100.9873, grad_fn=<NegBackward0>) tensor(11100.9932, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11100.986328125
tensor(11100.9873, grad_fn=<NegBackward0>) tensor(11100.9863, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11100.8505859375
tensor(11100.9863, grad_fn=<NegBackward0>) tensor(11100.8506, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11100.8515625
tensor(11100.8506, grad_fn=<NegBackward0>) tensor(11100.8516, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11100.84765625
tensor(11100.8506, grad_fn=<NegBackward0>) tensor(11100.8477, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11100.84765625
tensor(11100.8477, grad_fn=<NegBackward0>) tensor(11100.8477, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11100.853515625
tensor(11100.8477, grad_fn=<NegBackward0>) tensor(11100.8535, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11100.8466796875
tensor(11100.8477, grad_fn=<NegBackward0>) tensor(11100.8467, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11100.8466796875
tensor(11100.8467, grad_fn=<NegBackward0>) tensor(11100.8467, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11100.845703125
tensor(11100.8467, grad_fn=<NegBackward0>) tensor(11100.8457, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11100.8466796875
tensor(11100.8457, grad_fn=<NegBackward0>) tensor(11100.8467, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11100.8427734375
tensor(11100.8457, grad_fn=<NegBackward0>) tensor(11100.8428, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11100.853515625
tensor(11100.8428, grad_fn=<NegBackward0>) tensor(11100.8535, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11100.8427734375
tensor(11100.8428, grad_fn=<NegBackward0>) tensor(11100.8428, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11100.845703125
tensor(11100.8428, grad_fn=<NegBackward0>) tensor(11100.8457, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11100.8408203125
tensor(11100.8428, grad_fn=<NegBackward0>) tensor(11100.8408, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11100.8505859375
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8506, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11100.84375
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8438, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11100.8427734375
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8428, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11100.8408203125
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8408, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11100.8447265625
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8447, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11100.861328125
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8613, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11100.83984375
tensor(11100.8408, grad_fn=<NegBackward0>) tensor(11100.8398, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11100.8408203125
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8408, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11100.83984375
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8398, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11100.8408203125
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8408, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11100.83984375
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8398, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11100.83984375
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8398, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11100.837890625
tensor(11100.8398, grad_fn=<NegBackward0>) tensor(11100.8379, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11100.837890625
tensor(11100.8379, grad_fn=<NegBackward0>) tensor(11100.8379, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11100.8359375
tensor(11100.8379, grad_fn=<NegBackward0>) tensor(11100.8359, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11100.8349609375
tensor(11100.8359, grad_fn=<NegBackward0>) tensor(11100.8350, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11100.8349609375
tensor(11100.8350, grad_fn=<NegBackward0>) tensor(11100.8350, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11100.8271484375
tensor(11100.8350, grad_fn=<NegBackward0>) tensor(11100.8271, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11100.828125
tensor(11100.8271, grad_fn=<NegBackward0>) tensor(11100.8281, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11100.826171875
tensor(11100.8271, grad_fn=<NegBackward0>) tensor(11100.8262, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11100.826171875
tensor(11100.8262, grad_fn=<NegBackward0>) tensor(11100.8262, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11100.8251953125
tensor(11100.8262, grad_fn=<NegBackward0>) tensor(11100.8252, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11100.9052734375
tensor(11100.8252, grad_fn=<NegBackward0>) tensor(11100.9053, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11100.8232421875
tensor(11100.8252, grad_fn=<NegBackward0>) tensor(11100.8232, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11100.8232421875
tensor(11100.8232, grad_fn=<NegBackward0>) tensor(11100.8232, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11100.8291015625
tensor(11100.8232, grad_fn=<NegBackward0>) tensor(11100.8291, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11100.8232421875
tensor(11100.8232, grad_fn=<NegBackward0>) tensor(11100.8232, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11100.8359375
tensor(11100.8232, grad_fn=<NegBackward0>) tensor(11100.8359, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7706, 0.2294],
        [0.2653, 0.7347]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4799, 0.5201], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.0945],
         [0.6398, 0.2879]],

        [[0.5995, 0.1070],
         [0.5210, 0.6689]],

        [[0.6056, 0.1054],
         [0.7222, 0.6310]],

        [[0.5012, 0.1061],
         [0.7211, 0.5883]],

        [[0.6187, 0.0975],
         [0.5927, 0.5293]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9446732505061082
Average Adjusted Rand Index: 0.9446432137166513
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19989.45703125
inf tensor(19989.4570, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11330.9521484375
tensor(19989.4570, grad_fn=<NegBackward0>) tensor(11330.9521, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11329.962890625
tensor(11330.9521, grad_fn=<NegBackward0>) tensor(11329.9629, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11327.9072265625
tensor(11329.9629, grad_fn=<NegBackward0>) tensor(11327.9072, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11327.1875
tensor(11327.9072, grad_fn=<NegBackward0>) tensor(11327.1875, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11327.0107421875
tensor(11327.1875, grad_fn=<NegBackward0>) tensor(11327.0107, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11326.8623046875
tensor(11327.0107, grad_fn=<NegBackward0>) tensor(11326.8623, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11326.6806640625
tensor(11326.8623, grad_fn=<NegBackward0>) tensor(11326.6807, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11326.5556640625
tensor(11326.6807, grad_fn=<NegBackward0>) tensor(11326.5557, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11326.4755859375
tensor(11326.5557, grad_fn=<NegBackward0>) tensor(11326.4756, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11326.408203125
tensor(11326.4756, grad_fn=<NegBackward0>) tensor(11326.4082, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11326.345703125
tensor(11326.4082, grad_fn=<NegBackward0>) tensor(11326.3457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11326.27734375
tensor(11326.3457, grad_fn=<NegBackward0>) tensor(11326.2773, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11326.203125
tensor(11326.2773, grad_fn=<NegBackward0>) tensor(11326.2031, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11326.1123046875
tensor(11326.2031, grad_fn=<NegBackward0>) tensor(11326.1123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11326.0078125
tensor(11326.1123, grad_fn=<NegBackward0>) tensor(11326.0078, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11325.9150390625
tensor(11326.0078, grad_fn=<NegBackward0>) tensor(11325.9150, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11325.8486328125
tensor(11325.9150, grad_fn=<NegBackward0>) tensor(11325.8486, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11325.8037109375
tensor(11325.8486, grad_fn=<NegBackward0>) tensor(11325.8037, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11325.7666015625
tensor(11325.8037, grad_fn=<NegBackward0>) tensor(11325.7666, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11325.70703125
tensor(11325.7666, grad_fn=<NegBackward0>) tensor(11325.7070, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11325.5654296875
tensor(11325.7070, grad_fn=<NegBackward0>) tensor(11325.5654, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11325.435546875
tensor(11325.5654, grad_fn=<NegBackward0>) tensor(11325.4355, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11325.384765625
tensor(11325.4355, grad_fn=<NegBackward0>) tensor(11325.3848, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11325.36328125
tensor(11325.3848, grad_fn=<NegBackward0>) tensor(11325.3633, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11325.353515625
tensor(11325.3633, grad_fn=<NegBackward0>) tensor(11325.3535, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11325.3466796875
tensor(11325.3535, grad_fn=<NegBackward0>) tensor(11325.3467, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11325.3408203125
tensor(11325.3467, grad_fn=<NegBackward0>) tensor(11325.3408, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11325.337890625
tensor(11325.3408, grad_fn=<NegBackward0>) tensor(11325.3379, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11325.3349609375
tensor(11325.3379, grad_fn=<NegBackward0>) tensor(11325.3350, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11325.3349609375
tensor(11325.3350, grad_fn=<NegBackward0>) tensor(11325.3350, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11325.3310546875
tensor(11325.3350, grad_fn=<NegBackward0>) tensor(11325.3311, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11325.3310546875
tensor(11325.3311, grad_fn=<NegBackward0>) tensor(11325.3311, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11325.3271484375
tensor(11325.3311, grad_fn=<NegBackward0>) tensor(11325.3271, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11325.3271484375
tensor(11325.3271, grad_fn=<NegBackward0>) tensor(11325.3271, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11325.3251953125
tensor(11325.3271, grad_fn=<NegBackward0>) tensor(11325.3252, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11325.3251953125
tensor(11325.3252, grad_fn=<NegBackward0>) tensor(11325.3252, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11325.3232421875
tensor(11325.3252, grad_fn=<NegBackward0>) tensor(11325.3232, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11325.3232421875
tensor(11325.3232, grad_fn=<NegBackward0>) tensor(11325.3232, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11325.3232421875
tensor(11325.3232, grad_fn=<NegBackward0>) tensor(11325.3232, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11325.3232421875
tensor(11325.3232, grad_fn=<NegBackward0>) tensor(11325.3232, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11325.322265625
tensor(11325.3232, grad_fn=<NegBackward0>) tensor(11325.3223, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11325.3212890625
tensor(11325.3223, grad_fn=<NegBackward0>) tensor(11325.3213, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11325.322265625
tensor(11325.3213, grad_fn=<NegBackward0>) tensor(11325.3223, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11325.3212890625
tensor(11325.3213, grad_fn=<NegBackward0>) tensor(11325.3213, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11325.3203125
tensor(11325.3213, grad_fn=<NegBackward0>) tensor(11325.3203, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11325.3193359375
tensor(11325.3203, grad_fn=<NegBackward0>) tensor(11325.3193, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11325.3193359375
tensor(11325.3193, grad_fn=<NegBackward0>) tensor(11325.3193, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11325.3193359375
tensor(11325.3193, grad_fn=<NegBackward0>) tensor(11325.3193, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11325.318359375
tensor(11325.3193, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11325.318359375
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11325.318359375
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11325.318359375
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11325.318359375
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11325.3193359375
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3193, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11325.3173828125
tensor(11325.3184, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11325.3173828125
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11325.3173828125
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11325.318359375
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11325.3184, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11325.3173828125
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11325.31640625
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11325.31640625
tensor(11325.3164, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11325.31640625
tensor(11325.3164, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11325.31640625
tensor(11325.3164, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11325.3154296875
tensor(11325.3164, grad_fn=<NegBackward0>) tensor(11325.3154, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11325.3173828125
tensor(11325.3154, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11325.31640625
tensor(11325.3154, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11325.3173828125
tensor(11325.3154, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11325.31640625
tensor(11325.3154, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11325.31640625
tensor(11325.3154, grad_fn=<NegBackward0>) tensor(11325.3164, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[9.7497e-01, 2.5028e-02],
        [9.9993e-01, 6.6583e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8899, 0.1101], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1728, 0.1224],
         [0.6191, 0.1154]],

        [[0.5151, 0.2747],
         [0.5329, 0.6174]],

        [[0.6865, 0.2837],
         [0.6954, 0.6748]],

        [[0.6845, 0.0938],
         [0.5938, 0.5884]],

        [[0.5651, 0.1128],
         [0.6496, 0.5753]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00047371165206273484
Average Adjusted Rand Index: 0.0016609108986282129
[0.9446732505061082, 0.00047371165206273484] [0.9446432137166513, 0.0016609108986282129] [11100.82421875, 11325.31640625]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11167.941056096859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22624.353515625
inf tensor(22624.3535, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11363.6474609375
tensor(22624.3535, grad_fn=<NegBackward0>) tensor(11363.6475, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11363.0849609375
tensor(11363.6475, grad_fn=<NegBackward0>) tensor(11363.0850, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11362.9033203125
tensor(11363.0850, grad_fn=<NegBackward0>) tensor(11362.9033, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11362.7060546875
tensor(11362.9033, grad_fn=<NegBackward0>) tensor(11362.7061, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11362.453125
tensor(11362.7061, grad_fn=<NegBackward0>) tensor(11362.4531, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11362.29296875
tensor(11362.4531, grad_fn=<NegBackward0>) tensor(11362.2930, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11362.1875
tensor(11362.2930, grad_fn=<NegBackward0>) tensor(11362.1875, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11362.078125
tensor(11362.1875, grad_fn=<NegBackward0>) tensor(11362.0781, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11361.9765625
tensor(11362.0781, grad_fn=<NegBackward0>) tensor(11361.9766, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11361.8935546875
tensor(11361.9766, grad_fn=<NegBackward0>) tensor(11361.8936, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11361.833984375
tensor(11361.8936, grad_fn=<NegBackward0>) tensor(11361.8340, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11361.7880859375
tensor(11361.8340, grad_fn=<NegBackward0>) tensor(11361.7881, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11361.7509765625
tensor(11361.7881, grad_fn=<NegBackward0>) tensor(11361.7510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11361.720703125
tensor(11361.7510, grad_fn=<NegBackward0>) tensor(11361.7207, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11361.693359375
tensor(11361.7207, grad_fn=<NegBackward0>) tensor(11361.6934, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11361.6689453125
tensor(11361.6934, grad_fn=<NegBackward0>) tensor(11361.6689, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11361.642578125
tensor(11361.6689, grad_fn=<NegBackward0>) tensor(11361.6426, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11361.619140625
tensor(11361.6426, grad_fn=<NegBackward0>) tensor(11361.6191, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11361.59375
tensor(11361.6191, grad_fn=<NegBackward0>) tensor(11361.5938, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11361.5654296875
tensor(11361.5938, grad_fn=<NegBackward0>) tensor(11361.5654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11361.52734375
tensor(11361.5654, grad_fn=<NegBackward0>) tensor(11361.5273, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11361.4794921875
tensor(11361.5273, grad_fn=<NegBackward0>) tensor(11361.4795, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11361.40625
tensor(11361.4795, grad_fn=<NegBackward0>) tensor(11361.4062, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11361.2509765625
tensor(11361.4062, grad_fn=<NegBackward0>) tensor(11361.2510, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11360.5703125
tensor(11361.2510, grad_fn=<NegBackward0>) tensor(11360.5703, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11359.7568359375
tensor(11360.5703, grad_fn=<NegBackward0>) tensor(11359.7568, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11358.9697265625
tensor(11359.7568, grad_fn=<NegBackward0>) tensor(11358.9697, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11357.3125
tensor(11358.9697, grad_fn=<NegBackward0>) tensor(11357.3125, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11357.1259765625
tensor(11357.3125, grad_fn=<NegBackward0>) tensor(11357.1260, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11357.052734375
tensor(11357.1260, grad_fn=<NegBackward0>) tensor(11357.0527, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11357.013671875
tensor(11357.0527, grad_fn=<NegBackward0>) tensor(11357.0137, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11356.9853515625
tensor(11357.0137, grad_fn=<NegBackward0>) tensor(11356.9854, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11356.9677734375
tensor(11356.9854, grad_fn=<NegBackward0>) tensor(11356.9678, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11356.955078125
tensor(11356.9678, grad_fn=<NegBackward0>) tensor(11356.9551, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11356.9462890625
tensor(11356.9551, grad_fn=<NegBackward0>) tensor(11356.9463, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11356.9365234375
tensor(11356.9463, grad_fn=<NegBackward0>) tensor(11356.9365, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11356.9326171875
tensor(11356.9365, grad_fn=<NegBackward0>) tensor(11356.9326, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11356.9267578125
tensor(11356.9326, grad_fn=<NegBackward0>) tensor(11356.9268, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11356.921875
tensor(11356.9268, grad_fn=<NegBackward0>) tensor(11356.9219, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11356.91796875
tensor(11356.9219, grad_fn=<NegBackward0>) tensor(11356.9180, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11356.9150390625
tensor(11356.9180, grad_fn=<NegBackward0>) tensor(11356.9150, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11356.9130859375
tensor(11356.9150, grad_fn=<NegBackward0>) tensor(11356.9131, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11356.91015625
tensor(11356.9131, grad_fn=<NegBackward0>) tensor(11356.9102, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11356.908203125
tensor(11356.9102, grad_fn=<NegBackward0>) tensor(11356.9082, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11356.9072265625
tensor(11356.9082, grad_fn=<NegBackward0>) tensor(11356.9072, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11356.9052734375
tensor(11356.9072, grad_fn=<NegBackward0>) tensor(11356.9053, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11356.904296875
tensor(11356.9053, grad_fn=<NegBackward0>) tensor(11356.9043, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11356.9013671875
tensor(11356.9043, grad_fn=<NegBackward0>) tensor(11356.9014, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11356.900390625
tensor(11356.9014, grad_fn=<NegBackward0>) tensor(11356.9004, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11356.900390625
tensor(11356.9004, grad_fn=<NegBackward0>) tensor(11356.9004, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11356.8984375
tensor(11356.9004, grad_fn=<NegBackward0>) tensor(11356.8984, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11356.896484375
tensor(11356.8984, grad_fn=<NegBackward0>) tensor(11356.8965, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11356.8974609375
tensor(11356.8965, grad_fn=<NegBackward0>) tensor(11356.8975, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11356.896484375
tensor(11356.8965, grad_fn=<NegBackward0>) tensor(11356.8965, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11356.8955078125
tensor(11356.8965, grad_fn=<NegBackward0>) tensor(11356.8955, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11356.896484375
tensor(11356.8955, grad_fn=<NegBackward0>) tensor(11356.8965, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11356.89453125
tensor(11356.8955, grad_fn=<NegBackward0>) tensor(11356.8945, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11356.8935546875
tensor(11356.8945, grad_fn=<NegBackward0>) tensor(11356.8936, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11356.89453125
tensor(11356.8936, grad_fn=<NegBackward0>) tensor(11356.8945, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11356.892578125
tensor(11356.8936, grad_fn=<NegBackward0>) tensor(11356.8926, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11356.8935546875
tensor(11356.8926, grad_fn=<NegBackward0>) tensor(11356.8936, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11356.8916015625
tensor(11356.8926, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11356.8916015625
tensor(11356.8916, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11356.8916015625
tensor(11356.8916, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11356.8896484375
tensor(11356.8916, grad_fn=<NegBackward0>) tensor(11356.8896, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11356.8916015625
tensor(11356.8896, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11356.888671875
tensor(11356.8896, grad_fn=<NegBackward0>) tensor(11356.8887, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11356.8896484375
tensor(11356.8887, grad_fn=<NegBackward0>) tensor(11356.8896, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11356.8876953125
tensor(11356.8887, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11356.8876953125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11356.8876953125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11356.88671875
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11356.8876953125
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11356.8876953125
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11356.8876953125
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11356.88671875
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11356.884765625
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11356.88671875
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11356.884765625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11356.88671875
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11356.884765625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11356.8876953125
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11356.884765625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11356.900390625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.9004, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11356.8857421875
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11356.8857421875
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11356.8837890625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8838, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11356.884765625
tensor(11356.8838, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11356.888671875
tensor(11356.8838, grad_fn=<NegBackward0>) tensor(11356.8887, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11356.884765625
tensor(11356.8838, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11356.884765625
tensor(11356.8838, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11356.88671875
tensor(11356.8838, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[9.9266e-01, 7.3408e-03],
        [1.6808e-04, 9.9983e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9459, 0.0541], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1718, 0.2186],
         [0.5486, 0.3477]],

        [[0.7170, 0.2127],
         [0.7110, 0.6329]],

        [[0.6846, 0.1710],
         [0.7020, 0.5869]],

        [[0.6973, 0.1719],
         [0.6370, 0.7042]],

        [[0.6933, 0.0962],
         [0.6831, 0.5289]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.007911462598749978
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.007817426793427174
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.019407251391605235
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.024851437378340484
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006998325149571616
Global Adjusted Rand Index: -0.012996705537725622
Average Adjusted Rand Index: -0.013397180662338897
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23500.068359375
inf tensor(23500.0684, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11364.0869140625
tensor(23500.0684, grad_fn=<NegBackward0>) tensor(11364.0869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11363.1005859375
tensor(11364.0869, grad_fn=<NegBackward0>) tensor(11363.1006, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11362.9033203125
tensor(11363.1006, grad_fn=<NegBackward0>) tensor(11362.9033, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11362.7998046875
tensor(11362.9033, grad_fn=<NegBackward0>) tensor(11362.7998, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11362.6904296875
tensor(11362.7998, grad_fn=<NegBackward0>) tensor(11362.6904, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11362.564453125
tensor(11362.6904, grad_fn=<NegBackward0>) tensor(11362.5645, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11362.4609375
tensor(11362.5645, grad_fn=<NegBackward0>) tensor(11362.4609, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11362.38671875
tensor(11362.4609, grad_fn=<NegBackward0>) tensor(11362.3867, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11362.3173828125
tensor(11362.3867, grad_fn=<NegBackward0>) tensor(11362.3174, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11362.2421875
tensor(11362.3174, grad_fn=<NegBackward0>) tensor(11362.2422, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11362.16015625
tensor(11362.2422, grad_fn=<NegBackward0>) tensor(11362.1602, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11362.0751953125
tensor(11362.1602, grad_fn=<NegBackward0>) tensor(11362.0752, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11361.9990234375
tensor(11362.0752, grad_fn=<NegBackward0>) tensor(11361.9990, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11361.935546875
tensor(11361.9990, grad_fn=<NegBackward0>) tensor(11361.9355, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11361.88671875
tensor(11361.9355, grad_fn=<NegBackward0>) tensor(11361.8867, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11361.845703125
tensor(11361.8867, grad_fn=<NegBackward0>) tensor(11361.8457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11361.814453125
tensor(11361.8457, grad_fn=<NegBackward0>) tensor(11361.8145, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11361.7880859375
tensor(11361.8145, grad_fn=<NegBackward0>) tensor(11361.7881, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11361.763671875
tensor(11361.7881, grad_fn=<NegBackward0>) tensor(11361.7637, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11361.7451171875
tensor(11361.7637, grad_fn=<NegBackward0>) tensor(11361.7451, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11361.724609375
tensor(11361.7451, grad_fn=<NegBackward0>) tensor(11361.7246, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11361.7080078125
tensor(11361.7246, grad_fn=<NegBackward0>) tensor(11361.7080, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11361.69140625
tensor(11361.7080, grad_fn=<NegBackward0>) tensor(11361.6914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11361.6767578125
tensor(11361.6914, grad_fn=<NegBackward0>) tensor(11361.6768, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11361.6611328125
tensor(11361.6768, grad_fn=<NegBackward0>) tensor(11361.6611, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11361.6455078125
tensor(11361.6611, grad_fn=<NegBackward0>) tensor(11361.6455, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11361.6298828125
tensor(11361.6455, grad_fn=<NegBackward0>) tensor(11361.6299, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11361.6171875
tensor(11361.6299, grad_fn=<NegBackward0>) tensor(11361.6172, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11361.599609375
tensor(11361.6172, grad_fn=<NegBackward0>) tensor(11361.5996, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11361.5810546875
tensor(11361.5996, grad_fn=<NegBackward0>) tensor(11361.5811, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11361.55859375
tensor(11361.5811, grad_fn=<NegBackward0>) tensor(11361.5586, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11361.5302734375
tensor(11361.5586, grad_fn=<NegBackward0>) tensor(11361.5303, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11361.4873046875
tensor(11361.5303, grad_fn=<NegBackward0>) tensor(11361.4873, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11361.423828125
tensor(11361.4873, grad_fn=<NegBackward0>) tensor(11361.4238, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11361.302734375
tensor(11361.4238, grad_fn=<NegBackward0>) tensor(11361.3027, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11360.85546875
tensor(11361.3027, grad_fn=<NegBackward0>) tensor(11360.8555, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11359.9462890625
tensor(11360.8555, grad_fn=<NegBackward0>) tensor(11359.9463, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11359.111328125
tensor(11359.9463, grad_fn=<NegBackward0>) tensor(11359.1113, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11357.3916015625
tensor(11359.1113, grad_fn=<NegBackward0>) tensor(11357.3916, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11357.1484375
tensor(11357.3916, grad_fn=<NegBackward0>) tensor(11357.1484, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11357.0654296875
tensor(11357.1484, grad_fn=<NegBackward0>) tensor(11357.0654, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11357.0205078125
tensor(11357.0654, grad_fn=<NegBackward0>) tensor(11357.0205, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11356.9921875
tensor(11357.0205, grad_fn=<NegBackward0>) tensor(11356.9922, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11356.97265625
tensor(11356.9922, grad_fn=<NegBackward0>) tensor(11356.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11356.9599609375
tensor(11356.9727, grad_fn=<NegBackward0>) tensor(11356.9600, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11356.9482421875
tensor(11356.9600, grad_fn=<NegBackward0>) tensor(11356.9482, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11356.9404296875
tensor(11356.9482, grad_fn=<NegBackward0>) tensor(11356.9404, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11356.93359375
tensor(11356.9404, grad_fn=<NegBackward0>) tensor(11356.9336, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11356.9287109375
tensor(11356.9336, grad_fn=<NegBackward0>) tensor(11356.9287, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11356.923828125
tensor(11356.9287, grad_fn=<NegBackward0>) tensor(11356.9238, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11356.9189453125
tensor(11356.9238, grad_fn=<NegBackward0>) tensor(11356.9189, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11356.916015625
tensor(11356.9189, grad_fn=<NegBackward0>) tensor(11356.9160, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11356.9140625
tensor(11356.9160, grad_fn=<NegBackward0>) tensor(11356.9141, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11356.9111328125
tensor(11356.9141, grad_fn=<NegBackward0>) tensor(11356.9111, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11356.908203125
tensor(11356.9111, grad_fn=<NegBackward0>) tensor(11356.9082, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11356.9052734375
tensor(11356.9082, grad_fn=<NegBackward0>) tensor(11356.9053, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11356.9052734375
tensor(11356.9053, grad_fn=<NegBackward0>) tensor(11356.9053, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11356.9033203125
tensor(11356.9053, grad_fn=<NegBackward0>) tensor(11356.9033, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11356.9013671875
tensor(11356.9033, grad_fn=<NegBackward0>) tensor(11356.9014, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11356.8994140625
tensor(11356.9014, grad_fn=<NegBackward0>) tensor(11356.8994, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11356.900390625
tensor(11356.8994, grad_fn=<NegBackward0>) tensor(11356.9004, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11356.8994140625
tensor(11356.8994, grad_fn=<NegBackward0>) tensor(11356.8994, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11356.8984375
tensor(11356.8994, grad_fn=<NegBackward0>) tensor(11356.8984, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11356.8974609375
tensor(11356.8984, grad_fn=<NegBackward0>) tensor(11356.8975, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11356.896484375
tensor(11356.8975, grad_fn=<NegBackward0>) tensor(11356.8965, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11356.896484375
tensor(11356.8965, grad_fn=<NegBackward0>) tensor(11356.8965, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11356.8955078125
tensor(11356.8965, grad_fn=<NegBackward0>) tensor(11356.8955, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11356.8935546875
tensor(11356.8955, grad_fn=<NegBackward0>) tensor(11356.8936, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11356.8935546875
tensor(11356.8936, grad_fn=<NegBackward0>) tensor(11356.8936, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11356.892578125
tensor(11356.8936, grad_fn=<NegBackward0>) tensor(11356.8926, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11356.8916015625
tensor(11356.8926, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11356.892578125
tensor(11356.8916, grad_fn=<NegBackward0>) tensor(11356.8926, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11356.890625
tensor(11356.8916, grad_fn=<NegBackward0>) tensor(11356.8906, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11356.890625
tensor(11356.8906, grad_fn=<NegBackward0>) tensor(11356.8906, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11357.1123046875
tensor(11356.8906, grad_fn=<NegBackward0>) tensor(11357.1123, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11356.890625
tensor(11356.8906, grad_fn=<NegBackward0>) tensor(11356.8906, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11356.8896484375
tensor(11356.8906, grad_fn=<NegBackward0>) tensor(11356.8896, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11356.888671875
tensor(11356.8896, grad_fn=<NegBackward0>) tensor(11356.8887, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11356.8876953125
tensor(11356.8887, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11356.8876953125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11357.1689453125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11357.1689, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11356.8876953125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11356.8876953125
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8877, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11356.8994140625
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8994, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11356.88671875
tensor(11356.8877, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11356.8857421875
tensor(11356.8867, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11356.8916015625
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8916, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11356.88671875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11356.8857421875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11356.892578125
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8926, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11356.88671875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8867, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11356.931640625
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.9316, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11356.8857421875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11356.8857421875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11356.8857421875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11356.8857421875
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8857, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11357.1787109375
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11357.1787, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11356.884765625
tensor(11356.8857, grad_fn=<NegBackward0>) tensor(11356.8848, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11356.8837890625
tensor(11356.8848, grad_fn=<NegBackward0>) tensor(11356.8838, grad_fn=<NegBackward0>)
pi: tensor([[9.9982e-01, 1.8164e-04],
        [7.3424e-03, 9.9266e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0541, 0.9459], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3477, 0.2186],
         [0.6138, 0.1718]],

        [[0.6052, 0.2127],
         [0.6699, 0.5820]],

        [[0.6706, 0.1710],
         [0.5530, 0.5395]],

        [[0.7232, 0.1719],
         [0.6597, 0.6381]],

        [[0.7127, 0.0962],
         [0.7141, 0.5263]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.007911462598749978
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.007817426793427174
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.019407251391605235
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.024851437378340484
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006998325149571616
Global Adjusted Rand Index: -0.012996705537725622
Average Adjusted Rand Index: -0.013397180662338897
[-0.012996705537725622, -0.012996705537725622] [-0.013397180662338897, -0.013397180662338897] [11356.88671875, 11356.884765625]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11132.410844230853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24156.220703125
inf tensor(24156.2207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11399.4375
tensor(24156.2207, grad_fn=<NegBackward0>) tensor(11399.4375, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11262.8056640625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11262.8057, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11260.9697265625
tensor(11262.8057, grad_fn=<NegBackward0>) tensor(11260.9697, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11260.7880859375
tensor(11260.9697, grad_fn=<NegBackward0>) tensor(11260.7881, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11260.7109375
tensor(11260.7881, grad_fn=<NegBackward0>) tensor(11260.7109, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11260.6103515625
tensor(11260.7109, grad_fn=<NegBackward0>) tensor(11260.6104, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11260.3046875
tensor(11260.6104, grad_fn=<NegBackward0>) tensor(11260.3047, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11260.2841796875
tensor(11260.3047, grad_fn=<NegBackward0>) tensor(11260.2842, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11260.2724609375
tensor(11260.2842, grad_fn=<NegBackward0>) tensor(11260.2725, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11260.2626953125
tensor(11260.2725, grad_fn=<NegBackward0>) tensor(11260.2627, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11260.2548828125
tensor(11260.2627, grad_fn=<NegBackward0>) tensor(11260.2549, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11260.25
tensor(11260.2549, grad_fn=<NegBackward0>) tensor(11260.2500, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11260.24609375
tensor(11260.2500, grad_fn=<NegBackward0>) tensor(11260.2461, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11260.2412109375
tensor(11260.2461, grad_fn=<NegBackward0>) tensor(11260.2412, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11260.234375
tensor(11260.2412, grad_fn=<NegBackward0>) tensor(11260.2344, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11256.130859375
tensor(11260.2344, grad_fn=<NegBackward0>) tensor(11256.1309, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11254.7646484375
tensor(11256.1309, grad_fn=<NegBackward0>) tensor(11254.7646, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11254.7333984375
tensor(11254.7646, grad_fn=<NegBackward0>) tensor(11254.7334, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11254.7294921875
tensor(11254.7334, grad_fn=<NegBackward0>) tensor(11254.7295, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11254.728515625
tensor(11254.7295, grad_fn=<NegBackward0>) tensor(11254.7285, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11254.7236328125
tensor(11254.7285, grad_fn=<NegBackward0>) tensor(11254.7236, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11254.71875
tensor(11254.7236, grad_fn=<NegBackward0>) tensor(11254.7188, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11254.7158203125
tensor(11254.7188, grad_fn=<NegBackward0>) tensor(11254.7158, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11254.6953125
tensor(11254.7158, grad_fn=<NegBackward0>) tensor(11254.6953, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11254.5439453125
tensor(11254.6953, grad_fn=<NegBackward0>) tensor(11254.5439, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11232.7763671875
tensor(11254.5439, grad_fn=<NegBackward0>) tensor(11232.7764, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11211.1796875
tensor(11232.7764, grad_fn=<NegBackward0>) tensor(11211.1797, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11211.046875
tensor(11211.1797, grad_fn=<NegBackward0>) tensor(11211.0469, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11211.0322265625
tensor(11211.0469, grad_fn=<NegBackward0>) tensor(11211.0322, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11211.02734375
tensor(11211.0322, grad_fn=<NegBackward0>) tensor(11211.0273, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11211.0205078125
tensor(11211.0273, grad_fn=<NegBackward0>) tensor(11211.0205, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11211.0166015625
tensor(11211.0205, grad_fn=<NegBackward0>) tensor(11211.0166, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11211.017578125
tensor(11211.0166, grad_fn=<NegBackward0>) tensor(11211.0176, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11211.0146484375
tensor(11211.0166, grad_fn=<NegBackward0>) tensor(11211.0146, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11210.798828125
tensor(11211.0146, grad_fn=<NegBackward0>) tensor(11210.7988, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11210.5615234375
tensor(11210.7988, grad_fn=<NegBackward0>) tensor(11210.5615, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11210.5556640625
tensor(11210.5615, grad_fn=<NegBackward0>) tensor(11210.5557, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11210.556640625
tensor(11210.5557, grad_fn=<NegBackward0>) tensor(11210.5566, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11210.556640625
tensor(11210.5557, grad_fn=<NegBackward0>) tensor(11210.5566, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11210.556640625
tensor(11210.5557, grad_fn=<NegBackward0>) tensor(11210.5566, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -11210.5625
tensor(11210.5557, grad_fn=<NegBackward0>) tensor(11210.5625, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -11210.5634765625
tensor(11210.5557, grad_fn=<NegBackward0>) tensor(11210.5635, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.4013, 0.5987],
        [0.5835, 0.4165]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5497, 0.4503], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2380, 0.1016],
         [0.5854, 0.2620]],

        [[0.6809, 0.0962],
         [0.5430, 0.5165]],

        [[0.6647, 0.1088],
         [0.5181, 0.6380]],

        [[0.5998, 0.0835],
         [0.7224, 0.6610]],

        [[0.6546, 0.0921],
         [0.6394, 0.5721]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.772151675588645
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.04649095574872773
Average Adjusted Rand Index: 0.8395994549615093
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24742.56640625
inf tensor(24742.5664, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11114.4248046875
tensor(24742.5664, grad_fn=<NegBackward0>) tensor(11114.4248, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11112.365234375
tensor(11114.4248, grad_fn=<NegBackward0>) tensor(11112.3652, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11112.193359375
tensor(11112.3652, grad_fn=<NegBackward0>) tensor(11112.1934, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11112.123046875
tensor(11112.1934, grad_fn=<NegBackward0>) tensor(11112.1230, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11112.087890625
tensor(11112.1230, grad_fn=<NegBackward0>) tensor(11112.0879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11112.0673828125
tensor(11112.0879, grad_fn=<NegBackward0>) tensor(11112.0674, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11112.0537109375
tensor(11112.0674, grad_fn=<NegBackward0>) tensor(11112.0537, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11112.044921875
tensor(11112.0537, grad_fn=<NegBackward0>) tensor(11112.0449, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11112.037109375
tensor(11112.0449, grad_fn=<NegBackward0>) tensor(11112.0371, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11112.033203125
tensor(11112.0371, grad_fn=<NegBackward0>) tensor(11112.0332, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11112.029296875
tensor(11112.0332, grad_fn=<NegBackward0>) tensor(11112.0293, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11112.025390625
tensor(11112.0293, grad_fn=<NegBackward0>) tensor(11112.0254, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11112.0234375
tensor(11112.0254, grad_fn=<NegBackward0>) tensor(11112.0234, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11112.0205078125
tensor(11112.0234, grad_fn=<NegBackward0>) tensor(11112.0205, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11112.0205078125
tensor(11112.0205, grad_fn=<NegBackward0>) tensor(11112.0205, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11112.0185546875
tensor(11112.0205, grad_fn=<NegBackward0>) tensor(11112.0186, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11112.0166015625
tensor(11112.0186, grad_fn=<NegBackward0>) tensor(11112.0166, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11112.015625
tensor(11112.0166, grad_fn=<NegBackward0>) tensor(11112.0156, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11112.0146484375
tensor(11112.0156, grad_fn=<NegBackward0>) tensor(11112.0146, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11112.013671875
tensor(11112.0146, grad_fn=<NegBackward0>) tensor(11112.0137, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11112.013671875
tensor(11112.0137, grad_fn=<NegBackward0>) tensor(11112.0137, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11112.0146484375
tensor(11112.0137, grad_fn=<NegBackward0>) tensor(11112.0146, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11112.015625
tensor(11112.0137, grad_fn=<NegBackward0>) tensor(11112.0156, grad_fn=<NegBackward0>)
2
Iteration 2400: Loss = -11112.015625
tensor(11112.0137, grad_fn=<NegBackward0>) tensor(11112.0156, grad_fn=<NegBackward0>)
3
Iteration 2500: Loss = -11112.01171875
tensor(11112.0137, grad_fn=<NegBackward0>) tensor(11112.0117, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11112.01171875
tensor(11112.0117, grad_fn=<NegBackward0>) tensor(11112.0117, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11112.01171875
tensor(11112.0117, grad_fn=<NegBackward0>) tensor(11112.0117, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11112.0166015625
tensor(11112.0117, grad_fn=<NegBackward0>) tensor(11112.0166, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11112.0107421875
tensor(11112.0117, grad_fn=<NegBackward0>) tensor(11112.0107, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11112.0126953125
tensor(11112.0107, grad_fn=<NegBackward0>) tensor(11112.0127, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11112.01171875
tensor(11112.0107, grad_fn=<NegBackward0>) tensor(11112.0117, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -11112.009765625
tensor(11112.0107, grad_fn=<NegBackward0>) tensor(11112.0098, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11112.009765625
tensor(11112.0098, grad_fn=<NegBackward0>) tensor(11112.0098, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11112.009765625
tensor(11112.0098, grad_fn=<NegBackward0>) tensor(11112.0098, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11112.009765625
tensor(11112.0098, grad_fn=<NegBackward0>) tensor(11112.0098, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11112.0087890625
tensor(11112.0098, grad_fn=<NegBackward0>) tensor(11112.0088, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11112.0087890625
tensor(11112.0088, grad_fn=<NegBackward0>) tensor(11112.0088, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11112.0126953125
tensor(11112.0088, grad_fn=<NegBackward0>) tensor(11112.0127, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11112.009765625
tensor(11112.0088, grad_fn=<NegBackward0>) tensor(11112.0098, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11112.0068359375
tensor(11112.0088, grad_fn=<NegBackward0>) tensor(11112.0068, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11111.98046875
tensor(11112.0068, grad_fn=<NegBackward0>) tensor(11111.9805, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11111.9794921875
tensor(11111.9805, grad_fn=<NegBackward0>) tensor(11111.9795, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11111.98046875
tensor(11111.9795, grad_fn=<NegBackward0>) tensor(11111.9805, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11111.982421875
tensor(11111.9795, grad_fn=<NegBackward0>) tensor(11111.9824, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11111.986328125
tensor(11111.9795, grad_fn=<NegBackward0>) tensor(11111.9863, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11111.978515625
tensor(11111.9795, grad_fn=<NegBackward0>) tensor(11111.9785, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11111.9765625
tensor(11111.9785, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11111.9765625
tensor(11111.9766, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11111.978515625
tensor(11111.9766, grad_fn=<NegBackward0>) tensor(11111.9785, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11111.9765625
tensor(11111.9766, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11111.9775390625
tensor(11111.9766, grad_fn=<NegBackward0>) tensor(11111.9775, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11111.9755859375
tensor(11111.9766, grad_fn=<NegBackward0>) tensor(11111.9756, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11111.9775390625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9775, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11111.9755859375
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9756, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11111.9775390625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9775, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11111.9794921875
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9795, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11111.978515625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9785, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11111.9775390625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9775, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -11111.9755859375
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9756, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11111.98046875
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9805, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11111.9755859375
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9756, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11111.978515625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9785, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11111.98046875
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9805, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11111.9814453125
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9814, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11111.9755859375
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9756, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11111.9775390625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9775, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11111.9765625
tensor(11111.9756, grad_fn=<NegBackward0>) tensor(11111.9766, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7294, 0.2706],
        [0.2633, 0.7367]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5202, 0.4798], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1954, 0.1023],
         [0.5420, 0.3001]],

        [[0.5621, 0.0972],
         [0.6592, 0.5983]],

        [[0.6663, 0.1119],
         [0.6810, 0.5347]],

        [[0.7162, 0.0872],
         [0.5182, 0.5306]],

        [[0.5216, 0.0929],
         [0.6880, 0.6678]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
Global Adjusted Rand Index: 0.9291541854495776
Average Adjusted Rand Index: 0.9292910372639677
[0.04649095574872773, 0.9291541854495776] [0.8395994549615093, 0.9292910372639677] [11210.5634765625, 11111.9765625]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11254.719458504685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25452.341796875
inf tensor(25452.3418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11494.2880859375
tensor(25452.3418, grad_fn=<NegBackward0>) tensor(11494.2881, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11489.751953125
tensor(11494.2881, grad_fn=<NegBackward0>) tensor(11489.7520, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11483.3525390625
tensor(11489.7520, grad_fn=<NegBackward0>) tensor(11483.3525, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11396.90625
tensor(11483.3525, grad_fn=<NegBackward0>) tensor(11396.9062, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11309.5146484375
tensor(11396.9062, grad_fn=<NegBackward0>) tensor(11309.5146, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11293.6025390625
tensor(11309.5146, grad_fn=<NegBackward0>) tensor(11293.6025, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11291.2890625
tensor(11293.6025, grad_fn=<NegBackward0>) tensor(11291.2891, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11286.3232421875
tensor(11291.2891, grad_fn=<NegBackward0>) tensor(11286.3232, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11281.05078125
tensor(11286.3232, grad_fn=<NegBackward0>) tensor(11281.0508, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11280.8515625
tensor(11281.0508, grad_fn=<NegBackward0>) tensor(11280.8516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11280.8193359375
tensor(11280.8516, grad_fn=<NegBackward0>) tensor(11280.8193, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11280.794921875
tensor(11280.8193, grad_fn=<NegBackward0>) tensor(11280.7949, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11280.7763671875
tensor(11280.7949, grad_fn=<NegBackward0>) tensor(11280.7764, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11280.76171875
tensor(11280.7764, grad_fn=<NegBackward0>) tensor(11280.7617, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11280.7509765625
tensor(11280.7617, grad_fn=<NegBackward0>) tensor(11280.7510, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11280.7412109375
tensor(11280.7510, grad_fn=<NegBackward0>) tensor(11280.7412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11280.7314453125
tensor(11280.7412, grad_fn=<NegBackward0>) tensor(11280.7314, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11280.7216796875
tensor(11280.7314, grad_fn=<NegBackward0>) tensor(11280.7217, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11280.712890625
tensor(11280.7217, grad_fn=<NegBackward0>) tensor(11280.7129, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11280.70703125
tensor(11280.7129, grad_fn=<NegBackward0>) tensor(11280.7070, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11280.6982421875
tensor(11280.7070, grad_fn=<NegBackward0>) tensor(11280.6982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11280.6787109375
tensor(11280.6982, grad_fn=<NegBackward0>) tensor(11280.6787, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11280.6650390625
tensor(11280.6787, grad_fn=<NegBackward0>) tensor(11280.6650, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11280.6611328125
tensor(11280.6650, grad_fn=<NegBackward0>) tensor(11280.6611, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11280.658203125
tensor(11280.6611, grad_fn=<NegBackward0>) tensor(11280.6582, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11280.6552734375
tensor(11280.6582, grad_fn=<NegBackward0>) tensor(11280.6553, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11280.650390625
tensor(11280.6553, grad_fn=<NegBackward0>) tensor(11280.6504, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11280.6220703125
tensor(11280.6504, grad_fn=<NegBackward0>) tensor(11280.6221, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11280.62109375
tensor(11280.6221, grad_fn=<NegBackward0>) tensor(11280.6211, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11280.619140625
tensor(11280.6211, grad_fn=<NegBackward0>) tensor(11280.6191, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11280.6171875
tensor(11280.6191, grad_fn=<NegBackward0>) tensor(11280.6172, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11280.6181640625
tensor(11280.6172, grad_fn=<NegBackward0>) tensor(11280.6182, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11280.6162109375
tensor(11280.6172, grad_fn=<NegBackward0>) tensor(11280.6162, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11280.615234375
tensor(11280.6162, grad_fn=<NegBackward0>) tensor(11280.6152, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11280.61328125
tensor(11280.6152, grad_fn=<NegBackward0>) tensor(11280.6133, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11280.6123046875
tensor(11280.6133, grad_fn=<NegBackward0>) tensor(11280.6123, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11280.611328125
tensor(11280.6123, grad_fn=<NegBackward0>) tensor(11280.6113, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11280.609375
tensor(11280.6113, grad_fn=<NegBackward0>) tensor(11280.6094, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11280.603515625
tensor(11280.6094, grad_fn=<NegBackward0>) tensor(11280.6035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11275.2705078125
tensor(11280.6035, grad_fn=<NegBackward0>) tensor(11275.2705, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11275.2666015625
tensor(11275.2705, grad_fn=<NegBackward0>) tensor(11275.2666, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11275.2666015625
tensor(11275.2666, grad_fn=<NegBackward0>) tensor(11275.2666, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11275.265625
tensor(11275.2666, grad_fn=<NegBackward0>) tensor(11275.2656, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11275.265625
tensor(11275.2656, grad_fn=<NegBackward0>) tensor(11275.2656, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11275.2646484375
tensor(11275.2656, grad_fn=<NegBackward0>) tensor(11275.2646, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11275.263671875
tensor(11275.2646, grad_fn=<NegBackward0>) tensor(11275.2637, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11275.263671875
tensor(11275.2637, grad_fn=<NegBackward0>) tensor(11275.2637, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11275.2646484375
tensor(11275.2637, grad_fn=<NegBackward0>) tensor(11275.2646, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11275.263671875
tensor(11275.2637, grad_fn=<NegBackward0>) tensor(11275.2637, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11275.263671875
tensor(11275.2637, grad_fn=<NegBackward0>) tensor(11275.2637, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11275.26171875
tensor(11275.2637, grad_fn=<NegBackward0>) tensor(11275.2617, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11275.26171875
tensor(11275.2617, grad_fn=<NegBackward0>) tensor(11275.2617, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11275.26171875
tensor(11275.2617, grad_fn=<NegBackward0>) tensor(11275.2617, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11275.2607421875
tensor(11275.2617, grad_fn=<NegBackward0>) tensor(11275.2607, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11275.26953125
tensor(11275.2607, grad_fn=<NegBackward0>) tensor(11275.2695, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11275.2607421875
tensor(11275.2607, grad_fn=<NegBackward0>) tensor(11275.2607, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11275.27734375
tensor(11275.2607, grad_fn=<NegBackward0>) tensor(11275.2773, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11275.2607421875
tensor(11275.2607, grad_fn=<NegBackward0>) tensor(11275.2607, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11275.259765625
tensor(11275.2607, grad_fn=<NegBackward0>) tensor(11275.2598, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11275.2587890625
tensor(11275.2598, grad_fn=<NegBackward0>) tensor(11275.2588, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11275.2587890625
tensor(11275.2588, grad_fn=<NegBackward0>) tensor(11275.2588, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11275.2578125
tensor(11275.2588, grad_fn=<NegBackward0>) tensor(11275.2578, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11275.2587890625
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2588, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11275.2578125
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2578, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11275.2587890625
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2588, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11275.2578125
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2578, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11275.2587890625
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2588, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11275.2568359375
tensor(11275.2578, grad_fn=<NegBackward0>) tensor(11275.2568, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11275.2568359375
tensor(11275.2568, grad_fn=<NegBackward0>) tensor(11275.2568, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11275.25390625
tensor(11275.2568, grad_fn=<NegBackward0>) tensor(11275.2539, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11275.2548828125
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2549, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11275.2548828125
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2549, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11275.25390625
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2539, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11275.25390625
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2539, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11275.25390625
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2539, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11275.24609375
tensor(11275.2539, grad_fn=<NegBackward0>) tensor(11275.2461, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11275.2470703125
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11275.2470703125
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11275.2470703125
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11275.24609375
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2461, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11275.2509765625
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2510, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11275.2470703125
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11275.2470703125
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11275.24609375
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2461, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11275.248046875
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2480, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11275.248046875
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2480, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11275.2509765625
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2510, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11275.25390625
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2539, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11275.24609375
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2461, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11275.2529296875
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2529, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11275.2412109375
tensor(11275.2461, grad_fn=<NegBackward0>) tensor(11275.2412, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11275.2607421875
tensor(11275.2412, grad_fn=<NegBackward0>) tensor(11275.2607, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11274.826171875
tensor(11275.2412, grad_fn=<NegBackward0>) tensor(11274.8262, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11274.7548828125
tensor(11274.8262, grad_fn=<NegBackward0>) tensor(11274.7549, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11274.724609375
tensor(11274.7549, grad_fn=<NegBackward0>) tensor(11274.7246, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11274.6884765625
tensor(11274.7246, grad_fn=<NegBackward0>) tensor(11274.6885, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11274.693359375
tensor(11274.6885, grad_fn=<NegBackward0>) tensor(11274.6934, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11274.5419921875
tensor(11274.6885, grad_fn=<NegBackward0>) tensor(11274.5420, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11274.541015625
tensor(11274.5420, grad_fn=<NegBackward0>) tensor(11274.5410, grad_fn=<NegBackward0>)
pi: tensor([[0.7608, 0.2392],
        [0.3161, 0.6839]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0363, 0.9637], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3044, 0.1412],
         [0.5821, 0.1904]],

        [[0.6856, 0.0988],
         [0.7288, 0.6448]],

        [[0.5001, 0.1013],
         [0.6937, 0.6574]],

        [[0.7285, 0.1084],
         [0.6892, 0.5561]],

        [[0.6627, 0.1005],
         [0.6874, 0.6233]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.6392572342368168
Average Adjusted Rand Index: 0.7291259496134388
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22326.052734375
inf tensor(22326.0527, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11494.8095703125
tensor(22326.0527, grad_fn=<NegBackward0>) tensor(11494.8096, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11492.5732421875
tensor(11494.8096, grad_fn=<NegBackward0>) tensor(11492.5732, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11490.431640625
tensor(11492.5732, grad_fn=<NegBackward0>) tensor(11490.4316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11489.1787109375
tensor(11490.4316, grad_fn=<NegBackward0>) tensor(11489.1787, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11487.89453125
tensor(11489.1787, grad_fn=<NegBackward0>) tensor(11487.8945, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11471.53125
tensor(11487.8945, grad_fn=<NegBackward0>) tensor(11471.5312, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11395.3896484375
tensor(11471.5312, grad_fn=<NegBackward0>) tensor(11395.3896, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11319.6396484375
tensor(11395.3896, grad_fn=<NegBackward0>) tensor(11319.6396, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11301.859375
tensor(11319.6396, grad_fn=<NegBackward0>) tensor(11301.8594, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11288.9345703125
tensor(11301.8594, grad_fn=<NegBackward0>) tensor(11288.9346, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11288.6416015625
tensor(11288.9346, grad_fn=<NegBackward0>) tensor(11288.6416, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11288.4541015625
tensor(11288.6416, grad_fn=<NegBackward0>) tensor(11288.4541, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11288.375
tensor(11288.4541, grad_fn=<NegBackward0>) tensor(11288.3750, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11288.2060546875
tensor(11288.3750, grad_fn=<NegBackward0>) tensor(11288.2061, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11284.130859375
tensor(11288.2061, grad_fn=<NegBackward0>) tensor(11284.1309, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11282.140625
tensor(11284.1309, grad_fn=<NegBackward0>) tensor(11282.1406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11282.0498046875
tensor(11282.1406, grad_fn=<NegBackward0>) tensor(11282.0498, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11281.994140625
tensor(11282.0498, grad_fn=<NegBackward0>) tensor(11281.9941, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11281.9736328125
tensor(11281.9941, grad_fn=<NegBackward0>) tensor(11281.9736, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11281.9599609375
tensor(11281.9736, grad_fn=<NegBackward0>) tensor(11281.9600, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11281.9501953125
tensor(11281.9600, grad_fn=<NegBackward0>) tensor(11281.9502, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11281.9404296875
tensor(11281.9502, grad_fn=<NegBackward0>) tensor(11281.9404, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11281.931640625
tensor(11281.9404, grad_fn=<NegBackward0>) tensor(11281.9316, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11281.9248046875
tensor(11281.9316, grad_fn=<NegBackward0>) tensor(11281.9248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11281.9189453125
tensor(11281.9248, grad_fn=<NegBackward0>) tensor(11281.9189, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11281.912109375
tensor(11281.9189, grad_fn=<NegBackward0>) tensor(11281.9121, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11281.90625
tensor(11281.9121, grad_fn=<NegBackward0>) tensor(11281.9062, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11281.900390625
tensor(11281.9062, grad_fn=<NegBackward0>) tensor(11281.9004, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11281.8935546875
tensor(11281.9004, grad_fn=<NegBackward0>) tensor(11281.8936, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11281.8828125
tensor(11281.8936, grad_fn=<NegBackward0>) tensor(11281.8828, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11281.873046875
tensor(11281.8828, grad_fn=<NegBackward0>) tensor(11281.8730, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11281.853515625
tensor(11281.8730, grad_fn=<NegBackward0>) tensor(11281.8535, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11281.81640625
tensor(11281.8535, grad_fn=<NegBackward0>) tensor(11281.8164, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11281.7392578125
tensor(11281.8164, grad_fn=<NegBackward0>) tensor(11281.7393, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11281.6337890625
tensor(11281.7393, grad_fn=<NegBackward0>) tensor(11281.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11281.52734375
tensor(11281.6338, grad_fn=<NegBackward0>) tensor(11281.5273, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11281.41015625
tensor(11281.5273, grad_fn=<NegBackward0>) tensor(11281.4102, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11281.3232421875
tensor(11281.4102, grad_fn=<NegBackward0>) tensor(11281.3232, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11281.2470703125
tensor(11281.3232, grad_fn=<NegBackward0>) tensor(11281.2471, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11281.103515625
tensor(11281.2471, grad_fn=<NegBackward0>) tensor(11281.1035, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11274.623046875
tensor(11281.1035, grad_fn=<NegBackward0>) tensor(11274.6230, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11248.537109375
tensor(11274.6230, grad_fn=<NegBackward0>) tensor(11248.5371, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11235.91796875
tensor(11248.5371, grad_fn=<NegBackward0>) tensor(11235.9180, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11230.2578125
tensor(11235.9180, grad_fn=<NegBackward0>) tensor(11230.2578, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11229.962890625
tensor(11230.2578, grad_fn=<NegBackward0>) tensor(11229.9629, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11224.5732421875
tensor(11229.9629, grad_fn=<NegBackward0>) tensor(11224.5732, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11224.5712890625
tensor(11224.5732, grad_fn=<NegBackward0>) tensor(11224.5713, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11224.564453125
tensor(11224.5713, grad_fn=<NegBackward0>) tensor(11224.5645, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11224.5634765625
tensor(11224.5645, grad_fn=<NegBackward0>) tensor(11224.5635, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11224.5625
tensor(11224.5635, grad_fn=<NegBackward0>) tensor(11224.5625, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11224.5615234375
tensor(11224.5625, grad_fn=<NegBackward0>) tensor(11224.5615, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11224.560546875
tensor(11224.5615, grad_fn=<NegBackward0>) tensor(11224.5605, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11224.564453125
tensor(11224.5605, grad_fn=<NegBackward0>) tensor(11224.5645, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11224.57421875
tensor(11224.5605, grad_fn=<NegBackward0>) tensor(11224.5742, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11224.228515625
tensor(11224.5605, grad_fn=<NegBackward0>) tensor(11224.2285, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11224.2255859375
tensor(11224.2285, grad_fn=<NegBackward0>) tensor(11224.2256, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11224.224609375
tensor(11224.2256, grad_fn=<NegBackward0>) tensor(11224.2246, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11224.2236328125
tensor(11224.2246, grad_fn=<NegBackward0>) tensor(11224.2236, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11224.228515625
tensor(11224.2236, grad_fn=<NegBackward0>) tensor(11224.2285, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11224.2275390625
tensor(11224.2236, grad_fn=<NegBackward0>) tensor(11224.2275, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11224.22265625
tensor(11224.2236, grad_fn=<NegBackward0>) tensor(11224.2227, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11224.22265625
tensor(11224.2227, grad_fn=<NegBackward0>) tensor(11224.2227, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11224.2216796875
tensor(11224.2227, grad_fn=<NegBackward0>) tensor(11224.2217, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11224.22265625
tensor(11224.2217, grad_fn=<NegBackward0>) tensor(11224.2227, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11224.2216796875
tensor(11224.2217, grad_fn=<NegBackward0>) tensor(11224.2217, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11224.2216796875
tensor(11224.2217, grad_fn=<NegBackward0>) tensor(11224.2217, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11224.2216796875
tensor(11224.2217, grad_fn=<NegBackward0>) tensor(11224.2217, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11224.2197265625
tensor(11224.2217, grad_fn=<NegBackward0>) tensor(11224.2197, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11224.220703125
tensor(11224.2197, grad_fn=<NegBackward0>) tensor(11224.2207, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11224.21875
tensor(11224.2197, grad_fn=<NegBackward0>) tensor(11224.2188, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11224.220703125
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2207, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11224.2197265625
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2197, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11224.2197265625
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2197, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11224.2216796875
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2217, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11224.21875
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2188, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11224.21875
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2188, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11224.21875
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2188, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11224.2177734375
tensor(11224.2188, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11224.283203125
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2832, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11224.2177734375
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11224.2607421875
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2607, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11224.2666015625
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2666, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11224.2177734375
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11224.2177734375
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11224.3271484375
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.3271, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11224.216796875
tensor(11224.2178, grad_fn=<NegBackward0>) tensor(11224.2168, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11224.287109375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2871, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11224.21875
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2188, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11224.2177734375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11224.3232421875
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.3232, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11224.216796875
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2168, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11224.2197265625
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2197, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11224.2177734375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11224.2255859375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2256, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11224.2177734375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11224.2177734375
tensor(11224.2168, grad_fn=<NegBackward0>) tensor(11224.2178, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.7437, 0.2563],
        [0.2474, 0.7526]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6077, 0.3923], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.1044],
         [0.5781, 0.3011]],

        [[0.6114, 0.0967],
         [0.7259, 0.6263]],

        [[0.7063, 0.1007],
         [0.5774, 0.6499]],

        [[0.5489, 0.1080],
         [0.6493, 0.5625]],

        [[0.6247, 0.1004],
         [0.6542, 0.5698]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.9291540965879624
Average Adjusted Rand Index: 0.9291131465440629
[0.6392572342368168, 0.9291540965879624] [0.7291259496134388, 0.9291131465440629] [11274.5224609375, 11224.2177734375]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11013.293615683191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23212.296875
inf tensor(23212.2969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11290.916015625
tensor(23212.2969, grad_fn=<NegBackward0>) tensor(11290.9160, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11289.9189453125
tensor(11290.9160, grad_fn=<NegBackward0>) tensor(11289.9189, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11287.4560546875
tensor(11289.9189, grad_fn=<NegBackward0>) tensor(11287.4561, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11284.9267578125
tensor(11287.4561, grad_fn=<NegBackward0>) tensor(11284.9268, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11283.1298828125
tensor(11284.9268, grad_fn=<NegBackward0>) tensor(11283.1299, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11232.0595703125
tensor(11283.1299, grad_fn=<NegBackward0>) tensor(11232.0596, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11227.662109375
tensor(11232.0596, grad_fn=<NegBackward0>) tensor(11227.6621, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11227.3330078125
tensor(11227.6621, grad_fn=<NegBackward0>) tensor(11227.3330, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11227.16796875
tensor(11227.3330, grad_fn=<NegBackward0>) tensor(11227.1680, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11227.0576171875
tensor(11227.1680, grad_fn=<NegBackward0>) tensor(11227.0576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11226.9716796875
tensor(11227.0576, grad_fn=<NegBackward0>) tensor(11226.9717, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11226.9033203125
tensor(11226.9717, grad_fn=<NegBackward0>) tensor(11226.9033, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11226.8447265625
tensor(11226.9033, grad_fn=<NegBackward0>) tensor(11226.8447, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11226.798828125
tensor(11226.8447, grad_fn=<NegBackward0>) tensor(11226.7988, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11226.7587890625
tensor(11226.7988, grad_fn=<NegBackward0>) tensor(11226.7588, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11226.7294921875
tensor(11226.7588, grad_fn=<NegBackward0>) tensor(11226.7295, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11226.708984375
tensor(11226.7295, grad_fn=<NegBackward0>) tensor(11226.7090, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11226.6796875
tensor(11226.7090, grad_fn=<NegBackward0>) tensor(11226.6797, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11226.662109375
tensor(11226.6797, grad_fn=<NegBackward0>) tensor(11226.6621, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11226.646484375
tensor(11226.6621, grad_fn=<NegBackward0>) tensor(11226.6465, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11226.6328125
tensor(11226.6465, grad_fn=<NegBackward0>) tensor(11226.6328, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11226.6220703125
tensor(11226.6328, grad_fn=<NegBackward0>) tensor(11226.6221, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11226.61328125
tensor(11226.6221, grad_fn=<NegBackward0>) tensor(11226.6133, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11226.6044921875
tensor(11226.6133, grad_fn=<NegBackward0>) tensor(11226.6045, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11226.59765625
tensor(11226.6045, grad_fn=<NegBackward0>) tensor(11226.5977, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11226.591796875
tensor(11226.5977, grad_fn=<NegBackward0>) tensor(11226.5918, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11226.5859375
tensor(11226.5918, grad_fn=<NegBackward0>) tensor(11226.5859, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11226.580078125
tensor(11226.5859, grad_fn=<NegBackward0>) tensor(11226.5801, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11226.5771484375
tensor(11226.5801, grad_fn=<NegBackward0>) tensor(11226.5771, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11226.572265625
tensor(11226.5771, grad_fn=<NegBackward0>) tensor(11226.5723, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11226.5693359375
tensor(11226.5723, grad_fn=<NegBackward0>) tensor(11226.5693, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11226.5673828125
tensor(11226.5693, grad_fn=<NegBackward0>) tensor(11226.5674, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11226.5625
tensor(11226.5674, grad_fn=<NegBackward0>) tensor(11226.5625, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11226.5625
tensor(11226.5625, grad_fn=<NegBackward0>) tensor(11226.5625, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11226.55859375
tensor(11226.5625, grad_fn=<NegBackward0>) tensor(11226.5586, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11226.55859375
tensor(11226.5586, grad_fn=<NegBackward0>) tensor(11226.5586, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11226.5546875
tensor(11226.5586, grad_fn=<NegBackward0>) tensor(11226.5547, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11226.5537109375
tensor(11226.5547, grad_fn=<NegBackward0>) tensor(11226.5537, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11226.5517578125
tensor(11226.5537, grad_fn=<NegBackward0>) tensor(11226.5518, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11226.5498046875
tensor(11226.5518, grad_fn=<NegBackward0>) tensor(11226.5498, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11226.548828125
tensor(11226.5498, grad_fn=<NegBackward0>) tensor(11226.5488, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11226.546875
tensor(11226.5488, grad_fn=<NegBackward0>) tensor(11226.5469, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11226.5458984375
tensor(11226.5469, grad_fn=<NegBackward0>) tensor(11226.5459, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11226.544921875
tensor(11226.5459, grad_fn=<NegBackward0>) tensor(11226.5449, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11226.5439453125
tensor(11226.5449, grad_fn=<NegBackward0>) tensor(11226.5439, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11226.54296875
tensor(11226.5439, grad_fn=<NegBackward0>) tensor(11226.5430, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11226.5419921875
tensor(11226.5430, grad_fn=<NegBackward0>) tensor(11226.5420, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11226.541015625
tensor(11226.5420, grad_fn=<NegBackward0>) tensor(11226.5410, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11226.541015625
tensor(11226.5410, grad_fn=<NegBackward0>) tensor(11226.5410, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11226.5458984375
tensor(11226.5410, grad_fn=<NegBackward0>) tensor(11226.5459, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11226.5390625
tensor(11226.5410, grad_fn=<NegBackward0>) tensor(11226.5391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11226.5380859375
tensor(11226.5391, grad_fn=<NegBackward0>) tensor(11226.5381, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11226.5380859375
tensor(11226.5381, grad_fn=<NegBackward0>) tensor(11226.5381, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11226.5361328125
tensor(11226.5381, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11226.5361328125
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11226.5361328125
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11226.5361328125
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11226.53515625
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11226.53515625
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11226.53515625
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11226.5341796875
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5342, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11226.533203125
tensor(11226.5342, grad_fn=<NegBackward0>) tensor(11226.5332, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11226.5322265625
tensor(11226.5332, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11226.5322265625
tensor(11226.5322, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11226.533203125
tensor(11226.5322, grad_fn=<NegBackward0>) tensor(11226.5332, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11226.533203125
tensor(11226.5322, grad_fn=<NegBackward0>) tensor(11226.5332, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11226.53125
tensor(11226.5322, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11226.5322265625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11226.5322265625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11226.5302734375
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11226.5302734375
tensor(11226.5303, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11226.529296875
tensor(11226.5303, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11226.5302734375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11226.53515625
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11226.5283203125
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11226.529296875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11226.5478515625
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5479, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11226.5283203125
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11226.576171875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5762, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11226.5283203125
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11226.5537109375
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5537, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11226.5283203125
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11226.578125
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5781, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11226.529296875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11226.529296875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11226.529296875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11226.5263671875
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5264, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11226.53125
tensor(11226.5264, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11226.52734375
tensor(11226.5264, grad_fn=<NegBackward0>) tensor(11226.5273, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11226.580078125
tensor(11226.5264, grad_fn=<NegBackward0>) tensor(11226.5801, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11226.52734375
tensor(11226.5264, grad_fn=<NegBackward0>) tensor(11226.5273, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11226.5341796875
tensor(11226.5264, grad_fn=<NegBackward0>) tensor(11226.5342, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[3.4736e-05, 9.9997e-01],
        [5.6604e-02, 9.4340e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4907, 0.5093], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3157, 0.0987],
         [0.5544, 0.1682]],

        [[0.6375, 0.1057],
         [0.6912, 0.5693]],

        [[0.5328, 0.1971],
         [0.6587, 0.7207]],

        [[0.7103, 0.2350],
         [0.5537, 0.6261]],

        [[0.6667, 0.1988],
         [0.6498, 0.6099]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0580592008870284
Average Adjusted Rand Index: 0.18414925904043658
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22244.302734375
inf tensor(22244.3027, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11291.3857421875
tensor(22244.3027, grad_fn=<NegBackward0>) tensor(11291.3857, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11290.2431640625
tensor(11291.3857, grad_fn=<NegBackward0>) tensor(11290.2432, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11287.23828125
tensor(11290.2432, grad_fn=<NegBackward0>) tensor(11287.2383, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11286.03125
tensor(11287.2383, grad_fn=<NegBackward0>) tensor(11286.0312, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11284.365234375
tensor(11286.0312, grad_fn=<NegBackward0>) tensor(11284.3652, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11230.6953125
tensor(11284.3652, grad_fn=<NegBackward0>) tensor(11230.6953, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11228.0517578125
tensor(11230.6953, grad_fn=<NegBackward0>) tensor(11228.0518, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11227.611328125
tensor(11228.0518, grad_fn=<NegBackward0>) tensor(11227.6113, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11227.33203125
tensor(11227.6113, grad_fn=<NegBackward0>) tensor(11227.3320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11227.162109375
tensor(11227.3320, grad_fn=<NegBackward0>) tensor(11227.1621, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11227.0478515625
tensor(11227.1621, grad_fn=<NegBackward0>) tensor(11227.0479, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11226.958984375
tensor(11227.0479, grad_fn=<NegBackward0>) tensor(11226.9590, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11226.87890625
tensor(11226.9590, grad_fn=<NegBackward0>) tensor(11226.8789, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11226.8173828125
tensor(11226.8789, grad_fn=<NegBackward0>) tensor(11226.8174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11226.7626953125
tensor(11226.8174, grad_fn=<NegBackward0>) tensor(11226.7627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11226.720703125
tensor(11226.7627, grad_fn=<NegBackward0>) tensor(11226.7207, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11226.6884765625
tensor(11226.7207, grad_fn=<NegBackward0>) tensor(11226.6885, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11226.6611328125
tensor(11226.6885, grad_fn=<NegBackward0>) tensor(11226.6611, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11226.6435546875
tensor(11226.6611, grad_fn=<NegBackward0>) tensor(11226.6436, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11226.62890625
tensor(11226.6436, grad_fn=<NegBackward0>) tensor(11226.6289, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11226.6162109375
tensor(11226.6289, grad_fn=<NegBackward0>) tensor(11226.6162, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11226.6044921875
tensor(11226.6162, grad_fn=<NegBackward0>) tensor(11226.6045, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11226.595703125
tensor(11226.6045, grad_fn=<NegBackward0>) tensor(11226.5957, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11226.5888671875
tensor(11226.5957, grad_fn=<NegBackward0>) tensor(11226.5889, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11226.5830078125
tensor(11226.5889, grad_fn=<NegBackward0>) tensor(11226.5830, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11226.578125
tensor(11226.5830, grad_fn=<NegBackward0>) tensor(11226.5781, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11226.5712890625
tensor(11226.5781, grad_fn=<NegBackward0>) tensor(11226.5713, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11226.5693359375
tensor(11226.5713, grad_fn=<NegBackward0>) tensor(11226.5693, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11226.564453125
tensor(11226.5693, grad_fn=<NegBackward0>) tensor(11226.5645, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11226.5615234375
tensor(11226.5645, grad_fn=<NegBackward0>) tensor(11226.5615, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11226.5595703125
tensor(11226.5615, grad_fn=<NegBackward0>) tensor(11226.5596, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11226.556640625
tensor(11226.5596, grad_fn=<NegBackward0>) tensor(11226.5566, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11226.5546875
tensor(11226.5566, grad_fn=<NegBackward0>) tensor(11226.5547, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11226.5517578125
tensor(11226.5547, grad_fn=<NegBackward0>) tensor(11226.5518, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11226.5498046875
tensor(11226.5518, grad_fn=<NegBackward0>) tensor(11226.5498, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11226.55078125
tensor(11226.5498, grad_fn=<NegBackward0>) tensor(11226.5508, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11226.5478515625
tensor(11226.5498, grad_fn=<NegBackward0>) tensor(11226.5479, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11226.546875
tensor(11226.5479, grad_fn=<NegBackward0>) tensor(11226.5469, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11226.5458984375
tensor(11226.5469, grad_fn=<NegBackward0>) tensor(11226.5459, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11226.544921875
tensor(11226.5459, grad_fn=<NegBackward0>) tensor(11226.5449, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11226.5419921875
tensor(11226.5449, grad_fn=<NegBackward0>) tensor(11226.5420, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11226.5419921875
tensor(11226.5420, grad_fn=<NegBackward0>) tensor(11226.5420, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11226.541015625
tensor(11226.5420, grad_fn=<NegBackward0>) tensor(11226.5410, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11226.5400390625
tensor(11226.5410, grad_fn=<NegBackward0>) tensor(11226.5400, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11226.5390625
tensor(11226.5400, grad_fn=<NegBackward0>) tensor(11226.5391, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11226.5390625
tensor(11226.5391, grad_fn=<NegBackward0>) tensor(11226.5391, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11226.5380859375
tensor(11226.5391, grad_fn=<NegBackward0>) tensor(11226.5381, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11226.5390625
tensor(11226.5381, grad_fn=<NegBackward0>) tensor(11226.5391, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11226.5361328125
tensor(11226.5381, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11226.537109375
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5371, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11226.5361328125
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5361, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11226.537109375
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5371, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11226.53515625
tensor(11226.5361, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11226.5390625
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5391, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11226.53515625
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11226.5341796875
tensor(11226.5352, grad_fn=<NegBackward0>) tensor(11226.5342, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11226.54296875
tensor(11226.5342, grad_fn=<NegBackward0>) tensor(11226.5430, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11226.533203125
tensor(11226.5342, grad_fn=<NegBackward0>) tensor(11226.5332, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11226.53125
tensor(11226.5332, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11226.5322265625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11226.53515625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5352, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11226.5322265625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11226.5322265625
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11226.53125
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5312, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11226.5302734375
tensor(11226.5312, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11226.529296875
tensor(11226.5303, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11226.5322265625
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5322, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11226.5302734375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11226.5302734375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11226.5302734375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11226.5302734375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11226.5693359375
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5693, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11226.529296875
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11226.5283203125
tensor(11226.5293, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11226.5302734375
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11226.5302734375
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5303, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11226.52734375
tensor(11226.5283, grad_fn=<NegBackward0>) tensor(11226.5273, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11226.6318359375
tensor(11226.5273, grad_fn=<NegBackward0>) tensor(11226.6318, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11226.5283203125
tensor(11226.5273, grad_fn=<NegBackward0>) tensor(11226.5283, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11226.58203125
tensor(11226.5273, grad_fn=<NegBackward0>) tensor(11226.5820, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11226.529296875
tensor(11226.5273, grad_fn=<NegBackward0>) tensor(11226.5293, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11226.8515625
tensor(11226.5273, grad_fn=<NegBackward0>) tensor(11226.8516, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[9.4178e-01, 5.8224e-02],
        [9.9996e-01, 3.7798e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5075, 0.4925], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1662, 0.0988],
         [0.5818, 0.3157]],

        [[0.6985, 0.1057],
         [0.5196, 0.6472]],

        [[0.5844, 0.1971],
         [0.6227, 0.6779]],

        [[0.5782, 0.2350],
         [0.7268, 0.5997]],

        [[0.5134, 0.1988],
         [0.6728, 0.5549]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.00485601903559462
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0580592008870284
Average Adjusted Rand Index: 0.18414925904043658
[0.0580592008870284, 0.0580592008870284] [0.18414925904043658, 0.18414925904043658] [11226.5341796875, 11226.8515625]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11111.872048540668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23918.080078125
inf tensor(23918.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11435.3486328125
tensor(23918.0801, grad_fn=<NegBackward0>) tensor(11435.3486, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11424.921875
tensor(11435.3486, grad_fn=<NegBackward0>) tensor(11424.9219, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11389.9189453125
tensor(11424.9219, grad_fn=<NegBackward0>) tensor(11389.9189, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11324.4150390625
tensor(11389.9189, grad_fn=<NegBackward0>) tensor(11324.4150, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11179.6708984375
tensor(11324.4150, grad_fn=<NegBackward0>) tensor(11179.6709, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11106.302734375
tensor(11179.6709, grad_fn=<NegBackward0>) tensor(11106.3027, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11104.5517578125
tensor(11106.3027, grad_fn=<NegBackward0>) tensor(11104.5518, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11098.771484375
tensor(11104.5518, grad_fn=<NegBackward0>) tensor(11098.7715, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11097.474609375
tensor(11098.7715, grad_fn=<NegBackward0>) tensor(11097.4746, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11096.7119140625
tensor(11097.4746, grad_fn=<NegBackward0>) tensor(11096.7119, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11091.2236328125
tensor(11096.7119, grad_fn=<NegBackward0>) tensor(11091.2236, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11091.1083984375
tensor(11091.2236, grad_fn=<NegBackward0>) tensor(11091.1084, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11091.0498046875
tensor(11091.1084, grad_fn=<NegBackward0>) tensor(11091.0498, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11091.0166015625
tensor(11091.0498, grad_fn=<NegBackward0>) tensor(11091.0166, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11090.984375
tensor(11091.0166, grad_fn=<NegBackward0>) tensor(11090.9844, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11090.919921875
tensor(11090.9844, grad_fn=<NegBackward0>) tensor(11090.9199, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11090.7822265625
tensor(11090.9199, grad_fn=<NegBackward0>) tensor(11090.7822, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11090.7646484375
tensor(11090.7822, grad_fn=<NegBackward0>) tensor(11090.7646, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11090.7529296875
tensor(11090.7646, grad_fn=<NegBackward0>) tensor(11090.7529, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11090.7431640625
tensor(11090.7529, grad_fn=<NegBackward0>) tensor(11090.7432, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11090.7333984375
tensor(11090.7432, grad_fn=<NegBackward0>) tensor(11090.7334, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11090.724609375
tensor(11090.7334, grad_fn=<NegBackward0>) tensor(11090.7246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11090.712890625
tensor(11090.7246, grad_fn=<NegBackward0>) tensor(11090.7129, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11090.6865234375
tensor(11090.7129, grad_fn=<NegBackward0>) tensor(11090.6865, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11090.654296875
tensor(11090.6865, grad_fn=<NegBackward0>) tensor(11090.6543, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11089.44921875
tensor(11090.6543, grad_fn=<NegBackward0>) tensor(11089.4492, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11089.43359375
tensor(11089.4492, grad_fn=<NegBackward0>) tensor(11089.4336, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11089.4306640625
tensor(11089.4336, grad_fn=<NegBackward0>) tensor(11089.4307, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11089.4267578125
tensor(11089.4307, grad_fn=<NegBackward0>) tensor(11089.4268, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11089.423828125
tensor(11089.4268, grad_fn=<NegBackward0>) tensor(11089.4238, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11089.419921875
tensor(11089.4238, grad_fn=<NegBackward0>) tensor(11089.4199, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11089.4169921875
tensor(11089.4199, grad_fn=<NegBackward0>) tensor(11089.4170, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11089.4111328125
tensor(11089.4170, grad_fn=<NegBackward0>) tensor(11089.4111, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11089.36328125
tensor(11089.4111, grad_fn=<NegBackward0>) tensor(11089.3633, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11089.3623046875
tensor(11089.3633, grad_fn=<NegBackward0>) tensor(11089.3623, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11089.359375
tensor(11089.3623, grad_fn=<NegBackward0>) tensor(11089.3594, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11089.357421875
tensor(11089.3594, grad_fn=<NegBackward0>) tensor(11089.3574, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11089.3544921875
tensor(11089.3574, grad_fn=<NegBackward0>) tensor(11089.3545, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11089.3505859375
tensor(11089.3545, grad_fn=<NegBackward0>) tensor(11089.3506, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11089.34765625
tensor(11089.3506, grad_fn=<NegBackward0>) tensor(11089.3477, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11089.345703125
tensor(11089.3477, grad_fn=<NegBackward0>) tensor(11089.3457, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11089.33984375
tensor(11089.3457, grad_fn=<NegBackward0>) tensor(11089.3398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11089.326171875
tensor(11089.3398, grad_fn=<NegBackward0>) tensor(11089.3262, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11089.3115234375
tensor(11089.3262, grad_fn=<NegBackward0>) tensor(11089.3115, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11089.3115234375
tensor(11089.3115, grad_fn=<NegBackward0>) tensor(11089.3115, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11089.3095703125
tensor(11089.3115, grad_fn=<NegBackward0>) tensor(11089.3096, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11089.30859375
tensor(11089.3096, grad_fn=<NegBackward0>) tensor(11089.3086, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11089.3095703125
tensor(11089.3086, grad_fn=<NegBackward0>) tensor(11089.3096, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11089.3076171875
tensor(11089.3086, grad_fn=<NegBackward0>) tensor(11089.3076, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11089.306640625
tensor(11089.3076, grad_fn=<NegBackward0>) tensor(11089.3066, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11089.3017578125
tensor(11089.3066, grad_fn=<NegBackward0>) tensor(11089.3018, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11089.29296875
tensor(11089.3018, grad_fn=<NegBackward0>) tensor(11089.2930, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11089.2900390625
tensor(11089.2930, grad_fn=<NegBackward0>) tensor(11089.2900, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11089.2890625
tensor(11089.2900, grad_fn=<NegBackward0>) tensor(11089.2891, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11089.2880859375
tensor(11089.2891, grad_fn=<NegBackward0>) tensor(11089.2881, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11089.287109375
tensor(11089.2881, grad_fn=<NegBackward0>) tensor(11089.2871, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11089.2880859375
tensor(11089.2871, grad_fn=<NegBackward0>) tensor(11089.2881, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11089.2861328125
tensor(11089.2871, grad_fn=<NegBackward0>) tensor(11089.2861, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11089.2861328125
tensor(11089.2861, grad_fn=<NegBackward0>) tensor(11089.2861, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11089.2861328125
tensor(11089.2861, grad_fn=<NegBackward0>) tensor(11089.2861, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11089.28515625
tensor(11089.2861, grad_fn=<NegBackward0>) tensor(11089.2852, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11089.28515625
tensor(11089.2852, grad_fn=<NegBackward0>) tensor(11089.2852, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11089.2880859375
tensor(11089.2852, grad_fn=<NegBackward0>) tensor(11089.2881, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11089.283203125
tensor(11089.2852, grad_fn=<NegBackward0>) tensor(11089.2832, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11089.283203125
tensor(11089.2832, grad_fn=<NegBackward0>) tensor(11089.2832, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11089.3037109375
tensor(11089.2832, grad_fn=<NegBackward0>) tensor(11089.3037, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11089.2724609375
tensor(11089.2832, grad_fn=<NegBackward0>) tensor(11089.2725, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11089.271484375
tensor(11089.2725, grad_fn=<NegBackward0>) tensor(11089.2715, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11089.2666015625
tensor(11089.2715, grad_fn=<NegBackward0>) tensor(11089.2666, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11089.197265625
tensor(11089.2666, grad_fn=<NegBackward0>) tensor(11089.1973, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11089.19140625
tensor(11089.1973, grad_fn=<NegBackward0>) tensor(11089.1914, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11089.1904296875
tensor(11089.1914, grad_fn=<NegBackward0>) tensor(11089.1904, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11089.19140625
tensor(11089.1904, grad_fn=<NegBackward0>) tensor(11089.1914, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11089.1904296875
tensor(11089.1904, grad_fn=<NegBackward0>) tensor(11089.1904, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11089.19140625
tensor(11089.1904, grad_fn=<NegBackward0>) tensor(11089.1914, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11089.189453125
tensor(11089.1904, grad_fn=<NegBackward0>) tensor(11089.1895, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11089.197265625
tensor(11089.1895, grad_fn=<NegBackward0>) tensor(11089.1973, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11089.18359375
tensor(11089.1895, grad_fn=<NegBackward0>) tensor(11089.1836, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11089.18359375
tensor(11089.1836, grad_fn=<NegBackward0>) tensor(11089.1836, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11089.18359375
tensor(11089.1836, grad_fn=<NegBackward0>) tensor(11089.1836, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11089.1826171875
tensor(11089.1836, grad_fn=<NegBackward0>) tensor(11089.1826, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11089.185546875
tensor(11089.1826, grad_fn=<NegBackward0>) tensor(11089.1855, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11089.1826171875
tensor(11089.1826, grad_fn=<NegBackward0>) tensor(11089.1826, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11089.1826171875
tensor(11089.1826, grad_fn=<NegBackward0>) tensor(11089.1826, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11089.181640625
tensor(11089.1826, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11089.181640625
tensor(11089.1816, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11089.1806640625
tensor(11089.1816, grad_fn=<NegBackward0>) tensor(11089.1807, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11089.181640625
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11089.181640625
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11089.181640625
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11089.1845703125
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1846, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11089.1806640625
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1807, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11089.1796875
tensor(11089.1807, grad_fn=<NegBackward0>) tensor(11089.1797, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11089.18359375
tensor(11089.1797, grad_fn=<NegBackward0>) tensor(11089.1836, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11089.181640625
tensor(11089.1797, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11089.1787109375
tensor(11089.1797, grad_fn=<NegBackward0>) tensor(11089.1787, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11089.181640625
tensor(11089.1787, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11089.181640625
tensor(11089.1787, grad_fn=<NegBackward0>) tensor(11089.1816, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11089.19140625
tensor(11089.1787, grad_fn=<NegBackward0>) tensor(11089.1914, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7347, 0.2653],
        [0.1904, 0.8096]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4782, 0.5218], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3189, 0.1016],
         [0.6514, 0.1995]],

        [[0.5667, 0.1031],
         [0.6137, 0.5731]],

        [[0.6242, 0.0980],
         [0.7089, 0.7287]],

        [[0.5866, 0.0949],
         [0.5631, 0.6439]],

        [[0.5758, 0.0939],
         [0.5073, 0.6053]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
Global Adjusted Rand Index: 0.9603191620531613
Average Adjusted Rand Index: 0.959976680115967
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21523.619140625
inf tensor(21523.6191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11432.943359375
tensor(21523.6191, grad_fn=<NegBackward0>) tensor(11432.9434, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11424.44140625
tensor(11432.9434, grad_fn=<NegBackward0>) tensor(11424.4414, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11375.767578125
tensor(11424.4414, grad_fn=<NegBackward0>) tensor(11375.7676, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11139.3310546875
tensor(11375.7676, grad_fn=<NegBackward0>) tensor(11139.3311, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11109.986328125
tensor(11139.3311, grad_fn=<NegBackward0>) tensor(11109.9863, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11095.103515625
tensor(11109.9863, grad_fn=<NegBackward0>) tensor(11095.1035, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11092.78515625
tensor(11095.1035, grad_fn=<NegBackward0>) tensor(11092.7852, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11090.783203125
tensor(11092.7852, grad_fn=<NegBackward0>) tensor(11090.7832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11090.71875
tensor(11090.7832, grad_fn=<NegBackward0>) tensor(11090.7188, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11090.685546875
tensor(11090.7188, grad_fn=<NegBackward0>) tensor(11090.6855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11090.6416015625
tensor(11090.6855, grad_fn=<NegBackward0>) tensor(11090.6416, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11090.626953125
tensor(11090.6416, grad_fn=<NegBackward0>) tensor(11090.6270, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11090.6162109375
tensor(11090.6270, grad_fn=<NegBackward0>) tensor(11090.6162, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11090.60546875
tensor(11090.6162, grad_fn=<NegBackward0>) tensor(11090.6055, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11090.5947265625
tensor(11090.6055, grad_fn=<NegBackward0>) tensor(11090.5947, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11090.58984375
tensor(11090.5947, grad_fn=<NegBackward0>) tensor(11090.5898, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11090.5869140625
tensor(11090.5898, grad_fn=<NegBackward0>) tensor(11090.5869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11090.5810546875
tensor(11090.5869, grad_fn=<NegBackward0>) tensor(11090.5811, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11090.5751953125
tensor(11090.5811, grad_fn=<NegBackward0>) tensor(11090.5752, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11090.529296875
tensor(11090.5752, grad_fn=<NegBackward0>) tensor(11090.5293, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11090.4912109375
tensor(11090.5293, grad_fn=<NegBackward0>) tensor(11090.4912, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11090.4716796875
tensor(11090.4912, grad_fn=<NegBackward0>) tensor(11090.4717, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11089.2705078125
tensor(11090.4717, grad_fn=<NegBackward0>) tensor(11089.2705, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11089.26171875
tensor(11089.2705, grad_fn=<NegBackward0>) tensor(11089.2617, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11089.2607421875
tensor(11089.2617, grad_fn=<NegBackward0>) tensor(11089.2607, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11089.2587890625
tensor(11089.2607, grad_fn=<NegBackward0>) tensor(11089.2588, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11089.2685546875
tensor(11089.2588, grad_fn=<NegBackward0>) tensor(11089.2686, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11089.25
tensor(11089.2588, grad_fn=<NegBackward0>) tensor(11089.2500, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11089.2490234375
tensor(11089.2500, grad_fn=<NegBackward0>) tensor(11089.2490, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11089.2490234375
tensor(11089.2490, grad_fn=<NegBackward0>) tensor(11089.2490, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11089.248046875
tensor(11089.2490, grad_fn=<NegBackward0>) tensor(11089.2480, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11089.248046875
tensor(11089.2480, grad_fn=<NegBackward0>) tensor(11089.2480, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11089.2509765625
tensor(11089.2480, grad_fn=<NegBackward0>) tensor(11089.2510, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11089.24609375
tensor(11089.2480, grad_fn=<NegBackward0>) tensor(11089.2461, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11089.244140625
tensor(11089.2461, grad_fn=<NegBackward0>) tensor(11089.2441, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11089.240234375
tensor(11089.2441, grad_fn=<NegBackward0>) tensor(11089.2402, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11089.236328125
tensor(11089.2402, grad_fn=<NegBackward0>) tensor(11089.2363, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11089.224609375
tensor(11089.2363, grad_fn=<NegBackward0>) tensor(11089.2246, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11089.2236328125
tensor(11089.2246, grad_fn=<NegBackward0>) tensor(11089.2236, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11089.2158203125
tensor(11089.2236, grad_fn=<NegBackward0>) tensor(11089.2158, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11089.2119140625
tensor(11089.2158, grad_fn=<NegBackward0>) tensor(11089.2119, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11089.2119140625
tensor(11089.2119, grad_fn=<NegBackward0>) tensor(11089.2119, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11089.21484375
tensor(11089.2119, grad_fn=<NegBackward0>) tensor(11089.2148, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11089.2099609375
tensor(11089.2119, grad_fn=<NegBackward0>) tensor(11089.2100, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11089.2109375
tensor(11089.2100, grad_fn=<NegBackward0>) tensor(11089.2109, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11089.2177734375
tensor(11089.2100, grad_fn=<NegBackward0>) tensor(11089.2178, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11089.208984375
tensor(11089.2100, grad_fn=<NegBackward0>) tensor(11089.2090, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11089.208984375
tensor(11089.2090, grad_fn=<NegBackward0>) tensor(11089.2090, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11089.2119140625
tensor(11089.2090, grad_fn=<NegBackward0>) tensor(11089.2119, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11089.208984375
tensor(11089.2090, grad_fn=<NegBackward0>) tensor(11089.2090, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11089.208984375
tensor(11089.2090, grad_fn=<NegBackward0>) tensor(11089.2090, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11089.20703125
tensor(11089.2090, grad_fn=<NegBackward0>) tensor(11089.2070, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11089.20703125
tensor(11089.2070, grad_fn=<NegBackward0>) tensor(11089.2070, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11089.2060546875
tensor(11089.2070, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11089.2060546875
tensor(11089.2061, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11089.2060546875
tensor(11089.2061, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11089.205078125
tensor(11089.2061, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11089.2060546875
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11089.21484375
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2148, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11089.2060546875
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11089.212890625
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2129, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11089.205078125
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11089.2041015625
tensor(11089.2051, grad_fn=<NegBackward0>) tensor(11089.2041, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11089.2041015625
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2041, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11089.2041015625
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2041, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11089.2041015625
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2041, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11089.2080078125
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2080, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11089.205078125
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2051, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11089.25
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2500, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11089.2060546875
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11089.2060546875
tensor(11089.2041, grad_fn=<NegBackward0>) tensor(11089.2061, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.8096, 0.1904],
        [0.2648, 0.7352]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5212, 0.4788], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.1015],
         [0.5657, 0.3187]],

        [[0.5003, 0.1031],
         [0.6631, 0.5612]],

        [[0.6418, 0.0980],
         [0.7176, 0.6346]],

        [[0.6833, 0.0949],
         [0.7253, 0.6472]],

        [[0.5854, 0.0939],
         [0.7258, 0.6362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
Global Adjusted Rand Index: 0.9603191620531613
Average Adjusted Rand Index: 0.959976680115967
[0.9603191620531613, 0.9603191620531613] [0.959976680115967, 0.959976680115967] [11089.1787109375, 11089.2060546875]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11316.368285957491
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22970.025390625
inf tensor(22970.0254, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11619.3564453125
tensor(22970.0254, grad_fn=<NegBackward0>) tensor(11619.3564, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11616.4423828125
tensor(11619.3564, grad_fn=<NegBackward0>) tensor(11616.4424, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11606.9736328125
tensor(11616.4424, grad_fn=<NegBackward0>) tensor(11606.9736, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11605.6142578125
tensor(11606.9736, grad_fn=<NegBackward0>) tensor(11605.6143, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11605.2001953125
tensor(11605.6143, grad_fn=<NegBackward0>) tensor(11605.2002, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11605.095703125
tensor(11605.2002, grad_fn=<NegBackward0>) tensor(11605.0957, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11605.060546875
tensor(11605.0957, grad_fn=<NegBackward0>) tensor(11605.0605, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11605.048828125
tensor(11605.0605, grad_fn=<NegBackward0>) tensor(11605.0488, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11605.0390625
tensor(11605.0488, grad_fn=<NegBackward0>) tensor(11605.0391, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11605.0361328125
tensor(11605.0391, grad_fn=<NegBackward0>) tensor(11605.0361, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11605.0322265625
tensor(11605.0361, grad_fn=<NegBackward0>) tensor(11605.0322, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11605.029296875
tensor(11605.0322, grad_fn=<NegBackward0>) tensor(11605.0293, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11605.0263671875
tensor(11605.0293, grad_fn=<NegBackward0>) tensor(11605.0264, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11605.0224609375
tensor(11605.0264, grad_fn=<NegBackward0>) tensor(11605.0225, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11605.01953125
tensor(11605.0225, grad_fn=<NegBackward0>) tensor(11605.0195, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11605.0166015625
tensor(11605.0195, grad_fn=<NegBackward0>) tensor(11605.0166, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11605.0107421875
tensor(11605.0166, grad_fn=<NegBackward0>) tensor(11605.0107, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11604.9990234375
tensor(11605.0107, grad_fn=<NegBackward0>) tensor(11604.9990, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11604.9658203125
tensor(11604.9990, grad_fn=<NegBackward0>) tensor(11604.9658, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11603.994140625
tensor(11604.9658, grad_fn=<NegBackward0>) tensor(11603.9941, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11372.03515625
tensor(11603.9941, grad_fn=<NegBackward0>) tensor(11372.0352, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11341.9267578125
tensor(11372.0352, grad_fn=<NegBackward0>) tensor(11341.9268, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11339.060546875
tensor(11341.9268, grad_fn=<NegBackward0>) tensor(11339.0605, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11331.6396484375
tensor(11339.0605, grad_fn=<NegBackward0>) tensor(11331.6396, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11324.1640625
tensor(11331.6396, grad_fn=<NegBackward0>) tensor(11324.1641, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11317.712890625
tensor(11324.1641, grad_fn=<NegBackward0>) tensor(11317.7129, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11317.703125
tensor(11317.7129, grad_fn=<NegBackward0>) tensor(11317.7031, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11313.060546875
tensor(11317.7031, grad_fn=<NegBackward0>) tensor(11313.0605, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11313.0546875
tensor(11313.0605, grad_fn=<NegBackward0>) tensor(11313.0547, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11309.685546875
tensor(11313.0547, grad_fn=<NegBackward0>) tensor(11309.6855, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11306.2177734375
tensor(11309.6855, grad_fn=<NegBackward0>) tensor(11306.2178, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11306.2099609375
tensor(11306.2178, grad_fn=<NegBackward0>) tensor(11306.2100, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11302.1025390625
tensor(11306.2100, grad_fn=<NegBackward0>) tensor(11302.1025, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11302.107421875
tensor(11302.1025, grad_fn=<NegBackward0>) tensor(11302.1074, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11302.0947265625
tensor(11302.1025, grad_fn=<NegBackward0>) tensor(11302.0947, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11301.9541015625
tensor(11302.0947, grad_fn=<NegBackward0>) tensor(11301.9541, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11294.130859375
tensor(11301.9541, grad_fn=<NegBackward0>) tensor(11294.1309, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11294.1025390625
tensor(11294.1309, grad_fn=<NegBackward0>) tensor(11294.1025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11294.115234375
tensor(11294.1025, grad_fn=<NegBackward0>) tensor(11294.1152, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11294.09765625
tensor(11294.1025, grad_fn=<NegBackward0>) tensor(11294.0977, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11294.099609375
tensor(11294.0977, grad_fn=<NegBackward0>) tensor(11294.0996, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11294.0966796875
tensor(11294.0977, grad_fn=<NegBackward0>) tensor(11294.0967, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11294.09765625
tensor(11294.0967, grad_fn=<NegBackward0>) tensor(11294.0977, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11294.09765625
tensor(11294.0967, grad_fn=<NegBackward0>) tensor(11294.0977, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11294.09375
tensor(11294.0967, grad_fn=<NegBackward0>) tensor(11294.0938, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11294.08984375
tensor(11294.0938, grad_fn=<NegBackward0>) tensor(11294.0898, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11294.083984375
tensor(11294.0898, grad_fn=<NegBackward0>) tensor(11294.0840, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11294.0810546875
tensor(11294.0840, grad_fn=<NegBackward0>) tensor(11294.0811, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11294.0810546875
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0811, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11294.08203125
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0820, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11294.0810546875
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0811, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11294.0810546875
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0811, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11294.08203125
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0820, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11294.08203125
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0820, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11294.080078125
tensor(11294.0811, grad_fn=<NegBackward0>) tensor(11294.0801, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11294.080078125
tensor(11294.0801, grad_fn=<NegBackward0>) tensor(11294.0801, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11294.056640625
tensor(11294.0801, grad_fn=<NegBackward0>) tensor(11294.0566, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11294.0546875
tensor(11294.0566, grad_fn=<NegBackward0>) tensor(11294.0547, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11294.0546875
tensor(11294.0547, grad_fn=<NegBackward0>) tensor(11294.0547, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11294.0546875
tensor(11294.0547, grad_fn=<NegBackward0>) tensor(11294.0547, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11294.0537109375
tensor(11294.0547, grad_fn=<NegBackward0>) tensor(11294.0537, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11294.0546875
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0547, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11294.0576171875
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0576, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11294.0546875
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0547, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11294.0537109375
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0537, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11294.0537109375
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0537, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11294.0537109375
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0537, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11294.048828125
tensor(11294.0537, grad_fn=<NegBackward0>) tensor(11294.0488, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11294.0390625
tensor(11294.0488, grad_fn=<NegBackward0>) tensor(11294.0391, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11294.0302734375
tensor(11294.0391, grad_fn=<NegBackward0>) tensor(11294.0303, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11294.029296875
tensor(11294.0303, grad_fn=<NegBackward0>) tensor(11294.0293, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11294.029296875
tensor(11294.0293, grad_fn=<NegBackward0>) tensor(11294.0293, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11294.02734375
tensor(11294.0293, grad_fn=<NegBackward0>) tensor(11294.0273, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11294.0322265625
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0322, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11294.03125
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0312, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11294.0458984375
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0459, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11294.02734375
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0273, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11294.0283203125
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0283, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11294.02734375
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0273, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11294.0380859375
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0381, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11294.0263671875
tensor(11294.0273, grad_fn=<NegBackward0>) tensor(11294.0264, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11294.02734375
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0273, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11294.0263671875
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0264, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11294.03125
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0312, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11294.0263671875
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0264, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11294.052734375
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0527, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11294.02734375
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0273, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11294.025390625
tensor(11294.0264, grad_fn=<NegBackward0>) tensor(11294.0254, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11294.0283203125
tensor(11294.0254, grad_fn=<NegBackward0>) tensor(11294.0283, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11294.0390625
tensor(11294.0254, grad_fn=<NegBackward0>) tensor(11294.0391, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11294.0224609375
tensor(11294.0254, grad_fn=<NegBackward0>) tensor(11294.0225, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11294.0234375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0234, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11294.0234375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0234, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11294.0224609375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0225, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11294.205078125
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.2051, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11294.0234375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0234, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11294.0341796875
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0342, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11294.0234375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0234, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11294.068359375
tensor(11294.0225, grad_fn=<NegBackward0>) tensor(11294.0684, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7909, 0.2091],
        [0.2057, 0.7943]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5201, 0.4799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3030, 0.1004],
         [0.5472, 0.2042]],

        [[0.7262, 0.1009],
         [0.5011, 0.6471]],

        [[0.5970, 0.1035],
         [0.7270, 0.5617]],

        [[0.5044, 0.1078],
         [0.6670, 0.5565]],

        [[0.6663, 0.1026],
         [0.6819, 0.6139]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.9368977075382423
Average Adjusted Rand Index: 0.9371267812460605
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22357.923828125
inf tensor(22357.9238, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11621.04296875
tensor(22357.9238, grad_fn=<NegBackward0>) tensor(11621.0430, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11616.8359375
tensor(11621.0430, grad_fn=<NegBackward0>) tensor(11616.8359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11612.7275390625
tensor(11616.8359, grad_fn=<NegBackward0>) tensor(11612.7275, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11606.05859375
tensor(11612.7275, grad_fn=<NegBackward0>) tensor(11606.0586, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11522.072265625
tensor(11606.0586, grad_fn=<NegBackward0>) tensor(11522.0723, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11435.1884765625
tensor(11522.0723, grad_fn=<NegBackward0>) tensor(11435.1885, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11415.8115234375
tensor(11435.1885, grad_fn=<NegBackward0>) tensor(11415.8115, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11415.013671875
tensor(11415.8115, grad_fn=<NegBackward0>) tensor(11415.0137, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11413.9873046875
tensor(11415.0137, grad_fn=<NegBackward0>) tensor(11413.9873, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11406.1044921875
tensor(11413.9873, grad_fn=<NegBackward0>) tensor(11406.1045, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11400.3603515625
tensor(11406.1045, grad_fn=<NegBackward0>) tensor(11400.3604, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11400.2666015625
tensor(11400.3604, grad_fn=<NegBackward0>) tensor(11400.2666, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11390.404296875
tensor(11400.2666, grad_fn=<NegBackward0>) tensor(11390.4043, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11390.30078125
tensor(11390.4043, grad_fn=<NegBackward0>) tensor(11390.3008, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11390.2626953125
tensor(11390.3008, grad_fn=<NegBackward0>) tensor(11390.2627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11390.2431640625
tensor(11390.2627, grad_fn=<NegBackward0>) tensor(11390.2432, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11390.2265625
tensor(11390.2432, grad_fn=<NegBackward0>) tensor(11390.2266, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11390.212890625
tensor(11390.2266, grad_fn=<NegBackward0>) tensor(11390.2129, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11390.201171875
tensor(11390.2129, grad_fn=<NegBackward0>) tensor(11390.2012, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11390.17578125
tensor(11390.2012, grad_fn=<NegBackward0>) tensor(11390.1758, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11390.1474609375
tensor(11390.1758, grad_fn=<NegBackward0>) tensor(11390.1475, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11390.13671875
tensor(11390.1475, grad_fn=<NegBackward0>) tensor(11390.1367, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11389.208984375
tensor(11390.1367, grad_fn=<NegBackward0>) tensor(11389.2090, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11389.037109375
tensor(11389.2090, grad_fn=<NegBackward0>) tensor(11389.0371, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11389.013671875
tensor(11389.0371, grad_fn=<NegBackward0>) tensor(11389.0137, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11389.0078125
tensor(11389.0137, grad_fn=<NegBackward0>) tensor(11389.0078, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11389.001953125
tensor(11389.0078, grad_fn=<NegBackward0>) tensor(11389.0020, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11388.998046875
tensor(11389.0020, grad_fn=<NegBackward0>) tensor(11388.9980, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11388.9970703125
tensor(11388.9980, grad_fn=<NegBackward0>) tensor(11388.9971, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11388.9921875
tensor(11388.9971, grad_fn=<NegBackward0>) tensor(11388.9922, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11388.9921875
tensor(11388.9922, grad_fn=<NegBackward0>) tensor(11388.9922, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11388.9892578125
tensor(11388.9922, grad_fn=<NegBackward0>) tensor(11388.9893, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11388.986328125
tensor(11388.9893, grad_fn=<NegBackward0>) tensor(11388.9863, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11388.9912109375
tensor(11388.9863, grad_fn=<NegBackward0>) tensor(11388.9912, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11383.4208984375
tensor(11388.9863, grad_fn=<NegBackward0>) tensor(11383.4209, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11383.392578125
tensor(11383.4209, grad_fn=<NegBackward0>) tensor(11383.3926, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11383.390625
tensor(11383.3926, grad_fn=<NegBackward0>) tensor(11383.3906, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11383.3896484375
tensor(11383.3906, grad_fn=<NegBackward0>) tensor(11383.3896, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11383.3876953125
tensor(11383.3896, grad_fn=<NegBackward0>) tensor(11383.3877, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11383.38671875
tensor(11383.3877, grad_fn=<NegBackward0>) tensor(11383.3867, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11383.3828125
tensor(11383.3867, grad_fn=<NegBackward0>) tensor(11383.3828, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11383.375
tensor(11383.3828, grad_fn=<NegBackward0>) tensor(11383.3750, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11383.3701171875
tensor(11383.3750, grad_fn=<NegBackward0>) tensor(11383.3701, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11383.3681640625
tensor(11383.3701, grad_fn=<NegBackward0>) tensor(11383.3682, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11383.3681640625
tensor(11383.3682, grad_fn=<NegBackward0>) tensor(11383.3682, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11383.3671875
tensor(11383.3682, grad_fn=<NegBackward0>) tensor(11383.3672, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11383.3662109375
tensor(11383.3672, grad_fn=<NegBackward0>) tensor(11383.3662, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11383.365234375
tensor(11383.3662, grad_fn=<NegBackward0>) tensor(11383.3652, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11383.3642578125
tensor(11383.3652, grad_fn=<NegBackward0>) tensor(11383.3643, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11383.3642578125
tensor(11383.3643, grad_fn=<NegBackward0>) tensor(11383.3643, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11383.36328125
tensor(11383.3643, grad_fn=<NegBackward0>) tensor(11383.3633, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11383.3623046875
tensor(11383.3633, grad_fn=<NegBackward0>) tensor(11383.3623, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11383.05859375
tensor(11383.3623, grad_fn=<NegBackward0>) tensor(11383.0586, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11383.0419921875
tensor(11383.0586, grad_fn=<NegBackward0>) tensor(11383.0420, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11383.05859375
tensor(11383.0420, grad_fn=<NegBackward0>) tensor(11383.0586, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11383.041015625
tensor(11383.0420, grad_fn=<NegBackward0>) tensor(11383.0410, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11383.041015625
tensor(11383.0410, grad_fn=<NegBackward0>) tensor(11383.0410, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11383.041015625
tensor(11383.0410, grad_fn=<NegBackward0>) tensor(11383.0410, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11383.0400390625
tensor(11383.0410, grad_fn=<NegBackward0>) tensor(11383.0400, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11383.0400390625
tensor(11383.0400, grad_fn=<NegBackward0>) tensor(11383.0400, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11383.0400390625
tensor(11383.0400, grad_fn=<NegBackward0>) tensor(11383.0400, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11383.0419921875
tensor(11383.0400, grad_fn=<NegBackward0>) tensor(11383.0420, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11383.0390625
tensor(11383.0400, grad_fn=<NegBackward0>) tensor(11383.0391, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11383.044921875
tensor(11383.0391, grad_fn=<NegBackward0>) tensor(11383.0449, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11383.0361328125
tensor(11383.0391, grad_fn=<NegBackward0>) tensor(11383.0361, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11383.0380859375
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0381, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11383.037109375
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0371, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11383.0400390625
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0400, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11383.0361328125
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0361, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11383.048828125
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0488, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11383.03515625
tensor(11383.0361, grad_fn=<NegBackward0>) tensor(11383.0352, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11383.03515625
tensor(11383.0352, grad_fn=<NegBackward0>) tensor(11383.0352, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11383.041015625
tensor(11383.0352, grad_fn=<NegBackward0>) tensor(11383.0410, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11383.0341796875
tensor(11383.0352, grad_fn=<NegBackward0>) tensor(11383.0342, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11383.0439453125
tensor(11383.0342, grad_fn=<NegBackward0>) tensor(11383.0439, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11383.05078125
tensor(11383.0342, grad_fn=<NegBackward0>) tensor(11383.0508, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11383.0322265625
tensor(11383.0342, grad_fn=<NegBackward0>) tensor(11383.0322, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11383.037109375
tensor(11383.0322, grad_fn=<NegBackward0>) tensor(11383.0371, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11383.0322265625
tensor(11383.0322, grad_fn=<NegBackward0>) tensor(11383.0322, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11383.0654296875
tensor(11383.0322, grad_fn=<NegBackward0>) tensor(11383.0654, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11383.03125
tensor(11383.0322, grad_fn=<NegBackward0>) tensor(11383.0312, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11383.0322265625
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0322, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11383.03125
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0312, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11383.03125
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0312, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11383.0390625
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0391, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11383.0322265625
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0322, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11383.0322265625
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0322, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11383.06640625
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0664, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11383.033203125
tensor(11383.0312, grad_fn=<NegBackward0>) tensor(11383.0332, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.6595, 0.3405],
        [0.2309, 0.7691]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9994e-01, 5.8000e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1886, 0.2019],
         [0.6456, 0.3083]],

        [[0.5830, 0.1018],
         [0.5739, 0.5467]],

        [[0.5700, 0.1040],
         [0.6637, 0.7016]],

        [[0.5367, 0.1087],
         [0.6298, 0.7239]],

        [[0.5219, 0.1021],
         [0.5397, 0.5319]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.5767667187470551
Average Adjusted Rand Index: 0.7446428783447254
[0.9368977075382423, 0.5767667187470551] [0.9371267812460605, 0.7446428783447254] [11294.068359375, 11383.033203125]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11149.814267162514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23509.51953125
inf tensor(23509.5195, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11442.3828125
tensor(23509.5195, grad_fn=<NegBackward0>) tensor(11442.3828, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11434.037109375
tensor(11442.3828, grad_fn=<NegBackward0>) tensor(11434.0371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11355.783203125
tensor(11434.0371, grad_fn=<NegBackward0>) tensor(11355.7832, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11347.703125
tensor(11355.7832, grad_fn=<NegBackward0>) tensor(11347.7031, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11286.8271484375
tensor(11347.7031, grad_fn=<NegBackward0>) tensor(11286.8271, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11155.361328125
tensor(11286.8271, grad_fn=<NegBackward0>) tensor(11155.3613, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11145.294921875
tensor(11155.3613, grad_fn=<NegBackward0>) tensor(11145.2949, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11137.369140625
tensor(11145.2949, grad_fn=<NegBackward0>) tensor(11137.3691, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11137.2177734375
tensor(11137.3691, grad_fn=<NegBackward0>) tensor(11137.2178, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11137.138671875
tensor(11137.2178, grad_fn=<NegBackward0>) tensor(11137.1387, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11137.0908203125
tensor(11137.1387, grad_fn=<NegBackward0>) tensor(11137.0908, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11137.052734375
tensor(11137.0908, grad_fn=<NegBackward0>) tensor(11137.0527, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11133.5908203125
tensor(11137.0527, grad_fn=<NegBackward0>) tensor(11133.5908, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11133.453125
tensor(11133.5908, grad_fn=<NegBackward0>) tensor(11133.4531, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11133.3046875
tensor(11133.4531, grad_fn=<NegBackward0>) tensor(11133.3047, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11133.2900390625
tensor(11133.3047, grad_fn=<NegBackward0>) tensor(11133.2900, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11133.2802734375
tensor(11133.2900, grad_fn=<NegBackward0>) tensor(11133.2803, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11133.271484375
tensor(11133.2803, grad_fn=<NegBackward0>) tensor(11133.2715, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11133.2646484375
tensor(11133.2715, grad_fn=<NegBackward0>) tensor(11133.2646, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11133.25390625
tensor(11133.2646, grad_fn=<NegBackward0>) tensor(11133.2539, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11133.24609375
tensor(11133.2539, grad_fn=<NegBackward0>) tensor(11133.2461, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11133.234375
tensor(11133.2461, grad_fn=<NegBackward0>) tensor(11133.2344, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11133.23046875
tensor(11133.2344, grad_fn=<NegBackward0>) tensor(11133.2305, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11133.2265625
tensor(11133.2305, grad_fn=<NegBackward0>) tensor(11133.2266, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11133.22265625
tensor(11133.2266, grad_fn=<NegBackward0>) tensor(11133.2227, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11133.2197265625
tensor(11133.2227, grad_fn=<NegBackward0>) tensor(11133.2197, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11133.21484375
tensor(11133.2197, grad_fn=<NegBackward0>) tensor(11133.2148, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11133.1982421875
tensor(11133.2148, grad_fn=<NegBackward0>) tensor(11133.1982, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11133.197265625
tensor(11133.1982, grad_fn=<NegBackward0>) tensor(11133.1973, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11133.1943359375
tensor(11133.1973, grad_fn=<NegBackward0>) tensor(11133.1943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11133.1865234375
tensor(11133.1943, grad_fn=<NegBackward0>) tensor(11133.1865, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11132.5703125
tensor(11133.1865, grad_fn=<NegBackward0>) tensor(11132.5703, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11132.568359375
tensor(11132.5703, grad_fn=<NegBackward0>) tensor(11132.5684, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11132.56640625
tensor(11132.5684, grad_fn=<NegBackward0>) tensor(11132.5664, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11132.56640625
tensor(11132.5664, grad_fn=<NegBackward0>) tensor(11132.5664, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11132.5654296875
tensor(11132.5664, grad_fn=<NegBackward0>) tensor(11132.5654, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11132.564453125
tensor(11132.5654, grad_fn=<NegBackward0>) tensor(11132.5645, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11132.5712890625
tensor(11132.5645, grad_fn=<NegBackward0>) tensor(11132.5713, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11132.5634765625
tensor(11132.5645, grad_fn=<NegBackward0>) tensor(11132.5635, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11132.5615234375
tensor(11132.5635, grad_fn=<NegBackward0>) tensor(11132.5615, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11132.5615234375
tensor(11132.5615, grad_fn=<NegBackward0>) tensor(11132.5615, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11132.560546875
tensor(11132.5615, grad_fn=<NegBackward0>) tensor(11132.5605, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11132.560546875
tensor(11132.5605, grad_fn=<NegBackward0>) tensor(11132.5605, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11132.55859375
tensor(11132.5605, grad_fn=<NegBackward0>) tensor(11132.5586, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11132.5576171875
tensor(11132.5586, grad_fn=<NegBackward0>) tensor(11132.5576, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11132.5576171875
tensor(11132.5576, grad_fn=<NegBackward0>) tensor(11132.5576, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11132.548828125
tensor(11132.5576, grad_fn=<NegBackward0>) tensor(11132.5488, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11132.5458984375
tensor(11132.5488, grad_fn=<NegBackward0>) tensor(11132.5459, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11132.55078125
tensor(11132.5459, grad_fn=<NegBackward0>) tensor(11132.5508, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11132.2626953125
tensor(11132.5459, grad_fn=<NegBackward0>) tensor(11132.2627, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11132.2529296875
tensor(11132.2627, grad_fn=<NegBackward0>) tensor(11132.2529, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11132.251953125
tensor(11132.2529, grad_fn=<NegBackward0>) tensor(11132.2520, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11132.2529296875
tensor(11132.2520, grad_fn=<NegBackward0>) tensor(11132.2529, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11132.251953125
tensor(11132.2520, grad_fn=<NegBackward0>) tensor(11132.2520, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11132.2490234375
tensor(11132.2520, grad_fn=<NegBackward0>) tensor(11132.2490, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11132.25
tensor(11132.2490, grad_fn=<NegBackward0>) tensor(11132.2500, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11132.2451171875
tensor(11132.2490, grad_fn=<NegBackward0>) tensor(11132.2451, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11132.2333984375
tensor(11132.2451, grad_fn=<NegBackward0>) tensor(11132.2334, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11132.232421875
tensor(11132.2334, grad_fn=<NegBackward0>) tensor(11132.2324, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11132.232421875
tensor(11132.2324, grad_fn=<NegBackward0>) tensor(11132.2324, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11132.20703125
tensor(11132.2324, grad_fn=<NegBackward0>) tensor(11132.2070, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11132.205078125
tensor(11132.2070, grad_fn=<NegBackward0>) tensor(11132.2051, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11132.205078125
tensor(11132.2051, grad_fn=<NegBackward0>) tensor(11132.2051, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11132.2060546875
tensor(11132.2051, grad_fn=<NegBackward0>) tensor(11132.2061, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11132.2041015625
tensor(11132.2051, grad_fn=<NegBackward0>) tensor(11132.2041, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11132.203125
tensor(11132.2041, grad_fn=<NegBackward0>) tensor(11132.2031, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11132.2021484375
tensor(11132.2031, grad_fn=<NegBackward0>) tensor(11132.2021, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11132.20703125
tensor(11132.2021, grad_fn=<NegBackward0>) tensor(11132.2070, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11132.203125
tensor(11132.2021, grad_fn=<NegBackward0>) tensor(11132.2031, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11132.2021484375
tensor(11132.2021, grad_fn=<NegBackward0>) tensor(11132.2021, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11132.201171875
tensor(11132.2021, grad_fn=<NegBackward0>) tensor(11132.2012, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11132.2021484375
tensor(11132.2012, grad_fn=<NegBackward0>) tensor(11132.2021, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11132.2001953125
tensor(11132.2012, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11132.23828125
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2383, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11132.2001953125
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11132.2880859375
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2881, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11132.2001953125
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11132.2021484375
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2021, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11132.197265625
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.1973, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11132.1982421875
tensor(11132.1973, grad_fn=<NegBackward0>) tensor(11132.1982, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11132.1982421875
tensor(11132.1973, grad_fn=<NegBackward0>) tensor(11132.1982, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11132.201171875
tensor(11132.1973, grad_fn=<NegBackward0>) tensor(11132.2012, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11132.19921875
tensor(11132.1973, grad_fn=<NegBackward0>) tensor(11132.1992, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -11132.2001953125
tensor(11132.1973, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.7914, 0.2086],
        [0.2767, 0.7233]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4706, 0.5294], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2008, 0.0917],
         [0.5543, 0.3057]],

        [[0.6728, 0.0931],
         [0.6961, 0.6500]],

        [[0.7308, 0.1073],
         [0.6864, 0.7260]],

        [[0.6299, 0.1063],
         [0.5980, 0.6638]],

        [[0.5850, 0.0980],
         [0.7236, 0.6865]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
Global Adjusted Rand Index: 0.9681923880397061
Average Adjusted Rand Index: 0.9681534227806623
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25013.80078125
inf tensor(25013.8008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11443.2060546875
tensor(25013.8008, grad_fn=<NegBackward0>) tensor(11443.2061, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11437.9189453125
tensor(11443.2061, grad_fn=<NegBackward0>) tensor(11437.9189, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11430.70703125
tensor(11437.9189, grad_fn=<NegBackward0>) tensor(11430.7070, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11374.88671875
tensor(11430.7070, grad_fn=<NegBackward0>) tensor(11374.8867, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11265.94140625
tensor(11374.8867, grad_fn=<NegBackward0>) tensor(11265.9414, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11153.2578125
tensor(11265.9414, grad_fn=<NegBackward0>) tensor(11153.2578, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11143.025390625
tensor(11153.2578, grad_fn=<NegBackward0>) tensor(11143.0254, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11140.91015625
tensor(11143.0254, grad_fn=<NegBackward0>) tensor(11140.9102, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11140.619140625
tensor(11140.9102, grad_fn=<NegBackward0>) tensor(11140.6191, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11140.24609375
tensor(11140.6191, grad_fn=<NegBackward0>) tensor(11140.2461, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11137.109375
tensor(11140.2461, grad_fn=<NegBackward0>) tensor(11137.1094, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11136.94140625
tensor(11137.1094, grad_fn=<NegBackward0>) tensor(11136.9414, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11136.8408203125
tensor(11136.9414, grad_fn=<NegBackward0>) tensor(11136.8408, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11136.8173828125
tensor(11136.8408, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11136.80078125
tensor(11136.8174, grad_fn=<NegBackward0>) tensor(11136.8008, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11136.783203125
tensor(11136.8008, grad_fn=<NegBackward0>) tensor(11136.7832, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11136.7705078125
tensor(11136.7832, grad_fn=<NegBackward0>) tensor(11136.7705, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11136.759765625
tensor(11136.7705, grad_fn=<NegBackward0>) tensor(11136.7598, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11136.751953125
tensor(11136.7598, grad_fn=<NegBackward0>) tensor(11136.7520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11136.7421875
tensor(11136.7520, grad_fn=<NegBackward0>) tensor(11136.7422, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11136.7275390625
tensor(11136.7422, grad_fn=<NegBackward0>) tensor(11136.7275, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11136.6865234375
tensor(11136.7275, grad_fn=<NegBackward0>) tensor(11136.6865, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11132.857421875
tensor(11136.6865, grad_fn=<NegBackward0>) tensor(11132.8574, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11132.8076171875
tensor(11132.8574, grad_fn=<NegBackward0>) tensor(11132.8076, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11132.2255859375
tensor(11132.8076, grad_fn=<NegBackward0>) tensor(11132.2256, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11132.220703125
tensor(11132.2256, grad_fn=<NegBackward0>) tensor(11132.2207, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11132.216796875
tensor(11132.2207, grad_fn=<NegBackward0>) tensor(11132.2168, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11132.2119140625
tensor(11132.2168, grad_fn=<NegBackward0>) tensor(11132.2119, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11132.2109375
tensor(11132.2119, grad_fn=<NegBackward0>) tensor(11132.2109, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11132.2080078125
tensor(11132.2109, grad_fn=<NegBackward0>) tensor(11132.2080, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11132.205078125
tensor(11132.2080, grad_fn=<NegBackward0>) tensor(11132.2051, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11132.2021484375
tensor(11132.2051, grad_fn=<NegBackward0>) tensor(11132.2021, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11132.2001953125
tensor(11132.2021, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11132.2001953125
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.2002, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11132.1953125
tensor(11132.2002, grad_fn=<NegBackward0>) tensor(11132.1953, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11132.1953125
tensor(11132.1953, grad_fn=<NegBackward0>) tensor(11132.1953, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11132.1953125
tensor(11132.1953, grad_fn=<NegBackward0>) tensor(11132.1953, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11132.201171875
tensor(11132.1953, grad_fn=<NegBackward0>) tensor(11132.2012, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11132.1923828125
tensor(11132.1953, grad_fn=<NegBackward0>) tensor(11132.1924, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11132.19140625
tensor(11132.1924, grad_fn=<NegBackward0>) tensor(11132.1914, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11132.1904296875
tensor(11132.1914, grad_fn=<NegBackward0>) tensor(11132.1904, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11132.1904296875
tensor(11132.1904, grad_fn=<NegBackward0>) tensor(11132.1904, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11132.189453125
tensor(11132.1904, grad_fn=<NegBackward0>) tensor(11132.1895, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11132.1884765625
tensor(11132.1895, grad_fn=<NegBackward0>) tensor(11132.1885, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11132.189453125
tensor(11132.1885, grad_fn=<NegBackward0>) tensor(11132.1895, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11132.189453125
tensor(11132.1885, grad_fn=<NegBackward0>) tensor(11132.1895, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11132.1875
tensor(11132.1885, grad_fn=<NegBackward0>) tensor(11132.1875, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11132.1865234375
tensor(11132.1875, grad_fn=<NegBackward0>) tensor(11132.1865, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11132.19921875
tensor(11132.1865, grad_fn=<NegBackward0>) tensor(11132.1992, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11132.1865234375
tensor(11132.1865, grad_fn=<NegBackward0>) tensor(11132.1865, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11132.1865234375
tensor(11132.1865, grad_fn=<NegBackward0>) tensor(11132.1865, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11132.1865234375
tensor(11132.1865, grad_fn=<NegBackward0>) tensor(11132.1865, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11132.1845703125
tensor(11132.1865, grad_fn=<NegBackward0>) tensor(11132.1846, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11132.1845703125
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1846, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11132.1865234375
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1865, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11132.1845703125
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1846, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11132.1845703125
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1846, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11132.1875
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1875, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11132.1748046875
tensor(11132.1846, grad_fn=<NegBackward0>) tensor(11132.1748, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11132.1748046875
tensor(11132.1748, grad_fn=<NegBackward0>) tensor(11132.1748, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11132.173828125
tensor(11132.1748, grad_fn=<NegBackward0>) tensor(11132.1738, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11132.173828125
tensor(11132.1738, grad_fn=<NegBackward0>) tensor(11132.1738, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11132.173828125
tensor(11132.1738, grad_fn=<NegBackward0>) tensor(11132.1738, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11132.17578125
tensor(11132.1738, grad_fn=<NegBackward0>) tensor(11132.1758, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11132.171875
tensor(11132.1738, grad_fn=<NegBackward0>) tensor(11132.1719, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11132.1728515625
tensor(11132.1719, grad_fn=<NegBackward0>) tensor(11132.1729, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11132.1826171875
tensor(11132.1719, grad_fn=<NegBackward0>) tensor(11132.1826, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11132.1708984375
tensor(11132.1719, grad_fn=<NegBackward0>) tensor(11132.1709, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11132.171875
tensor(11132.1709, grad_fn=<NegBackward0>) tensor(11132.1719, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11132.171875
tensor(11132.1709, grad_fn=<NegBackward0>) tensor(11132.1719, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11132.171875
tensor(11132.1709, grad_fn=<NegBackward0>) tensor(11132.1719, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11132.16015625
tensor(11132.1709, grad_fn=<NegBackward0>) tensor(11132.1602, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11132.16015625
tensor(11132.1602, grad_fn=<NegBackward0>) tensor(11132.1602, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11132.16015625
tensor(11132.1602, grad_fn=<NegBackward0>) tensor(11132.1602, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11132.1591796875
tensor(11132.1602, grad_fn=<NegBackward0>) tensor(11132.1592, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11132.1591796875
tensor(11132.1592, grad_fn=<NegBackward0>) tensor(11132.1592, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11132.1591796875
tensor(11132.1592, grad_fn=<NegBackward0>) tensor(11132.1592, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11132.158203125
tensor(11132.1592, grad_fn=<NegBackward0>) tensor(11132.1582, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11132.1572265625
tensor(11132.1582, grad_fn=<NegBackward0>) tensor(11132.1572, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11132.1796875
tensor(11132.1572, grad_fn=<NegBackward0>) tensor(11132.1797, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11132.1630859375
tensor(11132.1572, grad_fn=<NegBackward0>) tensor(11132.1631, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11132.1572265625
tensor(11132.1572, grad_fn=<NegBackward0>) tensor(11132.1572, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11132.154296875
tensor(11132.1572, grad_fn=<NegBackward0>) tensor(11132.1543, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11132.15625
tensor(11132.1543, grad_fn=<NegBackward0>) tensor(11132.1562, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11132.154296875
tensor(11132.1543, grad_fn=<NegBackward0>) tensor(11132.1543, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11132.1552734375
tensor(11132.1543, grad_fn=<NegBackward0>) tensor(11132.1553, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11132.1533203125
tensor(11132.1543, grad_fn=<NegBackward0>) tensor(11132.1533, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11132.154296875
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1543, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11132.1552734375
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1553, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11132.1533203125
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1533, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11132.154296875
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1543, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11132.154296875
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1543, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11132.1494140625
tensor(11132.1533, grad_fn=<NegBackward0>) tensor(11132.1494, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11132.15234375
tensor(11132.1494, grad_fn=<NegBackward0>) tensor(11132.1523, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11132.1494140625
tensor(11132.1494, grad_fn=<NegBackward0>) tensor(11132.1494, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11132.1494140625
tensor(11132.1494, grad_fn=<NegBackward0>) tensor(11132.1494, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11132.228515625
tensor(11132.1494, grad_fn=<NegBackward0>) tensor(11132.2285, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11132.1484375
tensor(11132.1494, grad_fn=<NegBackward0>) tensor(11132.1484, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11132.150390625
tensor(11132.1484, grad_fn=<NegBackward0>) tensor(11132.1504, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7226, 0.2774],
        [0.2083, 0.7917]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5279, 0.4721], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3061, 0.0917],
         [0.6895, 0.2008]],

        [[0.6449, 0.0931],
         [0.6723, 0.5586]],

        [[0.7148, 0.1077],
         [0.6321, 0.6968]],

        [[0.5633, 0.1063],
         [0.6821, 0.5537]],

        [[0.6328, 0.0980],
         [0.7006, 0.5362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
Global Adjusted Rand Index: 0.9681923880397061
Average Adjusted Rand Index: 0.9681534227806623
[0.9681923880397061, 0.9681923880397061] [0.9681534227806623, 0.9681534227806623] [11132.2001953125, 11132.1513671875]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11144.31715580235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23757.232421875
inf tensor(23757.2324, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11456.029296875
tensor(23757.2324, grad_fn=<NegBackward0>) tensor(11456.0293, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11455.0830078125
tensor(11456.0293, grad_fn=<NegBackward0>) tensor(11455.0830, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11454.5634765625
tensor(11455.0830, grad_fn=<NegBackward0>) tensor(11454.5635, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11453.966796875
tensor(11454.5635, grad_fn=<NegBackward0>) tensor(11453.9668, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11453.73046875
tensor(11453.9668, grad_fn=<NegBackward0>) tensor(11453.7305, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11453.599609375
tensor(11453.7305, grad_fn=<NegBackward0>) tensor(11453.5996, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11453.521484375
tensor(11453.5996, grad_fn=<NegBackward0>) tensor(11453.5215, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11453.4775390625
tensor(11453.5215, grad_fn=<NegBackward0>) tensor(11453.4775, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11453.451171875
tensor(11453.4775, grad_fn=<NegBackward0>) tensor(11453.4512, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11453.43359375
tensor(11453.4512, grad_fn=<NegBackward0>) tensor(11453.4336, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11453.421875
tensor(11453.4336, grad_fn=<NegBackward0>) tensor(11453.4219, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11453.4111328125
tensor(11453.4219, grad_fn=<NegBackward0>) tensor(11453.4111, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11453.4033203125
tensor(11453.4111, grad_fn=<NegBackward0>) tensor(11453.4033, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11453.3994140625
tensor(11453.4033, grad_fn=<NegBackward0>) tensor(11453.3994, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11453.3935546875
tensor(11453.3994, grad_fn=<NegBackward0>) tensor(11453.3936, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11453.3876953125
tensor(11453.3936, grad_fn=<NegBackward0>) tensor(11453.3877, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11453.384765625
tensor(11453.3877, grad_fn=<NegBackward0>) tensor(11453.3848, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11453.3828125
tensor(11453.3848, grad_fn=<NegBackward0>) tensor(11453.3828, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11453.3798828125
tensor(11453.3828, grad_fn=<NegBackward0>) tensor(11453.3799, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11453.375
tensor(11453.3799, grad_fn=<NegBackward0>) tensor(11453.3750, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11453.3740234375
tensor(11453.3750, grad_fn=<NegBackward0>) tensor(11453.3740, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11453.37109375
tensor(11453.3740, grad_fn=<NegBackward0>) tensor(11453.3711, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11453.369140625
tensor(11453.3711, grad_fn=<NegBackward0>) tensor(11453.3691, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11453.3681640625
tensor(11453.3691, grad_fn=<NegBackward0>) tensor(11453.3682, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11453.365234375
tensor(11453.3682, grad_fn=<NegBackward0>) tensor(11453.3652, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11453.3642578125
tensor(11453.3652, grad_fn=<NegBackward0>) tensor(11453.3643, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11453.36328125
tensor(11453.3643, grad_fn=<NegBackward0>) tensor(11453.3633, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11453.361328125
tensor(11453.3633, grad_fn=<NegBackward0>) tensor(11453.3613, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11453.3603515625
tensor(11453.3613, grad_fn=<NegBackward0>) tensor(11453.3604, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11453.3583984375
tensor(11453.3604, grad_fn=<NegBackward0>) tensor(11453.3584, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11453.357421875
tensor(11453.3584, grad_fn=<NegBackward0>) tensor(11453.3574, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11453.3564453125
tensor(11453.3574, grad_fn=<NegBackward0>) tensor(11453.3564, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11453.3564453125
tensor(11453.3564, grad_fn=<NegBackward0>) tensor(11453.3564, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11453.35546875
tensor(11453.3564, grad_fn=<NegBackward0>) tensor(11453.3555, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11453.35546875
tensor(11453.3555, grad_fn=<NegBackward0>) tensor(11453.3555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11453.3544921875
tensor(11453.3555, grad_fn=<NegBackward0>) tensor(11453.3545, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11453.3515625
tensor(11453.3545, grad_fn=<NegBackward0>) tensor(11453.3516, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11453.3505859375
tensor(11453.3516, grad_fn=<NegBackward0>) tensor(11453.3506, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11453.3505859375
tensor(11453.3506, grad_fn=<NegBackward0>) tensor(11453.3506, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11453.349609375
tensor(11453.3506, grad_fn=<NegBackward0>) tensor(11453.3496, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11453.3486328125
tensor(11453.3496, grad_fn=<NegBackward0>) tensor(11453.3486, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11453.3486328125
tensor(11453.3486, grad_fn=<NegBackward0>) tensor(11453.3486, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11453.34765625
tensor(11453.3486, grad_fn=<NegBackward0>) tensor(11453.3477, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11453.3466796875
tensor(11453.3477, grad_fn=<NegBackward0>) tensor(11453.3467, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11453.345703125
tensor(11453.3467, grad_fn=<NegBackward0>) tensor(11453.3457, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11453.3486328125
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3486, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11453.345703125
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3457, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11453.34765625
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3477, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11453.3466796875
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3467, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11453.34765625
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3477, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11453.34765625
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3477, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11453.3447265625
tensor(11453.3457, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11453.3447265625
tensor(11453.3447, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11453.345703125
tensor(11453.3447, grad_fn=<NegBackward0>) tensor(11453.3457, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11453.34375
tensor(11453.3447, grad_fn=<NegBackward0>) tensor(11453.3438, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11453.345703125
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3457, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11453.34375
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3438, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11453.3447265625
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3447, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11453.349609375
tensor(11453.3438, grad_fn=<NegBackward0>) tensor(11453.3496, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[9.7107e-01, 2.8927e-02],
        [9.9961e-01, 3.9205e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9964, 0.0036], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1729, 0.1412],
         [0.5663, 0.3204]],

        [[0.5090, 0.2488],
         [0.6105, 0.5900]],

        [[0.7189, 0.2129],
         [0.6270, 0.6825]],

        [[0.5175, 0.0998],
         [0.5595, 0.5993]],

        [[0.5131, 0.2291],
         [0.5129, 0.6965]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.091685828802855e-05
Average Adjusted Rand Index: 0.0003266560983811308
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21307.890625
inf tensor(21307.8906, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11454.458984375
tensor(21307.8906, grad_fn=<NegBackward0>) tensor(11454.4590, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11452.0185546875
tensor(11454.4590, grad_fn=<NegBackward0>) tensor(11452.0186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11450.8779296875
tensor(11452.0186, grad_fn=<NegBackward0>) tensor(11450.8779, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11450.3115234375
tensor(11450.8779, grad_fn=<NegBackward0>) tensor(11450.3115, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11449.5244140625
tensor(11450.3115, grad_fn=<NegBackward0>) tensor(11449.5244, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11423.9716796875
tensor(11449.5244, grad_fn=<NegBackward0>) tensor(11423.9717, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11277.5029296875
tensor(11423.9717, grad_fn=<NegBackward0>) tensor(11277.5029, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11264.8740234375
tensor(11277.5029, grad_fn=<NegBackward0>) tensor(11264.8740, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11264.720703125
tensor(11264.8740, grad_fn=<NegBackward0>) tensor(11264.7207, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11264.6611328125
tensor(11264.7207, grad_fn=<NegBackward0>) tensor(11264.6611, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11264.5595703125
tensor(11264.6611, grad_fn=<NegBackward0>) tensor(11264.5596, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11264.5361328125
tensor(11264.5596, grad_fn=<NegBackward0>) tensor(11264.5361, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11264.5146484375
tensor(11264.5361, grad_fn=<NegBackward0>) tensor(11264.5146, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11264.5
tensor(11264.5146, grad_fn=<NegBackward0>) tensor(11264.5000, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11264.490234375
tensor(11264.5000, grad_fn=<NegBackward0>) tensor(11264.4902, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11264.4794921875
tensor(11264.4902, grad_fn=<NegBackward0>) tensor(11264.4795, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11264.4716796875
tensor(11264.4795, grad_fn=<NegBackward0>) tensor(11264.4717, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11264.4384765625
tensor(11264.4717, grad_fn=<NegBackward0>) tensor(11264.4385, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11264.404296875
tensor(11264.4385, grad_fn=<NegBackward0>) tensor(11264.4043, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11264.1806640625
tensor(11264.4043, grad_fn=<NegBackward0>) tensor(11264.1807, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11259.373046875
tensor(11264.1807, grad_fn=<NegBackward0>) tensor(11259.3730, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11259.35546875
tensor(11259.3730, grad_fn=<NegBackward0>) tensor(11259.3555, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11259.1806640625
tensor(11259.3555, grad_fn=<NegBackward0>) tensor(11259.1807, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11259.15234375
tensor(11259.1807, grad_fn=<NegBackward0>) tensor(11259.1523, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11259.1484375
tensor(11259.1523, grad_fn=<NegBackward0>) tensor(11259.1484, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11259.146484375
tensor(11259.1484, grad_fn=<NegBackward0>) tensor(11259.1465, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11259.140625
tensor(11259.1465, grad_fn=<NegBackward0>) tensor(11259.1406, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11258.12890625
tensor(11259.1406, grad_fn=<NegBackward0>) tensor(11258.1289, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11257.8671875
tensor(11258.1289, grad_fn=<NegBackward0>) tensor(11257.8672, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11257.86328125
tensor(11257.8672, grad_fn=<NegBackward0>) tensor(11257.8633, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11257.828125
tensor(11257.8633, grad_fn=<NegBackward0>) tensor(11257.8281, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11257.8251953125
tensor(11257.8281, grad_fn=<NegBackward0>) tensor(11257.8252, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11257.8193359375
tensor(11257.8252, grad_fn=<NegBackward0>) tensor(11257.8193, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11257.755859375
tensor(11257.8193, grad_fn=<NegBackward0>) tensor(11257.7559, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11257.748046875
tensor(11257.7559, grad_fn=<NegBackward0>) tensor(11257.7480, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11257.51953125
tensor(11257.7480, grad_fn=<NegBackward0>) tensor(11257.5195, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11257.509765625
tensor(11257.5195, grad_fn=<NegBackward0>) tensor(11257.5098, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11257.4912109375
tensor(11257.5098, grad_fn=<NegBackward0>) tensor(11257.4912, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11256.615234375
tensor(11257.4912, grad_fn=<NegBackward0>) tensor(11256.6152, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11256.6142578125
tensor(11256.6152, grad_fn=<NegBackward0>) tensor(11256.6143, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11256.619140625
tensor(11256.6143, grad_fn=<NegBackward0>) tensor(11256.6191, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11256.447265625
tensor(11256.6143, grad_fn=<NegBackward0>) tensor(11256.4473, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11226.1953125
tensor(11256.4473, grad_fn=<NegBackward0>) tensor(11226.1953, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11226.01171875
tensor(11226.1953, grad_fn=<NegBackward0>) tensor(11226.0117, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11225.978515625
tensor(11226.0117, grad_fn=<NegBackward0>) tensor(11225.9785, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11225.9609375
tensor(11225.9785, grad_fn=<NegBackward0>) tensor(11225.9609, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11224.8828125
tensor(11225.9609, grad_fn=<NegBackward0>) tensor(11224.8828, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11223.90625
tensor(11224.8828, grad_fn=<NegBackward0>) tensor(11223.9062, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11220.841796875
tensor(11223.9062, grad_fn=<NegBackward0>) tensor(11220.8418, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11220.841796875
tensor(11220.8418, grad_fn=<NegBackward0>) tensor(11220.8418, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11220.8408203125
tensor(11220.8418, grad_fn=<NegBackward0>) tensor(11220.8408, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11220.8427734375
tensor(11220.8408, grad_fn=<NegBackward0>) tensor(11220.8428, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11220.8408203125
tensor(11220.8408, grad_fn=<NegBackward0>) tensor(11220.8408, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11220.83984375
tensor(11220.8408, grad_fn=<NegBackward0>) tensor(11220.8398, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11220.5029296875
tensor(11220.8398, grad_fn=<NegBackward0>) tensor(11220.5029, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11219.611328125
tensor(11220.5029, grad_fn=<NegBackward0>) tensor(11219.6113, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11219.6044921875
tensor(11219.6113, grad_fn=<NegBackward0>) tensor(11219.6045, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11219.5556640625
tensor(11219.6045, grad_fn=<NegBackward0>) tensor(11219.5557, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11219.556640625
tensor(11219.5557, grad_fn=<NegBackward0>) tensor(11219.5566, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11219.5478515625
tensor(11219.5557, grad_fn=<NegBackward0>) tensor(11219.5479, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11219.5478515625
tensor(11219.5479, grad_fn=<NegBackward0>) tensor(11219.5479, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11219.544921875
tensor(11219.5479, grad_fn=<NegBackward0>) tensor(11219.5449, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11219.53515625
tensor(11219.5449, grad_fn=<NegBackward0>) tensor(11219.5352, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11219.5380859375
tensor(11219.5352, grad_fn=<NegBackward0>) tensor(11219.5381, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11219.5361328125
tensor(11219.5352, grad_fn=<NegBackward0>) tensor(11219.5361, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11219.537109375
tensor(11219.5352, grad_fn=<NegBackward0>) tensor(11219.5371, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11219.53125
tensor(11219.5352, grad_fn=<NegBackward0>) tensor(11219.5312, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11218.54296875
tensor(11219.5312, grad_fn=<NegBackward0>) tensor(11218.5430, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11218.5419921875
tensor(11218.5430, grad_fn=<NegBackward0>) tensor(11218.5420, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11218.46484375
tensor(11218.5420, grad_fn=<NegBackward0>) tensor(11218.4648, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11218.447265625
tensor(11218.4648, grad_fn=<NegBackward0>) tensor(11218.4473, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11218.451171875
tensor(11218.4473, grad_fn=<NegBackward0>) tensor(11218.4512, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11218.4462890625
tensor(11218.4473, grad_fn=<NegBackward0>) tensor(11218.4463, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11218.4365234375
tensor(11218.4463, grad_fn=<NegBackward0>) tensor(11218.4365, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11218.259765625
tensor(11218.4365, grad_fn=<NegBackward0>) tensor(11218.2598, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11218.279296875
tensor(11218.2598, grad_fn=<NegBackward0>) tensor(11218.2793, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11218.2080078125
tensor(11218.2598, grad_fn=<NegBackward0>) tensor(11218.2080, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11218.208984375
tensor(11218.2080, grad_fn=<NegBackward0>) tensor(11218.2090, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11218.208984375
tensor(11218.2080, grad_fn=<NegBackward0>) tensor(11218.2090, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11218.2080078125
tensor(11218.2080, grad_fn=<NegBackward0>) tensor(11218.2080, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11218.2080078125
tensor(11218.2080, grad_fn=<NegBackward0>) tensor(11218.2080, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11218.201171875
tensor(11218.2080, grad_fn=<NegBackward0>) tensor(11218.2012, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11218.1787109375
tensor(11218.2012, grad_fn=<NegBackward0>) tensor(11218.1787, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11218.1201171875
tensor(11218.1787, grad_fn=<NegBackward0>) tensor(11218.1201, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11218.11328125
tensor(11218.1201, grad_fn=<NegBackward0>) tensor(11218.1133, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11217.5263671875
tensor(11218.1133, grad_fn=<NegBackward0>) tensor(11217.5264, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11217.5224609375
tensor(11217.5264, grad_fn=<NegBackward0>) tensor(11217.5225, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11217.671875
tensor(11217.5225, grad_fn=<NegBackward0>) tensor(11217.6719, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11217.5205078125
tensor(11217.5225, grad_fn=<NegBackward0>) tensor(11217.5205, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11217.482421875
tensor(11217.5205, grad_fn=<NegBackward0>) tensor(11217.4824, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11217.48046875
tensor(11217.4824, grad_fn=<NegBackward0>) tensor(11217.4805, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11217.4677734375
tensor(11217.4805, grad_fn=<NegBackward0>) tensor(11217.4678, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11209.224609375
tensor(11217.4678, grad_fn=<NegBackward0>) tensor(11209.2246, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11209.1748046875
tensor(11209.2246, grad_fn=<NegBackward0>) tensor(11209.1748, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11209.169921875
tensor(11209.1748, grad_fn=<NegBackward0>) tensor(11209.1699, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11209.134765625
tensor(11209.1699, grad_fn=<NegBackward0>) tensor(11209.1348, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11209.134765625
tensor(11209.1348, grad_fn=<NegBackward0>) tensor(11209.1348, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11209.1318359375
tensor(11209.1348, grad_fn=<NegBackward0>) tensor(11209.1318, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11209.130859375
tensor(11209.1318, grad_fn=<NegBackward0>) tensor(11209.1309, grad_fn=<NegBackward0>)
pi: tensor([[0.4193, 0.5807],
        [0.5555, 0.4445]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4498, 0.5502], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2798, 0.0988],
         [0.5209, 0.2272]],

        [[0.7066, 0.0996],
         [0.6971, 0.7145]],

        [[0.6542, 0.0987],
         [0.5646, 0.6972]],

        [[0.5088, 0.0986],
         [0.5948, 0.5995]],

        [[0.6496, 0.0845],
         [0.7111, 0.6583]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363636363636364
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5734937529465501
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.06782989489101277
Average Adjusted Rand Index: 0.7950988799710391
[-3.091685828802855e-05, 0.06782989489101277] [0.0003266560983811308, 0.7950988799710391] [11453.349609375, 11209.130859375]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11209.444149975014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22265.7265625
inf tensor(22265.7266, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11495.275390625
tensor(22265.7266, grad_fn=<NegBackward0>) tensor(11495.2754, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11494.2021484375
tensor(11495.2754, grad_fn=<NegBackward0>) tensor(11494.2021, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11493.30859375
tensor(11494.2021, grad_fn=<NegBackward0>) tensor(11493.3086, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11491.7451171875
tensor(11493.3086, grad_fn=<NegBackward0>) tensor(11491.7451, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11487.8359375
tensor(11491.7451, grad_fn=<NegBackward0>) tensor(11487.8359, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11481.9697265625
tensor(11487.8359, grad_fn=<NegBackward0>) tensor(11481.9697, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11400.3203125
tensor(11481.9697, grad_fn=<NegBackward0>) tensor(11400.3203, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11250.9462890625
tensor(11400.3203, grad_fn=<NegBackward0>) tensor(11250.9463, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11230.3857421875
tensor(11250.9463, grad_fn=<NegBackward0>) tensor(11230.3857, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11229.982421875
tensor(11230.3857, grad_fn=<NegBackward0>) tensor(11229.9824, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11219.8798828125
tensor(11229.9824, grad_fn=<NegBackward0>) tensor(11219.8799, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11209.8369140625
tensor(11219.8799, grad_fn=<NegBackward0>) tensor(11209.8369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11209.525390625
tensor(11209.8369, grad_fn=<NegBackward0>) tensor(11209.5254, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11209.4853515625
tensor(11209.5254, grad_fn=<NegBackward0>) tensor(11209.4854, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11209.4296875
tensor(11209.4854, grad_fn=<NegBackward0>) tensor(11209.4297, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11209.3818359375
tensor(11209.4297, grad_fn=<NegBackward0>) tensor(11209.3818, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11209.3349609375
tensor(11209.3818, grad_fn=<NegBackward0>) tensor(11209.3350, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11202.3818359375
tensor(11209.3350, grad_fn=<NegBackward0>) tensor(11202.3818, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11202.3095703125
tensor(11202.3818, grad_fn=<NegBackward0>) tensor(11202.3096, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11200.501953125
tensor(11202.3096, grad_fn=<NegBackward0>) tensor(11200.5020, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11199.9072265625
tensor(11200.5020, grad_fn=<NegBackward0>) tensor(11199.9072, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11199.5185546875
tensor(11199.9072, grad_fn=<NegBackward0>) tensor(11199.5186, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11199.474609375
tensor(11199.5186, grad_fn=<NegBackward0>) tensor(11199.4746, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11199.3916015625
tensor(11199.4746, grad_fn=<NegBackward0>) tensor(11199.3916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11199.3818359375
tensor(11199.3916, grad_fn=<NegBackward0>) tensor(11199.3818, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11199.3671875
tensor(11199.3818, grad_fn=<NegBackward0>) tensor(11199.3672, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11199.361328125
tensor(11199.3672, grad_fn=<NegBackward0>) tensor(11199.3613, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11199.35546875
tensor(11199.3613, grad_fn=<NegBackward0>) tensor(11199.3555, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11199.3515625
tensor(11199.3555, grad_fn=<NegBackward0>) tensor(11199.3516, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11199.3486328125
tensor(11199.3516, grad_fn=<NegBackward0>) tensor(11199.3486, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11199.3466796875
tensor(11199.3486, grad_fn=<NegBackward0>) tensor(11199.3467, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11199.3427734375
tensor(11199.3467, grad_fn=<NegBackward0>) tensor(11199.3428, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11199.294921875
tensor(11199.3428, grad_fn=<NegBackward0>) tensor(11199.2949, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11199.279296875
tensor(11199.2949, grad_fn=<NegBackward0>) tensor(11199.2793, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11199.279296875
tensor(11199.2793, grad_fn=<NegBackward0>) tensor(11199.2793, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11199.2763671875
tensor(11199.2793, grad_fn=<NegBackward0>) tensor(11199.2764, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11199.275390625
tensor(11199.2764, grad_fn=<NegBackward0>) tensor(11199.2754, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11199.275390625
tensor(11199.2754, grad_fn=<NegBackward0>) tensor(11199.2754, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11199.2744140625
tensor(11199.2754, grad_fn=<NegBackward0>) tensor(11199.2744, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11199.271484375
tensor(11199.2744, grad_fn=<NegBackward0>) tensor(11199.2715, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11199.2724609375
tensor(11199.2715, grad_fn=<NegBackward0>) tensor(11199.2725, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11199.2734375
tensor(11199.2715, grad_fn=<NegBackward0>) tensor(11199.2734, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11199.2705078125
tensor(11199.2715, grad_fn=<NegBackward0>) tensor(11199.2705, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11192.744140625
tensor(11199.2705, grad_fn=<NegBackward0>) tensor(11192.7441, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11192.7421875
tensor(11192.7441, grad_fn=<NegBackward0>) tensor(11192.7422, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11192.7412109375
tensor(11192.7422, grad_fn=<NegBackward0>) tensor(11192.7412, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11192.7412109375
tensor(11192.7412, grad_fn=<NegBackward0>) tensor(11192.7412, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11192.73828125
tensor(11192.7412, grad_fn=<NegBackward0>) tensor(11192.7383, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11192.7353515625
tensor(11192.7383, grad_fn=<NegBackward0>) tensor(11192.7354, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11192.7236328125
tensor(11192.7354, grad_fn=<NegBackward0>) tensor(11192.7236, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11192.716796875
tensor(11192.7236, grad_fn=<NegBackward0>) tensor(11192.7168, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11192.716796875
tensor(11192.7168, grad_fn=<NegBackward0>) tensor(11192.7168, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11192.712890625
tensor(11192.7168, grad_fn=<NegBackward0>) tensor(11192.7129, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11192.7138671875
tensor(11192.7129, grad_fn=<NegBackward0>) tensor(11192.7139, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11192.7119140625
tensor(11192.7129, grad_fn=<NegBackward0>) tensor(11192.7119, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11192.7099609375
tensor(11192.7119, grad_fn=<NegBackward0>) tensor(11192.7100, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11192.708984375
tensor(11192.7100, grad_fn=<NegBackward0>) tensor(11192.7090, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11192.7109375
tensor(11192.7090, grad_fn=<NegBackward0>) tensor(11192.7109, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11192.708984375
tensor(11192.7090, grad_fn=<NegBackward0>) tensor(11192.7090, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11192.7080078125
tensor(11192.7090, grad_fn=<NegBackward0>) tensor(11192.7080, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11192.70703125
tensor(11192.7080, grad_fn=<NegBackward0>) tensor(11192.7070, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11192.7080078125
tensor(11192.7070, grad_fn=<NegBackward0>) tensor(11192.7080, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11192.71875
tensor(11192.7070, grad_fn=<NegBackward0>) tensor(11192.7188, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11192.7060546875
tensor(11192.7070, grad_fn=<NegBackward0>) tensor(11192.7061, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11192.7041015625
tensor(11192.7061, grad_fn=<NegBackward0>) tensor(11192.7041, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11192.6904296875
tensor(11192.7041, grad_fn=<NegBackward0>) tensor(11192.6904, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11192.685546875
tensor(11192.6904, grad_fn=<NegBackward0>) tensor(11192.6855, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11192.6845703125
tensor(11192.6855, grad_fn=<NegBackward0>) tensor(11192.6846, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11192.6875
tensor(11192.6846, grad_fn=<NegBackward0>) tensor(11192.6875, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11192.685546875
tensor(11192.6846, grad_fn=<NegBackward0>) tensor(11192.6855, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11192.6845703125
tensor(11192.6846, grad_fn=<NegBackward0>) tensor(11192.6846, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11192.68359375
tensor(11192.6846, grad_fn=<NegBackward0>) tensor(11192.6836, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11192.68359375
tensor(11192.6836, grad_fn=<NegBackward0>) tensor(11192.6836, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11192.68359375
tensor(11192.6836, grad_fn=<NegBackward0>) tensor(11192.6836, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11192.68359375
tensor(11192.6836, grad_fn=<NegBackward0>) tensor(11192.6836, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11192.6962890625
tensor(11192.6836, grad_fn=<NegBackward0>) tensor(11192.6963, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11192.6826171875
tensor(11192.6836, grad_fn=<NegBackward0>) tensor(11192.6826, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11192.6806640625
tensor(11192.6826, grad_fn=<NegBackward0>) tensor(11192.6807, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11192.6806640625
tensor(11192.6807, grad_fn=<NegBackward0>) tensor(11192.6807, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11192.689453125
tensor(11192.6807, grad_fn=<NegBackward0>) tensor(11192.6895, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11192.68359375
tensor(11192.6807, grad_fn=<NegBackward0>) tensor(11192.6836, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11192.6787109375
tensor(11192.6807, grad_fn=<NegBackward0>) tensor(11192.6787, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11192.6787109375
tensor(11192.6787, grad_fn=<NegBackward0>) tensor(11192.6787, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11192.76171875
tensor(11192.6787, grad_fn=<NegBackward0>) tensor(11192.7617, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11192.6611328125
tensor(11192.6787, grad_fn=<NegBackward0>) tensor(11192.6611, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11192.66015625
tensor(11192.6611, grad_fn=<NegBackward0>) tensor(11192.6602, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11192.654296875
tensor(11192.6602, grad_fn=<NegBackward0>) tensor(11192.6543, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11192.70703125
tensor(11192.6543, grad_fn=<NegBackward0>) tensor(11192.7070, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11192.6533203125
tensor(11192.6543, grad_fn=<NegBackward0>) tensor(11192.6533, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11192.6865234375
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6865, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11192.6533203125
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6533, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11192.6533203125
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6533, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11192.662109375
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6621, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11192.6572265625
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6572, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11192.654296875
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6543, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11192.65625
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6562, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11192.654296875
tensor(11192.6533, grad_fn=<NegBackward0>) tensor(11192.6543, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7528, 0.2472],
        [0.2515, 0.7485]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4956, 0.5044], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.1037],
         [0.7067, 0.3034]],

        [[0.5703, 0.0968],
         [0.6075, 0.5276]],

        [[0.6964, 0.0968],
         [0.6162, 0.6283]],

        [[0.7182, 0.1008],
         [0.7249, 0.6795]],

        [[0.7141, 0.1048],
         [0.7043, 0.6218]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731668894559
Average Adjusted Rand Index: 0.9446425197695675
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21909.3359375
inf tensor(21909.3359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11499.494140625
tensor(21909.3359, grad_fn=<NegBackward0>) tensor(11499.4941, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11495.9208984375
tensor(11499.4941, grad_fn=<NegBackward0>) tensor(11495.9209, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11494.3388671875
tensor(11495.9209, grad_fn=<NegBackward0>) tensor(11494.3389, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11493.36328125
tensor(11494.3389, grad_fn=<NegBackward0>) tensor(11493.3633, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11492.9833984375
tensor(11493.3633, grad_fn=<NegBackward0>) tensor(11492.9834, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11492.572265625
tensor(11492.9834, grad_fn=<NegBackward0>) tensor(11492.5723, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11492.2109375
tensor(11492.5723, grad_fn=<NegBackward0>) tensor(11492.2109, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11492.05078125
tensor(11492.2109, grad_fn=<NegBackward0>) tensor(11492.0508, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11491.9287109375
tensor(11492.0508, grad_fn=<NegBackward0>) tensor(11491.9287, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11491.8203125
tensor(11491.9287, grad_fn=<NegBackward0>) tensor(11491.8203, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11491.6728515625
tensor(11491.8203, grad_fn=<NegBackward0>) tensor(11491.6729, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11491.5888671875
tensor(11491.6729, grad_fn=<NegBackward0>) tensor(11491.5889, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11491.54296875
tensor(11491.5889, grad_fn=<NegBackward0>) tensor(11491.5430, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11491.4814453125
tensor(11491.5430, grad_fn=<NegBackward0>) tensor(11491.4814, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11491.4306640625
tensor(11491.4814, grad_fn=<NegBackward0>) tensor(11491.4307, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11491.3837890625
tensor(11491.4307, grad_fn=<NegBackward0>) tensor(11491.3838, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11491.36328125
tensor(11491.3838, grad_fn=<NegBackward0>) tensor(11491.3633, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11491.3486328125
tensor(11491.3633, grad_fn=<NegBackward0>) tensor(11491.3486, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11491.3359375
tensor(11491.3486, grad_fn=<NegBackward0>) tensor(11491.3359, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11491.3251953125
tensor(11491.3359, grad_fn=<NegBackward0>) tensor(11491.3252, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11491.314453125
tensor(11491.3252, grad_fn=<NegBackward0>) tensor(11491.3145, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11491.306640625
tensor(11491.3145, grad_fn=<NegBackward0>) tensor(11491.3066, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11491.298828125
tensor(11491.3066, grad_fn=<NegBackward0>) tensor(11491.2988, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11491.2939453125
tensor(11491.2988, grad_fn=<NegBackward0>) tensor(11491.2939, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11491.291015625
tensor(11491.2939, grad_fn=<NegBackward0>) tensor(11491.2910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11491.28515625
tensor(11491.2910, grad_fn=<NegBackward0>) tensor(11491.2852, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11491.283203125
tensor(11491.2852, grad_fn=<NegBackward0>) tensor(11491.2832, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11491.28125
tensor(11491.2832, grad_fn=<NegBackward0>) tensor(11491.2812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11491.279296875
tensor(11491.2812, grad_fn=<NegBackward0>) tensor(11491.2793, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11491.2763671875
tensor(11491.2793, grad_fn=<NegBackward0>) tensor(11491.2764, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11491.2744140625
tensor(11491.2764, grad_fn=<NegBackward0>) tensor(11491.2744, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11491.2724609375
tensor(11491.2744, grad_fn=<NegBackward0>) tensor(11491.2725, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11491.2705078125
tensor(11491.2725, grad_fn=<NegBackward0>) tensor(11491.2705, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11491.26953125
tensor(11491.2705, grad_fn=<NegBackward0>) tensor(11491.2695, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11491.2666015625
tensor(11491.2695, grad_fn=<NegBackward0>) tensor(11491.2666, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11491.2685546875
tensor(11491.2666, grad_fn=<NegBackward0>) tensor(11491.2686, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11491.265625
tensor(11491.2666, grad_fn=<NegBackward0>) tensor(11491.2656, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11491.2646484375
tensor(11491.2656, grad_fn=<NegBackward0>) tensor(11491.2646, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11491.263671875
tensor(11491.2646, grad_fn=<NegBackward0>) tensor(11491.2637, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11491.2626953125
tensor(11491.2637, grad_fn=<NegBackward0>) tensor(11491.2627, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11491.26171875
tensor(11491.2627, grad_fn=<NegBackward0>) tensor(11491.2617, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11491.2607421875
tensor(11491.2617, grad_fn=<NegBackward0>) tensor(11491.2607, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11491.259765625
tensor(11491.2607, grad_fn=<NegBackward0>) tensor(11491.2598, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11491.2587890625
tensor(11491.2598, grad_fn=<NegBackward0>) tensor(11491.2588, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11491.2587890625
tensor(11491.2588, grad_fn=<NegBackward0>) tensor(11491.2588, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11491.2587890625
tensor(11491.2588, grad_fn=<NegBackward0>) tensor(11491.2588, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11491.2587890625
tensor(11491.2588, grad_fn=<NegBackward0>) tensor(11491.2588, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11491.2568359375
tensor(11491.2588, grad_fn=<NegBackward0>) tensor(11491.2568, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11491.255859375
tensor(11491.2568, grad_fn=<NegBackward0>) tensor(11491.2559, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11491.2578125
tensor(11491.2559, grad_fn=<NegBackward0>) tensor(11491.2578, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11491.2568359375
tensor(11491.2559, grad_fn=<NegBackward0>) tensor(11491.2568, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11491.2568359375
tensor(11491.2559, grad_fn=<NegBackward0>) tensor(11491.2568, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11491.25390625
tensor(11491.2559, grad_fn=<NegBackward0>) tensor(11491.2539, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11491.255859375
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2559, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11491.25390625
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2539, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11491.2548828125
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2549, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11491.25390625
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2539, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11491.2578125
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2578, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11491.2529296875
tensor(11491.2539, grad_fn=<NegBackward0>) tensor(11491.2529, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11491.25390625
tensor(11491.2529, grad_fn=<NegBackward0>) tensor(11491.2539, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11491.25390625
tensor(11491.2529, grad_fn=<NegBackward0>) tensor(11491.2539, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11491.251953125
tensor(11491.2529, grad_fn=<NegBackward0>) tensor(11491.2520, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11491.2529296875
tensor(11491.2520, grad_fn=<NegBackward0>) tensor(11491.2529, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11491.251953125
tensor(11491.2520, grad_fn=<NegBackward0>) tensor(11491.2520, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11491.2509765625
tensor(11491.2520, grad_fn=<NegBackward0>) tensor(11491.2510, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11491.2529296875
tensor(11491.2510, grad_fn=<NegBackward0>) tensor(11491.2529, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11491.251953125
tensor(11491.2510, grad_fn=<NegBackward0>) tensor(11491.2520, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11491.25
tensor(11491.2510, grad_fn=<NegBackward0>) tensor(11491.2500, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11491.25
tensor(11491.2500, grad_fn=<NegBackward0>) tensor(11491.2500, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11491.2490234375
tensor(11491.2500, grad_fn=<NegBackward0>) tensor(11491.2490, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11491.2509765625
tensor(11491.2490, grad_fn=<NegBackward0>) tensor(11491.2510, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11491.251953125
tensor(11491.2490, grad_fn=<NegBackward0>) tensor(11491.2520, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11491.25
tensor(11491.2490, grad_fn=<NegBackward0>) tensor(11491.2500, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11491.25
tensor(11491.2490, grad_fn=<NegBackward0>) tensor(11491.2500, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11491.2509765625
tensor(11491.2490, grad_fn=<NegBackward0>) tensor(11491.2510, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[5.8562e-02, 9.4144e-01],
        [9.9999e-01, 5.5037e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.3411e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1686, 0.1798],
         [0.7256, 0.1735]],

        [[0.6897, 0.1538],
         [0.5571, 0.7240]],

        [[0.5523, 0.2215],
         [0.6320, 0.6258]],

        [[0.6171, 0.2487],
         [0.5475, 0.6660]],

        [[0.6563, 0.1991],
         [0.7162, 0.5510]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
Global Adjusted Rand Index: 0.0011939897125673405
Average Adjusted Rand Index: 0.000598601924438702
[0.9446731668894559, 0.0011939897125673405] [0.9446425197695675, 0.000598601924438702] [11192.654296875, 11491.2509765625]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11058.685463531672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22850.9453125
inf tensor(22850.9453, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11293.3955078125
tensor(22850.9453, grad_fn=<NegBackward0>) tensor(11293.3955, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11292.359375
tensor(11293.3955, grad_fn=<NegBackward0>) tensor(11292.3594, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11291.5390625
tensor(11292.3594, grad_fn=<NegBackward0>) tensor(11291.5391, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11289.974609375
tensor(11291.5391, grad_fn=<NegBackward0>) tensor(11289.9746, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11278.6484375
tensor(11289.9746, grad_fn=<NegBackward0>) tensor(11278.6484, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11238.6826171875
tensor(11278.6484, grad_fn=<NegBackward0>) tensor(11238.6826, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11238.4033203125
tensor(11238.6826, grad_fn=<NegBackward0>) tensor(11238.4033, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11238.2685546875
tensor(11238.4033, grad_fn=<NegBackward0>) tensor(11238.2686, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11238.19140625
tensor(11238.2686, grad_fn=<NegBackward0>) tensor(11238.1914, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11238.138671875
tensor(11238.1914, grad_fn=<NegBackward0>) tensor(11238.1387, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11238.09765625
tensor(11238.1387, grad_fn=<NegBackward0>) tensor(11238.0977, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11238.0576171875
tensor(11238.0977, grad_fn=<NegBackward0>) tensor(11238.0576, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11238.0166015625
tensor(11238.0576, grad_fn=<NegBackward0>) tensor(11238.0166, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11237.9638671875
tensor(11238.0166, grad_fn=<NegBackward0>) tensor(11237.9639, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11237.84765625
tensor(11237.9639, grad_fn=<NegBackward0>) tensor(11237.8477, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11237.505859375
tensor(11237.8477, grad_fn=<NegBackward0>) tensor(11237.5059, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11237.0625
tensor(11237.5059, grad_fn=<NegBackward0>) tensor(11237.0625, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11236.8818359375
tensor(11237.0625, grad_fn=<NegBackward0>) tensor(11236.8818, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11236.7919921875
tensor(11236.8818, grad_fn=<NegBackward0>) tensor(11236.7920, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11236.716796875
tensor(11236.7920, grad_fn=<NegBackward0>) tensor(11236.7168, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11236.6494140625
tensor(11236.7168, grad_fn=<NegBackward0>) tensor(11236.6494, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11236.611328125
tensor(11236.6494, grad_fn=<NegBackward0>) tensor(11236.6113, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11236.5869140625
tensor(11236.6113, grad_fn=<NegBackward0>) tensor(11236.5869, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11236.5693359375
tensor(11236.5869, grad_fn=<NegBackward0>) tensor(11236.5693, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11236.55859375
tensor(11236.5693, grad_fn=<NegBackward0>) tensor(11236.5586, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11236.546875
tensor(11236.5586, grad_fn=<NegBackward0>) tensor(11236.5469, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11236.5361328125
tensor(11236.5469, grad_fn=<NegBackward0>) tensor(11236.5361, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11236.5283203125
tensor(11236.5361, grad_fn=<NegBackward0>) tensor(11236.5283, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11236.51953125
tensor(11236.5283, grad_fn=<NegBackward0>) tensor(11236.5195, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11236.513671875
tensor(11236.5195, grad_fn=<NegBackward0>) tensor(11236.5137, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11236.5078125
tensor(11236.5137, grad_fn=<NegBackward0>) tensor(11236.5078, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11236.5048828125
tensor(11236.5078, grad_fn=<NegBackward0>) tensor(11236.5049, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11236.5009765625
tensor(11236.5049, grad_fn=<NegBackward0>) tensor(11236.5010, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11236.498046875
tensor(11236.5010, grad_fn=<NegBackward0>) tensor(11236.4980, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11236.49609375
tensor(11236.4980, grad_fn=<NegBackward0>) tensor(11236.4961, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11236.4931640625
tensor(11236.4961, grad_fn=<NegBackward0>) tensor(11236.4932, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11236.4921875
tensor(11236.4932, grad_fn=<NegBackward0>) tensor(11236.4922, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11236.4912109375
tensor(11236.4922, grad_fn=<NegBackward0>) tensor(11236.4912, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11236.494140625
tensor(11236.4912, grad_fn=<NegBackward0>) tensor(11236.4941, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11236.4873046875
tensor(11236.4912, grad_fn=<NegBackward0>) tensor(11236.4873, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11236.48828125
tensor(11236.4873, grad_fn=<NegBackward0>) tensor(11236.4883, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11236.486328125
tensor(11236.4873, grad_fn=<NegBackward0>) tensor(11236.4863, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11236.486328125
tensor(11236.4863, grad_fn=<NegBackward0>) tensor(11236.4863, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11236.4892578125
tensor(11236.4863, grad_fn=<NegBackward0>) tensor(11236.4893, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11236.484375
tensor(11236.4863, grad_fn=<NegBackward0>) tensor(11236.4844, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11236.484375
tensor(11236.4844, grad_fn=<NegBackward0>) tensor(11236.4844, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11236.4833984375
tensor(11236.4844, grad_fn=<NegBackward0>) tensor(11236.4834, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11236.4833984375
tensor(11236.4834, grad_fn=<NegBackward0>) tensor(11236.4834, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11236.482421875
tensor(11236.4834, grad_fn=<NegBackward0>) tensor(11236.4824, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11236.4833984375
tensor(11236.4824, grad_fn=<NegBackward0>) tensor(11236.4834, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11236.4814453125
tensor(11236.4824, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11236.482421875
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4824, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11236.4814453125
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11236.4814453125
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11236.4814453125
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11236.478515625
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11236.4794921875
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11236.48828125
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4883, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11236.48046875
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11236.4794921875
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -11236.4814453125
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[6.4426e-05, 9.9994e-01],
        [2.3190e-02, 9.7681e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4554, 0.5446], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3270, 0.1029],
         [0.5606, 0.1716]],

        [[0.6899, 0.1121],
         [0.6118, 0.6028]],

        [[0.6639, 0.0819],
         [0.5335, 0.7032]],

        [[0.7306, 0.1219],
         [0.5011, 0.5511]],

        [[0.6980, 0.2355],
         [0.5440, 0.7196]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.09523490876302283
Average Adjusted Rand Index: 0.19249372764371
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20027.576171875
inf tensor(20027.5762, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11292.1669921875
tensor(20027.5762, grad_fn=<NegBackward0>) tensor(11292.1670, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11291.4765625
tensor(11292.1670, grad_fn=<NegBackward0>) tensor(11291.4766, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11289.681640625
tensor(11291.4766, grad_fn=<NegBackward0>) tensor(11289.6816, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11288.966796875
tensor(11289.6816, grad_fn=<NegBackward0>) tensor(11288.9668, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11239.9443359375
tensor(11288.9668, grad_fn=<NegBackward0>) tensor(11239.9443, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11238.26171875
tensor(11239.9443, grad_fn=<NegBackward0>) tensor(11238.2617, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11238.1513671875
tensor(11238.2617, grad_fn=<NegBackward0>) tensor(11238.1514, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11238.087890625
tensor(11238.1514, grad_fn=<NegBackward0>) tensor(11238.0879, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11238.0478515625
tensor(11238.0879, grad_fn=<NegBackward0>) tensor(11238.0479, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11238.015625
tensor(11238.0479, grad_fn=<NegBackward0>) tensor(11238.0156, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11237.986328125
tensor(11238.0156, grad_fn=<NegBackward0>) tensor(11237.9863, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11237.95703125
tensor(11237.9863, grad_fn=<NegBackward0>) tensor(11237.9570, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11237.9150390625
tensor(11237.9570, grad_fn=<NegBackward0>) tensor(11237.9150, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11237.8447265625
tensor(11237.9150, grad_fn=<NegBackward0>) tensor(11237.8447, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11237.66796875
tensor(11237.8447, grad_fn=<NegBackward0>) tensor(11237.6680, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11237.1611328125
tensor(11237.6680, grad_fn=<NegBackward0>) tensor(11237.1611, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11236.7802734375
tensor(11237.1611, grad_fn=<NegBackward0>) tensor(11236.7803, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11236.6591796875
tensor(11236.7803, grad_fn=<NegBackward0>) tensor(11236.6592, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11236.611328125
tensor(11236.6592, grad_fn=<NegBackward0>) tensor(11236.6113, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11236.5810546875
tensor(11236.6113, grad_fn=<NegBackward0>) tensor(11236.5811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11236.556640625
tensor(11236.5811, grad_fn=<NegBackward0>) tensor(11236.5566, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11236.54296875
tensor(11236.5566, grad_fn=<NegBackward0>) tensor(11236.5430, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11236.5302734375
tensor(11236.5430, grad_fn=<NegBackward0>) tensor(11236.5303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11236.5224609375
tensor(11236.5303, grad_fn=<NegBackward0>) tensor(11236.5225, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11236.515625
tensor(11236.5225, grad_fn=<NegBackward0>) tensor(11236.5156, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11236.51171875
tensor(11236.5156, grad_fn=<NegBackward0>) tensor(11236.5117, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11236.5048828125
tensor(11236.5117, grad_fn=<NegBackward0>) tensor(11236.5049, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11236.501953125
tensor(11236.5049, grad_fn=<NegBackward0>) tensor(11236.5020, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11236.4990234375
tensor(11236.5020, grad_fn=<NegBackward0>) tensor(11236.4990, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11236.4970703125
tensor(11236.4990, grad_fn=<NegBackward0>) tensor(11236.4971, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11236.4951171875
tensor(11236.4971, grad_fn=<NegBackward0>) tensor(11236.4951, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11236.4921875
tensor(11236.4951, grad_fn=<NegBackward0>) tensor(11236.4922, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11236.4912109375
tensor(11236.4922, grad_fn=<NegBackward0>) tensor(11236.4912, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11236.4921875
tensor(11236.4912, grad_fn=<NegBackward0>) tensor(11236.4922, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11236.4912109375
tensor(11236.4912, grad_fn=<NegBackward0>) tensor(11236.4912, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11236.48828125
tensor(11236.4912, grad_fn=<NegBackward0>) tensor(11236.4883, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11236.48828125
tensor(11236.4883, grad_fn=<NegBackward0>) tensor(11236.4883, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11236.486328125
tensor(11236.4883, grad_fn=<NegBackward0>) tensor(11236.4863, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11236.486328125
tensor(11236.4863, grad_fn=<NegBackward0>) tensor(11236.4863, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11236.4853515625
tensor(11236.4863, grad_fn=<NegBackward0>) tensor(11236.4854, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11236.484375
tensor(11236.4854, grad_fn=<NegBackward0>) tensor(11236.4844, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11236.482421875
tensor(11236.4844, grad_fn=<NegBackward0>) tensor(11236.4824, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11236.4833984375
tensor(11236.4824, grad_fn=<NegBackward0>) tensor(11236.4834, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11236.4814453125
tensor(11236.4824, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11236.4833984375
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4834, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11236.482421875
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4824, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11236.482421875
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4824, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11236.4814453125
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4814, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11236.48046875
tensor(11236.4814, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11236.4794921875
tensor(11236.4805, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11236.48046875
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11236.48046875
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11236.4794921875
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11236.48046875
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11236.4794921875
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11236.478515625
tensor(11236.4795, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11236.484375
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4844, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11236.4775390625
tensor(11236.4785, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11236.4794921875
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4795, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11236.4775390625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11236.478515625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11236.48046875
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11236.478515625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11236.4775390625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11236.4775390625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11236.4775390625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11236.478515625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11236.4765625
tensor(11236.4775, grad_fn=<NegBackward0>) tensor(11236.4766, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11236.4775390625
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11236.4775390625
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11236.4765625
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.4766, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11236.4775390625
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.4775, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11236.5361328125
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.5361, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11236.4755859375
tensor(11236.4766, grad_fn=<NegBackward0>) tensor(11236.4756, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11236.478515625
tensor(11236.4756, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11236.4765625
tensor(11236.4756, grad_fn=<NegBackward0>) tensor(11236.4766, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11236.48046875
tensor(11236.4756, grad_fn=<NegBackward0>) tensor(11236.4805, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11236.474609375
tensor(11236.4756, grad_fn=<NegBackward0>) tensor(11236.4746, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11236.4755859375
tensor(11236.4746, grad_fn=<NegBackward0>) tensor(11236.4756, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11236.4755859375
tensor(11236.4746, grad_fn=<NegBackward0>) tensor(11236.4756, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11236.4765625
tensor(11236.4746, grad_fn=<NegBackward0>) tensor(11236.4766, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11236.478515625
tensor(11236.4746, grad_fn=<NegBackward0>) tensor(11236.4785, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11236.4755859375
tensor(11236.4746, grad_fn=<NegBackward0>) tensor(11236.4756, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[1.9672e-05, 9.9998e-01],
        [2.3207e-02, 9.7679e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4511, 0.5489], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3270, 0.1029],
         [0.6034, 0.1716]],

        [[0.5083, 0.1120],
         [0.6395, 0.5379]],

        [[0.6363, 0.0819],
         [0.5744, 0.6053]],

        [[0.6625, 0.1219],
         [0.6277, 0.5877]],

        [[0.7253, 0.2355],
         [0.6434, 0.5761]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.002468638218550053
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.09523490876302283
Average Adjusted Rand Index: 0.19249372764371
[0.09523490876302283, 0.09523490876302283] [0.19249372764371, 0.19249372764371] [11236.4814453125, 11236.4755859375]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11135.077939037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20549.142578125
inf tensor(20549.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11465.2490234375
tensor(20549.1426, grad_fn=<NegBackward0>) tensor(11465.2490, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11463.056640625
tensor(11465.2490, grad_fn=<NegBackward0>) tensor(11463.0566, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11460.9453125
tensor(11463.0566, grad_fn=<NegBackward0>) tensor(11460.9453, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11453.5224609375
tensor(11460.9453, grad_fn=<NegBackward0>) tensor(11453.5225, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11384.2197265625
tensor(11453.5225, grad_fn=<NegBackward0>) tensor(11384.2197, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11154.7490234375
tensor(11384.2197, grad_fn=<NegBackward0>) tensor(11154.7490, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11123.7646484375
tensor(11154.7490, grad_fn=<NegBackward0>) tensor(11123.7646, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11117.1494140625
tensor(11123.7646, grad_fn=<NegBackward0>) tensor(11117.1494, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11116.904296875
tensor(11117.1494, grad_fn=<NegBackward0>) tensor(11116.9043, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11116.80078125
tensor(11116.9043, grad_fn=<NegBackward0>) tensor(11116.8008, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11116.7333984375
tensor(11116.8008, grad_fn=<NegBackward0>) tensor(11116.7334, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11116.6787109375
tensor(11116.7334, grad_fn=<NegBackward0>) tensor(11116.6787, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11116.64453125
tensor(11116.6787, grad_fn=<NegBackward0>) tensor(11116.6445, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11116.6123046875
tensor(11116.6445, grad_fn=<NegBackward0>) tensor(11116.6123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11116.544921875
tensor(11116.6123, grad_fn=<NegBackward0>) tensor(11116.5449, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11116.5205078125
tensor(11116.5449, grad_fn=<NegBackward0>) tensor(11116.5205, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11116.5068359375
tensor(11116.5205, grad_fn=<NegBackward0>) tensor(11116.5068, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11116.4921875
tensor(11116.5068, grad_fn=<NegBackward0>) tensor(11116.4922, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11116.4697265625
tensor(11116.4922, grad_fn=<NegBackward0>) tensor(11116.4697, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11116.4580078125
tensor(11116.4697, grad_fn=<NegBackward0>) tensor(11116.4580, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11116.453125
tensor(11116.4580, grad_fn=<NegBackward0>) tensor(11116.4531, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11116.4482421875
tensor(11116.4531, grad_fn=<NegBackward0>) tensor(11116.4482, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11116.4423828125
tensor(11116.4482, grad_fn=<NegBackward0>) tensor(11116.4424, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11116.4384765625
tensor(11116.4424, grad_fn=<NegBackward0>) tensor(11116.4385, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11116.43359375
tensor(11116.4385, grad_fn=<NegBackward0>) tensor(11116.4336, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11116.427734375
tensor(11116.4336, grad_fn=<NegBackward0>) tensor(11116.4277, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11116.423828125
tensor(11116.4277, grad_fn=<NegBackward0>) tensor(11116.4238, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11116.4208984375
tensor(11116.4238, grad_fn=<NegBackward0>) tensor(11116.4209, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11116.41796875
tensor(11116.4209, grad_fn=<NegBackward0>) tensor(11116.4180, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11116.4140625
tensor(11116.4180, grad_fn=<NegBackward0>) tensor(11116.4141, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11116.40625
tensor(11116.4141, grad_fn=<NegBackward0>) tensor(11116.4062, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11116.39453125
tensor(11116.4062, grad_fn=<NegBackward0>) tensor(11116.3945, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11116.3916015625
tensor(11116.3945, grad_fn=<NegBackward0>) tensor(11116.3916, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11116.37109375
tensor(11116.3916, grad_fn=<NegBackward0>) tensor(11116.3711, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11116.2138671875
tensor(11116.3711, grad_fn=<NegBackward0>) tensor(11116.2139, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11116.212890625
tensor(11116.2139, grad_fn=<NegBackward0>) tensor(11116.2129, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11116.2119140625
tensor(11116.2129, grad_fn=<NegBackward0>) tensor(11116.2119, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11116.2109375
tensor(11116.2119, grad_fn=<NegBackward0>) tensor(11116.2109, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11116.2099609375
tensor(11116.2109, grad_fn=<NegBackward0>) tensor(11116.2100, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11116.208984375
tensor(11116.2100, grad_fn=<NegBackward0>) tensor(11116.2090, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11116.208984375
tensor(11116.2090, grad_fn=<NegBackward0>) tensor(11116.2090, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11116.208984375
tensor(11116.2090, grad_fn=<NegBackward0>) tensor(11116.2090, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11116.20703125
tensor(11116.2090, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11116.20703125
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11116.2060546875
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2061, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11116.2060546875
tensor(11116.2061, grad_fn=<NegBackward0>) tensor(11116.2061, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11116.205078125
tensor(11116.2061, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11116.205078125
tensor(11116.2051, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11116.205078125
tensor(11116.2051, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11116.205078125
tensor(11116.2051, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11116.2041015625
tensor(11116.2051, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11116.205078125
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11116.203125
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2031, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11116.2041015625
tensor(11116.2031, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11116.2041015625
tensor(11116.2031, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11116.2041015625
tensor(11116.2031, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11116.2041015625
tensor(11116.2031, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11116.2080078125
tensor(11116.2031, grad_fn=<NegBackward0>) tensor(11116.2080, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.7573, 0.2427],
        [0.2371, 0.7629]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5542, 0.4458], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2998, 0.0937],
         [0.5685, 0.2049]],

        [[0.6856, 0.1034],
         [0.6207, 0.5064]],

        [[0.6103, 0.0975],
         [0.6130, 0.7026]],

        [[0.7040, 0.0988],
         [0.6631, 0.6604]],

        [[0.7070, 0.0845],
         [0.6996, 0.5870]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681922570281124
Average Adjusted Rand Index: 0.9681601267189862
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21671.900390625
inf tensor(21671.9004, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11465.400390625
tensor(21671.9004, grad_fn=<NegBackward0>) tensor(11465.4004, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11462.654296875
tensor(11465.4004, grad_fn=<NegBackward0>) tensor(11462.6543, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11458.24609375
tensor(11462.6543, grad_fn=<NegBackward0>) tensor(11458.2461, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11405.107421875
tensor(11458.2461, grad_fn=<NegBackward0>) tensor(11405.1074, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11381.0556640625
tensor(11405.1074, grad_fn=<NegBackward0>) tensor(11381.0557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11378.97265625
tensor(11381.0557, grad_fn=<NegBackward0>) tensor(11378.9727, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11155.953125
tensor(11378.9727, grad_fn=<NegBackward0>) tensor(11155.9531, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11127.5703125
tensor(11155.9531, grad_fn=<NegBackward0>) tensor(11127.5703, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11118.2470703125
tensor(11127.5703, grad_fn=<NegBackward0>) tensor(11118.2471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11117.8232421875
tensor(11118.2471, grad_fn=<NegBackward0>) tensor(11117.8232, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11117.7197265625
tensor(11117.8232, grad_fn=<NegBackward0>) tensor(11117.7197, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11117.6513671875
tensor(11117.7197, grad_fn=<NegBackward0>) tensor(11117.6514, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11117.5966796875
tensor(11117.6514, grad_fn=<NegBackward0>) tensor(11117.5967, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11117.5595703125
tensor(11117.5967, grad_fn=<NegBackward0>) tensor(11117.5596, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11117.529296875
tensor(11117.5596, grad_fn=<NegBackward0>) tensor(11117.5293, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11117.4951171875
tensor(11117.5293, grad_fn=<NegBackward0>) tensor(11117.4951, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11117.396484375
tensor(11117.4951, grad_fn=<NegBackward0>) tensor(11117.3965, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11116.5703125
tensor(11117.3965, grad_fn=<NegBackward0>) tensor(11116.5703, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11116.5556640625
tensor(11116.5703, grad_fn=<NegBackward0>) tensor(11116.5557, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11116.5322265625
tensor(11116.5557, grad_fn=<NegBackward0>) tensor(11116.5322, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11116.521484375
tensor(11116.5322, grad_fn=<NegBackward0>) tensor(11116.5215, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11116.5087890625
tensor(11116.5215, grad_fn=<NegBackward0>) tensor(11116.5088, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11116.4912109375
tensor(11116.5088, grad_fn=<NegBackward0>) tensor(11116.4912, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11116.4794921875
tensor(11116.4912, grad_fn=<NegBackward0>) tensor(11116.4795, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11116.4326171875
tensor(11116.4795, grad_fn=<NegBackward0>) tensor(11116.4326, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11116.4287109375
tensor(11116.4326, grad_fn=<NegBackward0>) tensor(11116.4287, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11116.4248046875
tensor(11116.4287, grad_fn=<NegBackward0>) tensor(11116.4248, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11116.4189453125
tensor(11116.4248, grad_fn=<NegBackward0>) tensor(11116.4189, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11116.4140625
tensor(11116.4189, grad_fn=<NegBackward0>) tensor(11116.4141, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11116.4091796875
tensor(11116.4141, grad_fn=<NegBackward0>) tensor(11116.4092, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11116.408203125
tensor(11116.4092, grad_fn=<NegBackward0>) tensor(11116.4082, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11116.40625
tensor(11116.4082, grad_fn=<NegBackward0>) tensor(11116.4062, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11116.4033203125
tensor(11116.4062, grad_fn=<NegBackward0>) tensor(11116.4033, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11116.4013671875
tensor(11116.4033, grad_fn=<NegBackward0>) tensor(11116.4014, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11116.3994140625
tensor(11116.4014, grad_fn=<NegBackward0>) tensor(11116.3994, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11116.3994140625
tensor(11116.3994, grad_fn=<NegBackward0>) tensor(11116.3994, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11116.3974609375
tensor(11116.3994, grad_fn=<NegBackward0>) tensor(11116.3975, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11116.3974609375
tensor(11116.3975, grad_fn=<NegBackward0>) tensor(11116.3975, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11116.39453125
tensor(11116.3975, grad_fn=<NegBackward0>) tensor(11116.3945, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11116.3935546875
tensor(11116.3945, grad_fn=<NegBackward0>) tensor(11116.3936, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11116.390625
tensor(11116.3936, grad_fn=<NegBackward0>) tensor(11116.3906, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11116.3828125
tensor(11116.3906, grad_fn=<NegBackward0>) tensor(11116.3828, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11116.212890625
tensor(11116.3828, grad_fn=<NegBackward0>) tensor(11116.2129, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11116.21484375
tensor(11116.2129, grad_fn=<NegBackward0>) tensor(11116.2148, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11116.2119140625
tensor(11116.2129, grad_fn=<NegBackward0>) tensor(11116.2119, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11116.2119140625
tensor(11116.2119, grad_fn=<NegBackward0>) tensor(11116.2119, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11116.2099609375
tensor(11116.2119, grad_fn=<NegBackward0>) tensor(11116.2100, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11116.2099609375
tensor(11116.2100, grad_fn=<NegBackward0>) tensor(11116.2100, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11116.2109375
tensor(11116.2100, grad_fn=<NegBackward0>) tensor(11116.2109, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11116.2119140625
tensor(11116.2100, grad_fn=<NegBackward0>) tensor(11116.2119, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11116.2080078125
tensor(11116.2100, grad_fn=<NegBackward0>) tensor(11116.2080, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11116.208984375
tensor(11116.2080, grad_fn=<NegBackward0>) tensor(11116.2090, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11116.2119140625
tensor(11116.2080, grad_fn=<NegBackward0>) tensor(11116.2119, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11116.20703125
tensor(11116.2080, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11116.20703125
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11116.20703125
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11116.20703125
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11116.2041015625
tensor(11116.2070, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11116.2041015625
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11116.205078125
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11116.2060546875
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2061, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11116.2041015625
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2041, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11116.205078125
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2051, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11116.2099609375
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2100, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11116.2021484375
tensor(11116.2041, grad_fn=<NegBackward0>) tensor(11116.2021, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11116.203125
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2031, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11116.208984375
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2090, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11116.203125
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2031, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11116.2021484375
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2021, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11116.21484375
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2148, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11116.2001953125
tensor(11116.2021, grad_fn=<NegBackward0>) tensor(11116.2002, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11116.19921875
tensor(11116.2002, grad_fn=<NegBackward0>) tensor(11116.1992, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11116.2001953125
tensor(11116.1992, grad_fn=<NegBackward0>) tensor(11116.2002, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11116.20703125
tensor(11116.1992, grad_fn=<NegBackward0>) tensor(11116.2070, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11116.203125
tensor(11116.1992, grad_fn=<NegBackward0>) tensor(11116.2031, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11116.2001953125
tensor(11116.1992, grad_fn=<NegBackward0>) tensor(11116.2002, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11116.203125
tensor(11116.1992, grad_fn=<NegBackward0>) tensor(11116.2031, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7640, 0.2360],
        [0.2391, 0.7609]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4467, 0.5533], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2049, 0.0937],
         [0.6407, 0.3001]],

        [[0.5987, 0.1034],
         [0.6123, 0.6784]],

        [[0.6233, 0.0975],
         [0.6159, 0.5208]],

        [[0.5554, 0.0988],
         [0.6297, 0.7278]],

        [[0.5290, 0.0845],
         [0.6335, 0.7170]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681922570281124
Average Adjusted Rand Index: 0.9681601267189862
[0.9681922570281124, 0.9681922570281124] [0.9681601267189862, 0.9681601267189862] [11116.2080078125, 11116.203125]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11418.38453861485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19644.650390625
inf tensor(19644.6504, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11769.0224609375
tensor(19644.6504, grad_fn=<NegBackward0>) tensor(11769.0225, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11762.6181640625
tensor(11769.0225, grad_fn=<NegBackward0>) tensor(11762.6182, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11733.1689453125
tensor(11762.6182, grad_fn=<NegBackward0>) tensor(11733.1689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11493.7255859375
tensor(11733.1689, grad_fn=<NegBackward0>) tensor(11493.7256, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11456.478515625
tensor(11493.7256, grad_fn=<NegBackward0>) tensor(11456.4785, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11433.5703125
tensor(11456.4785, grad_fn=<NegBackward0>) tensor(11433.5703, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11419.1904296875
tensor(11433.5703, grad_fn=<NegBackward0>) tensor(11419.1904, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11405.8203125
tensor(11419.1904, grad_fn=<NegBackward0>) tensor(11405.8203, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11405.7255859375
tensor(11405.8203, grad_fn=<NegBackward0>) tensor(11405.7256, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11405.6806640625
tensor(11405.7256, grad_fn=<NegBackward0>) tensor(11405.6807, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11405.6337890625
tensor(11405.6807, grad_fn=<NegBackward0>) tensor(11405.6338, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11405.599609375
tensor(11405.6338, grad_fn=<NegBackward0>) tensor(11405.5996, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11404.6962890625
tensor(11405.5996, grad_fn=<NegBackward0>) tensor(11404.6963, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11404.630859375
tensor(11404.6963, grad_fn=<NegBackward0>) tensor(11404.6309, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11404.265625
tensor(11404.6309, grad_fn=<NegBackward0>) tensor(11404.2656, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11404.2529296875
tensor(11404.2656, grad_fn=<NegBackward0>) tensor(11404.2529, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11404.2451171875
tensor(11404.2529, grad_fn=<NegBackward0>) tensor(11404.2451, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11404.240234375
tensor(11404.2451, grad_fn=<NegBackward0>) tensor(11404.2402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11404.2080078125
tensor(11404.2402, grad_fn=<NegBackward0>) tensor(11404.2080, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11403.5556640625
tensor(11404.2080, grad_fn=<NegBackward0>) tensor(11403.5557, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11403.548828125
tensor(11403.5557, grad_fn=<NegBackward0>) tensor(11403.5488, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11403.0458984375
tensor(11403.5488, grad_fn=<NegBackward0>) tensor(11403.0459, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11398.0107421875
tensor(11403.0459, grad_fn=<NegBackward0>) tensor(11398.0107, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11398.0048828125
tensor(11398.0107, grad_fn=<NegBackward0>) tensor(11398.0049, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11397.9140625
tensor(11398.0049, grad_fn=<NegBackward0>) tensor(11397.9141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11397.908203125
tensor(11397.9141, grad_fn=<NegBackward0>) tensor(11397.9082, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11397.9052734375
tensor(11397.9082, grad_fn=<NegBackward0>) tensor(11397.9053, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11397.8984375
tensor(11397.9053, grad_fn=<NegBackward0>) tensor(11397.8984, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11397.89453125
tensor(11397.8984, grad_fn=<NegBackward0>) tensor(11397.8945, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11397.892578125
tensor(11397.8945, grad_fn=<NegBackward0>) tensor(11397.8926, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11397.890625
tensor(11397.8926, grad_fn=<NegBackward0>) tensor(11397.8906, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11397.8935546875
tensor(11397.8906, grad_fn=<NegBackward0>) tensor(11397.8936, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11397.8876953125
tensor(11397.8906, grad_fn=<NegBackward0>) tensor(11397.8877, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11397.88671875
tensor(11397.8877, grad_fn=<NegBackward0>) tensor(11397.8867, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11397.88671875
tensor(11397.8867, grad_fn=<NegBackward0>) tensor(11397.8867, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11397.8857421875
tensor(11397.8867, grad_fn=<NegBackward0>) tensor(11397.8857, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11397.8857421875
tensor(11397.8857, grad_fn=<NegBackward0>) tensor(11397.8857, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11397.884765625
tensor(11397.8857, grad_fn=<NegBackward0>) tensor(11397.8848, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11397.8857421875
tensor(11397.8848, grad_fn=<NegBackward0>) tensor(11397.8857, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11397.8818359375
tensor(11397.8848, grad_fn=<NegBackward0>) tensor(11397.8818, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11397.7919921875
tensor(11397.8818, grad_fn=<NegBackward0>) tensor(11397.7920, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11397.7890625
tensor(11397.7920, grad_fn=<NegBackward0>) tensor(11397.7891, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11397.7939453125
tensor(11397.7891, grad_fn=<NegBackward0>) tensor(11397.7939, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11397.787109375
tensor(11397.7891, grad_fn=<NegBackward0>) tensor(11397.7871, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11397.787109375
tensor(11397.7871, grad_fn=<NegBackward0>) tensor(11397.7871, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11397.787109375
tensor(11397.7871, grad_fn=<NegBackward0>) tensor(11397.7871, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11397.7861328125
tensor(11397.7871, grad_fn=<NegBackward0>) tensor(11397.7861, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11397.7890625
tensor(11397.7861, grad_fn=<NegBackward0>) tensor(11397.7891, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11397.78125
tensor(11397.7861, grad_fn=<NegBackward0>) tensor(11397.7812, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11397.7822265625
tensor(11397.7812, grad_fn=<NegBackward0>) tensor(11397.7822, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11397.7802734375
tensor(11397.7812, grad_fn=<NegBackward0>) tensor(11397.7803, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11397.78125
tensor(11397.7803, grad_fn=<NegBackward0>) tensor(11397.7812, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11397.7802734375
tensor(11397.7803, grad_fn=<NegBackward0>) tensor(11397.7803, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11397.779296875
tensor(11397.7803, grad_fn=<NegBackward0>) tensor(11397.7793, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11397.7734375
tensor(11397.7793, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11397.7744140625
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7744, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11397.775390625
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7754, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11397.7744140625
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7744, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11397.775390625
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7754, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11397.7734375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11397.7724609375
tensor(11397.7734, grad_fn=<NegBackward0>) tensor(11397.7725, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11397.78515625
tensor(11397.7725, grad_fn=<NegBackward0>) tensor(11397.7852, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11397.771484375
tensor(11397.7725, grad_fn=<NegBackward0>) tensor(11397.7715, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11397.875
tensor(11397.7715, grad_fn=<NegBackward0>) tensor(11397.8750, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11397.771484375
tensor(11397.7715, grad_fn=<NegBackward0>) tensor(11397.7715, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11397.8466796875
tensor(11397.7715, grad_fn=<NegBackward0>) tensor(11397.8467, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11397.7724609375
tensor(11397.7715, grad_fn=<NegBackward0>) tensor(11397.7725, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11397.7705078125
tensor(11397.7715, grad_fn=<NegBackward0>) tensor(11397.7705, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11397.771484375
tensor(11397.7705, grad_fn=<NegBackward0>) tensor(11397.7715, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11397.7705078125
tensor(11397.7705, grad_fn=<NegBackward0>) tensor(11397.7705, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11397.7734375
tensor(11397.7705, grad_fn=<NegBackward0>) tensor(11397.7734, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11395.6201171875
tensor(11397.7705, grad_fn=<NegBackward0>) tensor(11395.6201, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11395.6201171875
tensor(11395.6201, grad_fn=<NegBackward0>) tensor(11395.6201, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11395.6142578125
tensor(11395.6201, grad_fn=<NegBackward0>) tensor(11395.6143, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11395.6142578125
tensor(11395.6143, grad_fn=<NegBackward0>) tensor(11395.6143, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11395.615234375
tensor(11395.6143, grad_fn=<NegBackward0>) tensor(11395.6152, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11395.61328125
tensor(11395.6143, grad_fn=<NegBackward0>) tensor(11395.6133, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11395.6455078125
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6455, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11395.61328125
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6133, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11395.62109375
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6211, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11395.6142578125
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6143, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11395.6142578125
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6143, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11395.6162109375
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6162, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11395.6123046875
tensor(11395.6133, grad_fn=<NegBackward0>) tensor(11395.6123, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11395.615234375
tensor(11395.6123, grad_fn=<NegBackward0>) tensor(11395.6152, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11395.61328125
tensor(11395.6123, grad_fn=<NegBackward0>) tensor(11395.6133, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11395.61328125
tensor(11395.6123, grad_fn=<NegBackward0>) tensor(11395.6133, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11395.6083984375
tensor(11395.6123, grad_fn=<NegBackward0>) tensor(11395.6084, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11395.5400390625
tensor(11395.6084, grad_fn=<NegBackward0>) tensor(11395.5400, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11395.541015625
tensor(11395.5400, grad_fn=<NegBackward0>) tensor(11395.5410, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11395.564453125
tensor(11395.5400, grad_fn=<NegBackward0>) tensor(11395.5645, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11395.541015625
tensor(11395.5400, grad_fn=<NegBackward0>) tensor(11395.5410, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11395.6201171875
tensor(11395.5400, grad_fn=<NegBackward0>) tensor(11395.6201, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7344, 0.2656],
        [0.2396, 0.7604]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4381, 0.5619], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.1014],
         [0.7188, 0.3070]],

        [[0.5257, 0.0981],
         [0.6982, 0.6375]],

        [[0.6470, 0.0998],
         [0.6668, 0.6006]],

        [[0.6612, 0.1059],
         [0.5525, 0.6912]],

        [[0.5699, 0.0997],
         [0.6483, 0.5081]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8820944099269067
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9291516675203715
Average Adjusted Rand Index: 0.9287388482650847
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23623.84765625
inf tensor(23623.8477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11768.1318359375
tensor(23623.8477, grad_fn=<NegBackward0>) tensor(11768.1318, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11763.7890625
tensor(11768.1318, grad_fn=<NegBackward0>) tensor(11763.7891, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11761.8681640625
tensor(11763.7891, grad_fn=<NegBackward0>) tensor(11761.8682, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11758.0341796875
tensor(11761.8682, grad_fn=<NegBackward0>) tensor(11758.0342, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11734.7197265625
tensor(11758.0342, grad_fn=<NegBackward0>) tensor(11734.7197, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11525.9140625
tensor(11734.7197, grad_fn=<NegBackward0>) tensor(11525.9141, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11415.48046875
tensor(11525.9141, grad_fn=<NegBackward0>) tensor(11415.4805, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11398.0546875
tensor(11415.4805, grad_fn=<NegBackward0>) tensor(11398.0547, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11397.90625
tensor(11398.0547, grad_fn=<NegBackward0>) tensor(11397.9062, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11397.8466796875
tensor(11397.9062, grad_fn=<NegBackward0>) tensor(11397.8467, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11397.814453125
tensor(11397.8467, grad_fn=<NegBackward0>) tensor(11397.8145, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11397.796875
tensor(11397.8145, grad_fn=<NegBackward0>) tensor(11397.7969, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11397.78125
tensor(11397.7969, grad_fn=<NegBackward0>) tensor(11397.7812, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11397.7705078125
tensor(11397.7812, grad_fn=<NegBackward0>) tensor(11397.7705, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11397.75
tensor(11397.7705, grad_fn=<NegBackward0>) tensor(11397.7500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11397.73828125
tensor(11397.7500, grad_fn=<NegBackward0>) tensor(11397.7383, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11397.7294921875
tensor(11397.7383, grad_fn=<NegBackward0>) tensor(11397.7295, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11397.7158203125
tensor(11397.7295, grad_fn=<NegBackward0>) tensor(11397.7158, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11396.7958984375
tensor(11397.7158, grad_fn=<NegBackward0>) tensor(11396.7959, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11396.79296875
tensor(11396.7959, grad_fn=<NegBackward0>) tensor(11396.7930, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11396.7900390625
tensor(11396.7930, grad_fn=<NegBackward0>) tensor(11396.7900, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11396.7861328125
tensor(11396.7900, grad_fn=<NegBackward0>) tensor(11396.7861, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11396.78515625
tensor(11396.7861, grad_fn=<NegBackward0>) tensor(11396.7852, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11396.77734375
tensor(11396.7852, grad_fn=<NegBackward0>) tensor(11396.7773, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11396.76953125
tensor(11396.7773, grad_fn=<NegBackward0>) tensor(11396.7695, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11396.7666015625
tensor(11396.7695, grad_fn=<NegBackward0>) tensor(11396.7666, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11396.703125
tensor(11396.7666, grad_fn=<NegBackward0>) tensor(11396.7031, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11396.638671875
tensor(11396.7031, grad_fn=<NegBackward0>) tensor(11396.6387, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11396.6376953125
tensor(11396.6387, grad_fn=<NegBackward0>) tensor(11396.6377, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11396.6357421875
tensor(11396.6377, grad_fn=<NegBackward0>) tensor(11396.6357, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11396.6357421875
tensor(11396.6357, grad_fn=<NegBackward0>) tensor(11396.6357, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11396.634765625
tensor(11396.6357, grad_fn=<NegBackward0>) tensor(11396.6348, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11396.634765625
tensor(11396.6348, grad_fn=<NegBackward0>) tensor(11396.6348, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11396.634765625
tensor(11396.6348, grad_fn=<NegBackward0>) tensor(11396.6348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11396.6337890625
tensor(11396.6348, grad_fn=<NegBackward0>) tensor(11396.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11396.630859375
tensor(11396.6338, grad_fn=<NegBackward0>) tensor(11396.6309, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11396.630859375
tensor(11396.6309, grad_fn=<NegBackward0>) tensor(11396.6309, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11396.630859375
tensor(11396.6309, grad_fn=<NegBackward0>) tensor(11396.6309, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11396.634765625
tensor(11396.6309, grad_fn=<NegBackward0>) tensor(11396.6348, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11396.6298828125
tensor(11396.6309, grad_fn=<NegBackward0>) tensor(11396.6299, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11396.62890625
tensor(11396.6299, grad_fn=<NegBackward0>) tensor(11396.6289, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11396.62890625
tensor(11396.6289, grad_fn=<NegBackward0>) tensor(11396.6289, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11396.6279296875
tensor(11396.6289, grad_fn=<NegBackward0>) tensor(11396.6279, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11396.6279296875
tensor(11396.6279, grad_fn=<NegBackward0>) tensor(11396.6279, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11396.626953125
tensor(11396.6279, grad_fn=<NegBackward0>) tensor(11396.6270, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11396.6279296875
tensor(11396.6270, grad_fn=<NegBackward0>) tensor(11396.6279, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11396.6259765625
tensor(11396.6270, grad_fn=<NegBackward0>) tensor(11396.6260, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11396.6171875
tensor(11396.6260, grad_fn=<NegBackward0>) tensor(11396.6172, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11396.2685546875
tensor(11396.6172, grad_fn=<NegBackward0>) tensor(11396.2686, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11396.2685546875
tensor(11396.2686, grad_fn=<NegBackward0>) tensor(11396.2686, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11396.2685546875
tensor(11396.2686, grad_fn=<NegBackward0>) tensor(11396.2686, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11396.2666015625
tensor(11396.2686, grad_fn=<NegBackward0>) tensor(11396.2666, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11396.2666015625
tensor(11396.2666, grad_fn=<NegBackward0>) tensor(11396.2666, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11396.265625
tensor(11396.2666, grad_fn=<NegBackward0>) tensor(11396.2656, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11396.2939453125
tensor(11396.2656, grad_fn=<NegBackward0>) tensor(11396.2939, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11396.2646484375
tensor(11396.2656, grad_fn=<NegBackward0>) tensor(11396.2646, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11396.271484375
tensor(11396.2646, grad_fn=<NegBackward0>) tensor(11396.2715, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11396.263671875
tensor(11396.2646, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11396.2939453125
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2939, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11396.263671875
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11396.2646484375
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2646, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11396.2646484375
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2646, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11396.263671875
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11396.2646484375
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2646, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11396.263671875
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11396.2646484375
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2646, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11396.2626953125
tensor(11396.2637, grad_fn=<NegBackward0>) tensor(11396.2627, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11396.263671875
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11396.2626953125
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2627, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11396.263671875
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11396.2626953125
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2627, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11396.263671875
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2637, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11396.26171875
tensor(11396.2627, grad_fn=<NegBackward0>) tensor(11396.2617, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11396.2607421875
tensor(11396.2617, grad_fn=<NegBackward0>) tensor(11396.2607, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11396.2607421875
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2607, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11396.26171875
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2617, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11396.26171875
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2617, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11396.26171875
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2617, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11396.26171875
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2617, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11396.287109375
tensor(11396.2607, grad_fn=<NegBackward0>) tensor(11396.2871, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.7327, 0.2673],
        [0.2430, 0.7570]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4432, 0.5568], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2011, 0.1014],
         [0.5512, 0.3065]],

        [[0.7103, 0.0972],
         [0.6946, 0.5149]],

        [[0.7178, 0.0998],
         [0.5591, 0.5934]],

        [[0.5937, 0.1059],
         [0.5082, 0.6151]],

        [[0.7025, 0.0997],
         [0.6575, 0.7292]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8820944099269067
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9291516675203715
Average Adjusted Rand Index: 0.9287388482650847
[0.9291516675203715, 0.9291516675203715] [0.9287388482650847, 0.9287388482650847] [11395.5380859375, 11396.287109375]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11291.21245837653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19746.166015625
inf tensor(19746.1660, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11610.0390625
tensor(19746.1660, grad_fn=<NegBackward0>) tensor(11610.0391, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11606.1533203125
tensor(11610.0391, grad_fn=<NegBackward0>) tensor(11606.1533, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11591.83203125
tensor(11606.1533, grad_fn=<NegBackward0>) tensor(11591.8320, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11369.6201171875
tensor(11591.8320, grad_fn=<NegBackward0>) tensor(11369.6201, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11328.984375
tensor(11369.6201, grad_fn=<NegBackward0>) tensor(11328.9844, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11324.3427734375
tensor(11328.9844, grad_fn=<NegBackward0>) tensor(11324.3428, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11324.1181640625
tensor(11324.3428, grad_fn=<NegBackward0>) tensor(11324.1182, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11315.875
tensor(11324.1182, grad_fn=<NegBackward0>) tensor(11315.8750, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11315.689453125
tensor(11315.8750, grad_fn=<NegBackward0>) tensor(11315.6895, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11315.646484375
tensor(11315.6895, grad_fn=<NegBackward0>) tensor(11315.6465, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11315.5927734375
tensor(11315.6465, grad_fn=<NegBackward0>) tensor(11315.5928, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11315.2626953125
tensor(11315.5928, grad_fn=<NegBackward0>) tensor(11315.2627, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11315.1923828125
tensor(11315.2627, grad_fn=<NegBackward0>) tensor(11315.1924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11314.9423828125
tensor(11315.1924, grad_fn=<NegBackward0>) tensor(11314.9424, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11257.369140625
tensor(11314.9424, grad_fn=<NegBackward0>) tensor(11257.3691, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11256.8720703125
tensor(11257.3691, grad_fn=<NegBackward0>) tensor(11256.8721, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11256.7958984375
tensor(11256.8721, grad_fn=<NegBackward0>) tensor(11256.7959, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11256.7578125
tensor(11256.7959, grad_fn=<NegBackward0>) tensor(11256.7578, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11256.7421875
tensor(11256.7578, grad_fn=<NegBackward0>) tensor(11256.7422, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11256.734375
tensor(11256.7422, grad_fn=<NegBackward0>) tensor(11256.7344, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11256.7138671875
tensor(11256.7344, grad_fn=<NegBackward0>) tensor(11256.7139, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11253.669921875
tensor(11256.7139, grad_fn=<NegBackward0>) tensor(11253.6699, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11253.666015625
tensor(11253.6699, grad_fn=<NegBackward0>) tensor(11253.6660, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11253.66015625
tensor(11253.6660, grad_fn=<NegBackward0>) tensor(11253.6602, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11253.658203125
tensor(11253.6602, grad_fn=<NegBackward0>) tensor(11253.6582, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11253.6552734375
tensor(11253.6582, grad_fn=<NegBackward0>) tensor(11253.6553, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11253.6533203125
tensor(11253.6553, grad_fn=<NegBackward0>) tensor(11253.6533, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11253.65234375
tensor(11253.6533, grad_fn=<NegBackward0>) tensor(11253.6523, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11253.650390625
tensor(11253.6523, grad_fn=<NegBackward0>) tensor(11253.6504, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11253.6494140625
tensor(11253.6504, grad_fn=<NegBackward0>) tensor(11253.6494, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11253.6474609375
tensor(11253.6494, grad_fn=<NegBackward0>) tensor(11253.6475, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11253.6484375
tensor(11253.6475, grad_fn=<NegBackward0>) tensor(11253.6484, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11253.646484375
tensor(11253.6475, grad_fn=<NegBackward0>) tensor(11253.6465, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11253.6455078125
tensor(11253.6465, grad_fn=<NegBackward0>) tensor(11253.6455, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11253.64453125
tensor(11253.6455, grad_fn=<NegBackward0>) tensor(11253.6445, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11253.6435546875
tensor(11253.6445, grad_fn=<NegBackward0>) tensor(11253.6436, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11253.6416015625
tensor(11253.6436, grad_fn=<NegBackward0>) tensor(11253.6416, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11253.64453125
tensor(11253.6416, grad_fn=<NegBackward0>) tensor(11253.6445, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11253.6396484375
tensor(11253.6416, grad_fn=<NegBackward0>) tensor(11253.6396, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11253.638671875
tensor(11253.6396, grad_fn=<NegBackward0>) tensor(11253.6387, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11253.640625
tensor(11253.6387, grad_fn=<NegBackward0>) tensor(11253.6406, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11253.638671875
tensor(11253.6387, grad_fn=<NegBackward0>) tensor(11253.6387, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11253.63671875
tensor(11253.6387, grad_fn=<NegBackward0>) tensor(11253.6367, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11253.6376953125
tensor(11253.6367, grad_fn=<NegBackward0>) tensor(11253.6377, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11253.63671875
tensor(11253.6367, grad_fn=<NegBackward0>) tensor(11253.6367, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11253.634765625
tensor(11253.6367, grad_fn=<NegBackward0>) tensor(11253.6348, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11253.603515625
tensor(11253.6348, grad_fn=<NegBackward0>) tensor(11253.6035, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11253.607421875
tensor(11253.6035, grad_fn=<NegBackward0>) tensor(11253.6074, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11253.599609375
tensor(11253.6035, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11253.599609375
tensor(11253.5996, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11253.5986328125
tensor(11253.5996, grad_fn=<NegBackward0>) tensor(11253.5986, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11253.5986328125
tensor(11253.5986, grad_fn=<NegBackward0>) tensor(11253.5986, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11253.599609375
tensor(11253.5986, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11253.599609375
tensor(11253.5986, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11253.6123046875
tensor(11253.5986, grad_fn=<NegBackward0>) tensor(11253.6123, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11253.59765625
tensor(11253.5986, grad_fn=<NegBackward0>) tensor(11253.5977, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11253.6025390625
tensor(11253.5977, grad_fn=<NegBackward0>) tensor(11253.6025, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11253.59765625
tensor(11253.5977, grad_fn=<NegBackward0>) tensor(11253.5977, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11253.5986328125
tensor(11253.5977, grad_fn=<NegBackward0>) tensor(11253.5986, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11253.59765625
tensor(11253.5977, grad_fn=<NegBackward0>) tensor(11253.5977, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11253.5966796875
tensor(11253.5977, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11253.6064453125
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.6064, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11253.5966796875
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11253.6044921875
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.6045, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11253.5966796875
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11253.599609375
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11253.599609375
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.5996, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11253.6025390625
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.6025, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11253.6015625
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.6016, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11253.59375
tensor(11253.5967, grad_fn=<NegBackward0>) tensor(11253.5938, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11253.591796875
tensor(11253.5938, grad_fn=<NegBackward0>) tensor(11253.5918, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11253.591796875
tensor(11253.5918, grad_fn=<NegBackward0>) tensor(11253.5918, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11253.58984375
tensor(11253.5918, grad_fn=<NegBackward0>) tensor(11253.5898, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11253.669921875
tensor(11253.5898, grad_fn=<NegBackward0>) tensor(11253.6699, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11253.5908203125
tensor(11253.5898, grad_fn=<NegBackward0>) tensor(11253.5908, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11253.591796875
tensor(11253.5898, grad_fn=<NegBackward0>) tensor(11253.5918, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11253.5888671875
tensor(11253.5898, grad_fn=<NegBackward0>) tensor(11253.5889, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11253.5908203125
tensor(11253.5889, grad_fn=<NegBackward0>) tensor(11253.5908, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11253.58984375
tensor(11253.5889, grad_fn=<NegBackward0>) tensor(11253.5898, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11253.64453125
tensor(11253.5889, grad_fn=<NegBackward0>) tensor(11253.6445, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11253.5888671875
tensor(11253.5889, grad_fn=<NegBackward0>) tensor(11253.5889, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11253.587890625
tensor(11253.5889, grad_fn=<NegBackward0>) tensor(11253.5879, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11253.5830078125
tensor(11253.5879, grad_fn=<NegBackward0>) tensor(11253.5830, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11253.5830078125
tensor(11253.5830, grad_fn=<NegBackward0>) tensor(11253.5830, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11253.58203125
tensor(11253.5830, grad_fn=<NegBackward0>) tensor(11253.5820, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11253.58203125
tensor(11253.5820, grad_fn=<NegBackward0>) tensor(11253.5820, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11253.5908203125
tensor(11253.5820, grad_fn=<NegBackward0>) tensor(11253.5908, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11253.5419921875
tensor(11253.5820, grad_fn=<NegBackward0>) tensor(11253.5420, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11253.54296875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5430, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11253.5419921875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5420, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11253.5419921875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5420, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11253.54296875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5430, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11253.5419921875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5420, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11253.54296875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5430, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11253.5419921875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5420, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11253.544921875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5449, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11253.54296875
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5430, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11253.572265625
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5723, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11253.541015625
tensor(11253.5420, grad_fn=<NegBackward0>) tensor(11253.5410, grad_fn=<NegBackward0>)
pi: tensor([[0.7469, 0.2531],
        [0.2836, 0.7164]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4830, 0.5170], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3144, 0.1059],
         [0.5429, 0.1908]],

        [[0.7061, 0.1047],
         [0.7283, 0.6162]],

        [[0.6782, 0.1024],
         [0.6987, 0.5122]],

        [[0.5854, 0.0987],
         [0.7147, 0.5280]],

        [[0.5281, 0.0979],
         [0.5386, 0.5912]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.8833670319262217
Average Adjusted Rand Index: 0.8830555031090028
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21814.634765625
inf tensor(21814.6348, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11613.6708984375
tensor(21814.6348, grad_fn=<NegBackward0>) tensor(11613.6709, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11612.6708984375
tensor(11613.6709, grad_fn=<NegBackward0>) tensor(11612.6709, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11611.39453125
tensor(11612.6709, grad_fn=<NegBackward0>) tensor(11611.3945, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11597.783203125
tensor(11611.3945, grad_fn=<NegBackward0>) tensor(11597.7832, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11400.44921875
tensor(11597.7832, grad_fn=<NegBackward0>) tensor(11400.4492, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11351.4140625
tensor(11400.4492, grad_fn=<NegBackward0>) tensor(11351.4141, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11328.8671875
tensor(11351.4141, grad_fn=<NegBackward0>) tensor(11328.8672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11322.0810546875
tensor(11328.8672, grad_fn=<NegBackward0>) tensor(11322.0811, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11321.84375
tensor(11322.0811, grad_fn=<NegBackward0>) tensor(11321.8438, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11321.7119140625
tensor(11321.8438, grad_fn=<NegBackward0>) tensor(11321.7119, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11321.0078125
tensor(11321.7119, grad_fn=<NegBackward0>) tensor(11321.0078, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11320.8896484375
tensor(11321.0078, grad_fn=<NegBackward0>) tensor(11320.8896, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11320.748046875
tensor(11320.8896, grad_fn=<NegBackward0>) tensor(11320.7480, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11320.7099609375
tensor(11320.7480, grad_fn=<NegBackward0>) tensor(11320.7100, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11320.5849609375
tensor(11320.7100, grad_fn=<NegBackward0>) tensor(11320.5850, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11317.962890625
tensor(11320.5850, grad_fn=<NegBackward0>) tensor(11317.9629, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11317.7822265625
tensor(11317.9629, grad_fn=<NegBackward0>) tensor(11317.7822, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11317.7578125
tensor(11317.7822, grad_fn=<NegBackward0>) tensor(11317.7578, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11317.6181640625
tensor(11317.7578, grad_fn=<NegBackward0>) tensor(11317.6182, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11317.61328125
tensor(11317.6182, grad_fn=<NegBackward0>) tensor(11317.6133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11317.6064453125
tensor(11317.6133, grad_fn=<NegBackward0>) tensor(11317.6064, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11317.599609375
tensor(11317.6064, grad_fn=<NegBackward0>) tensor(11317.5996, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11317.5908203125
tensor(11317.5996, grad_fn=<NegBackward0>) tensor(11317.5908, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11317.5869140625
tensor(11317.5908, grad_fn=<NegBackward0>) tensor(11317.5869, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11317.5830078125
tensor(11317.5869, grad_fn=<NegBackward0>) tensor(11317.5830, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11317.5791015625
tensor(11317.5830, grad_fn=<NegBackward0>) tensor(11317.5791, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11317.5751953125
tensor(11317.5791, grad_fn=<NegBackward0>) tensor(11317.5752, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11317.57421875
tensor(11317.5752, grad_fn=<NegBackward0>) tensor(11317.5742, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11317.572265625
tensor(11317.5742, grad_fn=<NegBackward0>) tensor(11317.5723, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11317.5703125
tensor(11317.5723, grad_fn=<NegBackward0>) tensor(11317.5703, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11317.568359375
tensor(11317.5703, grad_fn=<NegBackward0>) tensor(11317.5684, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11317.564453125
tensor(11317.5684, grad_fn=<NegBackward0>) tensor(11317.5645, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11317.5634765625
tensor(11317.5645, grad_fn=<NegBackward0>) tensor(11317.5635, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11317.5615234375
tensor(11317.5635, grad_fn=<NegBackward0>) tensor(11317.5615, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11317.5595703125
tensor(11317.5615, grad_fn=<NegBackward0>) tensor(11317.5596, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11317.55859375
tensor(11317.5596, grad_fn=<NegBackward0>) tensor(11317.5586, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11317.556640625
tensor(11317.5586, grad_fn=<NegBackward0>) tensor(11317.5566, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11317.55078125
tensor(11317.5566, grad_fn=<NegBackward0>) tensor(11317.5508, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11317.54296875
tensor(11317.5508, grad_fn=<NegBackward0>) tensor(11317.5430, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11317.541015625
tensor(11317.5430, grad_fn=<NegBackward0>) tensor(11317.5410, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11317.5380859375
tensor(11317.5410, grad_fn=<NegBackward0>) tensor(11317.5381, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11317.537109375
tensor(11317.5381, grad_fn=<NegBackward0>) tensor(11317.5371, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11312.6005859375
tensor(11317.5371, grad_fn=<NegBackward0>) tensor(11312.6006, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11312.59375
tensor(11312.6006, grad_fn=<NegBackward0>) tensor(11312.5938, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11312.591796875
tensor(11312.5938, grad_fn=<NegBackward0>) tensor(11312.5918, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11312.591796875
tensor(11312.5918, grad_fn=<NegBackward0>) tensor(11312.5918, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11312.5908203125
tensor(11312.5918, grad_fn=<NegBackward0>) tensor(11312.5908, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11312.58984375
tensor(11312.5908, grad_fn=<NegBackward0>) tensor(11312.5898, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11312.58984375
tensor(11312.5898, grad_fn=<NegBackward0>) tensor(11312.5898, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11312.59765625
tensor(11312.5898, grad_fn=<NegBackward0>) tensor(11312.5977, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11312.5869140625
tensor(11312.5898, grad_fn=<NegBackward0>) tensor(11312.5869, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11312.5888671875
tensor(11312.5869, grad_fn=<NegBackward0>) tensor(11312.5889, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11312.5830078125
tensor(11312.5869, grad_fn=<NegBackward0>) tensor(11312.5830, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11312.5771484375
tensor(11312.5830, grad_fn=<NegBackward0>) tensor(11312.5771, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11312.40625
tensor(11312.5771, grad_fn=<NegBackward0>) tensor(11312.4062, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11254.3876953125
tensor(11312.4062, grad_fn=<NegBackward0>) tensor(11254.3877, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11254.314453125
tensor(11254.3877, grad_fn=<NegBackward0>) tensor(11254.3145, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11254.3095703125
tensor(11254.3145, grad_fn=<NegBackward0>) tensor(11254.3096, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11254.283203125
tensor(11254.3096, grad_fn=<NegBackward0>) tensor(11254.2832, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11254.2705078125
tensor(11254.2832, grad_fn=<NegBackward0>) tensor(11254.2705, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11254.26953125
tensor(11254.2705, grad_fn=<NegBackward0>) tensor(11254.2695, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11254.2685546875
tensor(11254.2695, grad_fn=<NegBackward0>) tensor(11254.2686, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11254.2685546875
tensor(11254.2686, grad_fn=<NegBackward0>) tensor(11254.2686, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11254.267578125
tensor(11254.2686, grad_fn=<NegBackward0>) tensor(11254.2676, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11254.265625
tensor(11254.2676, grad_fn=<NegBackward0>) tensor(11254.2656, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11253.689453125
tensor(11254.2656, grad_fn=<NegBackward0>) tensor(11253.6895, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11253.6865234375
tensor(11253.6895, grad_fn=<NegBackward0>) tensor(11253.6865, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11253.68359375
tensor(11253.6865, grad_fn=<NegBackward0>) tensor(11253.6836, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11253.6796875
tensor(11253.6836, grad_fn=<NegBackward0>) tensor(11253.6797, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11253.67578125
tensor(11253.6797, grad_fn=<NegBackward0>) tensor(11253.6758, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11253.6748046875
tensor(11253.6758, grad_fn=<NegBackward0>) tensor(11253.6748, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11253.67578125
tensor(11253.6748, grad_fn=<NegBackward0>) tensor(11253.6758, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11253.6748046875
tensor(11253.6748, grad_fn=<NegBackward0>) tensor(11253.6748, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11253.6845703125
tensor(11253.6748, grad_fn=<NegBackward0>) tensor(11253.6846, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11253.6279296875
tensor(11253.6748, grad_fn=<NegBackward0>) tensor(11253.6279, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11253.625
tensor(11253.6279, grad_fn=<NegBackward0>) tensor(11253.6250, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11253.626953125
tensor(11253.6250, grad_fn=<NegBackward0>) tensor(11253.6270, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11253.623046875
tensor(11253.6250, grad_fn=<NegBackward0>) tensor(11253.6230, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11253.7529296875
tensor(11253.6230, grad_fn=<NegBackward0>) tensor(11253.7529, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11253.62109375
tensor(11253.6230, grad_fn=<NegBackward0>) tensor(11253.6211, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11253.62109375
tensor(11253.6211, grad_fn=<NegBackward0>) tensor(11253.6211, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11253.6279296875
tensor(11253.6211, grad_fn=<NegBackward0>) tensor(11253.6279, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11253.6669921875
tensor(11253.6211, grad_fn=<NegBackward0>) tensor(11253.6670, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11253.6162109375
tensor(11253.6211, grad_fn=<NegBackward0>) tensor(11253.6162, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11253.609375
tensor(11253.6162, grad_fn=<NegBackward0>) tensor(11253.6094, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11253.6083984375
tensor(11253.6094, grad_fn=<NegBackward0>) tensor(11253.6084, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11253.6083984375
tensor(11253.6084, grad_fn=<NegBackward0>) tensor(11253.6084, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11253.607421875
tensor(11253.6084, grad_fn=<NegBackward0>) tensor(11253.6074, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11253.607421875
tensor(11253.6074, grad_fn=<NegBackward0>) tensor(11253.6074, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11253.60546875
tensor(11253.6074, grad_fn=<NegBackward0>) tensor(11253.6055, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11253.603515625
tensor(11253.6055, grad_fn=<NegBackward0>) tensor(11253.6035, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11253.9111328125
tensor(11253.6035, grad_fn=<NegBackward0>) tensor(11253.9111, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11253.603515625
tensor(11253.6035, grad_fn=<NegBackward0>) tensor(11253.6035, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11253.595703125
tensor(11253.6035, grad_fn=<NegBackward0>) tensor(11253.5957, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11253.5966796875
tensor(11253.5957, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11253.5966796875
tensor(11253.5957, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11253.5966796875
tensor(11253.5957, grad_fn=<NegBackward0>) tensor(11253.5967, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11253.607421875
tensor(11253.5957, grad_fn=<NegBackward0>) tensor(11253.6074, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11253.59765625
tensor(11253.5957, grad_fn=<NegBackward0>) tensor(11253.5977, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7117, 0.2883],
        [0.2482, 0.7518]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1930, 0.1049],
         [0.5862, 0.3114]],

        [[0.5517, 0.1036],
         [0.6498, 0.6754]],

        [[0.6304, 0.1012],
         [0.5341, 0.5826]],

        [[0.5985, 0.0975],
         [0.5094, 0.5315]],

        [[0.5652, 0.0967],
         [0.5844, 0.6752]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.8833670319262217
Average Adjusted Rand Index: 0.8830555031090028
[0.8833670319262217, 0.8833670319262217] [0.8830555031090028, 0.8830555031090028] [11253.916015625, 11253.59765625]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11274.044715388682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22271.61328125
inf tensor(22271.6133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11565.4423828125
tensor(22271.6133, grad_fn=<NegBackward0>) tensor(11565.4424, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11560.5615234375
tensor(11565.4424, grad_fn=<NegBackward0>) tensor(11560.5615, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11546.2880859375
tensor(11560.5615, grad_fn=<NegBackward0>) tensor(11546.2881, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11525.1474609375
tensor(11546.2881, grad_fn=<NegBackward0>) tensor(11525.1475, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11355.0712890625
tensor(11525.1475, grad_fn=<NegBackward0>) tensor(11355.0713, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11287.6611328125
tensor(11355.0713, grad_fn=<NegBackward0>) tensor(11287.6611, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11274.08984375
tensor(11287.6611, grad_fn=<NegBackward0>) tensor(11274.0898, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11268.6552734375
tensor(11274.0898, grad_fn=<NegBackward0>) tensor(11268.6553, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11260.150390625
tensor(11268.6553, grad_fn=<NegBackward0>) tensor(11260.1504, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11259.8583984375
tensor(11260.1504, grad_fn=<NegBackward0>) tensor(11259.8584, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11259.7822265625
tensor(11259.8584, grad_fn=<NegBackward0>) tensor(11259.7822, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11259.7109375
tensor(11259.7822, grad_fn=<NegBackward0>) tensor(11259.7109, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11259.5556640625
tensor(11259.7109, grad_fn=<NegBackward0>) tensor(11259.5557, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11259.4111328125
tensor(11259.5557, grad_fn=<NegBackward0>) tensor(11259.4111, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11259.3310546875
tensor(11259.4111, grad_fn=<NegBackward0>) tensor(11259.3311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11259.310546875
tensor(11259.3311, grad_fn=<NegBackward0>) tensor(11259.3105, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11259.2822265625
tensor(11259.3105, grad_fn=<NegBackward0>) tensor(11259.2822, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11259.0263671875
tensor(11259.2822, grad_fn=<NegBackward0>) tensor(11259.0264, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11257.951171875
tensor(11259.0264, grad_fn=<NegBackward0>) tensor(11257.9512, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11257.33203125
tensor(11257.9512, grad_fn=<NegBackward0>) tensor(11257.3320, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11257.302734375
tensor(11257.3320, grad_fn=<NegBackward0>) tensor(11257.3027, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11257.2939453125
tensor(11257.3027, grad_fn=<NegBackward0>) tensor(11257.2939, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11257.28515625
tensor(11257.2939, grad_fn=<NegBackward0>) tensor(11257.2852, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11257.2529296875
tensor(11257.2852, grad_fn=<NegBackward0>) tensor(11257.2529, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11257.2158203125
tensor(11257.2529, grad_fn=<NegBackward0>) tensor(11257.2158, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11254.7421875
tensor(11257.2158, grad_fn=<NegBackward0>) tensor(11254.7422, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11254.72265625
tensor(11254.7422, grad_fn=<NegBackward0>) tensor(11254.7227, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11254.689453125
tensor(11254.7227, grad_fn=<NegBackward0>) tensor(11254.6895, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11252.05859375
tensor(11254.6895, grad_fn=<NegBackward0>) tensor(11252.0586, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11251.9423828125
tensor(11252.0586, grad_fn=<NegBackward0>) tensor(11251.9424, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11251.9384765625
tensor(11251.9424, grad_fn=<NegBackward0>) tensor(11251.9385, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11251.935546875
tensor(11251.9385, grad_fn=<NegBackward0>) tensor(11251.9355, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11251.9326171875
tensor(11251.9355, grad_fn=<NegBackward0>) tensor(11251.9326, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11251.9306640625
tensor(11251.9326, grad_fn=<NegBackward0>) tensor(11251.9307, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11251.927734375
tensor(11251.9307, grad_fn=<NegBackward0>) tensor(11251.9277, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11251.923828125
tensor(11251.9277, grad_fn=<NegBackward0>) tensor(11251.9238, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11251.900390625
tensor(11251.9238, grad_fn=<NegBackward0>) tensor(11251.9004, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11248.8798828125
tensor(11251.9004, grad_fn=<NegBackward0>) tensor(11248.8799, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11248.876953125
tensor(11248.8799, grad_fn=<NegBackward0>) tensor(11248.8770, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11248.8720703125
tensor(11248.8770, grad_fn=<NegBackward0>) tensor(11248.8721, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11248.87109375
tensor(11248.8721, grad_fn=<NegBackward0>) tensor(11248.8711, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11248.869140625
tensor(11248.8711, grad_fn=<NegBackward0>) tensor(11248.8691, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11248.8671875
tensor(11248.8691, grad_fn=<NegBackward0>) tensor(11248.8672, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11248.8662109375
tensor(11248.8672, grad_fn=<NegBackward0>) tensor(11248.8662, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11248.8642578125
tensor(11248.8662, grad_fn=<NegBackward0>) tensor(11248.8643, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11248.8623046875
tensor(11248.8643, grad_fn=<NegBackward0>) tensor(11248.8623, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11248.8623046875
tensor(11248.8623, grad_fn=<NegBackward0>) tensor(11248.8623, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11248.8583984375
tensor(11248.8623, grad_fn=<NegBackward0>) tensor(11248.8584, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11248.7392578125
tensor(11248.8584, grad_fn=<NegBackward0>) tensor(11248.7393, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11248.7431640625
tensor(11248.7393, grad_fn=<NegBackward0>) tensor(11248.7432, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11248.7421875
tensor(11248.7393, grad_fn=<NegBackward0>) tensor(11248.7422, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11248.736328125
tensor(11248.7393, grad_fn=<NegBackward0>) tensor(11248.7363, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11248.736328125
tensor(11248.7363, grad_fn=<NegBackward0>) tensor(11248.7363, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11248.7353515625
tensor(11248.7363, grad_fn=<NegBackward0>) tensor(11248.7354, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11248.724609375
tensor(11248.7354, grad_fn=<NegBackward0>) tensor(11248.7246, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11246.50390625
tensor(11248.7246, grad_fn=<NegBackward0>) tensor(11246.5039, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11246.44140625
tensor(11246.5039, grad_fn=<NegBackward0>) tensor(11246.4414, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11246.439453125
tensor(11246.4414, grad_fn=<NegBackward0>) tensor(11246.4395, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11246.439453125
tensor(11246.4395, grad_fn=<NegBackward0>) tensor(11246.4395, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11246.447265625
tensor(11246.4395, grad_fn=<NegBackward0>) tensor(11246.4473, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11246.4189453125
tensor(11246.4395, grad_fn=<NegBackward0>) tensor(11246.4189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11246.3818359375
tensor(11246.4189, grad_fn=<NegBackward0>) tensor(11246.3818, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11246.3818359375
tensor(11246.3818, grad_fn=<NegBackward0>) tensor(11246.3818, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11246.3818359375
tensor(11246.3818, grad_fn=<NegBackward0>) tensor(11246.3818, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11246.380859375
tensor(11246.3818, grad_fn=<NegBackward0>) tensor(11246.3809, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11246.3798828125
tensor(11246.3809, grad_fn=<NegBackward0>) tensor(11246.3799, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11246.3798828125
tensor(11246.3799, grad_fn=<NegBackward0>) tensor(11246.3799, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11246.380859375
tensor(11246.3799, grad_fn=<NegBackward0>) tensor(11246.3809, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11246.3798828125
tensor(11246.3799, grad_fn=<NegBackward0>) tensor(11246.3799, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11246.3779296875
tensor(11246.3799, grad_fn=<NegBackward0>) tensor(11246.3779, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11246.37890625
tensor(11246.3779, grad_fn=<NegBackward0>) tensor(11246.3789, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11246.37890625
tensor(11246.3779, grad_fn=<NegBackward0>) tensor(11246.3789, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11246.3759765625
tensor(11246.3779, grad_fn=<NegBackward0>) tensor(11246.3760, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11246.3740234375
tensor(11246.3760, grad_fn=<NegBackward0>) tensor(11246.3740, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11246.376953125
tensor(11246.3740, grad_fn=<NegBackward0>) tensor(11246.3770, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11246.375
tensor(11246.3740, grad_fn=<NegBackward0>) tensor(11246.3750, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11246.3740234375
tensor(11246.3740, grad_fn=<NegBackward0>) tensor(11246.3740, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11246.373046875
tensor(11246.3740, grad_fn=<NegBackward0>) tensor(11246.3730, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11246.3740234375
tensor(11246.3730, grad_fn=<NegBackward0>) tensor(11246.3740, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11246.3720703125
tensor(11246.3730, grad_fn=<NegBackward0>) tensor(11246.3721, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11246.3994140625
tensor(11246.3721, grad_fn=<NegBackward0>) tensor(11246.3994, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11246.373046875
tensor(11246.3721, grad_fn=<NegBackward0>) tensor(11246.3730, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11246.3740234375
tensor(11246.3721, grad_fn=<NegBackward0>) tensor(11246.3740, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11246.373046875
tensor(11246.3721, grad_fn=<NegBackward0>) tensor(11246.3730, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11246.3740234375
tensor(11246.3721, grad_fn=<NegBackward0>) tensor(11246.3740, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7710, 0.2290],
        [0.2482, 0.7518]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4300, 0.5700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.1128],
         [0.5562, 0.2992]],

        [[0.5426, 0.1022],
         [0.6985, 0.5944]],

        [[0.5989, 0.1006],
         [0.5385, 0.6035]],

        [[0.5734, 0.0930],
         [0.5293, 0.6977]],

        [[0.5793, 0.0992],
         [0.6690, 0.5561]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.944673345271342
Average Adjusted Rand Index: 0.944805241719234
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21340.224609375
inf tensor(21340.2246, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11565.8681640625
tensor(21340.2246, grad_fn=<NegBackward0>) tensor(11565.8682, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11560.78515625
tensor(11565.8682, grad_fn=<NegBackward0>) tensor(11560.7852, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11549.69140625
tensor(11560.7852, grad_fn=<NegBackward0>) tensor(11549.6914, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11430.326171875
tensor(11549.6914, grad_fn=<NegBackward0>) tensor(11430.3262, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11268.12890625
tensor(11430.3262, grad_fn=<NegBackward0>) tensor(11268.1289, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11250.9345703125
tensor(11268.1289, grad_fn=<NegBackward0>) tensor(11250.9346, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11250.4755859375
tensor(11250.9346, grad_fn=<NegBackward0>) tensor(11250.4756, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11250.2509765625
tensor(11250.4756, grad_fn=<NegBackward0>) tensor(11250.2510, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11250.0830078125
tensor(11250.2510, grad_fn=<NegBackward0>) tensor(11250.0830, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11250.015625
tensor(11250.0830, grad_fn=<NegBackward0>) tensor(11250.0156, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11249.9150390625
tensor(11250.0156, grad_fn=<NegBackward0>) tensor(11249.9150, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11249.5849609375
tensor(11249.9150, grad_fn=<NegBackward0>) tensor(11249.5850, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11249.546875
tensor(11249.5850, grad_fn=<NegBackward0>) tensor(11249.5469, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11249.43359375
tensor(11249.5469, grad_fn=<NegBackward0>) tensor(11249.4336, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11246.6103515625
tensor(11249.4336, grad_fn=<NegBackward0>) tensor(11246.6104, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11246.583984375
tensor(11246.6104, grad_fn=<NegBackward0>) tensor(11246.5840, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11246.5673828125
tensor(11246.5840, grad_fn=<NegBackward0>) tensor(11246.5674, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11246.5576171875
tensor(11246.5674, grad_fn=<NegBackward0>) tensor(11246.5576, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11246.5517578125
tensor(11246.5576, grad_fn=<NegBackward0>) tensor(11246.5518, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11246.5302734375
tensor(11246.5518, grad_fn=<NegBackward0>) tensor(11246.5303, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11246.521484375
tensor(11246.5303, grad_fn=<NegBackward0>) tensor(11246.5215, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11246.517578125
tensor(11246.5215, grad_fn=<NegBackward0>) tensor(11246.5176, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11246.5126953125
tensor(11246.5176, grad_fn=<NegBackward0>) tensor(11246.5127, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11246.4951171875
tensor(11246.5127, grad_fn=<NegBackward0>) tensor(11246.4951, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11246.2939453125
tensor(11246.4951, grad_fn=<NegBackward0>) tensor(11246.2939, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11246.2880859375
tensor(11246.2939, grad_fn=<NegBackward0>) tensor(11246.2881, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11246.28515625
tensor(11246.2881, grad_fn=<NegBackward0>) tensor(11246.2852, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11246.28125
tensor(11246.2852, grad_fn=<NegBackward0>) tensor(11246.2812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11246.2802734375
tensor(11246.2812, grad_fn=<NegBackward0>) tensor(11246.2803, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11246.2822265625
tensor(11246.2803, grad_fn=<NegBackward0>) tensor(11246.2822, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11246.2783203125
tensor(11246.2803, grad_fn=<NegBackward0>) tensor(11246.2783, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11246.27734375
tensor(11246.2783, grad_fn=<NegBackward0>) tensor(11246.2773, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11246.275390625
tensor(11246.2773, grad_fn=<NegBackward0>) tensor(11246.2754, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11246.275390625
tensor(11246.2754, grad_fn=<NegBackward0>) tensor(11246.2754, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11246.2744140625
tensor(11246.2754, grad_fn=<NegBackward0>) tensor(11246.2744, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11246.2744140625
tensor(11246.2744, grad_fn=<NegBackward0>) tensor(11246.2744, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11246.2734375
tensor(11246.2744, grad_fn=<NegBackward0>) tensor(11246.2734, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11246.2724609375
tensor(11246.2734, grad_fn=<NegBackward0>) tensor(11246.2725, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11246.2724609375
tensor(11246.2725, grad_fn=<NegBackward0>) tensor(11246.2725, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11246.271484375
tensor(11246.2725, grad_fn=<NegBackward0>) tensor(11246.2715, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11246.271484375
tensor(11246.2715, grad_fn=<NegBackward0>) tensor(11246.2715, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11246.2705078125
tensor(11246.2715, grad_fn=<NegBackward0>) tensor(11246.2705, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11246.2763671875
tensor(11246.2705, grad_fn=<NegBackward0>) tensor(11246.2764, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11246.26953125
tensor(11246.2705, grad_fn=<NegBackward0>) tensor(11246.2695, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11246.267578125
tensor(11246.2695, grad_fn=<NegBackward0>) tensor(11246.2676, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11246.259765625
tensor(11246.2676, grad_fn=<NegBackward0>) tensor(11246.2598, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11246.19921875
tensor(11246.2598, grad_fn=<NegBackward0>) tensor(11246.1992, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11246.1982421875
tensor(11246.1992, grad_fn=<NegBackward0>) tensor(11246.1982, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11246.197265625
tensor(11246.1982, grad_fn=<NegBackward0>) tensor(11246.1973, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11246.189453125
tensor(11246.1973, grad_fn=<NegBackward0>) tensor(11246.1895, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11246.177734375
tensor(11246.1895, grad_fn=<NegBackward0>) tensor(11246.1777, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11246.17578125
tensor(11246.1777, grad_fn=<NegBackward0>) tensor(11246.1758, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11246.17578125
tensor(11246.1758, grad_fn=<NegBackward0>) tensor(11246.1758, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11246.1748046875
tensor(11246.1758, grad_fn=<NegBackward0>) tensor(11246.1748, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11246.173828125
tensor(11246.1748, grad_fn=<NegBackward0>) tensor(11246.1738, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11246.173828125
tensor(11246.1738, grad_fn=<NegBackward0>) tensor(11246.1738, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11246.181640625
tensor(11246.1738, grad_fn=<NegBackward0>) tensor(11246.1816, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11246.169921875
tensor(11246.1738, grad_fn=<NegBackward0>) tensor(11246.1699, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11246.1708984375
tensor(11246.1699, grad_fn=<NegBackward0>) tensor(11246.1709, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11246.17578125
tensor(11246.1699, grad_fn=<NegBackward0>) tensor(11246.1758, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11246.1689453125
tensor(11246.1699, grad_fn=<NegBackward0>) tensor(11246.1689, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11246.169921875
tensor(11246.1689, grad_fn=<NegBackward0>) tensor(11246.1699, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11246.1748046875
tensor(11246.1689, grad_fn=<NegBackward0>) tensor(11246.1748, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11246.16796875
tensor(11246.1689, grad_fn=<NegBackward0>) tensor(11246.1680, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11246.16796875
tensor(11246.1680, grad_fn=<NegBackward0>) tensor(11246.1680, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11246.16796875
tensor(11246.1680, grad_fn=<NegBackward0>) tensor(11246.1680, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11246.1669921875
tensor(11246.1680, grad_fn=<NegBackward0>) tensor(11246.1670, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11246.171875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1719, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11246.1669921875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1670, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11246.1904296875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1904, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11246.1669921875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1670, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11246.169921875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1699, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11246.1669921875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1670, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11246.2529296875
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.2529, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11246.1650390625
tensor(11246.1670, grad_fn=<NegBackward0>) tensor(11246.1650, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11246.1875
tensor(11246.1650, grad_fn=<NegBackward0>) tensor(11246.1875, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11246.1640625
tensor(11246.1650, grad_fn=<NegBackward0>) tensor(11246.1641, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11246.1650390625
tensor(11246.1641, grad_fn=<NegBackward0>) tensor(11246.1650, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11246.1640625
tensor(11246.1641, grad_fn=<NegBackward0>) tensor(11246.1641, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11246.1572265625
tensor(11246.1641, grad_fn=<NegBackward0>) tensor(11246.1572, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11246.1572265625
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1572, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11246.1572265625
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1572, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11246.1572265625
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1572, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11246.158203125
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1582, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11246.158203125
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1582, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11246.158203125
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1582, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11246.1591796875
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1592, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11246.1572265625
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1572, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11246.158203125
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1582, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11246.140625
tensor(11246.1572, grad_fn=<NegBackward0>) tensor(11246.1406, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11246.140625
tensor(11246.1406, grad_fn=<NegBackward0>) tensor(11246.1406, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11246.140625
tensor(11246.1406, grad_fn=<NegBackward0>) tensor(11246.1406, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11246.1376953125
tensor(11246.1406, grad_fn=<NegBackward0>) tensor(11246.1377, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11246.1455078125
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1455, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11246.1513671875
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1514, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11246.1630859375
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1631, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11246.142578125
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1426, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11246.1396484375
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1396, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7719, 0.2281],
        [0.2472, 0.7528]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4281, 0.5719], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1125],
         [0.5481, 0.2987]],

        [[0.7146, 0.1021],
         [0.7009, 0.6757]],

        [[0.5922, 0.1006],
         [0.6431, 0.6784]],

        [[0.5046, 0.0929],
         [0.7034, 0.6200]],

        [[0.6513, 0.0991],
         [0.5420, 0.6316]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.944673345271342
Average Adjusted Rand Index: 0.944805241719234
[0.944673345271342, 0.944673345271342] [0.944805241719234, 0.944805241719234] [11246.3740234375, 11246.1396484375]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11385.674947106323
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21585.72265625
inf tensor(21585.7227, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11730.345703125
tensor(21585.7227, grad_fn=<NegBackward0>) tensor(11730.3457, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11724.8681640625
tensor(11730.3457, grad_fn=<NegBackward0>) tensor(11724.8682, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11626.671875
tensor(11724.8682, grad_fn=<NegBackward0>) tensor(11626.6719, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11410.4091796875
tensor(11626.6719, grad_fn=<NegBackward0>) tensor(11410.4092, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11375.638671875
tensor(11410.4092, grad_fn=<NegBackward0>) tensor(11375.6387, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11375.017578125
tensor(11375.6387, grad_fn=<NegBackward0>) tensor(11375.0176, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11369.2685546875
tensor(11375.0176, grad_fn=<NegBackward0>) tensor(11369.2686, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11369.0625
tensor(11369.2686, grad_fn=<NegBackward0>) tensor(11369.0625, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11368.9638671875
tensor(11369.0625, grad_fn=<NegBackward0>) tensor(11368.9639, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11368.798828125
tensor(11368.9639, grad_fn=<NegBackward0>) tensor(11368.7988, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11368.7578125
tensor(11368.7988, grad_fn=<NegBackward0>) tensor(11368.7578, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11368.7314453125
tensor(11368.7578, grad_fn=<NegBackward0>) tensor(11368.7314, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11368.61328125
tensor(11368.7314, grad_fn=<NegBackward0>) tensor(11368.6133, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11367.0888671875
tensor(11368.6133, grad_fn=<NegBackward0>) tensor(11367.0889, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11365.9931640625
tensor(11367.0889, grad_fn=<NegBackward0>) tensor(11365.9932, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11365.8369140625
tensor(11365.9932, grad_fn=<NegBackward0>) tensor(11365.8369, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11365.8271484375
tensor(11365.8369, grad_fn=<NegBackward0>) tensor(11365.8271, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11365.8203125
tensor(11365.8271, grad_fn=<NegBackward0>) tensor(11365.8203, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11365.814453125
tensor(11365.8203, grad_fn=<NegBackward0>) tensor(11365.8145, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11365.8115234375
tensor(11365.8145, grad_fn=<NegBackward0>) tensor(11365.8115, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11365.8076171875
tensor(11365.8115, grad_fn=<NegBackward0>) tensor(11365.8076, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11365.8037109375
tensor(11365.8076, grad_fn=<NegBackward0>) tensor(11365.8037, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11365.7998046875
tensor(11365.8037, grad_fn=<NegBackward0>) tensor(11365.7998, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11365.796875
tensor(11365.7998, grad_fn=<NegBackward0>) tensor(11365.7969, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11365.7939453125
tensor(11365.7969, grad_fn=<NegBackward0>) tensor(11365.7939, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11365.7939453125
tensor(11365.7939, grad_fn=<NegBackward0>) tensor(11365.7939, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11365.083984375
tensor(11365.7939, grad_fn=<NegBackward0>) tensor(11365.0840, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11364.9677734375
tensor(11365.0840, grad_fn=<NegBackward0>) tensor(11364.9678, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11364.9658203125
tensor(11364.9678, grad_fn=<NegBackward0>) tensor(11364.9658, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11364.94921875
tensor(11364.9658, grad_fn=<NegBackward0>) tensor(11364.9492, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11364.939453125
tensor(11364.9492, grad_fn=<NegBackward0>) tensor(11364.9395, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11364.8701171875
tensor(11364.9395, grad_fn=<NegBackward0>) tensor(11364.8701, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11364.869140625
tensor(11364.8701, grad_fn=<NegBackward0>) tensor(11364.8691, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11364.8671875
tensor(11364.8691, grad_fn=<NegBackward0>) tensor(11364.8672, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11364.8642578125
tensor(11364.8672, grad_fn=<NegBackward0>) tensor(11364.8643, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11364.861328125
tensor(11364.8643, grad_fn=<NegBackward0>) tensor(11364.8613, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11364.8603515625
tensor(11364.8613, grad_fn=<NegBackward0>) tensor(11364.8604, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11364.861328125
tensor(11364.8604, grad_fn=<NegBackward0>) tensor(11364.8613, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11364.8544921875
tensor(11364.8604, grad_fn=<NegBackward0>) tensor(11364.8545, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11364.8544921875
tensor(11364.8545, grad_fn=<NegBackward0>) tensor(11364.8545, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11364.8525390625
tensor(11364.8545, grad_fn=<NegBackward0>) tensor(11364.8525, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11364.8544921875
tensor(11364.8525, grad_fn=<NegBackward0>) tensor(11364.8545, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11364.8515625
tensor(11364.8525, grad_fn=<NegBackward0>) tensor(11364.8516, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11364.8544921875
tensor(11364.8516, grad_fn=<NegBackward0>) tensor(11364.8545, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11364.8515625
tensor(11364.8516, grad_fn=<NegBackward0>) tensor(11364.8516, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11364.8515625
tensor(11364.8516, grad_fn=<NegBackward0>) tensor(11364.8516, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11364.8505859375
tensor(11364.8516, grad_fn=<NegBackward0>) tensor(11364.8506, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11364.8505859375
tensor(11364.8506, grad_fn=<NegBackward0>) tensor(11364.8506, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11364.8515625
tensor(11364.8506, grad_fn=<NegBackward0>) tensor(11364.8516, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11364.8486328125
tensor(11364.8506, grad_fn=<NegBackward0>) tensor(11364.8486, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11364.8505859375
tensor(11364.8486, grad_fn=<NegBackward0>) tensor(11364.8506, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11364.8466796875
tensor(11364.8486, grad_fn=<NegBackward0>) tensor(11364.8467, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11364.8681640625
tensor(11364.8467, grad_fn=<NegBackward0>) tensor(11364.8682, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11364.845703125
tensor(11364.8467, grad_fn=<NegBackward0>) tensor(11364.8457, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11364.861328125
tensor(11364.8457, grad_fn=<NegBackward0>) tensor(11364.8613, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11364.8330078125
tensor(11364.8457, grad_fn=<NegBackward0>) tensor(11364.8330, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11364.8310546875
tensor(11364.8330, grad_fn=<NegBackward0>) tensor(11364.8311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11364.8330078125
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8330, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11364.83203125
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8320, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11364.8310546875
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8311, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11364.8310546875
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8311, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11364.8310546875
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8311, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11364.8291015625
tensor(11364.8311, grad_fn=<NegBackward0>) tensor(11364.8291, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11364.7822265625
tensor(11364.8291, grad_fn=<NegBackward0>) tensor(11364.7822, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11364.78515625
tensor(11364.7822, grad_fn=<NegBackward0>) tensor(11364.7852, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11364.78125
tensor(11364.7822, grad_fn=<NegBackward0>) tensor(11364.7812, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11364.7763671875
tensor(11364.7812, grad_fn=<NegBackward0>) tensor(11364.7764, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11364.7890625
tensor(11364.7764, grad_fn=<NegBackward0>) tensor(11364.7891, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11364.7763671875
tensor(11364.7764, grad_fn=<NegBackward0>) tensor(11364.7764, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11364.783203125
tensor(11364.7764, grad_fn=<NegBackward0>) tensor(11364.7832, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11364.78515625
tensor(11364.7764, grad_fn=<NegBackward0>) tensor(11364.7852, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11363.5263671875
tensor(11364.7764, grad_fn=<NegBackward0>) tensor(11363.5264, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11363.298828125
tensor(11363.5264, grad_fn=<NegBackward0>) tensor(11363.2988, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11363.2998046875
tensor(11363.2988, grad_fn=<NegBackward0>) tensor(11363.2998, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11363.2998046875
tensor(11363.2988, grad_fn=<NegBackward0>) tensor(11363.2998, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11363.296875
tensor(11363.2988, grad_fn=<NegBackward0>) tensor(11363.2969, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11363.294921875
tensor(11363.2969, grad_fn=<NegBackward0>) tensor(11363.2949, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11363.294921875
tensor(11363.2949, grad_fn=<NegBackward0>) tensor(11363.2949, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11363.3017578125
tensor(11363.2949, grad_fn=<NegBackward0>) tensor(11363.3018, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11363.2998046875
tensor(11363.2949, grad_fn=<NegBackward0>) tensor(11363.2998, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11363.2939453125
tensor(11363.2949, grad_fn=<NegBackward0>) tensor(11363.2939, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11363.2958984375
tensor(11363.2939, grad_fn=<NegBackward0>) tensor(11363.2959, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11363.29296875
tensor(11363.2939, grad_fn=<NegBackward0>) tensor(11363.2930, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11363.2939453125
tensor(11363.2930, grad_fn=<NegBackward0>) tensor(11363.2939, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11363.2939453125
tensor(11363.2930, grad_fn=<NegBackward0>) tensor(11363.2939, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11363.2919921875
tensor(11363.2930, grad_fn=<NegBackward0>) tensor(11363.2920, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11363.318359375
tensor(11363.2920, grad_fn=<NegBackward0>) tensor(11363.3184, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11363.2900390625
tensor(11363.2920, grad_fn=<NegBackward0>) tensor(11363.2900, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11363.3017578125
tensor(11363.2900, grad_fn=<NegBackward0>) tensor(11363.3018, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11363.2900390625
tensor(11363.2900, grad_fn=<NegBackward0>) tensor(11363.2900, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11363.4677734375
tensor(11363.2900, grad_fn=<NegBackward0>) tensor(11363.4678, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11363.2880859375
tensor(11363.2900, grad_fn=<NegBackward0>) tensor(11363.2881, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11363.29296875
tensor(11363.2881, grad_fn=<NegBackward0>) tensor(11363.2930, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11363.2900390625
tensor(11363.2881, grad_fn=<NegBackward0>) tensor(11363.2900, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11363.2919921875
tensor(11363.2881, grad_fn=<NegBackward0>) tensor(11363.2920, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11363.2890625
tensor(11363.2881, grad_fn=<NegBackward0>) tensor(11363.2891, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11363.287109375
tensor(11363.2881, grad_fn=<NegBackward0>) tensor(11363.2871, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11363.2900390625
tensor(11363.2871, grad_fn=<NegBackward0>) tensor(11363.2900, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11363.333984375
tensor(11363.2871, grad_fn=<NegBackward0>) tensor(11363.3340, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7992, 0.2008],
        [0.1947, 0.8053]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5896, 0.4104], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3032, 0.1045],
         [0.5484, 0.2026]],

        [[0.6698, 0.1044],
         [0.6137, 0.6908]],

        [[0.5329, 0.0954],
         [0.6741, 0.5639]],

        [[0.6530, 0.0993],
         [0.7287, 0.6434]],

        [[0.5365, 0.1068],
         [0.7092, 0.7224]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9681921716431269
Average Adjusted Rand Index: 0.9679880504268226
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21980.51953125
inf tensor(21980.5195, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11729.0869140625
tensor(21980.5195, grad_fn=<NegBackward0>) tensor(11729.0869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11721.9599609375
tensor(11729.0869, grad_fn=<NegBackward0>) tensor(11721.9600, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11644.423828125
tensor(11721.9600, grad_fn=<NegBackward0>) tensor(11644.4238, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11491.65234375
tensor(11644.4238, grad_fn=<NegBackward0>) tensor(11491.6523, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11457.4560546875
tensor(11491.6523, grad_fn=<NegBackward0>) tensor(11457.4561, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11448.974609375
tensor(11457.4561, grad_fn=<NegBackward0>) tensor(11448.9746, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11448.474609375
tensor(11448.9746, grad_fn=<NegBackward0>) tensor(11448.4746, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11448.4072265625
tensor(11448.4746, grad_fn=<NegBackward0>) tensor(11448.4072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11448.369140625
tensor(11448.4072, grad_fn=<NegBackward0>) tensor(11448.3691, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11448.34375
tensor(11448.3691, grad_fn=<NegBackward0>) tensor(11448.3438, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11448.3251953125
tensor(11448.3438, grad_fn=<NegBackward0>) tensor(11448.3252, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11448.314453125
tensor(11448.3252, grad_fn=<NegBackward0>) tensor(11448.3145, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11448.3037109375
tensor(11448.3145, grad_fn=<NegBackward0>) tensor(11448.3037, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11448.2958984375
tensor(11448.3037, grad_fn=<NegBackward0>) tensor(11448.2959, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11448.287109375
tensor(11448.2959, grad_fn=<NegBackward0>) tensor(11448.2871, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11448.28125
tensor(11448.2871, grad_fn=<NegBackward0>) tensor(11448.2812, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11448.2744140625
tensor(11448.2812, grad_fn=<NegBackward0>) tensor(11448.2744, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11447.5478515625
tensor(11448.2744, grad_fn=<NegBackward0>) tensor(11447.5479, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11447.50390625
tensor(11447.5479, grad_fn=<NegBackward0>) tensor(11447.5039, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11447.5
tensor(11447.5039, grad_fn=<NegBackward0>) tensor(11447.5000, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11447.498046875
tensor(11447.5000, grad_fn=<NegBackward0>) tensor(11447.4980, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11447.49609375
tensor(11447.4980, grad_fn=<NegBackward0>) tensor(11447.4961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11447.494140625
tensor(11447.4961, grad_fn=<NegBackward0>) tensor(11447.4941, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11447.494140625
tensor(11447.4941, grad_fn=<NegBackward0>) tensor(11447.4941, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11447.490234375
tensor(11447.4941, grad_fn=<NegBackward0>) tensor(11447.4902, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11447.490234375
tensor(11447.4902, grad_fn=<NegBackward0>) tensor(11447.4902, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11447.48828125
tensor(11447.4902, grad_fn=<NegBackward0>) tensor(11447.4883, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11447.486328125
tensor(11447.4883, grad_fn=<NegBackward0>) tensor(11447.4863, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11447.484375
tensor(11447.4863, grad_fn=<NegBackward0>) tensor(11447.4844, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11447.4833984375
tensor(11447.4844, grad_fn=<NegBackward0>) tensor(11447.4834, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11447.4814453125
tensor(11447.4834, grad_fn=<NegBackward0>) tensor(11447.4814, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11447.4794921875
tensor(11447.4814, grad_fn=<NegBackward0>) tensor(11447.4795, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11447.4794921875
tensor(11447.4795, grad_fn=<NegBackward0>) tensor(11447.4795, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11447.478515625
tensor(11447.4795, grad_fn=<NegBackward0>) tensor(11447.4785, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11447.478515625
tensor(11447.4785, grad_fn=<NegBackward0>) tensor(11447.4785, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11447.4775390625
tensor(11447.4785, grad_fn=<NegBackward0>) tensor(11447.4775, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11447.4775390625
tensor(11447.4775, grad_fn=<NegBackward0>) tensor(11447.4775, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11447.4755859375
tensor(11447.4775, grad_fn=<NegBackward0>) tensor(11447.4756, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11447.474609375
tensor(11447.4756, grad_fn=<NegBackward0>) tensor(11447.4746, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11447.4736328125
tensor(11447.4746, grad_fn=<NegBackward0>) tensor(11447.4736, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11447.470703125
tensor(11447.4736, grad_fn=<NegBackward0>) tensor(11447.4707, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11447.46875
tensor(11447.4707, grad_fn=<NegBackward0>) tensor(11447.4688, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11447.4658203125
tensor(11447.4688, grad_fn=<NegBackward0>) tensor(11447.4658, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11447.462890625
tensor(11447.4658, grad_fn=<NegBackward0>) tensor(11447.4629, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11447.4619140625
tensor(11447.4629, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11447.466796875
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4668, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11447.462890625
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4629, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11447.4619140625
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11447.4619140625
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11447.4619140625
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11447.4609375
tensor(11447.4619, grad_fn=<NegBackward0>) tensor(11447.4609, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11447.4609375
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4609, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11447.4619140625
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11447.466796875
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4668, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11447.4619140625
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4619, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11447.4609375
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4609, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11447.4599609375
tensor(11447.4609, grad_fn=<NegBackward0>) tensor(11447.4600, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11447.458984375
tensor(11447.4600, grad_fn=<NegBackward0>) tensor(11447.4590, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11447.4638671875
tensor(11447.4590, grad_fn=<NegBackward0>) tensor(11447.4639, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11447.4599609375
tensor(11447.4590, grad_fn=<NegBackward0>) tensor(11447.4600, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11447.458984375
tensor(11447.4590, grad_fn=<NegBackward0>) tensor(11447.4590, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11447.458984375
tensor(11447.4590, grad_fn=<NegBackward0>) tensor(11447.4590, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11447.4560546875
tensor(11447.4590, grad_fn=<NegBackward0>) tensor(11447.4561, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11447.4501953125
tensor(11447.4561, grad_fn=<NegBackward0>) tensor(11447.4502, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11447.4580078125
tensor(11447.4502, grad_fn=<NegBackward0>) tensor(11447.4580, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11447.44921875
tensor(11447.4502, grad_fn=<NegBackward0>) tensor(11447.4492, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11447.447265625
tensor(11447.4492, grad_fn=<NegBackward0>) tensor(11447.4473, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11447.4482421875
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4482, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11447.4482421875
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4482, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11447.447265625
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4473, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11447.4521484375
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4521, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11447.5244140625
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.5244, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11447.453125
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4531, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11447.4482421875
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4482, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11447.4501953125
tensor(11447.4473, grad_fn=<NegBackward0>) tensor(11447.4502, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7708, 0.2292],
        [0.3488, 0.6512]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4104, 0.5896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2125, 0.1042],
         [0.6400, 0.3003]],

        [[0.7015, 0.1047],
         [0.5321, 0.6397]],

        [[0.6499, 0.0948],
         [0.7225, 0.5186]],

        [[0.6221, 0.1006],
         [0.5952, 0.5356]],

        [[0.5085, 0.1028],
         [0.5373, 0.6928]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 26
Adjusted Rand Index: 0.22444941185092895
Global Adjusted Rand Index: 0.472289661020367
Average Adjusted Rand Index: 0.8130403112598147
[0.9681921716431269, 0.472289661020367] [0.9679880504268226, 0.8130403112598147] [11363.2900390625, 11447.4501953125]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11014.163753394521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22054.83984375
inf tensor(22054.8398, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11276.962890625
tensor(22054.8398, grad_fn=<NegBackward0>) tensor(11276.9629, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11276.1767578125
tensor(11276.9629, grad_fn=<NegBackward0>) tensor(11276.1768, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11275.955078125
tensor(11276.1768, grad_fn=<NegBackward0>) tensor(11275.9551, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11275.7353515625
tensor(11275.9551, grad_fn=<NegBackward0>) tensor(11275.7354, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11275.2509765625
tensor(11275.7354, grad_fn=<NegBackward0>) tensor(11275.2510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11274.4697265625
tensor(11275.2510, grad_fn=<NegBackward0>) tensor(11274.4697, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11273.77734375
tensor(11274.4697, grad_fn=<NegBackward0>) tensor(11273.7773, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11272.890625
tensor(11273.7773, grad_fn=<NegBackward0>) tensor(11272.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11272.373046875
tensor(11272.8906, grad_fn=<NegBackward0>) tensor(11272.3730, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11272.04296875
tensor(11272.3730, grad_fn=<NegBackward0>) tensor(11272.0430, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11271.873046875
tensor(11272.0430, grad_fn=<NegBackward0>) tensor(11271.8730, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11271.724609375
tensor(11271.8730, grad_fn=<NegBackward0>) tensor(11271.7246, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11271.611328125
tensor(11271.7246, grad_fn=<NegBackward0>) tensor(11271.6113, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11271.51171875
tensor(11271.6113, grad_fn=<NegBackward0>) tensor(11271.5117, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11271.4130859375
tensor(11271.5117, grad_fn=<NegBackward0>) tensor(11271.4131, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11271.2490234375
tensor(11271.4131, grad_fn=<NegBackward0>) tensor(11271.2490, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11267.0732421875
tensor(11271.2490, grad_fn=<NegBackward0>) tensor(11267.0732, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11012.740234375
tensor(11267.0732, grad_fn=<NegBackward0>) tensor(11012.7402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10998.75390625
tensor(11012.7402, grad_fn=<NegBackward0>) tensor(10998.7539, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10993.302734375
tensor(10998.7539, grad_fn=<NegBackward0>) tensor(10993.3027, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10993.0478515625
tensor(10993.3027, grad_fn=<NegBackward0>) tensor(10993.0479, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10993.005859375
tensor(10993.0479, grad_fn=<NegBackward0>) tensor(10993.0059, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10992.974609375
tensor(10993.0059, grad_fn=<NegBackward0>) tensor(10992.9746, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10992.94140625
tensor(10992.9746, grad_fn=<NegBackward0>) tensor(10992.9414, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10992.9248046875
tensor(10992.9414, grad_fn=<NegBackward0>) tensor(10992.9248, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10992.9091796875
tensor(10992.9248, grad_fn=<NegBackward0>) tensor(10992.9092, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10992.880859375
tensor(10992.9092, grad_fn=<NegBackward0>) tensor(10992.8809, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10992.8701171875
tensor(10992.8809, grad_fn=<NegBackward0>) tensor(10992.8701, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10992.83203125
tensor(10992.8701, grad_fn=<NegBackward0>) tensor(10992.8320, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10991.5595703125
tensor(10992.8320, grad_fn=<NegBackward0>) tensor(10991.5596, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10991.5556640625
tensor(10991.5596, grad_fn=<NegBackward0>) tensor(10991.5557, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10991.5498046875
tensor(10991.5557, grad_fn=<NegBackward0>) tensor(10991.5498, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10991.544921875
tensor(10991.5498, grad_fn=<NegBackward0>) tensor(10991.5449, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10991.537109375
tensor(10991.5449, grad_fn=<NegBackward0>) tensor(10991.5371, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10991.53125
tensor(10991.5371, grad_fn=<NegBackward0>) tensor(10991.5312, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10991.5302734375
tensor(10991.5312, grad_fn=<NegBackward0>) tensor(10991.5303, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10991.52734375
tensor(10991.5303, grad_fn=<NegBackward0>) tensor(10991.5273, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10991.52734375
tensor(10991.5273, grad_fn=<NegBackward0>) tensor(10991.5273, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10991.525390625
tensor(10991.5273, grad_fn=<NegBackward0>) tensor(10991.5254, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10991.525390625
tensor(10991.5254, grad_fn=<NegBackward0>) tensor(10991.5254, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10991.525390625
tensor(10991.5254, grad_fn=<NegBackward0>) tensor(10991.5254, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10991.5244140625
tensor(10991.5254, grad_fn=<NegBackward0>) tensor(10991.5244, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10991.5224609375
tensor(10991.5244, grad_fn=<NegBackward0>) tensor(10991.5225, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10991.5234375
tensor(10991.5225, grad_fn=<NegBackward0>) tensor(10991.5234, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10991.5205078125
tensor(10991.5225, grad_fn=<NegBackward0>) tensor(10991.5205, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10991.51953125
tensor(10991.5205, grad_fn=<NegBackward0>) tensor(10991.5195, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10991.517578125
tensor(10991.5195, grad_fn=<NegBackward0>) tensor(10991.5176, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10991.5185546875
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5186, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10991.5205078125
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5205, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10991.5166015625
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5166, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10991.517578125
tensor(10991.5166, grad_fn=<NegBackward0>) tensor(10991.5176, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10991.515625
tensor(10991.5166, grad_fn=<NegBackward0>) tensor(10991.5156, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10991.513671875
tensor(10991.5156, grad_fn=<NegBackward0>) tensor(10991.5137, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10991.5126953125
tensor(10991.5137, grad_fn=<NegBackward0>) tensor(10991.5127, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10991.5126953125
tensor(10991.5127, grad_fn=<NegBackward0>) tensor(10991.5127, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10991.51953125
tensor(10991.5127, grad_fn=<NegBackward0>) tensor(10991.5195, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10991.513671875
tensor(10991.5127, grad_fn=<NegBackward0>) tensor(10991.5137, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10991.51171875
tensor(10991.5127, grad_fn=<NegBackward0>) tensor(10991.5117, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10991.5107421875
tensor(10991.5117, grad_fn=<NegBackward0>) tensor(10991.5107, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10991.5126953125
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5127, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10991.5107421875
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5107, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10991.51171875
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5117, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10991.515625
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5156, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10991.5107421875
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5107, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10991.509765625
tensor(10991.5107, grad_fn=<NegBackward0>) tensor(10991.5098, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10991.4853515625
tensor(10991.5098, grad_fn=<NegBackward0>) tensor(10991.4854, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10991.48046875
tensor(10991.4854, grad_fn=<NegBackward0>) tensor(10991.4805, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10991.4794921875
tensor(10991.4805, grad_fn=<NegBackward0>) tensor(10991.4795, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10991.48046875
tensor(10991.4795, grad_fn=<NegBackward0>) tensor(10991.4805, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10991.48046875
tensor(10991.4795, grad_fn=<NegBackward0>) tensor(10991.4805, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10991.4775390625
tensor(10991.4795, grad_fn=<NegBackward0>) tensor(10991.4775, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10991.4814453125
tensor(10991.4775, grad_fn=<NegBackward0>) tensor(10991.4814, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10991.4765625
tensor(10991.4775, grad_fn=<NegBackward0>) tensor(10991.4766, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10991.4775390625
tensor(10991.4766, grad_fn=<NegBackward0>) tensor(10991.4775, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10991.4765625
tensor(10991.4766, grad_fn=<NegBackward0>) tensor(10991.4766, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10991.4755859375
tensor(10991.4766, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10991.4755859375
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10991.482421875
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10991.4775390625
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4775, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10991.4765625
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4766, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10991.5009765625
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.5010, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10991.4755859375
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10991.474609375
tensor(10991.4756, grad_fn=<NegBackward0>) tensor(10991.4746, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10991.4755859375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10991.4755859375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -10991.4765625
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4766, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -10991.4794921875
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4795, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -10991.474609375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4746, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10991.474609375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4746, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10991.5068359375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.5068, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10991.484375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4844, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10991.55859375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.5586, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10991.4755859375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -10991.4755859375
tensor(10991.4746, grad_fn=<NegBackward0>) tensor(10991.4756, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7485, 0.2515],
        [0.2856, 0.7144]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5864, 0.4136], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1032],
         [0.6597, 0.3089]],

        [[0.5737, 0.1018],
         [0.7223, 0.5624]],

        [[0.6614, 0.0943],
         [0.5715, 0.6122]],

        [[0.5129, 0.0929],
         [0.5935, 0.6728]],

        [[0.5999, 0.0969],
         [0.5070, 0.6898]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9291470599341588
Average Adjusted Rand Index: 0.929281786450883
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23184.693359375
inf tensor(23184.6934, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11275.7880859375
tensor(23184.6934, grad_fn=<NegBackward0>) tensor(11275.7881, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11275.0048828125
tensor(11275.7881, grad_fn=<NegBackward0>) tensor(11275.0049, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11274.5556640625
tensor(11275.0049, grad_fn=<NegBackward0>) tensor(11274.5557, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11274.287109375
tensor(11274.5557, grad_fn=<NegBackward0>) tensor(11274.2871, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11273.9189453125
tensor(11274.2871, grad_fn=<NegBackward0>) tensor(11273.9189, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11273.2509765625
tensor(11273.9189, grad_fn=<NegBackward0>) tensor(11273.2510, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11272.26171875
tensor(11273.2510, grad_fn=<NegBackward0>) tensor(11272.2617, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11271.9462890625
tensor(11272.2617, grad_fn=<NegBackward0>) tensor(11271.9463, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11271.73828125
tensor(11271.9463, grad_fn=<NegBackward0>) tensor(11271.7383, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11271.4462890625
tensor(11271.7383, grad_fn=<NegBackward0>) tensor(11271.4463, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11270.90234375
tensor(11271.4463, grad_fn=<NegBackward0>) tensor(11270.9023, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11010.6630859375
tensor(11270.9023, grad_fn=<NegBackward0>) tensor(11010.6631, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10992.0810546875
tensor(11010.6631, grad_fn=<NegBackward0>) tensor(10992.0811, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10991.892578125
tensor(10992.0811, grad_fn=<NegBackward0>) tensor(10991.8926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10991.8193359375
tensor(10991.8926, grad_fn=<NegBackward0>) tensor(10991.8193, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10991.7861328125
tensor(10991.8193, grad_fn=<NegBackward0>) tensor(10991.7861, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10991.7646484375
tensor(10991.7861, grad_fn=<NegBackward0>) tensor(10991.7646, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10991.755859375
tensor(10991.7646, grad_fn=<NegBackward0>) tensor(10991.7559, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10991.75
tensor(10991.7559, grad_fn=<NegBackward0>) tensor(10991.7500, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10991.6923828125
tensor(10991.7500, grad_fn=<NegBackward0>) tensor(10991.6924, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10991.6884765625
tensor(10991.6924, grad_fn=<NegBackward0>) tensor(10991.6885, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10991.685546875
tensor(10991.6885, grad_fn=<NegBackward0>) tensor(10991.6855, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10991.68359375
tensor(10991.6855, grad_fn=<NegBackward0>) tensor(10991.6836, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10991.681640625
tensor(10991.6836, grad_fn=<NegBackward0>) tensor(10991.6816, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10991.681640625
tensor(10991.6816, grad_fn=<NegBackward0>) tensor(10991.6816, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10991.6767578125
tensor(10991.6816, grad_fn=<NegBackward0>) tensor(10991.6768, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10991.6728515625
tensor(10991.6768, grad_fn=<NegBackward0>) tensor(10991.6729, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10991.6611328125
tensor(10991.6729, grad_fn=<NegBackward0>) tensor(10991.6611, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10991.65234375
tensor(10991.6611, grad_fn=<NegBackward0>) tensor(10991.6523, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10991.650390625
tensor(10991.6523, grad_fn=<NegBackward0>) tensor(10991.6504, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10991.62890625
tensor(10991.6504, grad_fn=<NegBackward0>) tensor(10991.6289, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10991.6015625
tensor(10991.6289, grad_fn=<NegBackward0>) tensor(10991.6016, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10991.6064453125
tensor(10991.6016, grad_fn=<NegBackward0>) tensor(10991.6064, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10991.5849609375
tensor(10991.6016, grad_fn=<NegBackward0>) tensor(10991.5850, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10991.57421875
tensor(10991.5850, grad_fn=<NegBackward0>) tensor(10991.5742, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10991.5791015625
tensor(10991.5742, grad_fn=<NegBackward0>) tensor(10991.5791, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10991.568359375
tensor(10991.5742, grad_fn=<NegBackward0>) tensor(10991.5684, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10991.56640625
tensor(10991.5684, grad_fn=<NegBackward0>) tensor(10991.5664, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10991.5576171875
tensor(10991.5664, grad_fn=<NegBackward0>) tensor(10991.5576, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10991.556640625
tensor(10991.5576, grad_fn=<NegBackward0>) tensor(10991.5566, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10991.560546875
tensor(10991.5566, grad_fn=<NegBackward0>) tensor(10991.5605, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10991.556640625
tensor(10991.5566, grad_fn=<NegBackward0>) tensor(10991.5566, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10991.556640625
tensor(10991.5566, grad_fn=<NegBackward0>) tensor(10991.5566, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10991.556640625
tensor(10991.5566, grad_fn=<NegBackward0>) tensor(10991.5566, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10991.5341796875
tensor(10991.5566, grad_fn=<NegBackward0>) tensor(10991.5342, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10991.5322265625
tensor(10991.5342, grad_fn=<NegBackward0>) tensor(10991.5322, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10991.5322265625
tensor(10991.5322, grad_fn=<NegBackward0>) tensor(10991.5322, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10991.53125
tensor(10991.5322, grad_fn=<NegBackward0>) tensor(10991.5312, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10991.5302734375
tensor(10991.5312, grad_fn=<NegBackward0>) tensor(10991.5303, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10991.529296875
tensor(10991.5303, grad_fn=<NegBackward0>) tensor(10991.5293, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10991.5283203125
tensor(10991.5293, grad_fn=<NegBackward0>) tensor(10991.5283, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10991.5234375
tensor(10991.5283, grad_fn=<NegBackward0>) tensor(10991.5234, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10991.5244140625
tensor(10991.5234, grad_fn=<NegBackward0>) tensor(10991.5244, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10991.5244140625
tensor(10991.5234, grad_fn=<NegBackward0>) tensor(10991.5244, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10991.5234375
tensor(10991.5234, grad_fn=<NegBackward0>) tensor(10991.5234, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10991.521484375
tensor(10991.5234, grad_fn=<NegBackward0>) tensor(10991.5215, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10991.5224609375
tensor(10991.5215, grad_fn=<NegBackward0>) tensor(10991.5225, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10991.51953125
tensor(10991.5215, grad_fn=<NegBackward0>) tensor(10991.5195, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10991.5185546875
tensor(10991.5195, grad_fn=<NegBackward0>) tensor(10991.5186, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10991.5185546875
tensor(10991.5186, grad_fn=<NegBackward0>) tensor(10991.5186, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10991.517578125
tensor(10991.5186, grad_fn=<NegBackward0>) tensor(10991.5176, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10991.5185546875
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5186, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10991.5302734375
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5303, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10991.5146484375
tensor(10991.5176, grad_fn=<NegBackward0>) tensor(10991.5146, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10991.5146484375
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5146, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10991.5166015625
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5166, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10991.5146484375
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5146, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10991.515625
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5156, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10991.5146484375
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5146, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10991.513671875
tensor(10991.5146, grad_fn=<NegBackward0>) tensor(10991.5137, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10991.513671875
tensor(10991.5137, grad_fn=<NegBackward0>) tensor(10991.5137, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10991.5146484375
tensor(10991.5137, grad_fn=<NegBackward0>) tensor(10991.5146, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10991.513671875
tensor(10991.5137, grad_fn=<NegBackward0>) tensor(10991.5137, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10991.4853515625
tensor(10991.5137, grad_fn=<NegBackward0>) tensor(10991.4854, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10991.484375
tensor(10991.4854, grad_fn=<NegBackward0>) tensor(10991.4844, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10991.4873046875
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4873, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10991.4853515625
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4854, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10991.486328125
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4863, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10991.49609375
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4961, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -10991.484375
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4844, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10991.484375
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4844, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10991.4853515625
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4854, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10991.484375
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4844, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10991.515625
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.5156, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10991.482421875
tensor(10991.4844, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10991.4833984375
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4834, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10991.482421875
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10991.4833984375
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4834, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10991.482421875
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10991.4833984375
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4834, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10991.482421875
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10991.482421875
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10991.486328125
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4863, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10991.482421875
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4824, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10991.4814453125
tensor(10991.4824, grad_fn=<NegBackward0>) tensor(10991.4814, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10991.4814453125
tensor(10991.4814, grad_fn=<NegBackward0>) tensor(10991.4814, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10991.478515625
tensor(10991.4814, grad_fn=<NegBackward0>) tensor(10991.4785, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10991.4814453125
tensor(10991.4785, grad_fn=<NegBackward0>) tensor(10991.4814, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10991.478515625
tensor(10991.4785, grad_fn=<NegBackward0>) tensor(10991.4785, grad_fn=<NegBackward0>)
pi: tensor([[0.7483, 0.2517],
        [0.2862, 0.7138]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5866, 0.4134], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1975, 0.1033],
         [0.5326, 0.3090]],

        [[0.6386, 0.1018],
         [0.6883, 0.5113]],

        [[0.6706, 0.0943],
         [0.5811, 0.6917]],

        [[0.5174, 0.0929],
         [0.6051, 0.5317]],

        [[0.5892, 0.0969],
         [0.5224, 0.6251]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9291470599341588
Average Adjusted Rand Index: 0.929281786450883
[0.9291470599341588, 0.9291470599341588] [0.929281786450883, 0.929281786450883] [10991.4755859375, 10991.4794921875]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11065.614925269521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22056.490234375
inf tensor(22056.4902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11346.3251953125
tensor(22056.4902, grad_fn=<NegBackward0>) tensor(11346.3252, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11344.7578125
tensor(11346.3252, grad_fn=<NegBackward0>) tensor(11344.7578, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11344.2109375
tensor(11344.7578, grad_fn=<NegBackward0>) tensor(11344.2109, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11344.078125
tensor(11344.2109, grad_fn=<NegBackward0>) tensor(11344.0781, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11344.005859375
tensor(11344.0781, grad_fn=<NegBackward0>) tensor(11344.0059, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11343.966796875
tensor(11344.0059, grad_fn=<NegBackward0>) tensor(11343.9668, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11343.9462890625
tensor(11343.9668, grad_fn=<NegBackward0>) tensor(11343.9463, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11343.9306640625
tensor(11343.9463, grad_fn=<NegBackward0>) tensor(11343.9307, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11343.921875
tensor(11343.9307, grad_fn=<NegBackward0>) tensor(11343.9219, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11343.9150390625
tensor(11343.9219, grad_fn=<NegBackward0>) tensor(11343.9150, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11343.91015625
tensor(11343.9150, grad_fn=<NegBackward0>) tensor(11343.9102, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11343.90625
tensor(11343.9102, grad_fn=<NegBackward0>) tensor(11343.9062, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11343.9033203125
tensor(11343.9062, grad_fn=<NegBackward0>) tensor(11343.9033, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11343.9013671875
tensor(11343.9033, grad_fn=<NegBackward0>) tensor(11343.9014, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11343.8984375
tensor(11343.9014, grad_fn=<NegBackward0>) tensor(11343.8984, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11343.89453125
tensor(11343.8984, grad_fn=<NegBackward0>) tensor(11343.8945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11343.892578125
tensor(11343.8945, grad_fn=<NegBackward0>) tensor(11343.8926, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11343.8916015625
tensor(11343.8926, grad_fn=<NegBackward0>) tensor(11343.8916, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11343.888671875
tensor(11343.8916, grad_fn=<NegBackward0>) tensor(11343.8887, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11343.88671875
tensor(11343.8887, grad_fn=<NegBackward0>) tensor(11343.8867, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11343.8857421875
tensor(11343.8867, grad_fn=<NegBackward0>) tensor(11343.8857, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11343.8837890625
tensor(11343.8857, grad_fn=<NegBackward0>) tensor(11343.8838, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11343.880859375
tensor(11343.8838, grad_fn=<NegBackward0>) tensor(11343.8809, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11343.87890625
tensor(11343.8809, grad_fn=<NegBackward0>) tensor(11343.8789, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11343.876953125
tensor(11343.8789, grad_fn=<NegBackward0>) tensor(11343.8770, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11343.873046875
tensor(11343.8770, grad_fn=<NegBackward0>) tensor(11343.8730, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11343.8701171875
tensor(11343.8730, grad_fn=<NegBackward0>) tensor(11343.8701, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11343.8662109375
tensor(11343.8701, grad_fn=<NegBackward0>) tensor(11343.8662, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11343.861328125
tensor(11343.8662, grad_fn=<NegBackward0>) tensor(11343.8613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11343.8544921875
tensor(11343.8613, grad_fn=<NegBackward0>) tensor(11343.8545, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11343.8515625
tensor(11343.8545, grad_fn=<NegBackward0>) tensor(11343.8516, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11343.845703125
tensor(11343.8516, grad_fn=<NegBackward0>) tensor(11343.8457, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11343.8408203125
tensor(11343.8457, grad_fn=<NegBackward0>) tensor(11343.8408, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11343.8369140625
tensor(11343.8408, grad_fn=<NegBackward0>) tensor(11343.8369, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11343.8310546875
tensor(11343.8369, grad_fn=<NegBackward0>) tensor(11343.8311, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11343.828125
tensor(11343.8311, grad_fn=<NegBackward0>) tensor(11343.8281, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11343.8232421875
tensor(11343.8281, grad_fn=<NegBackward0>) tensor(11343.8232, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11343.818359375
tensor(11343.8232, grad_fn=<NegBackward0>) tensor(11343.8184, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11343.814453125
tensor(11343.8184, grad_fn=<NegBackward0>) tensor(11343.8145, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11343.8095703125
tensor(11343.8145, grad_fn=<NegBackward0>) tensor(11343.8096, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11343.8056640625
tensor(11343.8096, grad_fn=<NegBackward0>) tensor(11343.8057, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11343.7998046875
tensor(11343.8057, grad_fn=<NegBackward0>) tensor(11343.7998, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11343.8115234375
tensor(11343.7998, grad_fn=<NegBackward0>) tensor(11343.8115, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11343.7919921875
tensor(11343.7998, grad_fn=<NegBackward0>) tensor(11343.7920, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11343.7939453125
tensor(11343.7920, grad_fn=<NegBackward0>) tensor(11343.7939, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11343.7900390625
tensor(11343.7920, grad_fn=<NegBackward0>) tensor(11343.7900, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11343.7890625
tensor(11343.7900, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11343.7890625
tensor(11343.7891, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11343.7890625
tensor(11343.7891, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11343.7890625
tensor(11343.7891, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11343.7890625
tensor(11343.7891, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11343.7880859375
tensor(11343.7891, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11343.7890625
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11343.7900390625
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7900, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11343.7880859375
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11343.79296875
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7930, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11343.7919921875
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7920, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11343.7880859375
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11343.7890625
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11343.7919921875
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7920, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11343.7919921875
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7920, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11343.794921875
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7949, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11343.787109375
tensor(11343.7881, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11343.7890625
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11343.810546875
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.8105, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11343.7890625
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11343.796875
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7969, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11343.7958984375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7959, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11343.787109375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11343.7939453125
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7939, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11343.7890625
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11343.7890625
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7891, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11343.7880859375
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7881, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.5851e-01, 4.1489e-02],
        [9.9988e-01, 1.1725e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6432, 0.3568], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1710, 0.1747],
         [0.6836, 0.1784]],

        [[0.6830, 0.2779],
         [0.6509, 0.6713]],

        [[0.6851, 0.1997],
         [0.5618, 0.5924]],

        [[0.6291, 0.1101],
         [0.6613, 0.5195]],

        [[0.6373, 0.1321],
         [0.5229, 0.6956]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007718940776607481
Average Adjusted Rand Index: 0.0001633280491905654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21449.552734375
inf tensor(21449.5527, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11239.3720703125
tensor(21449.5527, grad_fn=<NegBackward0>) tensor(11239.3721, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11216.5126953125
tensor(11239.3721, grad_fn=<NegBackward0>) tensor(11216.5127, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11153.697265625
tensor(11216.5127, grad_fn=<NegBackward0>) tensor(11153.6973, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11118.0791015625
tensor(11153.6973, grad_fn=<NegBackward0>) tensor(11118.0791, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11115.041015625
tensor(11118.0791, grad_fn=<NegBackward0>) tensor(11115.0410, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11114.9375
tensor(11115.0410, grad_fn=<NegBackward0>) tensor(11114.9375, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11112.072265625
tensor(11114.9375, grad_fn=<NegBackward0>) tensor(11112.0723, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11111.927734375
tensor(11112.0723, grad_fn=<NegBackward0>) tensor(11111.9277, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11111.892578125
tensor(11111.9277, grad_fn=<NegBackward0>) tensor(11111.8926, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11111.5224609375
tensor(11111.8926, grad_fn=<NegBackward0>) tensor(11111.5225, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11111.48046875
tensor(11111.5225, grad_fn=<NegBackward0>) tensor(11111.4805, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11111.462890625
tensor(11111.4805, grad_fn=<NegBackward0>) tensor(11111.4629, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11111.45703125
tensor(11111.4629, grad_fn=<NegBackward0>) tensor(11111.4570, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11111.455078125
tensor(11111.4570, grad_fn=<NegBackward0>) tensor(11111.4551, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11111.451171875
tensor(11111.4551, grad_fn=<NegBackward0>) tensor(11111.4512, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11111.447265625
tensor(11111.4512, grad_fn=<NegBackward0>) tensor(11111.4473, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11111.4462890625
tensor(11111.4473, grad_fn=<NegBackward0>) tensor(11111.4463, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11111.4443359375
tensor(11111.4463, grad_fn=<NegBackward0>) tensor(11111.4443, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11111.4443359375
tensor(11111.4443, grad_fn=<NegBackward0>) tensor(11111.4443, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11111.443359375
tensor(11111.4443, grad_fn=<NegBackward0>) tensor(11111.4434, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11111.44140625
tensor(11111.4434, grad_fn=<NegBackward0>) tensor(11111.4414, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11111.44140625
tensor(11111.4414, grad_fn=<NegBackward0>) tensor(11111.4414, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11111.439453125
tensor(11111.4414, grad_fn=<NegBackward0>) tensor(11111.4395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11110.9716796875
tensor(11111.4395, grad_fn=<NegBackward0>) tensor(11110.9717, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11110.90234375
tensor(11110.9717, grad_fn=<NegBackward0>) tensor(11110.9023, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11110.8994140625
tensor(11110.9023, grad_fn=<NegBackward0>) tensor(11110.8994, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11110.869140625
tensor(11110.8994, grad_fn=<NegBackward0>) tensor(11110.8691, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11110.8681640625
tensor(11110.8691, grad_fn=<NegBackward0>) tensor(11110.8682, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11110.8681640625
tensor(11110.8682, grad_fn=<NegBackward0>) tensor(11110.8682, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11110.8662109375
tensor(11110.8682, grad_fn=<NegBackward0>) tensor(11110.8662, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11110.8662109375
tensor(11110.8662, grad_fn=<NegBackward0>) tensor(11110.8662, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11110.865234375
tensor(11110.8662, grad_fn=<NegBackward0>) tensor(11110.8652, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11110.869140625
tensor(11110.8652, grad_fn=<NegBackward0>) tensor(11110.8691, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11110.8623046875
tensor(11110.8652, grad_fn=<NegBackward0>) tensor(11110.8623, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11110.861328125
tensor(11110.8623, grad_fn=<NegBackward0>) tensor(11110.8613, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11110.86328125
tensor(11110.8613, grad_fn=<NegBackward0>) tensor(11110.8633, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11110.8603515625
tensor(11110.8613, grad_fn=<NegBackward0>) tensor(11110.8604, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11110.8603515625
tensor(11110.8604, grad_fn=<NegBackward0>) tensor(11110.8604, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11110.87890625
tensor(11110.8604, grad_fn=<NegBackward0>) tensor(11110.8789, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11110.859375
tensor(11110.8604, grad_fn=<NegBackward0>) tensor(11110.8594, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11110.865234375
tensor(11110.8594, grad_fn=<NegBackward0>) tensor(11110.8652, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11110.859375
tensor(11110.8594, grad_fn=<NegBackward0>) tensor(11110.8594, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11110.86328125
tensor(11110.8594, grad_fn=<NegBackward0>) tensor(11110.8633, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11110.8232421875
tensor(11110.8594, grad_fn=<NegBackward0>) tensor(11110.8232, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11110.822265625
tensor(11110.8232, grad_fn=<NegBackward0>) tensor(11110.8223, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11110.8212890625
tensor(11110.8223, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11110.8212890625
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11110.8369140625
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8369, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11110.8212890625
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11110.8212890625
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11110.8212890625
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11110.8203125
tensor(11110.8213, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11110.8212890625
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11110.8349609375
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8350, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11110.8212890625
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11110.8203125
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11110.8212890625
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11110.8203125
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11110.8212890625
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11110.8212890625
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8213, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11110.8203125
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11110.8193359375
tensor(11110.8203, grad_fn=<NegBackward0>) tensor(11110.8193, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11110.8203125
tensor(11110.8193, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11110.8203125
tensor(11110.8193, grad_fn=<NegBackward0>) tensor(11110.8203, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11110.822265625
tensor(11110.8193, grad_fn=<NegBackward0>) tensor(11110.8223, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11110.751953125
tensor(11110.8193, grad_fn=<NegBackward0>) tensor(11110.7520, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11110.751953125
tensor(11110.7520, grad_fn=<NegBackward0>) tensor(11110.7520, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11110.7509765625
tensor(11110.7520, grad_fn=<NegBackward0>) tensor(11110.7510, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11110.75
tensor(11110.7510, grad_fn=<NegBackward0>) tensor(11110.7500, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11110.7890625
tensor(11110.7500, grad_fn=<NegBackward0>) tensor(11110.7891, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11110.7470703125
tensor(11110.7500, grad_fn=<NegBackward0>) tensor(11110.7471, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11110.755859375
tensor(11110.7471, grad_fn=<NegBackward0>) tensor(11110.7559, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11110.7451171875
tensor(11110.7471, grad_fn=<NegBackward0>) tensor(11110.7451, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11110.7578125
tensor(11110.7451, grad_fn=<NegBackward0>) tensor(11110.7578, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11110.744140625
tensor(11110.7451, grad_fn=<NegBackward0>) tensor(11110.7441, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11110.79296875
tensor(11110.7441, grad_fn=<NegBackward0>) tensor(11110.7930, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11110.7451171875
tensor(11110.7441, grad_fn=<NegBackward0>) tensor(11110.7451, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11110.8076171875
tensor(11110.7441, grad_fn=<NegBackward0>) tensor(11110.8076, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11110.7451171875
tensor(11110.7441, grad_fn=<NegBackward0>) tensor(11110.7451, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11110.7451171875
tensor(11110.7441, grad_fn=<NegBackward0>) tensor(11110.7451, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.2588, 0.7412],
        [0.7400, 0.2600]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4477, 0.5523], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2588, 0.1027],
         [0.5002, 0.2358]],

        [[0.6965, 0.0942],
         [0.5461, 0.6997]],

        [[0.6648, 0.0964],
         [0.5709, 0.6788]],

        [[0.6858, 0.0954],
         [0.5427, 0.6013]],

        [[0.6352, 0.0966],
         [0.6983, 0.6859]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026223303595567
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369954580512469
Global Adjusted Rand Index: 0.01878341590240099
Average Adjusted Rand Index: 0.87992277401189
[0.0007718940776607481, 0.01878341590240099] [0.0001633280491905654, 0.87992277401189] [11343.7880859375, 11110.7451171875]
-------------------------------------
This iteration is 28
True Objective function: Loss = -10999.593235656203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21115.6640625
inf tensor(21115.6641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11204.0
tensor(21115.6641, grad_fn=<NegBackward0>) tensor(11204., grad_fn=<NegBackward0>)
Iteration 200: Loss = -11203.1044921875
tensor(11204., grad_fn=<NegBackward0>) tensor(11203.1045, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11201.326171875
tensor(11203.1045, grad_fn=<NegBackward0>) tensor(11201.3262, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11200.599609375
tensor(11201.3262, grad_fn=<NegBackward0>) tensor(11200.5996, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11200.2216796875
tensor(11200.5996, grad_fn=<NegBackward0>) tensor(11200.2217, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11199.40625
tensor(11200.2217, grad_fn=<NegBackward0>) tensor(11199.4062, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11181.939453125
tensor(11199.4062, grad_fn=<NegBackward0>) tensor(11181.9395, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11129.9853515625
tensor(11181.9395, grad_fn=<NegBackward0>) tensor(11129.9854, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11126.5029296875
tensor(11129.9854, grad_fn=<NegBackward0>) tensor(11126.5029, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11125.634765625
tensor(11126.5029, grad_fn=<NegBackward0>) tensor(11125.6348, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11125.5087890625
tensor(11125.6348, grad_fn=<NegBackward0>) tensor(11125.5088, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11125.4169921875
tensor(11125.5088, grad_fn=<NegBackward0>) tensor(11125.4170, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11119.9326171875
tensor(11125.4170, grad_fn=<NegBackward0>) tensor(11119.9326, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11119.849609375
tensor(11119.9326, grad_fn=<NegBackward0>) tensor(11119.8496, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11119.7978515625
tensor(11119.8496, grad_fn=<NegBackward0>) tensor(11119.7979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11118.8623046875
tensor(11119.7979, grad_fn=<NegBackward0>) tensor(11118.8623, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11118.8359375
tensor(11118.8623, grad_fn=<NegBackward0>) tensor(11118.8359, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11118.822265625
tensor(11118.8359, grad_fn=<NegBackward0>) tensor(11118.8223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11118.806640625
tensor(11118.8223, grad_fn=<NegBackward0>) tensor(11118.8066, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11118.7705078125
tensor(11118.8066, grad_fn=<NegBackward0>) tensor(11118.7705, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11118.634765625
tensor(11118.7705, grad_fn=<NegBackward0>) tensor(11118.6348, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11118.626953125
tensor(11118.6348, grad_fn=<NegBackward0>) tensor(11118.6270, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11118.6220703125
tensor(11118.6270, grad_fn=<NegBackward0>) tensor(11118.6221, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11118.6181640625
tensor(11118.6221, grad_fn=<NegBackward0>) tensor(11118.6182, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11118.61328125
tensor(11118.6182, grad_fn=<NegBackward0>) tensor(11118.6133, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11118.611328125
tensor(11118.6133, grad_fn=<NegBackward0>) tensor(11118.6113, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11118.6083984375
tensor(11118.6113, grad_fn=<NegBackward0>) tensor(11118.6084, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11118.60546875
tensor(11118.6084, grad_fn=<NegBackward0>) tensor(11118.6055, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11118.6044921875
tensor(11118.6055, grad_fn=<NegBackward0>) tensor(11118.6045, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11118.6025390625
tensor(11118.6045, grad_fn=<NegBackward0>) tensor(11118.6025, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11118.6005859375
tensor(11118.6025, grad_fn=<NegBackward0>) tensor(11118.6006, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11118.5986328125
tensor(11118.6006, grad_fn=<NegBackward0>) tensor(11118.5986, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11118.5009765625
tensor(11118.5986, grad_fn=<NegBackward0>) tensor(11118.5010, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11118.5009765625
tensor(11118.5010, grad_fn=<NegBackward0>) tensor(11118.5010, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11118.4990234375
tensor(11118.5010, grad_fn=<NegBackward0>) tensor(11118.4990, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11118.4970703125
tensor(11118.4990, grad_fn=<NegBackward0>) tensor(11118.4971, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11118.494140625
tensor(11118.4971, grad_fn=<NegBackward0>) tensor(11118.4941, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11118.4912109375
tensor(11118.4941, grad_fn=<NegBackward0>) tensor(11118.4912, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11118.4892578125
tensor(11118.4912, grad_fn=<NegBackward0>) tensor(11118.4893, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11118.4931640625
tensor(11118.4893, grad_fn=<NegBackward0>) tensor(11118.4932, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11118.4765625
tensor(11118.4893, grad_fn=<NegBackward0>) tensor(11118.4766, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11118.4775390625
tensor(11118.4766, grad_fn=<NegBackward0>) tensor(11118.4775, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11118.4736328125
tensor(11118.4766, grad_fn=<NegBackward0>) tensor(11118.4736, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11118.47265625
tensor(11118.4736, grad_fn=<NegBackward0>) tensor(11118.4727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11118.470703125
tensor(11118.4727, grad_fn=<NegBackward0>) tensor(11118.4707, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11118.46875
tensor(11118.4707, grad_fn=<NegBackward0>) tensor(11118.4688, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11118.458984375
tensor(11118.4688, grad_fn=<NegBackward0>) tensor(11118.4590, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11118.45703125
tensor(11118.4590, grad_fn=<NegBackward0>) tensor(11118.4570, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11118.4521484375
tensor(11118.4570, grad_fn=<NegBackward0>) tensor(11118.4521, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11118.4501953125
tensor(11118.4521, grad_fn=<NegBackward0>) tensor(11118.4502, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11118.4501953125
tensor(11118.4502, grad_fn=<NegBackward0>) tensor(11118.4502, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11118.44921875
tensor(11118.4502, grad_fn=<NegBackward0>) tensor(11118.4492, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11118.4482421875
tensor(11118.4492, grad_fn=<NegBackward0>) tensor(11118.4482, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11118.44921875
tensor(11118.4482, grad_fn=<NegBackward0>) tensor(11118.4492, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11118.3798828125
tensor(11118.4482, grad_fn=<NegBackward0>) tensor(11118.3799, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11118.3798828125
tensor(11118.3799, grad_fn=<NegBackward0>) tensor(11118.3799, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11118.3798828125
tensor(11118.3799, grad_fn=<NegBackward0>) tensor(11118.3799, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11118.3779296875
tensor(11118.3799, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11118.37890625
tensor(11118.3779, grad_fn=<NegBackward0>) tensor(11118.3789, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11118.37890625
tensor(11118.3779, grad_fn=<NegBackward0>) tensor(11118.3789, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11118.4111328125
tensor(11118.3779, grad_fn=<NegBackward0>) tensor(11118.4111, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11118.3779296875
tensor(11118.3779, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11118.376953125
tensor(11118.3779, grad_fn=<NegBackward0>) tensor(11118.3770, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11118.3779296875
tensor(11118.3770, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11118.3779296875
tensor(11118.3770, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11118.3779296875
tensor(11118.3770, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11118.380859375
tensor(11118.3770, grad_fn=<NegBackward0>) tensor(11118.3809, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11118.3759765625
tensor(11118.3770, grad_fn=<NegBackward0>) tensor(11118.3760, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11118.3779296875
tensor(11118.3760, grad_fn=<NegBackward0>) tensor(11118.3779, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11118.3759765625
tensor(11118.3760, grad_fn=<NegBackward0>) tensor(11118.3760, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11118.37109375
tensor(11118.3760, grad_fn=<NegBackward0>) tensor(11118.3711, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11116.15234375
tensor(11118.3711, grad_fn=<NegBackward0>) tensor(11116.1523, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11115.6650390625
tensor(11116.1523, grad_fn=<NegBackward0>) tensor(11115.6650, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11115.595703125
tensor(11115.6650, grad_fn=<NegBackward0>) tensor(11115.5957, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11115.49609375
tensor(11115.5957, grad_fn=<NegBackward0>) tensor(11115.4961, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11115.40234375
tensor(11115.4961, grad_fn=<NegBackward0>) tensor(11115.4023, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11115.087890625
tensor(11115.4023, grad_fn=<NegBackward0>) tensor(11115.0879, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11114.9736328125
tensor(11115.0879, grad_fn=<NegBackward0>) tensor(11114.9736, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11099.4404296875
tensor(11114.9736, grad_fn=<NegBackward0>) tensor(11099.4404, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11082.58203125
tensor(11099.4404, grad_fn=<NegBackward0>) tensor(11082.5820, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11065.509765625
tensor(11082.5820, grad_fn=<NegBackward0>) tensor(11065.5098, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11061.88671875
tensor(11065.5098, grad_fn=<NegBackward0>) tensor(11061.8867, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11050.2255859375
tensor(11061.8867, grad_fn=<NegBackward0>) tensor(11050.2256, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11050.1669921875
tensor(11050.2256, grad_fn=<NegBackward0>) tensor(11050.1670, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11049.990234375
tensor(11050.1670, grad_fn=<NegBackward0>) tensor(11049.9902, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11049.9453125
tensor(11049.9902, grad_fn=<NegBackward0>) tensor(11049.9453, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11049.9404296875
tensor(11049.9453, grad_fn=<NegBackward0>) tensor(11049.9404, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11049.9384765625
tensor(11049.9404, grad_fn=<NegBackward0>) tensor(11049.9385, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11049.931640625
tensor(11049.9385, grad_fn=<NegBackward0>) tensor(11049.9316, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11049.9306640625
tensor(11049.9316, grad_fn=<NegBackward0>) tensor(11049.9307, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11049.857421875
tensor(11049.9307, grad_fn=<NegBackward0>) tensor(11049.8574, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11049.80078125
tensor(11049.8574, grad_fn=<NegBackward0>) tensor(11049.8008, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11049.802734375
tensor(11049.8008, grad_fn=<NegBackward0>) tensor(11049.8027, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11049.7998046875
tensor(11049.8008, grad_fn=<NegBackward0>) tensor(11049.7998, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11049.814453125
tensor(11049.7998, grad_fn=<NegBackward0>) tensor(11049.8145, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11049.798828125
tensor(11049.7998, grad_fn=<NegBackward0>) tensor(11049.7988, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11049.798828125
tensor(11049.7988, grad_fn=<NegBackward0>) tensor(11049.7988, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11049.7978515625
tensor(11049.7988, grad_fn=<NegBackward0>) tensor(11049.7979, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11049.796875
tensor(11049.7979, grad_fn=<NegBackward0>) tensor(11049.7969, grad_fn=<NegBackward0>)
pi: tensor([[0.3273, 0.6727],
        [0.5631, 0.4369]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4487, 0.5513], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2593, 0.0954],
         [0.5839, 0.2219]],

        [[0.6004, 0.0967],
         [0.7134, 0.7047]],

        [[0.6965, 0.0950],
         [0.5458, 0.5863]],

        [[0.7306, 0.0982],
         [0.6221, 0.5353]],

        [[0.6987, 0.1089],
         [0.7143, 0.5162]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 89
Adjusted Rand Index: 0.6044583919171261
Global Adjusted Rand Index: 0.05004165530010992
Average Adjusted Rand Index: 0.7979017793935261
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22627.080078125
inf tensor(22627.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11204.501953125
tensor(22627.0801, grad_fn=<NegBackward0>) tensor(11204.5020, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11203.6982421875
tensor(11204.5020, grad_fn=<NegBackward0>) tensor(11203.6982, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11201.80859375
tensor(11203.6982, grad_fn=<NegBackward0>) tensor(11201.8086, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11198.2724609375
tensor(11201.8086, grad_fn=<NegBackward0>) tensor(11198.2725, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11147.30859375
tensor(11198.2725, grad_fn=<NegBackward0>) tensor(11147.3086, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11103.1552734375
tensor(11147.3086, grad_fn=<NegBackward0>) tensor(11103.1553, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11100.3564453125
tensor(11103.1553, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11099.9189453125
tensor(11100.3564, grad_fn=<NegBackward0>) tensor(11099.9189, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11099.5419921875
tensor(11099.9189, grad_fn=<NegBackward0>) tensor(11099.5420, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11099.4423828125
tensor(11099.5420, grad_fn=<NegBackward0>) tensor(11099.4424, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11099.3935546875
tensor(11099.4424, grad_fn=<NegBackward0>) tensor(11099.3936, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11099.3603515625
tensor(11099.3936, grad_fn=<NegBackward0>) tensor(11099.3604, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11099.3359375
tensor(11099.3604, grad_fn=<NegBackward0>) tensor(11099.3359, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11099.3154296875
tensor(11099.3359, grad_fn=<NegBackward0>) tensor(11099.3154, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11099.23828125
tensor(11099.3154, grad_fn=<NegBackward0>) tensor(11099.2383, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11099.1142578125
tensor(11099.2383, grad_fn=<NegBackward0>) tensor(11099.1143, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11099.1064453125
tensor(11099.1143, grad_fn=<NegBackward0>) tensor(11099.1064, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11099.09765625
tensor(11099.1064, grad_fn=<NegBackward0>) tensor(11099.0977, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11099.091796875
tensor(11099.0977, grad_fn=<NegBackward0>) tensor(11099.0918, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11099.0859375
tensor(11099.0918, grad_fn=<NegBackward0>) tensor(11099.0859, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11099.0830078125
tensor(11099.0859, grad_fn=<NegBackward0>) tensor(11099.0830, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11099.078125
tensor(11099.0830, grad_fn=<NegBackward0>) tensor(11099.0781, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11099.0751953125
tensor(11099.0781, grad_fn=<NegBackward0>) tensor(11099.0752, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11099.0703125
tensor(11099.0752, grad_fn=<NegBackward0>) tensor(11099.0703, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11099.068359375
tensor(11099.0703, grad_fn=<NegBackward0>) tensor(11099.0684, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11099.06640625
tensor(11099.0684, grad_fn=<NegBackward0>) tensor(11099.0664, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11099.0634765625
tensor(11099.0664, grad_fn=<NegBackward0>) tensor(11099.0635, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11099.060546875
tensor(11099.0635, grad_fn=<NegBackward0>) tensor(11099.0605, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11099.05859375
tensor(11099.0605, grad_fn=<NegBackward0>) tensor(11099.0586, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11099.056640625
tensor(11099.0586, grad_fn=<NegBackward0>) tensor(11099.0566, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11099.0556640625
tensor(11099.0566, grad_fn=<NegBackward0>) tensor(11099.0557, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11099.0546875
tensor(11099.0557, grad_fn=<NegBackward0>) tensor(11099.0547, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11099.052734375
tensor(11099.0547, grad_fn=<NegBackward0>) tensor(11099.0527, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11099.0537109375
tensor(11099.0527, grad_fn=<NegBackward0>) tensor(11099.0537, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11099.052734375
tensor(11099.0527, grad_fn=<NegBackward0>) tensor(11099.0527, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11099.0498046875
tensor(11099.0527, grad_fn=<NegBackward0>) tensor(11099.0498, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11099.048828125
tensor(11099.0498, grad_fn=<NegBackward0>) tensor(11099.0488, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11099.0478515625
tensor(11099.0488, grad_fn=<NegBackward0>) tensor(11099.0479, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11099.0478515625
tensor(11099.0479, grad_fn=<NegBackward0>) tensor(11099.0479, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11099.046875
tensor(11099.0479, grad_fn=<NegBackward0>) tensor(11099.0469, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11099.0458984375
tensor(11099.0469, grad_fn=<NegBackward0>) tensor(11099.0459, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11099.0458984375
tensor(11099.0459, grad_fn=<NegBackward0>) tensor(11099.0459, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11099.04296875
tensor(11099.0459, grad_fn=<NegBackward0>) tensor(11099.0430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11099.0419921875
tensor(11099.0430, grad_fn=<NegBackward0>) tensor(11099.0420, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11099.0439453125
tensor(11099.0420, grad_fn=<NegBackward0>) tensor(11099.0439, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11099.04296875
tensor(11099.0420, grad_fn=<NegBackward0>) tensor(11099.0430, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11099.0419921875
tensor(11099.0420, grad_fn=<NegBackward0>) tensor(11099.0420, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11099.0400390625
tensor(11099.0420, grad_fn=<NegBackward0>) tensor(11099.0400, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11099.0390625
tensor(11099.0400, grad_fn=<NegBackward0>) tensor(11099.0391, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11099.0390625
tensor(11099.0391, grad_fn=<NegBackward0>) tensor(11099.0391, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11099.0390625
tensor(11099.0391, grad_fn=<NegBackward0>) tensor(11099.0391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11099.0380859375
tensor(11099.0391, grad_fn=<NegBackward0>) tensor(11099.0381, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11099.0380859375
tensor(11099.0381, grad_fn=<NegBackward0>) tensor(11099.0381, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11099.0380859375
tensor(11099.0381, grad_fn=<NegBackward0>) tensor(11099.0381, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11099.0380859375
tensor(11099.0381, grad_fn=<NegBackward0>) tensor(11099.0381, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11099.037109375
tensor(11099.0381, grad_fn=<NegBackward0>) tensor(11099.0371, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11099.037109375
tensor(11099.0371, grad_fn=<NegBackward0>) tensor(11099.0371, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11099.037109375
tensor(11099.0371, grad_fn=<NegBackward0>) tensor(11099.0371, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11099.0380859375
tensor(11099.0371, grad_fn=<NegBackward0>) tensor(11099.0381, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11099.037109375
tensor(11099.0371, grad_fn=<NegBackward0>) tensor(11099.0371, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11099.0361328125
tensor(11099.0371, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11099.0361328125
tensor(11099.0361, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11099.0361328125
tensor(11099.0361, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11099.0361328125
tensor(11099.0361, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11099.037109375
tensor(11099.0361, grad_fn=<NegBackward0>) tensor(11099.0371, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11099.03515625
tensor(11099.0361, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11099.03515625
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11099.0791015625
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0791, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11099.03515625
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11099.03515625
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11099.03515625
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11099.046875
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0469, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11099.0361328125
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0361, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11099.0341796875
tensor(11099.0352, grad_fn=<NegBackward0>) tensor(11099.0342, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11099.03515625
tensor(11099.0342, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11099.03515625
tensor(11099.0342, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11099.03515625
tensor(11099.0342, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11099.0390625
tensor(11099.0342, grad_fn=<NegBackward0>) tensor(11099.0391, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11099.03515625
tensor(11099.0342, grad_fn=<NegBackward0>) tensor(11099.0352, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.5289, 0.4711],
        [0.9753, 0.0247]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9950, 0.0050], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1762, 0.2933],
         [0.5608, 0.3133]],

        [[0.6112, 0.1003],
         [0.5359, 0.7180]],

        [[0.7211, 0.1250],
         [0.5460, 0.5167]],

        [[0.5020, 0.1038],
         [0.6114, 0.6619]],

        [[0.5213, 0.1206],
         [0.5228, 0.6235]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.018276026440026624
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 20
Adjusted Rand Index: 0.3531704204833287
Global Adjusted Rand Index: 0.05025744600074896
Average Adjusted Rand Index: 0.40519755108011635
[0.05004165530010992, 0.05025744600074896] [0.7979017793935261, 0.40519755108011635] [11049.798828125, 11099.03515625]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11059.772069736695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22208.921875
inf tensor(22208.9219, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11284.98046875
tensor(22208.9219, grad_fn=<NegBackward0>) tensor(11284.9805, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11283.779296875
tensor(11284.9805, grad_fn=<NegBackward0>) tensor(11283.7793, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11282.6787109375
tensor(11283.7793, grad_fn=<NegBackward0>) tensor(11282.6787, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11280.5634765625
tensor(11282.6787, grad_fn=<NegBackward0>) tensor(11280.5635, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11161.64453125
tensor(11280.5635, grad_fn=<NegBackward0>) tensor(11161.6445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11084.4541015625
tensor(11161.6445, grad_fn=<NegBackward0>) tensor(11084.4541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11084.212890625
tensor(11084.4541, grad_fn=<NegBackward0>) tensor(11084.2129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11084.1171875
tensor(11084.2129, grad_fn=<NegBackward0>) tensor(11084.1172, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11081.9482421875
tensor(11084.1172, grad_fn=<NegBackward0>) tensor(11081.9482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11081.8955078125
tensor(11081.9482, grad_fn=<NegBackward0>) tensor(11081.8955, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11081.2978515625
tensor(11081.8955, grad_fn=<NegBackward0>) tensor(11081.2979, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11081.140625
tensor(11081.2979, grad_fn=<NegBackward0>) tensor(11081.1406, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11079.7666015625
tensor(11081.1406, grad_fn=<NegBackward0>) tensor(11079.7666, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11079.271484375
tensor(11079.7666, grad_fn=<NegBackward0>) tensor(11079.2715, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11079.251953125
tensor(11079.2715, grad_fn=<NegBackward0>) tensor(11079.2520, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11079.24609375
tensor(11079.2520, grad_fn=<NegBackward0>) tensor(11079.2461, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11079.240234375
tensor(11079.2461, grad_fn=<NegBackward0>) tensor(11079.2402, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11079.2373046875
tensor(11079.2402, grad_fn=<NegBackward0>) tensor(11079.2373, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11079.236328125
tensor(11079.2373, grad_fn=<NegBackward0>) tensor(11079.2363, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11079.234375
tensor(11079.2363, grad_fn=<NegBackward0>) tensor(11079.2344, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11079.2294921875
tensor(11079.2344, grad_fn=<NegBackward0>) tensor(11079.2295, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11079.2138671875
tensor(11079.2295, grad_fn=<NegBackward0>) tensor(11079.2139, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11079.2138671875
tensor(11079.2139, grad_fn=<NegBackward0>) tensor(11079.2139, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11079.212890625
tensor(11079.2139, grad_fn=<NegBackward0>) tensor(11079.2129, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11079.2109375
tensor(11079.2129, grad_fn=<NegBackward0>) tensor(11079.2109, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11079.2109375
tensor(11079.2109, grad_fn=<NegBackward0>) tensor(11079.2109, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11079.2099609375
tensor(11079.2109, grad_fn=<NegBackward0>) tensor(11079.2100, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11079.2099609375
tensor(11079.2100, grad_fn=<NegBackward0>) tensor(11079.2100, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11079.208984375
tensor(11079.2100, grad_fn=<NegBackward0>) tensor(11079.2090, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11079.2080078125
tensor(11079.2090, grad_fn=<NegBackward0>) tensor(11079.2080, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11079.20703125
tensor(11079.2080, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11079.2060546875
tensor(11079.2070, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11079.20703125
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11079.20703125
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11079.2060546875
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11079.20703125
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11079.20703125
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11079.2060546875
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11079.2060546875
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11079.2060546875
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11079.2060546875
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11079.205078125
tensor(11079.2061, grad_fn=<NegBackward0>) tensor(11079.2051, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11079.2060546875
tensor(11079.2051, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11079.205078125
tensor(11079.2051, grad_fn=<NegBackward0>) tensor(11079.2051, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11079.2060546875
tensor(11079.2051, grad_fn=<NegBackward0>) tensor(11079.2061, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11079.20703125
tensor(11079.2051, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11079.2041015625
tensor(11079.2051, grad_fn=<NegBackward0>) tensor(11079.2041, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11079.205078125
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2051, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11079.205078125
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2051, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11079.20703125
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11079.2041015625
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2041, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11079.205078125
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2051, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11079.2041015625
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.2041, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11079.1982421875
tensor(11079.2041, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11079.197265625
tensor(11079.1982, grad_fn=<NegBackward0>) tensor(11079.1973, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11079.1982421875
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11079.197265625
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1973, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11079.1982421875
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11079.1982421875
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11079.197265625
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1973, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11079.1982421875
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11079.1982421875
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11079.1962890625
tensor(11079.1973, grad_fn=<NegBackward0>) tensor(11079.1963, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11079.1982421875
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11079.1982421875
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11079.1962890625
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1963, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11079.20703125
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.2070, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11079.197265625
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1973, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11079.1982421875
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11079.197265625
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1973, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11079.1982421875
tensor(11079.1963, grad_fn=<NegBackward0>) tensor(11079.1982, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6774, 0.3226],
        [0.2964, 0.7036]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9666, 0.0334], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1820, 0.0958],
         [0.6545, 0.3134]],

        [[0.5335, 0.0975],
         [0.6695, 0.5124]],

        [[0.5384, 0.0982],
         [0.5789, 0.6775]],

        [[0.6169, 0.1093],
         [0.5148, 0.6729]],

        [[0.6561, 0.1007],
         [0.5078, 0.5269]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844814436176263
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.5827742276999384
Average Adjusted Rand Index: 0.7043292286642829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21743.615234375
inf tensor(21743.6152, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11285.7900390625
tensor(21743.6152, grad_fn=<NegBackward0>) tensor(11285.7900, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11284.794921875
tensor(11285.7900, grad_fn=<NegBackward0>) tensor(11284.7949, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11283.568359375
tensor(11284.7949, grad_fn=<NegBackward0>) tensor(11283.5684, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11282.193359375
tensor(11283.5684, grad_fn=<NegBackward0>) tensor(11282.1934, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11278.3662109375
tensor(11282.1934, grad_fn=<NegBackward0>) tensor(11278.3662, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11115.3916015625
tensor(11278.3662, grad_fn=<NegBackward0>) tensor(11115.3916, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11092.85546875
tensor(11115.3916, grad_fn=<NegBackward0>) tensor(11092.8555, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11085.568359375
tensor(11092.8555, grad_fn=<NegBackward0>) tensor(11085.5684, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11085.3798828125
tensor(11085.5684, grad_fn=<NegBackward0>) tensor(11085.3799, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11085.26171875
tensor(11085.3799, grad_fn=<NegBackward0>) tensor(11085.2617, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11085.0380859375
tensor(11085.2617, grad_fn=<NegBackward0>) tensor(11085.0381, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11084.7021484375
tensor(11085.0381, grad_fn=<NegBackward0>) tensor(11084.7021, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11083.3291015625
tensor(11084.7021, grad_fn=<NegBackward0>) tensor(11083.3291, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11083.283203125
tensor(11083.3291, grad_fn=<NegBackward0>) tensor(11083.2832, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11083.2666015625
tensor(11083.2832, grad_fn=<NegBackward0>) tensor(11083.2666, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11083.255859375
tensor(11083.2666, grad_fn=<NegBackward0>) tensor(11083.2559, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11083.2470703125
tensor(11083.2559, grad_fn=<NegBackward0>) tensor(11083.2471, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11083.23828125
tensor(11083.2471, grad_fn=<NegBackward0>) tensor(11083.2383, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11083.228515625
tensor(11083.2383, grad_fn=<NegBackward0>) tensor(11083.2285, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11083.2109375
tensor(11083.2285, grad_fn=<NegBackward0>) tensor(11083.2109, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11083.2001953125
tensor(11083.2109, grad_fn=<NegBackward0>) tensor(11083.2002, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11083.1787109375
tensor(11083.2002, grad_fn=<NegBackward0>) tensor(11083.1787, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11083.171875
tensor(11083.1787, grad_fn=<NegBackward0>) tensor(11083.1719, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11083.166015625
tensor(11083.1719, grad_fn=<NegBackward0>) tensor(11083.1660, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11083.1640625
tensor(11083.1660, grad_fn=<NegBackward0>) tensor(11083.1641, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11083.15234375
tensor(11083.1641, grad_fn=<NegBackward0>) tensor(11083.1523, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11083.150390625
tensor(11083.1523, grad_fn=<NegBackward0>) tensor(11083.1504, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11083.1484375
tensor(11083.1504, grad_fn=<NegBackward0>) tensor(11083.1484, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11083.1474609375
tensor(11083.1484, grad_fn=<NegBackward0>) tensor(11083.1475, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11083.1484375
tensor(11083.1475, grad_fn=<NegBackward0>) tensor(11083.1484, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11083.1455078125
tensor(11083.1475, grad_fn=<NegBackward0>) tensor(11083.1455, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11083.14453125
tensor(11083.1455, grad_fn=<NegBackward0>) tensor(11083.1445, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11083.1435546875
tensor(11083.1445, grad_fn=<NegBackward0>) tensor(11083.1436, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11083.140625
tensor(11083.1436, grad_fn=<NegBackward0>) tensor(11083.1406, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11083.138671875
tensor(11083.1406, grad_fn=<NegBackward0>) tensor(11083.1387, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11083.1357421875
tensor(11083.1387, grad_fn=<NegBackward0>) tensor(11083.1357, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11083.134765625
tensor(11083.1357, grad_fn=<NegBackward0>) tensor(11083.1348, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11083.13671875
tensor(11083.1348, grad_fn=<NegBackward0>) tensor(11083.1367, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11083.126953125
tensor(11083.1348, grad_fn=<NegBackward0>) tensor(11083.1270, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11083.125
tensor(11083.1270, grad_fn=<NegBackward0>) tensor(11083.1250, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11083.125
tensor(11083.1250, grad_fn=<NegBackward0>) tensor(11083.1250, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11083.125
tensor(11083.1250, grad_fn=<NegBackward0>) tensor(11083.1250, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11083.125
tensor(11083.1250, grad_fn=<NegBackward0>) tensor(11083.1250, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11083.1240234375
tensor(11083.1250, grad_fn=<NegBackward0>) tensor(11083.1240, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11083.126953125
tensor(11083.1240, grad_fn=<NegBackward0>) tensor(11083.1270, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11083.1240234375
tensor(11083.1240, grad_fn=<NegBackward0>) tensor(11083.1240, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11083.1240234375
tensor(11083.1240, grad_fn=<NegBackward0>) tensor(11083.1240, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11083.130859375
tensor(11083.1240, grad_fn=<NegBackward0>) tensor(11083.1309, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11083.1220703125
tensor(11083.1240, grad_fn=<NegBackward0>) tensor(11083.1221, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11079.232421875
tensor(11083.1221, grad_fn=<NegBackward0>) tensor(11079.2324, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11079.2177734375
tensor(11079.2324, grad_fn=<NegBackward0>) tensor(11079.2178, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11079.216796875
tensor(11079.2178, grad_fn=<NegBackward0>) tensor(11079.2168, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11079.21484375
tensor(11079.2168, grad_fn=<NegBackward0>) tensor(11079.2148, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11079.216796875
tensor(11079.2148, grad_fn=<NegBackward0>) tensor(11079.2168, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11079.216796875
tensor(11079.2148, grad_fn=<NegBackward0>) tensor(11079.2168, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11079.2158203125
tensor(11079.2148, grad_fn=<NegBackward0>) tensor(11079.2158, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11079.2158203125
tensor(11079.2148, grad_fn=<NegBackward0>) tensor(11079.2158, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11079.2587890625
tensor(11079.2148, grad_fn=<NegBackward0>) tensor(11079.2588, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.6826, 0.3174],
        [0.2905, 0.7095]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9666, 0.0334], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1820, 0.0957],
         [0.6173, 0.3133]],

        [[0.6744, 0.0974],
         [0.5994, 0.5022]],

        [[0.6324, 0.0982],
         [0.6870, 0.7101]],

        [[0.7265, 0.1093],
         [0.5283, 0.5843]],

        [[0.5552, 0.1007],
         [0.5492, 0.6656]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844814436176263
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.5827742276999384
Average Adjusted Rand Index: 0.7043292286642829
[0.5827742276999384, 0.5827742276999384] [0.7043292286642829, 0.7043292286642829] [11079.1982421875, 11079.2587890625]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11258.537756308191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22149.314453125
inf tensor(22149.3145, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11504.822265625
tensor(22149.3145, grad_fn=<NegBackward0>) tensor(11504.8223, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11500.2294921875
tensor(11504.8223, grad_fn=<NegBackward0>) tensor(11500.2295, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11497.6484375
tensor(11500.2295, grad_fn=<NegBackward0>) tensor(11497.6484, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11495.0126953125
tensor(11497.6484, grad_fn=<NegBackward0>) tensor(11495.0127, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11488.9814453125
tensor(11495.0127, grad_fn=<NegBackward0>) tensor(11488.9814, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11388.515625
tensor(11488.9814, grad_fn=<NegBackward0>) tensor(11388.5156, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11360.5009765625
tensor(11388.5156, grad_fn=<NegBackward0>) tensor(11360.5010, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11355.02734375
tensor(11360.5010, grad_fn=<NegBackward0>) tensor(11355.0273, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11353.517578125
tensor(11355.0273, grad_fn=<NegBackward0>) tensor(11353.5176, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11347.3359375
tensor(11353.5176, grad_fn=<NegBackward0>) tensor(11347.3359, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11347.1640625
tensor(11347.3359, grad_fn=<NegBackward0>) tensor(11347.1641, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11345.5810546875
tensor(11347.1641, grad_fn=<NegBackward0>) tensor(11345.5811, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11345.224609375
tensor(11345.5811, grad_fn=<NegBackward0>) tensor(11345.2246, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11345.1708984375
tensor(11345.2246, grad_fn=<NegBackward0>) tensor(11345.1709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11345.099609375
tensor(11345.1709, grad_fn=<NegBackward0>) tensor(11345.0996, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11344.3759765625
tensor(11345.0996, grad_fn=<NegBackward0>) tensor(11344.3760, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11341.216796875
tensor(11344.3760, grad_fn=<NegBackward0>) tensor(11341.2168, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11338.091796875
tensor(11341.2168, grad_fn=<NegBackward0>) tensor(11338.0918, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11337.666015625
tensor(11338.0918, grad_fn=<NegBackward0>) tensor(11337.6660, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11337.12109375
tensor(11337.6660, grad_fn=<NegBackward0>) tensor(11337.1211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11337.0302734375
tensor(11337.1211, grad_fn=<NegBackward0>) tensor(11337.0303, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11336.9541015625
tensor(11337.0303, grad_fn=<NegBackward0>) tensor(11336.9541, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11336.818359375
tensor(11336.9541, grad_fn=<NegBackward0>) tensor(11336.8184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11326.7861328125
tensor(11336.8184, grad_fn=<NegBackward0>) tensor(11326.7861, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11322.3349609375
tensor(11326.7861, grad_fn=<NegBackward0>) tensor(11322.3350, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11317.7578125
tensor(11322.3350, grad_fn=<NegBackward0>) tensor(11317.7578, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11317.7109375
tensor(11317.7578, grad_fn=<NegBackward0>) tensor(11317.7109, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11317.6962890625
tensor(11317.7109, grad_fn=<NegBackward0>) tensor(11317.6963, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11317.6572265625
tensor(11317.6963, grad_fn=<NegBackward0>) tensor(11317.6572, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11310.318359375
tensor(11317.6572, grad_fn=<NegBackward0>) tensor(11310.3184, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11310.1171875
tensor(11310.3184, grad_fn=<NegBackward0>) tensor(11310.1172, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11307.5732421875
tensor(11310.1172, grad_fn=<NegBackward0>) tensor(11307.5732, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11303.8935546875
tensor(11307.5732, grad_fn=<NegBackward0>) tensor(11303.8936, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11303.8427734375
tensor(11303.8936, grad_fn=<NegBackward0>) tensor(11303.8428, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11303.8349609375
tensor(11303.8428, grad_fn=<NegBackward0>) tensor(11303.8350, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11302.9482421875
tensor(11303.8350, grad_fn=<NegBackward0>) tensor(11302.9482, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11302.8486328125
tensor(11302.9482, grad_fn=<NegBackward0>) tensor(11302.8486, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11302.8447265625
tensor(11302.8486, grad_fn=<NegBackward0>) tensor(11302.8447, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11302.826171875
tensor(11302.8447, grad_fn=<NegBackward0>) tensor(11302.8262, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11302.8193359375
tensor(11302.8262, grad_fn=<NegBackward0>) tensor(11302.8193, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11302.8134765625
tensor(11302.8193, grad_fn=<NegBackward0>) tensor(11302.8135, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11302.8115234375
tensor(11302.8135, grad_fn=<NegBackward0>) tensor(11302.8115, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11302.80859375
tensor(11302.8115, grad_fn=<NegBackward0>) tensor(11302.8086, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11302.8056640625
tensor(11302.8086, grad_fn=<NegBackward0>) tensor(11302.8057, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11302.8056640625
tensor(11302.8057, grad_fn=<NegBackward0>) tensor(11302.8057, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11302.8037109375
tensor(11302.8057, grad_fn=<NegBackward0>) tensor(11302.8037, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11302.802734375
tensor(11302.8037, grad_fn=<NegBackward0>) tensor(11302.8027, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11302.802734375
tensor(11302.8027, grad_fn=<NegBackward0>) tensor(11302.8027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11302.802734375
tensor(11302.8027, grad_fn=<NegBackward0>) tensor(11302.8027, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11302.8046875
tensor(11302.8027, grad_fn=<NegBackward0>) tensor(11302.8047, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11302.80078125
tensor(11302.8027, grad_fn=<NegBackward0>) tensor(11302.8008, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11302.80078125
tensor(11302.8008, grad_fn=<NegBackward0>) tensor(11302.8008, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11302.796875
tensor(11302.8008, grad_fn=<NegBackward0>) tensor(11302.7969, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11302.7958984375
tensor(11302.7969, grad_fn=<NegBackward0>) tensor(11302.7959, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11302.79296875
tensor(11302.7959, grad_fn=<NegBackward0>) tensor(11302.7930, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11302.798828125
tensor(11302.7930, grad_fn=<NegBackward0>) tensor(11302.7988, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11302.7900390625
tensor(11302.7930, grad_fn=<NegBackward0>) tensor(11302.7900, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11302.7890625
tensor(11302.7900, grad_fn=<NegBackward0>) tensor(11302.7891, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11302.7900390625
tensor(11302.7891, grad_fn=<NegBackward0>) tensor(11302.7900, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11302.7880859375
tensor(11302.7891, grad_fn=<NegBackward0>) tensor(11302.7881, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11302.7900390625
tensor(11302.7881, grad_fn=<NegBackward0>) tensor(11302.7900, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11302.7880859375
tensor(11302.7881, grad_fn=<NegBackward0>) tensor(11302.7881, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11302.7861328125
tensor(11302.7881, grad_fn=<NegBackward0>) tensor(11302.7861, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11302.787109375
tensor(11302.7861, grad_fn=<NegBackward0>) tensor(11302.7871, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11302.787109375
tensor(11302.7861, grad_fn=<NegBackward0>) tensor(11302.7871, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11302.791015625
tensor(11302.7861, grad_fn=<NegBackward0>) tensor(11302.7910, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11302.78125
tensor(11302.7861, grad_fn=<NegBackward0>) tensor(11302.7812, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11302.783203125
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7832, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11302.7900390625
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7900, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11302.7822265625
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7822, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11302.78125
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7812, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11302.7822265625
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7822, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11302.78125
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7812, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11302.779296875
tensor(11302.7812, grad_fn=<NegBackward0>) tensor(11302.7793, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11302.783203125
tensor(11302.7793, grad_fn=<NegBackward0>) tensor(11302.7832, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11302.78125
tensor(11302.7793, grad_fn=<NegBackward0>) tensor(11302.7812, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11302.7802734375
tensor(11302.7793, grad_fn=<NegBackward0>) tensor(11302.7803, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11302.775390625
tensor(11302.7793, grad_fn=<NegBackward0>) tensor(11302.7754, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11302.7763671875
tensor(11302.7754, grad_fn=<NegBackward0>) tensor(11302.7764, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11302.8701171875
tensor(11302.7754, grad_fn=<NegBackward0>) tensor(11302.8701, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11302.7763671875
tensor(11302.7754, grad_fn=<NegBackward0>) tensor(11302.7764, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11302.9013671875
tensor(11302.7754, grad_fn=<NegBackward0>) tensor(11302.9014, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11302.7763671875
tensor(11302.7754, grad_fn=<NegBackward0>) tensor(11302.7764, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.6398, 0.3602],
        [0.3936, 0.6064]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3979, 0.6021], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2736, 0.0955],
         [0.6479, 0.2244]],

        [[0.5832, 0.0929],
         [0.7069, 0.5843]],

        [[0.7242, 0.1086],
         [0.6836, 0.6557]],

        [[0.5872, 0.0998],
         [0.5951, 0.6551]],

        [[0.5251, 0.1047],
         [0.5836, 0.5280]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.573395733075917
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 83
Adjusted Rand Index: 0.43018028932640784
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207385189720222
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.07432354628332206
Average Adjusted Rand Index: 0.7229596055391531
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24787.734375
inf tensor(24787.7344, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11504.8515625
tensor(24787.7344, grad_fn=<NegBackward0>) tensor(11504.8516, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11503.9853515625
tensor(11504.8516, grad_fn=<NegBackward0>) tensor(11503.9854, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11502.6064453125
tensor(11503.9854, grad_fn=<NegBackward0>) tensor(11502.6064, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11500.623046875
tensor(11502.6064, grad_fn=<NegBackward0>) tensor(11500.6230, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11499.708984375
tensor(11500.6230, grad_fn=<NegBackward0>) tensor(11499.7090, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11499.3056640625
tensor(11499.7090, grad_fn=<NegBackward0>) tensor(11499.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11498.8056640625
tensor(11499.3057, grad_fn=<NegBackward0>) tensor(11498.8057, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11498.353515625
tensor(11498.8057, grad_fn=<NegBackward0>) tensor(11498.3535, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11497.8984375
tensor(11498.3535, grad_fn=<NegBackward0>) tensor(11497.8984, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11496.984375
tensor(11497.8984, grad_fn=<NegBackward0>) tensor(11496.9844, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11496.0048828125
tensor(11496.9844, grad_fn=<NegBackward0>) tensor(11496.0049, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11495.9296875
tensor(11496.0049, grad_fn=<NegBackward0>) tensor(11495.9297, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11495.8994140625
tensor(11495.9297, grad_fn=<NegBackward0>) tensor(11495.8994, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11495.8740234375
tensor(11495.8994, grad_fn=<NegBackward0>) tensor(11495.8740, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11495.84375
tensor(11495.8740, grad_fn=<NegBackward0>) tensor(11495.8438, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11495.7861328125
tensor(11495.8438, grad_fn=<NegBackward0>) tensor(11495.7861, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11495.62890625
tensor(11495.7861, grad_fn=<NegBackward0>) tensor(11495.6289, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11494.8994140625
tensor(11495.6289, grad_fn=<NegBackward0>) tensor(11494.8994, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11493.958984375
tensor(11494.8994, grad_fn=<NegBackward0>) tensor(11493.9590, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11400.19921875
tensor(11493.9590, grad_fn=<NegBackward0>) tensor(11400.1992, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11397.814453125
tensor(11400.1992, grad_fn=<NegBackward0>) tensor(11397.8145, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11397.4228515625
tensor(11397.8145, grad_fn=<NegBackward0>) tensor(11397.4229, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11371.3388671875
tensor(11397.4229, grad_fn=<NegBackward0>) tensor(11371.3389, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11356.146484375
tensor(11371.3389, grad_fn=<NegBackward0>) tensor(11356.1465, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11334.125
tensor(11356.1465, grad_fn=<NegBackward0>) tensor(11334.1250, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11315.841796875
tensor(11334.1250, grad_fn=<NegBackward0>) tensor(11315.8418, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11305.587890625
tensor(11315.8418, grad_fn=<NegBackward0>) tensor(11305.5879, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11298.2861328125
tensor(11305.5879, grad_fn=<NegBackward0>) tensor(11298.2861, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11290.6005859375
tensor(11298.2861, grad_fn=<NegBackward0>) tensor(11290.6006, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11290.4892578125
tensor(11290.6006, grad_fn=<NegBackward0>) tensor(11290.4893, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11286.0751953125
tensor(11290.4893, grad_fn=<NegBackward0>) tensor(11286.0752, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11277.255859375
tensor(11286.0752, grad_fn=<NegBackward0>) tensor(11277.2559, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11265.3955078125
tensor(11277.2559, grad_fn=<NegBackward0>) tensor(11265.3955, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11265.3896484375
tensor(11265.3955, grad_fn=<NegBackward0>) tensor(11265.3896, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11265.259765625
tensor(11265.3896, grad_fn=<NegBackward0>) tensor(11265.2598, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11261.2001953125
tensor(11265.2598, grad_fn=<NegBackward0>) tensor(11261.2002, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11260.95703125
tensor(11261.2002, grad_fn=<NegBackward0>) tensor(11260.9570, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11254.6572265625
tensor(11260.9570, grad_fn=<NegBackward0>) tensor(11254.6572, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11254.6142578125
tensor(11254.6572, grad_fn=<NegBackward0>) tensor(11254.6143, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11254.40625
tensor(11254.6143, grad_fn=<NegBackward0>) tensor(11254.4062, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11254.4052734375
tensor(11254.4062, grad_fn=<NegBackward0>) tensor(11254.4053, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11254.404296875
tensor(11254.4053, grad_fn=<NegBackward0>) tensor(11254.4043, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11247.4140625
tensor(11254.4043, grad_fn=<NegBackward0>) tensor(11247.4141, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11247.4013671875
tensor(11247.4141, grad_fn=<NegBackward0>) tensor(11247.4014, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11237.044921875
tensor(11247.4014, grad_fn=<NegBackward0>) tensor(11237.0449, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11237.0458984375
tensor(11237.0449, grad_fn=<NegBackward0>) tensor(11237.0459, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11237.04296875
tensor(11237.0449, grad_fn=<NegBackward0>) tensor(11237.0430, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11237.0439453125
tensor(11237.0430, grad_fn=<NegBackward0>) tensor(11237.0439, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11237.0439453125
tensor(11237.0430, grad_fn=<NegBackward0>) tensor(11237.0439, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11237.0419921875
tensor(11237.0430, grad_fn=<NegBackward0>) tensor(11237.0420, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11237.04296875
tensor(11237.0420, grad_fn=<NegBackward0>) tensor(11237.0430, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11237.041015625
tensor(11237.0420, grad_fn=<NegBackward0>) tensor(11237.0410, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11237.0380859375
tensor(11237.0410, grad_fn=<NegBackward0>) tensor(11237.0381, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11237.0380859375
tensor(11237.0381, grad_fn=<NegBackward0>) tensor(11237.0381, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11237.0390625
tensor(11237.0381, grad_fn=<NegBackward0>) tensor(11237.0391, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11237.0361328125
tensor(11237.0381, grad_fn=<NegBackward0>) tensor(11237.0361, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11237.0302734375
tensor(11237.0361, grad_fn=<NegBackward0>) tensor(11237.0303, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11237.025390625
tensor(11237.0303, grad_fn=<NegBackward0>) tensor(11237.0254, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11237.0234375
tensor(11237.0254, grad_fn=<NegBackward0>) tensor(11237.0234, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11237.0244140625
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0244, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11237.0244140625
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0244, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11237.0234375
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0234, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11237.025390625
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0254, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11237.0244140625
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0244, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11237.0244140625
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237.0244, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11237.0
tensor(11237.0234, grad_fn=<NegBackward0>) tensor(11237., grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11236.994140625
tensor(11237., grad_fn=<NegBackward0>) tensor(11236.9941, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11236.994140625
tensor(11236.9941, grad_fn=<NegBackward0>) tensor(11236.9941, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11236.9951171875
tensor(11236.9941, grad_fn=<NegBackward0>) tensor(11236.9951, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11236.9931640625
tensor(11236.9941, grad_fn=<NegBackward0>) tensor(11236.9932, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11236.9912109375
tensor(11236.9932, grad_fn=<NegBackward0>) tensor(11236.9912, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11236.9462890625
tensor(11236.9912, grad_fn=<NegBackward0>) tensor(11236.9463, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11236.955078125
tensor(11236.9463, grad_fn=<NegBackward0>) tensor(11236.9551, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11236.94921875
tensor(11236.9463, grad_fn=<NegBackward0>) tensor(11236.9492, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11236.9501953125
tensor(11236.9463, grad_fn=<NegBackward0>) tensor(11236.9502, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11236.9453125
tensor(11236.9463, grad_fn=<NegBackward0>) tensor(11236.9453, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11236.9453125
tensor(11236.9453, grad_fn=<NegBackward0>) tensor(11236.9453, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11236.9443359375
tensor(11236.9453, grad_fn=<NegBackward0>) tensor(11236.9443, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11236.9443359375
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9443, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11236.9443359375
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9443, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11236.947265625
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9473, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11236.9453125
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9453, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11236.9462890625
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9463, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11236.9501953125
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9502, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11236.9580078125
tensor(11236.9443, grad_fn=<NegBackward0>) tensor(11236.9580, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7556, 0.2444],
        [0.2842, 0.7158]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2930, 0.0997],
         [0.6191, 0.1992]],

        [[0.6836, 0.0949],
         [0.7225, 0.7090]],

        [[0.5012, 0.1106],
         [0.6390, 0.6621]],

        [[0.7307, 0.1030],
         [0.5844, 0.6498]],

        [[0.5783, 0.1064],
         [0.7038, 0.5518]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.9137634242965411
Average Adjusted Rand Index: 0.9140886406117504
[0.07432354628332206, 0.9137634242965411] [0.7229596055391531, 0.9140886406117504] [11302.7763671875, 11236.9580078125]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11198.63042712653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21964.52734375
inf tensor(21964.5273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11474.5625
tensor(21964.5273, grad_fn=<NegBackward0>) tensor(11474.5625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11461.248046875
tensor(11474.5625, grad_fn=<NegBackward0>) tensor(11461.2480, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11411.1416015625
tensor(11461.2480, grad_fn=<NegBackward0>) tensor(11411.1416, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11243.3037109375
tensor(11411.1416, grad_fn=<NegBackward0>) tensor(11243.3037, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11182.064453125
tensor(11243.3037, grad_fn=<NegBackward0>) tensor(11182.0645, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11177.333984375
tensor(11182.0645, grad_fn=<NegBackward0>) tensor(11177.3340, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11170.693359375
tensor(11177.3340, grad_fn=<NegBackward0>) tensor(11170.6934, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11165.7666015625
tensor(11170.6934, grad_fn=<NegBackward0>) tensor(11165.7666, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11165.599609375
tensor(11165.7666, grad_fn=<NegBackward0>) tensor(11165.5996, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11165.556640625
tensor(11165.5996, grad_fn=<NegBackward0>) tensor(11165.5566, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11165.4990234375
tensor(11165.5566, grad_fn=<NegBackward0>) tensor(11165.4990, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11165.240234375
tensor(11165.4990, grad_fn=<NegBackward0>) tensor(11165.2402, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11165.1689453125
tensor(11165.2402, grad_fn=<NegBackward0>) tensor(11165.1689, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11165.15234375
tensor(11165.1689, grad_fn=<NegBackward0>) tensor(11165.1523, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11165.142578125
tensor(11165.1523, grad_fn=<NegBackward0>) tensor(11165.1426, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11165.134765625
tensor(11165.1426, grad_fn=<NegBackward0>) tensor(11165.1348, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11165.126953125
tensor(11165.1348, grad_fn=<NegBackward0>) tensor(11165.1270, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11165.1044921875
tensor(11165.1270, grad_fn=<NegBackward0>) tensor(11165.1045, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11165.08984375
tensor(11165.1045, grad_fn=<NegBackward0>) tensor(11165.0898, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11165.025390625
tensor(11165.0898, grad_fn=<NegBackward0>) tensor(11165.0254, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11165.0234375
tensor(11165.0254, grad_fn=<NegBackward0>) tensor(11165.0234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11164.892578125
tensor(11165.0234, grad_fn=<NegBackward0>) tensor(11164.8926, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11164.8603515625
tensor(11164.8926, grad_fn=<NegBackward0>) tensor(11164.8604, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11164.857421875
tensor(11164.8604, grad_fn=<NegBackward0>) tensor(11164.8574, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11164.8544921875
tensor(11164.8574, grad_fn=<NegBackward0>) tensor(11164.8545, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11164.853515625
tensor(11164.8545, grad_fn=<NegBackward0>) tensor(11164.8535, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11164.8505859375
tensor(11164.8535, grad_fn=<NegBackward0>) tensor(11164.8506, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11164.84765625
tensor(11164.8506, grad_fn=<NegBackward0>) tensor(11164.8477, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11164.8486328125
tensor(11164.8477, grad_fn=<NegBackward0>) tensor(11164.8486, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11164.8466796875
tensor(11164.8477, grad_fn=<NegBackward0>) tensor(11164.8467, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11164.84375
tensor(11164.8467, grad_fn=<NegBackward0>) tensor(11164.8438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11164.8369140625
tensor(11164.8438, grad_fn=<NegBackward0>) tensor(11164.8369, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11164.8369140625
tensor(11164.8369, grad_fn=<NegBackward0>) tensor(11164.8369, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11164.833984375
tensor(11164.8369, grad_fn=<NegBackward0>) tensor(11164.8340, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11164.833984375
tensor(11164.8340, grad_fn=<NegBackward0>) tensor(11164.8340, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11164.8330078125
tensor(11164.8340, grad_fn=<NegBackward0>) tensor(11164.8330, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11164.8310546875
tensor(11164.8330, grad_fn=<NegBackward0>) tensor(11164.8311, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11164.830078125
tensor(11164.8311, grad_fn=<NegBackward0>) tensor(11164.8301, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11164.8291015625
tensor(11164.8301, grad_fn=<NegBackward0>) tensor(11164.8291, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11164.830078125
tensor(11164.8291, grad_fn=<NegBackward0>) tensor(11164.8301, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11164.8212890625
tensor(11164.8291, grad_fn=<NegBackward0>) tensor(11164.8213, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11164.7783203125
tensor(11164.8213, grad_fn=<NegBackward0>) tensor(11164.7783, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11164.7724609375
tensor(11164.7783, grad_fn=<NegBackward0>) tensor(11164.7725, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11164.322265625
tensor(11164.7725, grad_fn=<NegBackward0>) tensor(11164.3223, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11164.1474609375
tensor(11164.3223, grad_fn=<NegBackward0>) tensor(11164.1475, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11164.146484375
tensor(11164.1475, grad_fn=<NegBackward0>) tensor(11164.1465, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11164.1474609375
tensor(11164.1465, grad_fn=<NegBackward0>) tensor(11164.1475, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11164.1435546875
tensor(11164.1465, grad_fn=<NegBackward0>) tensor(11164.1436, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11164.1435546875
tensor(11164.1436, grad_fn=<NegBackward0>) tensor(11164.1436, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11164.1435546875
tensor(11164.1436, grad_fn=<NegBackward0>) tensor(11164.1436, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11164.1494140625
tensor(11164.1436, grad_fn=<NegBackward0>) tensor(11164.1494, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11164.142578125
tensor(11164.1436, grad_fn=<NegBackward0>) tensor(11164.1426, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11164.1396484375
tensor(11164.1426, grad_fn=<NegBackward0>) tensor(11164.1396, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11164.1396484375
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1396, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11164.1416015625
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1416, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11164.138671875
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11164.138671875
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11164.138671875
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11164.1396484375
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1396, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11164.1396484375
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1396, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11164.138671875
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11164.138671875
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11164.13671875
tensor(11164.1387, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11164.1376953125
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1377, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11164.1376953125
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1377, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11164.13671875
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11164.13671875
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11164.1376953125
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1377, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11164.150390625
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1504, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11164.13671875
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11164.13671875
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11164.1357421875
tensor(11164.1367, grad_fn=<NegBackward0>) tensor(11164.1357, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11164.13671875
tensor(11164.1357, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11164.173828125
tensor(11164.1357, grad_fn=<NegBackward0>) tensor(11164.1738, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11164.1337890625
tensor(11164.1357, grad_fn=<NegBackward0>) tensor(11164.1338, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11164.1337890625
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1338, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11164.134765625
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1348, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11164.142578125
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1426, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11164.125
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1250, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11164.126953125
tensor(11164.1250, grad_fn=<NegBackward0>) tensor(11164.1270, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11164.125
tensor(11164.1250, grad_fn=<NegBackward0>) tensor(11164.1250, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11164.125
tensor(11164.1250, grad_fn=<NegBackward0>) tensor(11164.1250, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11164.1240234375
tensor(11164.1250, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11164.125
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1250, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11164.1240234375
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11164.1240234375
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11164.1240234375
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11164.1640625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1641, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11164.1240234375
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11164.1259765625
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1260, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11164.1669921875
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1670, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11164.1201171875
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1201, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11164.12109375
tensor(11164.1201, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11164.16796875
tensor(11164.1201, grad_fn=<NegBackward0>) tensor(11164.1680, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7312, 0.2688],
        [0.2544, 0.7456]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5857, 0.4143], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2981, 0.0962],
         [0.6399, 0.1922]],

        [[0.5793, 0.1116],
         [0.5732, 0.5031]],

        [[0.5917, 0.1017],
         [0.7116, 0.5773]],

        [[0.5109, 0.0943],
         [0.5784, 0.6268]],

        [[0.6232, 0.0966],
         [0.5207, 0.6686]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.9061162282179316
Average Adjusted Rand Index: 0.90593048170787
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22452.421875
inf tensor(22452.4219, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11477.02734375
tensor(22452.4219, grad_fn=<NegBackward0>) tensor(11477.0273, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11473.7255859375
tensor(11477.0273, grad_fn=<NegBackward0>) tensor(11473.7256, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11463.939453125
tensor(11473.7256, grad_fn=<NegBackward0>) tensor(11463.9395, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11229.2041015625
tensor(11463.9395, grad_fn=<NegBackward0>) tensor(11229.2041, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11166.7939453125
tensor(11229.2041, grad_fn=<NegBackward0>) tensor(11166.7939, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11165.802734375
tensor(11166.7939, grad_fn=<NegBackward0>) tensor(11165.8027, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11165.326171875
tensor(11165.8027, grad_fn=<NegBackward0>) tensor(11165.3262, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11165.095703125
tensor(11165.3262, grad_fn=<NegBackward0>) tensor(11165.0957, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11164.431640625
tensor(11165.0957, grad_fn=<NegBackward0>) tensor(11164.4316, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11164.380859375
tensor(11164.4316, grad_fn=<NegBackward0>) tensor(11164.3809, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11164.3115234375
tensor(11164.3809, grad_fn=<NegBackward0>) tensor(11164.3115, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11164.2900390625
tensor(11164.3115, grad_fn=<NegBackward0>) tensor(11164.2900, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11164.2578125
tensor(11164.2900, grad_fn=<NegBackward0>) tensor(11164.2578, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11164.248046875
tensor(11164.2578, grad_fn=<NegBackward0>) tensor(11164.2480, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11164.2392578125
tensor(11164.2480, grad_fn=<NegBackward0>) tensor(11164.2393, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11164.232421875
tensor(11164.2393, grad_fn=<NegBackward0>) tensor(11164.2324, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11164.2216796875
tensor(11164.2324, grad_fn=<NegBackward0>) tensor(11164.2217, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11164.1826171875
tensor(11164.2217, grad_fn=<NegBackward0>) tensor(11164.1826, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11164.1787109375
tensor(11164.1826, grad_fn=<NegBackward0>) tensor(11164.1787, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11164.1708984375
tensor(11164.1787, grad_fn=<NegBackward0>) tensor(11164.1709, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11164.1884765625
tensor(11164.1709, grad_fn=<NegBackward0>) tensor(11164.1885, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11164.1640625
tensor(11164.1709, grad_fn=<NegBackward0>) tensor(11164.1641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11164.162109375
tensor(11164.1641, grad_fn=<NegBackward0>) tensor(11164.1621, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11164.1591796875
tensor(11164.1621, grad_fn=<NegBackward0>) tensor(11164.1592, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11164.1572265625
tensor(11164.1592, grad_fn=<NegBackward0>) tensor(11164.1572, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11164.1533203125
tensor(11164.1572, grad_fn=<NegBackward0>) tensor(11164.1533, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11164.15234375
tensor(11164.1533, grad_fn=<NegBackward0>) tensor(11164.1523, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11164.1494140625
tensor(11164.1523, grad_fn=<NegBackward0>) tensor(11164.1494, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11164.1474609375
tensor(11164.1494, grad_fn=<NegBackward0>) tensor(11164.1475, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11164.146484375
tensor(11164.1475, grad_fn=<NegBackward0>) tensor(11164.1465, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11164.150390625
tensor(11164.1465, grad_fn=<NegBackward0>) tensor(11164.1504, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11164.1455078125
tensor(11164.1465, grad_fn=<NegBackward0>) tensor(11164.1455, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11164.158203125
tensor(11164.1455, grad_fn=<NegBackward0>) tensor(11164.1582, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11164.142578125
tensor(11164.1455, grad_fn=<NegBackward0>) tensor(11164.1426, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11164.142578125
tensor(11164.1426, grad_fn=<NegBackward0>) tensor(11164.1426, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11164.1435546875
tensor(11164.1426, grad_fn=<NegBackward0>) tensor(11164.1436, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11164.1396484375
tensor(11164.1426, grad_fn=<NegBackward0>) tensor(11164.1396, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11164.140625
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1406, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11164.1416015625
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1416, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11164.1376953125
tensor(11164.1396, grad_fn=<NegBackward0>) tensor(11164.1377, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11164.138671875
tensor(11164.1377, grad_fn=<NegBackward0>) tensor(11164.1387, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11164.1357421875
tensor(11164.1377, grad_fn=<NegBackward0>) tensor(11164.1357, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11164.1357421875
tensor(11164.1357, grad_fn=<NegBackward0>) tensor(11164.1357, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11164.134765625
tensor(11164.1357, grad_fn=<NegBackward0>) tensor(11164.1348, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11164.1357421875
tensor(11164.1348, grad_fn=<NegBackward0>) tensor(11164.1357, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11164.13671875
tensor(11164.1348, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11164.134765625
tensor(11164.1348, grad_fn=<NegBackward0>) tensor(11164.1348, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11164.1337890625
tensor(11164.1348, grad_fn=<NegBackward0>) tensor(11164.1338, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11164.134765625
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1348, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11164.134765625
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1348, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11164.1337890625
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1338, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11164.1328125
tensor(11164.1338, grad_fn=<NegBackward0>) tensor(11164.1328, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11164.1328125
tensor(11164.1328, grad_fn=<NegBackward0>) tensor(11164.1328, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11164.1337890625
tensor(11164.1328, grad_fn=<NegBackward0>) tensor(11164.1338, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11164.1376953125
tensor(11164.1328, grad_fn=<NegBackward0>) tensor(11164.1377, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11164.130859375
tensor(11164.1328, grad_fn=<NegBackward0>) tensor(11164.1309, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11164.126953125
tensor(11164.1309, grad_fn=<NegBackward0>) tensor(11164.1270, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11164.1240234375
tensor(11164.1270, grad_fn=<NegBackward0>) tensor(11164.1240, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11164.1318359375
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1318, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11164.1220703125
tensor(11164.1240, grad_fn=<NegBackward0>) tensor(11164.1221, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11164.12109375
tensor(11164.1221, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11164.1220703125
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1221, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11164.142578125
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1426, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11164.1220703125
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1221, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11164.12109375
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1211, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11164.123046875
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1230, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11164.126953125
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1270, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11164.123046875
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1230, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11164.12890625
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1289, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11164.13671875
tensor(11164.1211, grad_fn=<NegBackward0>) tensor(11164.1367, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7301, 0.2699],
        [0.2552, 0.7448]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5839, 0.4161], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2987, 0.0962],
         [0.5628, 0.1922]],

        [[0.7103, 0.1117],
         [0.5112, 0.6237]],

        [[0.5455, 0.1016],
         [0.5672, 0.7164]],

        [[0.7252, 0.0942],
         [0.7098, 0.5698]],

        [[0.7094, 0.0966],
         [0.7051, 0.7165]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.9061162282179316
Average Adjusted Rand Index: 0.90593048170787
[0.9061162282179316, 0.9061162282179316] [0.90593048170787, 0.90593048170787] [11164.12109375, 11164.13671875]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11037.410680040199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21025.587890625
inf tensor(21025.5879, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11347.314453125
tensor(21025.5879, grad_fn=<NegBackward0>) tensor(11347.3145, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11337.4365234375
tensor(11347.3145, grad_fn=<NegBackward0>) tensor(11337.4365, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11262.9013671875
tensor(11337.4365, grad_fn=<NegBackward0>) tensor(11262.9014, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11069.5126953125
tensor(11262.9014, grad_fn=<NegBackward0>) tensor(11069.5127, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11043.861328125
tensor(11069.5127, grad_fn=<NegBackward0>) tensor(11043.8613, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11014.55859375
tensor(11043.8613, grad_fn=<NegBackward0>) tensor(11014.5586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11012.8369140625
tensor(11014.5586, grad_fn=<NegBackward0>) tensor(11012.8369, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11012.6240234375
tensor(11012.8369, grad_fn=<NegBackward0>) tensor(11012.6240, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11012.5625
tensor(11012.6240, grad_fn=<NegBackward0>) tensor(11012.5625, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11012.5234375
tensor(11012.5625, grad_fn=<NegBackward0>) tensor(11012.5234, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11012.4970703125
tensor(11012.5234, grad_fn=<NegBackward0>) tensor(11012.4971, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11012.4755859375
tensor(11012.4971, grad_fn=<NegBackward0>) tensor(11012.4756, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11012.4560546875
tensor(11012.4756, grad_fn=<NegBackward0>) tensor(11012.4561, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11012.3603515625
tensor(11012.4561, grad_fn=<NegBackward0>) tensor(11012.3604, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11012.19921875
tensor(11012.3604, grad_fn=<NegBackward0>) tensor(11012.1992, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11012.17578125
tensor(11012.1992, grad_fn=<NegBackward0>) tensor(11012.1758, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11012.14453125
tensor(11012.1758, grad_fn=<NegBackward0>) tensor(11012.1445, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11012.1396484375
tensor(11012.1445, grad_fn=<NegBackward0>) tensor(11012.1396, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11012.134765625
tensor(11012.1396, grad_fn=<NegBackward0>) tensor(11012.1348, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11012.1298828125
tensor(11012.1348, grad_fn=<NegBackward0>) tensor(11012.1299, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11012.1337890625
tensor(11012.1299, grad_fn=<NegBackward0>) tensor(11012.1338, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11012.12109375
tensor(11012.1299, grad_fn=<NegBackward0>) tensor(11012.1211, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11012.1171875
tensor(11012.1211, grad_fn=<NegBackward0>) tensor(11012.1172, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11012.1162109375
tensor(11012.1172, grad_fn=<NegBackward0>) tensor(11012.1162, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11012.11328125
tensor(11012.1162, grad_fn=<NegBackward0>) tensor(11012.1133, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11012.1123046875
tensor(11012.1133, grad_fn=<NegBackward0>) tensor(11012.1123, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11012.1103515625
tensor(11012.1123, grad_fn=<NegBackward0>) tensor(11012.1104, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11012.109375
tensor(11012.1104, grad_fn=<NegBackward0>) tensor(11012.1094, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11012.1123046875
tensor(11012.1094, grad_fn=<NegBackward0>) tensor(11012.1123, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11012.107421875
tensor(11012.1094, grad_fn=<NegBackward0>) tensor(11012.1074, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11012.1064453125
tensor(11012.1074, grad_fn=<NegBackward0>) tensor(11012.1064, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11012.10546875
tensor(11012.1064, grad_fn=<NegBackward0>) tensor(11012.1055, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11012.10546875
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1055, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11012.1044921875
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11012.103515625
tensor(11012.1045, grad_fn=<NegBackward0>) tensor(11012.1035, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11012.103515625
tensor(11012.1035, grad_fn=<NegBackward0>) tensor(11012.1035, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11012.1025390625
tensor(11012.1035, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11012.1025390625
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11012.1005859375
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1006, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11012.1005859375
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1006, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11012.1015625
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1016, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11012.1015625
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1016, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11012.099609375
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.0996, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11012.099609375
tensor(11012.0996, grad_fn=<NegBackward0>) tensor(11012.0996, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11012.1025390625
tensor(11012.0996, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11012.099609375
tensor(11012.0996, grad_fn=<NegBackward0>) tensor(11012.0996, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11012.0986328125
tensor(11012.0996, grad_fn=<NegBackward0>) tensor(11012.0986, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11012.099609375
tensor(11012.0986, grad_fn=<NegBackward0>) tensor(11012.0996, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11012.0986328125
tensor(11012.0986, grad_fn=<NegBackward0>) tensor(11012.0986, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11012.09765625
tensor(11012.0986, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11012.09765625
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11012.09765625
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11012.09765625
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11012.09765625
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11012.09765625
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11012.095703125
tensor(11012.0977, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11012.09765625
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11012.0966796875
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11012.0966796875
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11012.0966796875
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11012.0966796875
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11012.1025390625
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11012.0986328125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0986, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11012.095703125
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11012.0947265625
tensor(11012.0957, grad_fn=<NegBackward0>) tensor(11012.0947, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11012.1044921875
tensor(11012.0947, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11012.095703125
tensor(11012.0947, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11012.0966796875
tensor(11012.0947, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11012.115234375
tensor(11012.0947, grad_fn=<NegBackward0>) tensor(11012.1152, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11012.09375
tensor(11012.0947, grad_fn=<NegBackward0>) tensor(11012.0938, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11012.095703125
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11012.09375
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0938, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11012.095703125
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11012.095703125
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11012.095703125
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11012.09765625
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0977, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11012.095703125
tensor(11012.0938, grad_fn=<NegBackward0>) tensor(11012.0957, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.6894, 0.3106],
        [0.2590, 0.7410]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5189, 0.4811], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1824, 0.1038],
         [0.5228, 0.3029]],

        [[0.7240, 0.0932],
         [0.5174, 0.6062]],

        [[0.7004, 0.0954],
         [0.5109, 0.7125]],

        [[0.7295, 0.0999],
         [0.6095, 0.5480]],

        [[0.7205, 0.0921],
         [0.6613, 0.6720]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9291543574908578
Average Adjusted Rand Index: 0.9297718258455238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22047.12109375
inf tensor(22047.1211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11347.8525390625
tensor(22047.1211, grad_fn=<NegBackward0>) tensor(11347.8525, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11343.076171875
tensor(11347.8525, grad_fn=<NegBackward0>) tensor(11343.0762, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11333.998046875
tensor(11343.0762, grad_fn=<NegBackward0>) tensor(11333.9980, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11262.41015625
tensor(11333.9980, grad_fn=<NegBackward0>) tensor(11262.4102, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11081.052734375
tensor(11262.4102, grad_fn=<NegBackward0>) tensor(11081.0527, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11033.794921875
tensor(11081.0527, grad_fn=<NegBackward0>) tensor(11033.7949, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11026.4716796875
tensor(11033.7949, grad_fn=<NegBackward0>) tensor(11026.4717, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11019.052734375
tensor(11026.4717, grad_fn=<NegBackward0>) tensor(11019.0527, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11018.8701171875
tensor(11019.0527, grad_fn=<NegBackward0>) tensor(11018.8701, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11018.78125
tensor(11018.8701, grad_fn=<NegBackward0>) tensor(11018.7812, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11018.7216796875
tensor(11018.7812, grad_fn=<NegBackward0>) tensor(11018.7217, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11018.669921875
tensor(11018.7217, grad_fn=<NegBackward0>) tensor(11018.6699, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11013.6162109375
tensor(11018.6699, grad_fn=<NegBackward0>) tensor(11013.6162, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11013.5751953125
tensor(11013.6162, grad_fn=<NegBackward0>) tensor(11013.5752, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11013.5556640625
tensor(11013.5752, grad_fn=<NegBackward0>) tensor(11013.5557, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11013.5390625
tensor(11013.5557, grad_fn=<NegBackward0>) tensor(11013.5391, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11013.5263671875
tensor(11013.5391, grad_fn=<NegBackward0>) tensor(11013.5264, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11013.515625
tensor(11013.5264, grad_fn=<NegBackward0>) tensor(11013.5156, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11013.5078125
tensor(11013.5156, grad_fn=<NegBackward0>) tensor(11013.5078, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11013.4990234375
tensor(11013.5078, grad_fn=<NegBackward0>) tensor(11013.4990, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11013.484375
tensor(11013.4990, grad_fn=<NegBackward0>) tensor(11013.4844, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11013.2060546875
tensor(11013.4844, grad_fn=<NegBackward0>) tensor(11013.2061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11012.625
tensor(11013.2061, grad_fn=<NegBackward0>) tensor(11012.6250, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11012.3408203125
tensor(11012.6250, grad_fn=<NegBackward0>) tensor(11012.3408, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11012.3359375
tensor(11012.3408, grad_fn=<NegBackward0>) tensor(11012.3359, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11012.3330078125
tensor(11012.3359, grad_fn=<NegBackward0>) tensor(11012.3330, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11012.3310546875
tensor(11012.3330, grad_fn=<NegBackward0>) tensor(11012.3311, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11012.328125
tensor(11012.3311, grad_fn=<NegBackward0>) tensor(11012.3281, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11012.333984375
tensor(11012.3281, grad_fn=<NegBackward0>) tensor(11012.3340, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11012.32421875
tensor(11012.3281, grad_fn=<NegBackward0>) tensor(11012.3242, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11012.3212890625
tensor(11012.3242, grad_fn=<NegBackward0>) tensor(11012.3213, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11012.3193359375
tensor(11012.3213, grad_fn=<NegBackward0>) tensor(11012.3193, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11012.3173828125
tensor(11012.3193, grad_fn=<NegBackward0>) tensor(11012.3174, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11012.314453125
tensor(11012.3174, grad_fn=<NegBackward0>) tensor(11012.3145, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11012.283203125
tensor(11012.3145, grad_fn=<NegBackward0>) tensor(11012.2832, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11012.267578125
tensor(11012.2832, grad_fn=<NegBackward0>) tensor(11012.2676, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11012.1484375
tensor(11012.2676, grad_fn=<NegBackward0>) tensor(11012.1484, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11012.1396484375
tensor(11012.1484, grad_fn=<NegBackward0>) tensor(11012.1396, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11012.1337890625
tensor(11012.1396, grad_fn=<NegBackward0>) tensor(11012.1338, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11012.12890625
tensor(11012.1338, grad_fn=<NegBackward0>) tensor(11012.1289, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11012.1357421875
tensor(11012.1289, grad_fn=<NegBackward0>) tensor(11012.1357, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11012.1259765625
tensor(11012.1289, grad_fn=<NegBackward0>) tensor(11012.1260, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11012.1220703125
tensor(11012.1260, grad_fn=<NegBackward0>) tensor(11012.1221, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11012.115234375
tensor(11012.1221, grad_fn=<NegBackward0>) tensor(11012.1152, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11012.1123046875
tensor(11012.1152, grad_fn=<NegBackward0>) tensor(11012.1123, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11012.1103515625
tensor(11012.1123, grad_fn=<NegBackward0>) tensor(11012.1104, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11012.111328125
tensor(11012.1104, grad_fn=<NegBackward0>) tensor(11012.1113, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11012.10546875
tensor(11012.1104, grad_fn=<NegBackward0>) tensor(11012.1055, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11012.1201171875
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1201, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11012.10546875
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1055, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11012.1103515625
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1104, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11012.1044921875
tensor(11012.1055, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11012.1044921875
tensor(11012.1045, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11012.103515625
tensor(11012.1045, grad_fn=<NegBackward0>) tensor(11012.1035, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11012.1025390625
tensor(11012.1035, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11012.1044921875
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11012.1044921875
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1045, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11012.1064453125
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1064, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11012.1005859375
tensor(11012.1025, grad_fn=<NegBackward0>) tensor(11012.1006, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11012.1025390625
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11012.1025390625
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1025, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11012.1103515625
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.1104, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11012.0966796875
tensor(11012.1006, grad_fn=<NegBackward0>) tensor(11012.0967, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11012.0654296875
tensor(11012.0967, grad_fn=<NegBackward0>) tensor(11012.0654, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11012.06640625
tensor(11012.0654, grad_fn=<NegBackward0>) tensor(11012.0664, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11012.064453125
tensor(11012.0654, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11012.064453125
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11012.0654296875
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0654, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11012.068359375
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0684, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11012.064453125
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11012.064453125
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11012.0634765625
tensor(11012.0645, grad_fn=<NegBackward0>) tensor(11012.0635, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11012.064453125
tensor(11012.0635, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11012.0634765625
tensor(11012.0635, grad_fn=<NegBackward0>) tensor(11012.0635, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11012.0634765625
tensor(11012.0635, grad_fn=<NegBackward0>) tensor(11012.0635, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11012.060546875
tensor(11012.0635, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11012.0673828125
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0674, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11012.060546875
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11012.064453125
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0645, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11012.060546875
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11012.060546875
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11012.060546875
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11012.0595703125
tensor(11012.0605, grad_fn=<NegBackward0>) tensor(11012.0596, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11012.1083984375
tensor(11012.0596, grad_fn=<NegBackward0>) tensor(11012.1084, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11012.060546875
tensor(11012.0596, grad_fn=<NegBackward0>) tensor(11012.0605, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11012.0615234375
tensor(11012.0596, grad_fn=<NegBackward0>) tensor(11012.0615, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11012.0615234375
tensor(11012.0596, grad_fn=<NegBackward0>) tensor(11012.0615, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11012.0693359375
tensor(11012.0596, grad_fn=<NegBackward0>) tensor(11012.0693, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7430, 0.2570],
        [0.3112, 0.6888]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4835, 0.5165], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3022, 0.1038],
         [0.7309, 0.1824]],

        [[0.6249, 0.0932],
         [0.7092, 0.6552]],

        [[0.6232, 0.0954],
         [0.6815, 0.5516]],

        [[0.7029, 0.0999],
         [0.6102, 0.5296]],

        [[0.6313, 0.0921],
         [0.6643, 0.7015]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9291543574908578
Average Adjusted Rand Index: 0.9297718258455238
[0.9291543574908578, 0.9291543574908578] [0.9297718258455238, 0.9297718258455238] [11012.095703125, 11012.0693359375]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11140.551387135527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21874.181640625
inf tensor(21874.1816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11397.4453125
tensor(21874.1816, grad_fn=<NegBackward0>) tensor(11397.4453, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11396.7158203125
tensor(11397.4453, grad_fn=<NegBackward0>) tensor(11396.7158, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11395.4169921875
tensor(11396.7158, grad_fn=<NegBackward0>) tensor(11395.4170, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11386.923828125
tensor(11395.4170, grad_fn=<NegBackward0>) tensor(11386.9238, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11284.142578125
tensor(11386.9238, grad_fn=<NegBackward0>) tensor(11284.1426, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11178.09765625
tensor(11284.1426, grad_fn=<NegBackward0>) tensor(11178.0977, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11164.875
tensor(11178.0977, grad_fn=<NegBackward0>) tensor(11164.8750, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11160.6279296875
tensor(11164.8750, grad_fn=<NegBackward0>) tensor(11160.6279, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11157.3544921875
tensor(11160.6279, grad_fn=<NegBackward0>) tensor(11157.3545, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11157.171875
tensor(11157.3545, grad_fn=<NegBackward0>) tensor(11157.1719, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11151.7568359375
tensor(11157.1719, grad_fn=<NegBackward0>) tensor(11151.7568, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11151.7080078125
tensor(11151.7568, grad_fn=<NegBackward0>) tensor(11151.7080, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11151.6591796875
tensor(11151.7080, grad_fn=<NegBackward0>) tensor(11151.6592, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11151.4521484375
tensor(11151.6592, grad_fn=<NegBackward0>) tensor(11151.4521, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11151.1142578125
tensor(11151.4521, grad_fn=<NegBackward0>) tensor(11151.1143, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11150.4853515625
tensor(11151.1143, grad_fn=<NegBackward0>) tensor(11150.4854, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11150.4755859375
tensor(11150.4854, grad_fn=<NegBackward0>) tensor(11150.4756, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11150.466796875
tensor(11150.4756, grad_fn=<NegBackward0>) tensor(11150.4668, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11150.4619140625
tensor(11150.4668, grad_fn=<NegBackward0>) tensor(11150.4619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11150.45703125
tensor(11150.4619, grad_fn=<NegBackward0>) tensor(11150.4570, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11150.4521484375
tensor(11150.4570, grad_fn=<NegBackward0>) tensor(11150.4521, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11150.451171875
tensor(11150.4521, grad_fn=<NegBackward0>) tensor(11150.4512, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11150.4482421875
tensor(11150.4512, grad_fn=<NegBackward0>) tensor(11150.4482, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11150.4453125
tensor(11150.4482, grad_fn=<NegBackward0>) tensor(11150.4453, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11150.4423828125
tensor(11150.4453, grad_fn=<NegBackward0>) tensor(11150.4424, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11150.4404296875
tensor(11150.4424, grad_fn=<NegBackward0>) tensor(11150.4404, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11150.435546875
tensor(11150.4404, grad_fn=<NegBackward0>) tensor(11150.4355, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11150.4169921875
tensor(11150.4355, grad_fn=<NegBackward0>) tensor(11150.4170, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11150.4130859375
tensor(11150.4170, grad_fn=<NegBackward0>) tensor(11150.4131, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11150.41015625
tensor(11150.4131, grad_fn=<NegBackward0>) tensor(11150.4102, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11150.41015625
tensor(11150.4102, grad_fn=<NegBackward0>) tensor(11150.4102, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11150.4072265625
tensor(11150.4102, grad_fn=<NegBackward0>) tensor(11150.4072, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11150.40625
tensor(11150.4072, grad_fn=<NegBackward0>) tensor(11150.4062, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11150.392578125
tensor(11150.4062, grad_fn=<NegBackward0>) tensor(11150.3926, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11150.3896484375
tensor(11150.3926, grad_fn=<NegBackward0>) tensor(11150.3896, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11150.388671875
tensor(11150.3896, grad_fn=<NegBackward0>) tensor(11150.3887, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11150.3876953125
tensor(11150.3887, grad_fn=<NegBackward0>) tensor(11150.3877, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11150.38671875
tensor(11150.3877, grad_fn=<NegBackward0>) tensor(11150.3867, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11150.3857421875
tensor(11150.3867, grad_fn=<NegBackward0>) tensor(11150.3857, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11150.38671875
tensor(11150.3857, grad_fn=<NegBackward0>) tensor(11150.3867, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11150.3857421875
tensor(11150.3857, grad_fn=<NegBackward0>) tensor(11150.3857, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11150.3837890625
tensor(11150.3857, grad_fn=<NegBackward0>) tensor(11150.3838, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11150.39453125
tensor(11150.3838, grad_fn=<NegBackward0>) tensor(11150.3945, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11150.3837890625
tensor(11150.3838, grad_fn=<NegBackward0>) tensor(11150.3838, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11150.3828125
tensor(11150.3838, grad_fn=<NegBackward0>) tensor(11150.3828, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11150.3828125
tensor(11150.3828, grad_fn=<NegBackward0>) tensor(11150.3828, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11150.380859375
tensor(11150.3828, grad_fn=<NegBackward0>) tensor(11150.3809, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11150.39453125
tensor(11150.3809, grad_fn=<NegBackward0>) tensor(11150.3945, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11150.37890625
tensor(11150.3809, grad_fn=<NegBackward0>) tensor(11150.3789, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11150.3779296875
tensor(11150.3789, grad_fn=<NegBackward0>) tensor(11150.3779, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11150.37890625
tensor(11150.3779, grad_fn=<NegBackward0>) tensor(11150.3789, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11150.3779296875
tensor(11150.3779, grad_fn=<NegBackward0>) tensor(11150.3779, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11150.3798828125
tensor(11150.3779, grad_fn=<NegBackward0>) tensor(11150.3799, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11150.3779296875
tensor(11150.3779, grad_fn=<NegBackward0>) tensor(11150.3779, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11150.376953125
tensor(11150.3779, grad_fn=<NegBackward0>) tensor(11150.3770, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11150.375
tensor(11150.3770, grad_fn=<NegBackward0>) tensor(11150.3750, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11150.3759765625
tensor(11150.3750, grad_fn=<NegBackward0>) tensor(11150.3760, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11150.3759765625
tensor(11150.3750, grad_fn=<NegBackward0>) tensor(11150.3760, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11150.3740234375
tensor(11150.3750, grad_fn=<NegBackward0>) tensor(11150.3740, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11150.3740234375
tensor(11150.3740, grad_fn=<NegBackward0>) tensor(11150.3740, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11150.3662109375
tensor(11150.3740, grad_fn=<NegBackward0>) tensor(11150.3662, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11150.3642578125
tensor(11150.3662, grad_fn=<NegBackward0>) tensor(11150.3643, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11150.35546875
tensor(11150.3643, grad_fn=<NegBackward0>) tensor(11150.3555, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11149.3681640625
tensor(11150.3555, grad_fn=<NegBackward0>) tensor(11149.3682, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11149.318359375
tensor(11149.3682, grad_fn=<NegBackward0>) tensor(11149.3184, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11149.3125
tensor(11149.3184, grad_fn=<NegBackward0>) tensor(11149.3125, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11149.3125
tensor(11149.3125, grad_fn=<NegBackward0>) tensor(11149.3125, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11149.3125
tensor(11149.3125, grad_fn=<NegBackward0>) tensor(11149.3125, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11149.1943359375
tensor(11149.3125, grad_fn=<NegBackward0>) tensor(11149.1943, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11149.19140625
tensor(11149.1943, grad_fn=<NegBackward0>) tensor(11149.1914, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11149.193359375
tensor(11149.1914, grad_fn=<NegBackward0>) tensor(11149.1934, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11149.025390625
tensor(11149.1914, grad_fn=<NegBackward0>) tensor(11149.0254, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11149.0234375
tensor(11149.0254, grad_fn=<NegBackward0>) tensor(11149.0234, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11148.9912109375
tensor(11149.0234, grad_fn=<NegBackward0>) tensor(11148.9912, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11148.990234375
tensor(11148.9912, grad_fn=<NegBackward0>) tensor(11148.9902, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11148.9912109375
tensor(11148.9902, grad_fn=<NegBackward0>) tensor(11148.9912, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11148.990234375
tensor(11148.9902, grad_fn=<NegBackward0>) tensor(11148.9902, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11148.9892578125
tensor(11148.9902, grad_fn=<NegBackward0>) tensor(11148.9893, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11148.990234375
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9902, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11148.990234375
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9902, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11148.990234375
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9902, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11148.9892578125
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9893, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11148.9892578125
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9893, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11148.9873046875
tensor(11148.9893, grad_fn=<NegBackward0>) tensor(11148.9873, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11148.986328125
tensor(11148.9873, grad_fn=<NegBackward0>) tensor(11148.9863, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11148.9873046875
tensor(11148.9863, grad_fn=<NegBackward0>) tensor(11148.9873, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11148.9853515625
tensor(11148.9863, grad_fn=<NegBackward0>) tensor(11148.9854, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11148.986328125
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9863, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11148.986328125
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9863, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11149.099609375
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11149.0996, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11148.9853515625
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9854, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11148.986328125
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9863, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11148.9853515625
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9854, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11148.9169921875
tensor(11148.9854, grad_fn=<NegBackward0>) tensor(11148.9170, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11148.84375
tensor(11148.9170, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11148.8427734375
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8428, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11148.841796875
tensor(11148.8428, grad_fn=<NegBackward0>) tensor(11148.8418, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11148.83203125
tensor(11148.8418, grad_fn=<NegBackward0>) tensor(11148.8320, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11148.8515625
tensor(11148.8320, grad_fn=<NegBackward0>) tensor(11148.8516, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6305, 0.3695],
        [0.2475, 0.7525]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9642, 0.0358], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1840, 0.1044],
         [0.5119, 0.2984]],

        [[0.5311, 0.1045],
         [0.5538, 0.6873]],

        [[0.6883, 0.0921],
         [0.7103, 0.5103]],

        [[0.5296, 0.1022],
         [0.5370, 0.5323]],

        [[0.6869, 0.0938],
         [0.7215, 0.6737]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.6076324857379214
Average Adjusted Rand Index: 0.7366991492210535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22732.984375
inf tensor(22732.9844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11397.443359375
tensor(22732.9844, grad_fn=<NegBackward0>) tensor(11397.4434, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11395.8291015625
tensor(11397.4434, grad_fn=<NegBackward0>) tensor(11395.8291, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11394.1953125
tensor(11395.8291, grad_fn=<NegBackward0>) tensor(11394.1953, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11389.814453125
tensor(11394.1953, grad_fn=<NegBackward0>) tensor(11389.8145, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11382.6962890625
tensor(11389.8145, grad_fn=<NegBackward0>) tensor(11382.6963, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11294.7939453125
tensor(11382.6963, grad_fn=<NegBackward0>) tensor(11294.7939, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11196.185546875
tensor(11294.7939, grad_fn=<NegBackward0>) tensor(11196.1855, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11187.314453125
tensor(11196.1855, grad_fn=<NegBackward0>) tensor(11187.3145, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11174.2685546875
tensor(11187.3145, grad_fn=<NegBackward0>) tensor(11174.2686, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11165.041015625
tensor(11174.2686, grad_fn=<NegBackward0>) tensor(11165.0410, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11162.349609375
tensor(11165.0410, grad_fn=<NegBackward0>) tensor(11162.3496, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11162.0419921875
tensor(11162.3496, grad_fn=<NegBackward0>) tensor(11162.0420, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11161.45703125
tensor(11162.0420, grad_fn=<NegBackward0>) tensor(11161.4570, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11161.38671875
tensor(11161.4570, grad_fn=<NegBackward0>) tensor(11161.3867, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11160.89453125
tensor(11161.3867, grad_fn=<NegBackward0>) tensor(11160.8945, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11156.0185546875
tensor(11160.8945, grad_fn=<NegBackward0>) tensor(11156.0186, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11155.8974609375
tensor(11156.0186, grad_fn=<NegBackward0>) tensor(11155.8975, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11155.873046875
tensor(11155.8975, grad_fn=<NegBackward0>) tensor(11155.8730, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11155.8427734375
tensor(11155.8730, grad_fn=<NegBackward0>) tensor(11155.8428, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11155.8056640625
tensor(11155.8428, grad_fn=<NegBackward0>) tensor(11155.8057, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11151.99609375
tensor(11155.8057, grad_fn=<NegBackward0>) tensor(11151.9961, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11151.9794921875
tensor(11151.9961, grad_fn=<NegBackward0>) tensor(11151.9795, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11151.9716796875
tensor(11151.9795, grad_fn=<NegBackward0>) tensor(11151.9717, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11151.966796875
tensor(11151.9717, grad_fn=<NegBackward0>) tensor(11151.9668, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11151.830078125
tensor(11151.9668, grad_fn=<NegBackward0>) tensor(11151.8301, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11151.796875
tensor(11151.8301, grad_fn=<NegBackward0>) tensor(11151.7969, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11151.7900390625
tensor(11151.7969, grad_fn=<NegBackward0>) tensor(11151.7900, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11151.78515625
tensor(11151.7900, grad_fn=<NegBackward0>) tensor(11151.7852, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11151.7734375
tensor(11151.7852, grad_fn=<NegBackward0>) tensor(11151.7734, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11151.7529296875
tensor(11151.7734, grad_fn=<NegBackward0>) tensor(11151.7529, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11151.759765625
tensor(11151.7529, grad_fn=<NegBackward0>) tensor(11151.7598, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11151.7421875
tensor(11151.7529, grad_fn=<NegBackward0>) tensor(11151.7422, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11151.7314453125
tensor(11151.7422, grad_fn=<NegBackward0>) tensor(11151.7314, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11151.697265625
tensor(11151.7314, grad_fn=<NegBackward0>) tensor(11151.6973, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11150.7119140625
tensor(11151.6973, grad_fn=<NegBackward0>) tensor(11150.7119, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11150.6044921875
tensor(11150.7119, grad_fn=<NegBackward0>) tensor(11150.6045, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11150.48828125
tensor(11150.6045, grad_fn=<NegBackward0>) tensor(11150.4883, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11150.4775390625
tensor(11150.4883, grad_fn=<NegBackward0>) tensor(11150.4775, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11149.7841796875
tensor(11150.4775, grad_fn=<NegBackward0>) tensor(11149.7842, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11149.76953125
tensor(11149.7842, grad_fn=<NegBackward0>) tensor(11149.7695, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11149.767578125
tensor(11149.7695, grad_fn=<NegBackward0>) tensor(11149.7676, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11149.7626953125
tensor(11149.7676, grad_fn=<NegBackward0>) tensor(11149.7627, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11149.7607421875
tensor(11149.7627, grad_fn=<NegBackward0>) tensor(11149.7607, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11149.7548828125
tensor(11149.7607, grad_fn=<NegBackward0>) tensor(11149.7549, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11149.6875
tensor(11149.7549, grad_fn=<NegBackward0>) tensor(11149.6875, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11149.6923828125
tensor(11149.6875, grad_fn=<NegBackward0>) tensor(11149.6924, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11149.681640625
tensor(11149.6875, grad_fn=<NegBackward0>) tensor(11149.6816, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11149.681640625
tensor(11149.6816, grad_fn=<NegBackward0>) tensor(11149.6816, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11149.6787109375
tensor(11149.6816, grad_fn=<NegBackward0>) tensor(11149.6787, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11149.6845703125
tensor(11149.6787, grad_fn=<NegBackward0>) tensor(11149.6846, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11149.6767578125
tensor(11149.6787, grad_fn=<NegBackward0>) tensor(11149.6768, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11149.6767578125
tensor(11149.6768, grad_fn=<NegBackward0>) tensor(11149.6768, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11149.67578125
tensor(11149.6768, grad_fn=<NegBackward0>) tensor(11149.6758, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11149.67578125
tensor(11149.6758, grad_fn=<NegBackward0>) tensor(11149.6758, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11149.662109375
tensor(11149.6758, grad_fn=<NegBackward0>) tensor(11149.6621, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11149.5986328125
tensor(11149.6621, grad_fn=<NegBackward0>) tensor(11149.5986, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11149.59765625
tensor(11149.5986, grad_fn=<NegBackward0>) tensor(11149.5977, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11149.5947265625
tensor(11149.5977, grad_fn=<NegBackward0>) tensor(11149.5947, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11148.8955078125
tensor(11149.5947, grad_fn=<NegBackward0>) tensor(11148.8955, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11148.8828125
tensor(11148.8955, grad_fn=<NegBackward0>) tensor(11148.8828, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11148.8828125
tensor(11148.8828, grad_fn=<NegBackward0>) tensor(11148.8828, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11148.8828125
tensor(11148.8828, grad_fn=<NegBackward0>) tensor(11148.8828, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11148.8828125
tensor(11148.8828, grad_fn=<NegBackward0>) tensor(11148.8828, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11148.8818359375
tensor(11148.8828, grad_fn=<NegBackward0>) tensor(11148.8818, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11148.880859375
tensor(11148.8818, grad_fn=<NegBackward0>) tensor(11148.8809, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11148.875
tensor(11148.8809, grad_fn=<NegBackward0>) tensor(11148.8750, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11148.8720703125
tensor(11148.8750, grad_fn=<NegBackward0>) tensor(11148.8721, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11148.87109375
tensor(11148.8721, grad_fn=<NegBackward0>) tensor(11148.8711, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11148.8623046875
tensor(11148.8711, grad_fn=<NegBackward0>) tensor(11148.8623, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11148.8662109375
tensor(11148.8623, grad_fn=<NegBackward0>) tensor(11148.8662, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11148.8623046875
tensor(11148.8623, grad_fn=<NegBackward0>) tensor(11148.8623, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11148.8603515625
tensor(11148.8623, grad_fn=<NegBackward0>) tensor(11148.8604, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11148.861328125
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 35%|███▌      | 35/100 [9:18:21<18:05:36, 1002.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 36%|███▌      | 36/100 [9:34:31<17:38:36, 992.44s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 37%|███▋      | 37/100 [9:49:05<16:44:40, 956.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 38%|███▊      | 38/100 [10:03:34<16:01:35, 930.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 39%|███▉      | 39/100 [10:19:41<15:56:56, 941.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 40%|████      | 40/100 [10:32:17<14:45:55, 885.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 41%|████      | 41/100 [10:48:01<14:48:03, 903.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 42%|████▏     | 42/100 [11:02:12<14:18:02, 887.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 43%|████▎     | 43/100 [11:18:51<14:35:01, 921.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 44%|████▍     | 44/100 [11:33:28<14:07:16, 907.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 45%|████▌     | 45/100 [11:46:45<13:21:42, 874.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 46%|████▌     | 46/100 [11:59:33<12:38:15, 842.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 47%|████▋     | 47/100 [12:11:56<11:57:49, 812.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 48%|████▊     | 48/100 [12:22:49<11:02:47, 764.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 49%|████▉     | 49/100 [12:35:44<10:52:50, 768.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 50%|█████     | 50/100 [12:53:47<11:58:33, 862.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 51%|█████     | 51/100 [13:12:10<12:43:19, 934.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 52%|█████▏    | 52/100 [13:29:05<12:46:51, 958.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 53%|█████▎    | 53/100 [13:43:24<12:07:38, 928.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 54%|█████▍    | 54/100 [13:57:20<11:30:42, 900.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 55%|█████▌    | 55/100 [14:13:52<11:36:17, 928.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 56%|█████▌    | 56/100 [14:29:59<11:29:20, 940.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 57%|█████▋    | 57/100 [14:48:25<11:49:21, 989.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 58%|█████▊    | 58/100 [15:02:19<11:00:03, 942.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 59%|█████▉    | 59/100 [15:16:53<10:30:15, 922.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 60%|██████    | 60/100 [15:33:25<10:28:46, 943.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 61%|██████    | 61/100 [15:51:40<10:42:39, 988.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 62%|██████▏   | 62/100 [16:06:14<10:04:26, 954.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 63%|██████▎   | 63/100 [16:20:47<9:33:26, 929.91s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 64%|██████▍   | 64/100 [16:37:37<9:32:14, 953.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 65%|██████▌   | 65/100 [16:53:47<9:19:16, 958.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 66%|██████▌   | 66/100 [17:05:09<8:16:17, 875.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 67%|██████▋   | 67/100 [17:18:14<7:46:41, 848.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 68%|██████▊   | 68/100 [17:36:16<8:09:49, 918.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8613, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11148.8623046875
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8623, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11148.865234375
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8652, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11148.8603515625
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8604, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11148.8671875
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8672, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11148.86328125
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8633, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11148.859375
tensor(11148.8604, grad_fn=<NegBackward0>) tensor(11148.8594, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11148.8720703125
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8721, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11148.8603515625
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8604, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11148.8603515625
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8604, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11148.859375
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8594, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11148.8603515625
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8604, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11148.859375
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8594, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11148.8642578125
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8643, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11148.859375
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8594, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11148.8583984375
tensor(11148.8594, grad_fn=<NegBackward0>) tensor(11148.8584, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11148.859375
tensor(11148.8584, grad_fn=<NegBackward0>) tensor(11148.8594, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11148.857421875
tensor(11148.8584, grad_fn=<NegBackward0>) tensor(11148.8574, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11148.873046875
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8730, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11148.857421875
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8574, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11148.861328125
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8613, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11148.8583984375
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8584, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11148.8583984375
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8584, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11148.8828125
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8828, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11148.8583984375
tensor(11148.8574, grad_fn=<NegBackward0>) tensor(11148.8584, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7526, 0.2474],
        [0.3692, 0.6308]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0368, 0.9632], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2985, 0.1059],
         [0.5247, 0.1840]],

        [[0.5871, 0.1045],
         [0.6190, 0.6058]],

        [[0.7308, 0.0921],
         [0.5801, 0.6705]],

        [[0.6741, 0.1022],
         [0.5145, 0.6685]],

        [[0.6978, 0.0938],
         [0.6192, 0.5376]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.6076324857379214
Average Adjusted Rand Index: 0.7366991492210535
[0.6076324857379214, 0.6076324857379214] [0.7366991492210535, 0.7366991492210535] [11148.8310546875, 11148.8583984375]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11166.890274846859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20680.58203125
inf tensor(20680.5820, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11458.7099609375
tensor(20680.5820, grad_fn=<NegBackward0>) tensor(11458.7100, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11452.880859375
tensor(11458.7100, grad_fn=<NegBackward0>) tensor(11452.8809, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11378.37890625
tensor(11452.8809, grad_fn=<NegBackward0>) tensor(11378.3789, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11186.3759765625
tensor(11378.3789, grad_fn=<NegBackward0>) tensor(11186.3760, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11152.7197265625
tensor(11186.3760, grad_fn=<NegBackward0>) tensor(11152.7197, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11152.05859375
tensor(11152.7197, grad_fn=<NegBackward0>) tensor(11152.0586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11151.845703125
tensor(11152.0586, grad_fn=<NegBackward0>) tensor(11151.8457, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11151.5439453125
tensor(11151.8457, grad_fn=<NegBackward0>) tensor(11151.5439, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11151.44140625
tensor(11151.5439, grad_fn=<NegBackward0>) tensor(11151.4414, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11151.3291015625
tensor(11151.4414, grad_fn=<NegBackward0>) tensor(11151.3291, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11149.4990234375
tensor(11151.3291, grad_fn=<NegBackward0>) tensor(11149.4990, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11149.46484375
tensor(11149.4990, grad_fn=<NegBackward0>) tensor(11149.4648, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11148.77734375
tensor(11149.4648, grad_fn=<NegBackward0>) tensor(11148.7773, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11148.7548828125
tensor(11148.7773, grad_fn=<NegBackward0>) tensor(11148.7549, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11148.091796875
tensor(11148.7549, grad_fn=<NegBackward0>) tensor(11148.0918, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11147.962890625
tensor(11148.0918, grad_fn=<NegBackward0>) tensor(11147.9629, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11147.958984375
tensor(11147.9629, grad_fn=<NegBackward0>) tensor(11147.9590, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11147.953125
tensor(11147.9590, grad_fn=<NegBackward0>) tensor(11147.9531, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11147.9482421875
tensor(11147.9531, grad_fn=<NegBackward0>) tensor(11147.9482, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11147.9462890625
tensor(11147.9482, grad_fn=<NegBackward0>) tensor(11147.9463, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11147.9423828125
tensor(11147.9463, grad_fn=<NegBackward0>) tensor(11147.9424, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11147.9404296875
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9404, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11147.9375
tensor(11147.9404, grad_fn=<NegBackward0>) tensor(11147.9375, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11147.935546875
tensor(11147.9375, grad_fn=<NegBackward0>) tensor(11147.9355, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11147.93359375
tensor(11147.9355, grad_fn=<NegBackward0>) tensor(11147.9336, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11147.9306640625
tensor(11147.9336, grad_fn=<NegBackward0>) tensor(11147.9307, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11147.9296875
tensor(11147.9307, grad_fn=<NegBackward0>) tensor(11147.9297, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11147.927734375
tensor(11147.9297, grad_fn=<NegBackward0>) tensor(11147.9277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11147.927734375
tensor(11147.9277, grad_fn=<NegBackward0>) tensor(11147.9277, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11147.92578125
tensor(11147.9277, grad_fn=<NegBackward0>) tensor(11147.9258, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11147.9248046875
tensor(11147.9258, grad_fn=<NegBackward0>) tensor(11147.9248, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11147.9248046875
tensor(11147.9248, grad_fn=<NegBackward0>) tensor(11147.9248, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11147.923828125
tensor(11147.9248, grad_fn=<NegBackward0>) tensor(11147.9238, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11147.9228515625
tensor(11147.9238, grad_fn=<NegBackward0>) tensor(11147.9229, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11147.921875
tensor(11147.9229, grad_fn=<NegBackward0>) tensor(11147.9219, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11147.921875
tensor(11147.9219, grad_fn=<NegBackward0>) tensor(11147.9219, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11147.9208984375
tensor(11147.9219, grad_fn=<NegBackward0>) tensor(11147.9209, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11147.921875
tensor(11147.9209, grad_fn=<NegBackward0>) tensor(11147.9219, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11147.919921875
tensor(11147.9209, grad_fn=<NegBackward0>) tensor(11147.9199, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11147.9189453125
tensor(11147.9199, grad_fn=<NegBackward0>) tensor(11147.9189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11147.919921875
tensor(11147.9189, grad_fn=<NegBackward0>) tensor(11147.9199, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11147.919921875
tensor(11147.9189, grad_fn=<NegBackward0>) tensor(11147.9199, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11147.91796875
tensor(11147.9189, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11147.91796875
tensor(11147.9180, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11147.91796875
tensor(11147.9180, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11147.91796875
tensor(11147.9180, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11147.921875
tensor(11147.9180, grad_fn=<NegBackward0>) tensor(11147.9219, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11147.9169921875
tensor(11147.9180, grad_fn=<NegBackward0>) tensor(11147.9170, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11147.9169921875
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9170, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11147.91796875
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11147.91796875
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9180, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11147.9189453125
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9189, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11147.9169921875
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9170, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11147.916015625
tensor(11147.9170, grad_fn=<NegBackward0>) tensor(11147.9160, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11147.9150390625
tensor(11147.9160, grad_fn=<NegBackward0>) tensor(11147.9150, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11147.9130859375
tensor(11147.9150, grad_fn=<NegBackward0>) tensor(11147.9131, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11147.9130859375
tensor(11147.9131, grad_fn=<NegBackward0>) tensor(11147.9131, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11147.9130859375
tensor(11147.9131, grad_fn=<NegBackward0>) tensor(11147.9131, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11147.908203125
tensor(11147.9131, grad_fn=<NegBackward0>) tensor(11147.9082, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11147.9091796875
tensor(11147.9082, grad_fn=<NegBackward0>) tensor(11147.9092, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11147.90625
tensor(11147.9082, grad_fn=<NegBackward0>) tensor(11147.9062, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11147.90625
tensor(11147.9062, grad_fn=<NegBackward0>) tensor(11147.9062, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11147.904296875
tensor(11147.9062, grad_fn=<NegBackward0>) tensor(11147.9043, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11147.9033203125
tensor(11147.9043, grad_fn=<NegBackward0>) tensor(11147.9033, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11147.91015625
tensor(11147.9033, grad_fn=<NegBackward0>) tensor(11147.9102, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11147.9033203125
tensor(11147.9033, grad_fn=<NegBackward0>) tensor(11147.9033, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11147.9033203125
tensor(11147.9033, grad_fn=<NegBackward0>) tensor(11147.9033, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11147.9052734375
tensor(11147.9033, grad_fn=<NegBackward0>) tensor(11147.9053, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11147.8994140625
tensor(11147.9033, grad_fn=<NegBackward0>) tensor(11147.8994, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11147.9013671875
tensor(11147.8994, grad_fn=<NegBackward0>) tensor(11147.9014, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11147.8984375
tensor(11147.8994, grad_fn=<NegBackward0>) tensor(11147.8984, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11147.8955078125
tensor(11147.8984, grad_fn=<NegBackward0>) tensor(11147.8955, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11147.8916015625
tensor(11147.8955, grad_fn=<NegBackward0>) tensor(11147.8916, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11147.890625
tensor(11147.8916, grad_fn=<NegBackward0>) tensor(11147.8906, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11147.8916015625
tensor(11147.8906, grad_fn=<NegBackward0>) tensor(11147.8916, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11147.8955078125
tensor(11147.8906, grad_fn=<NegBackward0>) tensor(11147.8955, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11147.8896484375
tensor(11147.8906, grad_fn=<NegBackward0>) tensor(11147.8896, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11147.888671875
tensor(11147.8896, grad_fn=<NegBackward0>) tensor(11147.8887, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11147.8876953125
tensor(11147.8887, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11147.8896484375
tensor(11147.8877, grad_fn=<NegBackward0>) tensor(11147.8896, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11147.8896484375
tensor(11147.8877, grad_fn=<NegBackward0>) tensor(11147.8896, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11147.90625
tensor(11147.8877, grad_fn=<NegBackward0>) tensor(11147.9062, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11147.88671875
tensor(11147.8877, grad_fn=<NegBackward0>) tensor(11147.8867, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11147.90234375
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.9023, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11147.8916015625
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8916, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11147.88671875
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8867, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11147.88671875
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8867, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11147.88671875
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8867, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11147.9111328125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.9111, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11147.8876953125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8877, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11147.892578125
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8926, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11147.8994140625
tensor(11147.8867, grad_fn=<NegBackward0>) tensor(11147.8994, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7467, 0.2533],
        [0.2747, 0.7253]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4588, 0.5412], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.0950],
         [0.6190, 0.2970]],

        [[0.5178, 0.1016],
         [0.6371, 0.5351]],

        [[0.7233, 0.1020],
         [0.5344, 0.6820]],

        [[0.5642, 0.0889],
         [0.5430, 0.5329]],

        [[0.6733, 0.1049],
         [0.5062, 0.5283]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.9681922876370956
Average Adjusted Rand Index: 0.9681572436588309
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20241.998046875
inf tensor(20241.9980, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11458.5361328125
tensor(20241.9980, grad_fn=<NegBackward0>) tensor(11458.5361, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11450.486328125
tensor(11458.5361, grad_fn=<NegBackward0>) tensor(11450.4863, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11344.2021484375
tensor(11450.4863, grad_fn=<NegBackward0>) tensor(11344.2021, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11200.98046875
tensor(11344.2021, grad_fn=<NegBackward0>) tensor(11200.9805, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11156.4814453125
tensor(11200.9805, grad_fn=<NegBackward0>) tensor(11156.4814, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11152.30078125
tensor(11156.4814, grad_fn=<NegBackward0>) tensor(11152.3008, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11151.9296875
tensor(11152.3008, grad_fn=<NegBackward0>) tensor(11151.9297, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11151.83984375
tensor(11151.9297, grad_fn=<NegBackward0>) tensor(11151.8398, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11151.4912109375
tensor(11151.8398, grad_fn=<NegBackward0>) tensor(11151.4912, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11151.01171875
tensor(11151.4912, grad_fn=<NegBackward0>) tensor(11151.0117, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11149.70703125
tensor(11151.0117, grad_fn=<NegBackward0>) tensor(11149.7070, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11149.11328125
tensor(11149.7070, grad_fn=<NegBackward0>) tensor(11149.1133, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11149.087890625
tensor(11149.1133, grad_fn=<NegBackward0>) tensor(11149.0879, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11149.07421875
tensor(11149.0879, grad_fn=<NegBackward0>) tensor(11149.0742, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11149.0595703125
tensor(11149.0742, grad_fn=<NegBackward0>) tensor(11149.0596, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11148.3935546875
tensor(11149.0596, grad_fn=<NegBackward0>) tensor(11148.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11148.2763671875
tensor(11148.3936, grad_fn=<NegBackward0>) tensor(11148.2764, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11148.267578125
tensor(11148.2764, grad_fn=<NegBackward0>) tensor(11148.2676, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11148.1748046875
tensor(11148.2676, grad_fn=<NegBackward0>) tensor(11148.1748, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11148.1611328125
tensor(11148.1748, grad_fn=<NegBackward0>) tensor(11148.1611, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11148.15625
tensor(11148.1611, grad_fn=<NegBackward0>) tensor(11148.1562, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11148.1513671875
tensor(11148.1562, grad_fn=<NegBackward0>) tensor(11148.1514, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11148.146484375
tensor(11148.1514, grad_fn=<NegBackward0>) tensor(11148.1465, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11148.14453125
tensor(11148.1465, grad_fn=<NegBackward0>) tensor(11148.1445, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11148.1396484375
tensor(11148.1445, grad_fn=<NegBackward0>) tensor(11148.1396, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11148.1396484375
tensor(11148.1396, grad_fn=<NegBackward0>) tensor(11148.1396, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11148.1337890625
tensor(11148.1396, grad_fn=<NegBackward0>) tensor(11148.1338, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11148.1328125
tensor(11148.1338, grad_fn=<NegBackward0>) tensor(11148.1328, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11148.1318359375
tensor(11148.1328, grad_fn=<NegBackward0>) tensor(11148.1318, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11148.1298828125
tensor(11148.1318, grad_fn=<NegBackward0>) tensor(11148.1299, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11148.126953125
tensor(11148.1299, grad_fn=<NegBackward0>) tensor(11148.1270, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11148.1259765625
tensor(11148.1270, grad_fn=<NegBackward0>) tensor(11148.1260, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11148.125
tensor(11148.1260, grad_fn=<NegBackward0>) tensor(11148.1250, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11148.126953125
tensor(11148.1250, grad_fn=<NegBackward0>) tensor(11148.1270, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11148.123046875
tensor(11148.1250, grad_fn=<NegBackward0>) tensor(11148.1230, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11148.107421875
tensor(11148.1230, grad_fn=<NegBackward0>) tensor(11148.1074, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11148.0595703125
tensor(11148.1074, grad_fn=<NegBackward0>) tensor(11148.0596, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11148.01171875
tensor(11148.0596, grad_fn=<NegBackward0>) tensor(11148.0117, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11148.01171875
tensor(11148.0117, grad_fn=<NegBackward0>) tensor(11148.0117, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11148.0107421875
tensor(11148.0117, grad_fn=<NegBackward0>) tensor(11148.0107, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11148.0205078125
tensor(11148.0107, grad_fn=<NegBackward0>) tensor(11148.0205, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11148.009765625
tensor(11148.0107, grad_fn=<NegBackward0>) tensor(11148.0098, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11148.0087890625
tensor(11148.0098, grad_fn=<NegBackward0>) tensor(11148.0088, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11148.0087890625
tensor(11148.0088, grad_fn=<NegBackward0>) tensor(11148.0088, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11148.009765625
tensor(11148.0088, grad_fn=<NegBackward0>) tensor(11148.0098, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11148.0068359375
tensor(11148.0088, grad_fn=<NegBackward0>) tensor(11148.0068, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11147.9970703125
tensor(11148.0068, grad_fn=<NegBackward0>) tensor(11147.9971, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11148.001953125
tensor(11147.9971, grad_fn=<NegBackward0>) tensor(11148.0020, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11147.99609375
tensor(11147.9971, grad_fn=<NegBackward0>) tensor(11147.9961, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11147.99609375
tensor(11147.9961, grad_fn=<NegBackward0>) tensor(11147.9961, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11147.99609375
tensor(11147.9961, grad_fn=<NegBackward0>) tensor(11147.9961, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11147.99609375
tensor(11147.9961, grad_fn=<NegBackward0>) tensor(11147.9961, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11147.9951171875
tensor(11147.9961, grad_fn=<NegBackward0>) tensor(11147.9951, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11147.994140625
tensor(11147.9951, grad_fn=<NegBackward0>) tensor(11147.9941, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11147.9931640625
tensor(11147.9941, grad_fn=<NegBackward0>) tensor(11147.9932, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11147.990234375
tensor(11147.9932, grad_fn=<NegBackward0>) tensor(11147.9902, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11147.990234375
tensor(11147.9902, grad_fn=<NegBackward0>) tensor(11147.9902, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11147.9892578125
tensor(11147.9902, grad_fn=<NegBackward0>) tensor(11147.9893, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11147.986328125
tensor(11147.9893, grad_fn=<NegBackward0>) tensor(11147.9863, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11147.9853515625
tensor(11147.9863, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11147.9853515625
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11147.986328125
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9863, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11147.9853515625
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11147.9853515625
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11147.986328125
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9863, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11147.9853515625
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11147.9853515625
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11147.984375
tensor(11147.9854, grad_fn=<NegBackward0>) tensor(11147.9844, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11147.984375
tensor(11147.9844, grad_fn=<NegBackward0>) tensor(11147.9844, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11147.9853515625
tensor(11147.9844, grad_fn=<NegBackward0>) tensor(11147.9854, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11148.0361328125
tensor(11147.9844, grad_fn=<NegBackward0>) tensor(11148.0361, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11147.9814453125
tensor(11147.9844, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11147.984375
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9844, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11147.9892578125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9893, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11147.9873046875
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9873, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11147.98828125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9883, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11147.9814453125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9814, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11147.9580078125
tensor(11147.9814, grad_fn=<NegBackward0>) tensor(11147.9580, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11147.9609375
tensor(11147.9580, grad_fn=<NegBackward0>) tensor(11147.9609, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11147.95703125
tensor(11147.9580, grad_fn=<NegBackward0>) tensor(11147.9570, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11147.9423828125
tensor(11147.9570, grad_fn=<NegBackward0>) tensor(11147.9424, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11147.9443359375
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9443, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11147.970703125
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9707, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11147.9423828125
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9424, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11147.994140625
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9941, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11147.9404296875
tensor(11147.9424, grad_fn=<NegBackward0>) tensor(11147.9404, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11147.94140625
tensor(11147.9404, grad_fn=<NegBackward0>) tensor(11147.9414, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11147.94140625
tensor(11147.9404, grad_fn=<NegBackward0>) tensor(11147.9414, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11147.9765625
tensor(11147.9404, grad_fn=<NegBackward0>) tensor(11147.9766, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11147.8740234375
tensor(11147.9404, grad_fn=<NegBackward0>) tensor(11147.8740, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11147.8740234375
tensor(11147.8740, grad_fn=<NegBackward0>) tensor(11147.8740, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11147.8720703125
tensor(11147.8740, grad_fn=<NegBackward0>) tensor(11147.8721, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11147.8720703125
tensor(11147.8721, grad_fn=<NegBackward0>) tensor(11147.8721, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11147.8720703125
tensor(11147.8721, grad_fn=<NegBackward0>) tensor(11147.8721, grad_fn=<NegBackward0>)
pi: tensor([[0.7480, 0.2520],
        [0.2759, 0.7241]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4602, 0.5398], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.0950],
         [0.6713, 0.2974]],

        [[0.5099, 0.1017],
         [0.5156, 0.6221]],

        [[0.5223, 0.1019],
         [0.5502, 0.7141]],

        [[0.6671, 0.0887],
         [0.6323, 0.5944]],

        [[0.6199, 0.1052],
         [0.7176, 0.7098]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.9681922876370956
Average Adjusted Rand Index: 0.9681572436588309
[0.9681922876370956, 0.9681922876370956] [0.9681572436588309, 0.9681572436588309] [11147.8994140625, 11147.873046875]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11068.605467502
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21162.71484375
inf tensor(21162.7148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11339.2626953125
tensor(21162.7148, grad_fn=<NegBackward0>) tensor(11339.2627, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11338.7705078125
tensor(11339.2627, grad_fn=<NegBackward0>) tensor(11338.7705, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11338.2705078125
tensor(11338.7705, grad_fn=<NegBackward0>) tensor(11338.2705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11337.6884765625
tensor(11338.2705, grad_fn=<NegBackward0>) tensor(11337.6885, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11337.109375
tensor(11337.6885, grad_fn=<NegBackward0>) tensor(11337.1094, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11333.9580078125
tensor(11337.1094, grad_fn=<NegBackward0>) tensor(11333.9580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11149.3037109375
tensor(11333.9580, grad_fn=<NegBackward0>) tensor(11149.3037, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11062.6611328125
tensor(11149.3037, grad_fn=<NegBackward0>) tensor(11062.6611, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11048.611328125
tensor(11062.6611, grad_fn=<NegBackward0>) tensor(11048.6113, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11044.9560546875
tensor(11048.6113, grad_fn=<NegBackward0>) tensor(11044.9561, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11042.697265625
tensor(11044.9561, grad_fn=<NegBackward0>) tensor(11042.6973, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11042.5966796875
tensor(11042.6973, grad_fn=<NegBackward0>) tensor(11042.5967, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11042.521484375
tensor(11042.5967, grad_fn=<NegBackward0>) tensor(11042.5215, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11042.482421875
tensor(11042.5215, grad_fn=<NegBackward0>) tensor(11042.4824, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11042.4580078125
tensor(11042.4824, grad_fn=<NegBackward0>) tensor(11042.4580, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11042.44140625
tensor(11042.4580, grad_fn=<NegBackward0>) tensor(11042.4414, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11042.4287109375
tensor(11042.4414, grad_fn=<NegBackward0>) tensor(11042.4287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11042.41796875
tensor(11042.4287, grad_fn=<NegBackward0>) tensor(11042.4180, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11042.408203125
tensor(11042.4180, grad_fn=<NegBackward0>) tensor(11042.4082, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11042.40234375
tensor(11042.4082, grad_fn=<NegBackward0>) tensor(11042.4023, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11042.2109375
tensor(11042.4023, grad_fn=<NegBackward0>) tensor(11042.2109, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11042.1953125
tensor(11042.2109, grad_fn=<NegBackward0>) tensor(11042.1953, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11042.1875
tensor(11042.1953, grad_fn=<NegBackward0>) tensor(11042.1875, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11042.1669921875
tensor(11042.1875, grad_fn=<NegBackward0>) tensor(11042.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11042.14453125
tensor(11042.1670, grad_fn=<NegBackward0>) tensor(11042.1445, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11042.138671875
tensor(11042.1445, grad_fn=<NegBackward0>) tensor(11042.1387, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11042.1357421875
tensor(11042.1387, grad_fn=<NegBackward0>) tensor(11042.1357, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11042.1328125
tensor(11042.1357, grad_fn=<NegBackward0>) tensor(11042.1328, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11042.1298828125
tensor(11042.1328, grad_fn=<NegBackward0>) tensor(11042.1299, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11042.1279296875
tensor(11042.1299, grad_fn=<NegBackward0>) tensor(11042.1279, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11042.1240234375
tensor(11042.1279, grad_fn=<NegBackward0>) tensor(11042.1240, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11042.1376953125
tensor(11042.1240, grad_fn=<NegBackward0>) tensor(11042.1377, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11042.119140625
tensor(11042.1240, grad_fn=<NegBackward0>) tensor(11042.1191, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11042.1171875
tensor(11042.1191, grad_fn=<NegBackward0>) tensor(11042.1172, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11042.1171875
tensor(11042.1172, grad_fn=<NegBackward0>) tensor(11042.1172, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11042.1142578125
tensor(11042.1172, grad_fn=<NegBackward0>) tensor(11042.1143, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11042.109375
tensor(11042.1143, grad_fn=<NegBackward0>) tensor(11042.1094, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11042.10546875
tensor(11042.1094, grad_fn=<NegBackward0>) tensor(11042.1055, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11042.115234375
tensor(11042.1055, grad_fn=<NegBackward0>) tensor(11042.1152, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11042.103515625
tensor(11042.1055, grad_fn=<NegBackward0>) tensor(11042.1035, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11042.1025390625
tensor(11042.1035, grad_fn=<NegBackward0>) tensor(11042.1025, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11042.1025390625
tensor(11042.1025, grad_fn=<NegBackward0>) tensor(11042.1025, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11042.1015625
tensor(11042.1025, grad_fn=<NegBackward0>) tensor(11042.1016, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11042.103515625
tensor(11042.1016, grad_fn=<NegBackward0>) tensor(11042.1035, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11042.1005859375
tensor(11042.1016, grad_fn=<NegBackward0>) tensor(11042.1006, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11042.1005859375
tensor(11042.1006, grad_fn=<NegBackward0>) tensor(11042.1006, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11042.099609375
tensor(11042.1006, grad_fn=<NegBackward0>) tensor(11042.0996, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11042.0986328125
tensor(11042.0996, grad_fn=<NegBackward0>) tensor(11042.0986, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11042.0986328125
tensor(11042.0986, grad_fn=<NegBackward0>) tensor(11042.0986, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11042.09765625
tensor(11042.0986, grad_fn=<NegBackward0>) tensor(11042.0977, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11042.107421875
tensor(11042.0977, grad_fn=<NegBackward0>) tensor(11042.1074, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11042.0966796875
tensor(11042.0977, grad_fn=<NegBackward0>) tensor(11042.0967, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11042.095703125
tensor(11042.0967, grad_fn=<NegBackward0>) tensor(11042.0957, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11042.095703125
tensor(11042.0957, grad_fn=<NegBackward0>) tensor(11042.0957, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11042.095703125
tensor(11042.0957, grad_fn=<NegBackward0>) tensor(11042.0957, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11042.09375
tensor(11042.0957, grad_fn=<NegBackward0>) tensor(11042.0938, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11042.0947265625
tensor(11042.0938, grad_fn=<NegBackward0>) tensor(11042.0947, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11042.095703125
tensor(11042.0938, grad_fn=<NegBackward0>) tensor(11042.0957, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11042.0947265625
tensor(11042.0938, grad_fn=<NegBackward0>) tensor(11042.0947, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11042.0927734375
tensor(11042.0938, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11042.0927734375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11042.0927734375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11042.0927734375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11042.0927734375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11042.09375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0938, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11042.0927734375
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11042.091796875
tensor(11042.0928, grad_fn=<NegBackward0>) tensor(11042.0918, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11042.0927734375
tensor(11042.0918, grad_fn=<NegBackward0>) tensor(11042.0928, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11042.0908203125
tensor(11042.0918, grad_fn=<NegBackward0>) tensor(11042.0908, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11042.09375
tensor(11042.0908, grad_fn=<NegBackward0>) tensor(11042.0938, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11042.0908203125
tensor(11042.0908, grad_fn=<NegBackward0>) tensor(11042.0908, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11042.091796875
tensor(11042.0908, grad_fn=<NegBackward0>) tensor(11042.0918, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11042.091796875
tensor(11042.0908, grad_fn=<NegBackward0>) tensor(11042.0918, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11042.0888671875
tensor(11042.0908, grad_fn=<NegBackward0>) tensor(11042.0889, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11042.0888671875
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0889, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11042.08984375
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0898, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11042.0888671875
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0889, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11042.0888671875
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0889, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11042.0888671875
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0889, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11042.08984375
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0898, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11042.087890625
tensor(11042.0889, grad_fn=<NegBackward0>) tensor(11042.0879, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11042.05078125
tensor(11042.0879, grad_fn=<NegBackward0>) tensor(11042.0508, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11042.029296875
tensor(11042.0508, grad_fn=<NegBackward0>) tensor(11042.0293, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11042.0546875
tensor(11042.0293, grad_fn=<NegBackward0>) tensor(11042.0547, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11042.0283203125
tensor(11042.0293, grad_fn=<NegBackward0>) tensor(11042.0283, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11042.029296875
tensor(11042.0283, grad_fn=<NegBackward0>) tensor(11042.0293, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11042.02734375
tensor(11042.0283, grad_fn=<NegBackward0>) tensor(11042.0273, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11042.029296875
tensor(11042.0273, grad_fn=<NegBackward0>) tensor(11042.0293, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11042.0263671875
tensor(11042.0273, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11042.0263671875
tensor(11042.0264, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11042.0244140625
tensor(11042.0264, grad_fn=<NegBackward0>) tensor(11042.0244, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11042.0263671875
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11042.0263671875
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11042.052734375
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0527, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11042.0244140625
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0244, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11042.0263671875
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11042.0244140625
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0244, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11042.0263671875
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0264, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11042.0234375
tensor(11042.0244, grad_fn=<NegBackward0>) tensor(11042.0234, grad_fn=<NegBackward0>)
pi: tensor([[0.7488, 0.2512],
        [0.1982, 0.8018]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5137, 0.4863], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3016, 0.0962],
         [0.6965, 0.1925]],

        [[0.7246, 0.1005],
         [0.5490, 0.6564]],

        [[0.6564, 0.0955],
         [0.5995, 0.6748]],

        [[0.6922, 0.1059],
         [0.5591, 0.6391]],

        [[0.6834, 0.1006],
         [0.6025, 0.7189]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9291542885870415
Average Adjusted Rand Index: 0.9289553158274465
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22623.51953125
inf tensor(22623.5195, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11339.5771484375
tensor(22623.5195, grad_fn=<NegBackward0>) tensor(11339.5771, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11338.9228515625
tensor(11339.5771, grad_fn=<NegBackward0>) tensor(11338.9229, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11337.8994140625
tensor(11338.9229, grad_fn=<NegBackward0>) tensor(11337.8994, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11332.7001953125
tensor(11337.8994, grad_fn=<NegBackward0>) tensor(11332.7002, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11076.1201171875
tensor(11332.7002, grad_fn=<NegBackward0>) tensor(11076.1201, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11046.935546875
tensor(11076.1201, grad_fn=<NegBackward0>) tensor(11046.9355, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11046.56640625
tensor(11046.9355, grad_fn=<NegBackward0>) tensor(11046.5664, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11046.4140625
tensor(11046.5664, grad_fn=<NegBackward0>) tensor(11046.4141, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11046.349609375
tensor(11046.4141, grad_fn=<NegBackward0>) tensor(11046.3496, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11046.3125
tensor(11046.3496, grad_fn=<NegBackward0>) tensor(11046.3125, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11046.2802734375
tensor(11046.3125, grad_fn=<NegBackward0>) tensor(11046.2803, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11046.1220703125
tensor(11046.2803, grad_fn=<NegBackward0>) tensor(11046.1221, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11046.0654296875
tensor(11046.1221, grad_fn=<NegBackward0>) tensor(11046.0654, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11046.0087890625
tensor(11046.0654, grad_fn=<NegBackward0>) tensor(11046.0088, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11043.662109375
tensor(11046.0088, grad_fn=<NegBackward0>) tensor(11043.6621, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11043.6337890625
tensor(11043.6621, grad_fn=<NegBackward0>) tensor(11043.6338, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11043.6220703125
tensor(11043.6338, grad_fn=<NegBackward0>) tensor(11043.6221, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11043.5400390625
tensor(11043.6221, grad_fn=<NegBackward0>) tensor(11043.5400, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11043.53515625
tensor(11043.5400, grad_fn=<NegBackward0>) tensor(11043.5352, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11043.529296875
tensor(11043.5352, grad_fn=<NegBackward0>) tensor(11043.5293, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11043.517578125
tensor(11043.5293, grad_fn=<NegBackward0>) tensor(11043.5176, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11043.5146484375
tensor(11043.5176, grad_fn=<NegBackward0>) tensor(11043.5146, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11043.5126953125
tensor(11043.5146, grad_fn=<NegBackward0>) tensor(11043.5127, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11043.5087890625
tensor(11043.5127, grad_fn=<NegBackward0>) tensor(11043.5088, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11043.5029296875
tensor(11043.5088, grad_fn=<NegBackward0>) tensor(11043.5029, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11043.501953125
tensor(11043.5029, grad_fn=<NegBackward0>) tensor(11043.5020, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11043.5
tensor(11043.5020, grad_fn=<NegBackward0>) tensor(11043.5000, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11043.4990234375
tensor(11043.5000, grad_fn=<NegBackward0>) tensor(11043.4990, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11043.494140625
tensor(11043.4990, grad_fn=<NegBackward0>) tensor(11043.4941, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11043.4921875
tensor(11043.4941, grad_fn=<NegBackward0>) tensor(11043.4922, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11043.4921875
tensor(11043.4922, grad_fn=<NegBackward0>) tensor(11043.4922, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11043.4892578125
tensor(11043.4922, grad_fn=<NegBackward0>) tensor(11043.4893, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11042.173828125
tensor(11043.4893, grad_fn=<NegBackward0>) tensor(11042.1738, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11042.056640625
tensor(11042.1738, grad_fn=<NegBackward0>) tensor(11042.0566, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11042.0537109375
tensor(11042.0566, grad_fn=<NegBackward0>) tensor(11042.0537, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11042.056640625
tensor(11042.0537, grad_fn=<NegBackward0>) tensor(11042.0566, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11042.0537109375
tensor(11042.0537, grad_fn=<NegBackward0>) tensor(11042.0537, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11042.052734375
tensor(11042.0537, grad_fn=<NegBackward0>) tensor(11042.0527, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11042.05078125
tensor(11042.0527, grad_fn=<NegBackward0>) tensor(11042.0508, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11042.05078125
tensor(11042.0508, grad_fn=<NegBackward0>) tensor(11042.0508, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11042.0498046875
tensor(11042.0508, grad_fn=<NegBackward0>) tensor(11042.0498, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11042.0498046875
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0498, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11042.0517578125
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0518, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11042.0517578125
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0518, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11042.05078125
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0508, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11042.052734375
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0527, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -11042.0478515625
tensor(11042.0498, grad_fn=<NegBackward0>) tensor(11042.0479, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11042.048828125
tensor(11042.0479, grad_fn=<NegBackward0>) tensor(11042.0488, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11042.044921875
tensor(11042.0479, grad_fn=<NegBackward0>) tensor(11042.0449, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11042.0439453125
tensor(11042.0449, grad_fn=<NegBackward0>) tensor(11042.0439, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11042.044921875
tensor(11042.0439, grad_fn=<NegBackward0>) tensor(11042.0449, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11042.0439453125
tensor(11042.0439, grad_fn=<NegBackward0>) tensor(11042.0439, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11042.0439453125
tensor(11042.0439, grad_fn=<NegBackward0>) tensor(11042.0439, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11042.0419921875
tensor(11042.0439, grad_fn=<NegBackward0>) tensor(11042.0420, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11042.04296875
tensor(11042.0420, grad_fn=<NegBackward0>) tensor(11042.0430, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11042.041015625
tensor(11042.0420, grad_fn=<NegBackward0>) tensor(11042.0410, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11042.044921875
tensor(11042.0410, grad_fn=<NegBackward0>) tensor(11042.0449, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11042.04296875
tensor(11042.0410, grad_fn=<NegBackward0>) tensor(11042.0430, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11042.041015625
tensor(11042.0410, grad_fn=<NegBackward0>) tensor(11042.0410, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11042.0400390625
tensor(11042.0410, grad_fn=<NegBackward0>) tensor(11042.0400, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11042.0439453125
tensor(11042.0400, grad_fn=<NegBackward0>) tensor(11042.0439, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11042.0380859375
tensor(11042.0400, grad_fn=<NegBackward0>) tensor(11042.0381, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11042.0361328125
tensor(11042.0381, grad_fn=<NegBackward0>) tensor(11042.0361, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11042.0361328125
tensor(11042.0361, grad_fn=<NegBackward0>) tensor(11042.0361, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11042.0361328125
tensor(11042.0361, grad_fn=<NegBackward0>) tensor(11042.0361, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11042.0400390625
tensor(11042.0361, grad_fn=<NegBackward0>) tensor(11042.0400, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11042.0341796875
tensor(11042.0361, grad_fn=<NegBackward0>) tensor(11042.0342, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11042.037109375
tensor(11042.0342, grad_fn=<NegBackward0>) tensor(11042.0371, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11042.033203125
tensor(11042.0342, grad_fn=<NegBackward0>) tensor(11042.0332, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11042.0400390625
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0400, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11042.0341796875
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0342, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11042.033203125
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0332, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11042.0341796875
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0342, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11042.0361328125
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0361, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11042.03515625
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0352, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11042.0341796875
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0342, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11042.0341796875
tensor(11042.0332, grad_fn=<NegBackward0>) tensor(11042.0342, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.8013, 0.1987],
        [0.2516, 0.7484]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.0962],
         [0.6897, 0.3015]],

        [[0.7033, 0.1006],
         [0.7161, 0.5492]],

        [[0.7249, 0.0955],
         [0.6166, 0.6397]],

        [[0.6597, 0.1059],
         [0.6852, 0.6324]],

        [[0.5328, 0.1006],
         [0.6905, 0.6686]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
Global Adjusted Rand Index: 0.9291542885870415
Average Adjusted Rand Index: 0.9289553158274465
[0.9291542885870415, 0.9291542885870415] [0.9289553158274465, 0.9289553158274465] [11042.0244140625, 11042.0341796875]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11224.886532787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21538.08984375
inf tensor(21538.0898, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11574.353515625
tensor(21538.0898, grad_fn=<NegBackward0>) tensor(11574.3535, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11570.1484375
tensor(11574.3535, grad_fn=<NegBackward0>) tensor(11570.1484, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11560.2822265625
tensor(11570.1484, grad_fn=<NegBackward0>) tensor(11560.2822, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11559.4794921875
tensor(11560.2822, grad_fn=<NegBackward0>) tensor(11559.4795, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11559.0419921875
tensor(11559.4795, grad_fn=<NegBackward0>) tensor(11559.0420, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11558.71875
tensor(11559.0420, grad_fn=<NegBackward0>) tensor(11558.7188, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11545.6455078125
tensor(11558.7188, grad_fn=<NegBackward0>) tensor(11545.6455, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11455.806640625
tensor(11545.6455, grad_fn=<NegBackward0>) tensor(11455.8066, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11412.591796875
tensor(11455.8066, grad_fn=<NegBackward0>) tensor(11412.5918, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11398.11328125
tensor(11412.5918, grad_fn=<NegBackward0>) tensor(11398.1133, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11393.822265625
tensor(11398.1133, grad_fn=<NegBackward0>) tensor(11393.8223, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11388.3486328125
tensor(11393.8223, grad_fn=<NegBackward0>) tensor(11388.3486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11381.958984375
tensor(11388.3486, grad_fn=<NegBackward0>) tensor(11381.9590, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11381.89453125
tensor(11381.9590, grad_fn=<NegBackward0>) tensor(11381.8945, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11379.19140625
tensor(11381.8945, grad_fn=<NegBackward0>) tensor(11379.1914, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11371.9345703125
tensor(11379.1914, grad_fn=<NegBackward0>) tensor(11371.9346, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11361.3955078125
tensor(11371.9346, grad_fn=<NegBackward0>) tensor(11361.3955, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11361.3544921875
tensor(11361.3955, grad_fn=<NegBackward0>) tensor(11361.3545, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11350.9248046875
tensor(11361.3545, grad_fn=<NegBackward0>) tensor(11350.9248, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11345.748046875
tensor(11350.9248, grad_fn=<NegBackward0>) tensor(11345.7480, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11342.8515625
tensor(11345.7480, grad_fn=<NegBackward0>) tensor(11342.8516, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11335.3349609375
tensor(11342.8516, grad_fn=<NegBackward0>) tensor(11335.3350, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11331.830078125
tensor(11335.3350, grad_fn=<NegBackward0>) tensor(11331.8301, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11331.82421875
tensor(11331.8301, grad_fn=<NegBackward0>) tensor(11331.8242, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11331.8203125
tensor(11331.8242, grad_fn=<NegBackward0>) tensor(11331.8203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11331.8193359375
tensor(11331.8203, grad_fn=<NegBackward0>) tensor(11331.8193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11331.81640625
tensor(11331.8193, grad_fn=<NegBackward0>) tensor(11331.8164, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11331.8134765625
tensor(11331.8164, grad_fn=<NegBackward0>) tensor(11331.8135, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11329.998046875
tensor(11331.8135, grad_fn=<NegBackward0>) tensor(11329.9980, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11327.6494140625
tensor(11329.9980, grad_fn=<NegBackward0>) tensor(11327.6494, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11327.6083984375
tensor(11327.6494, grad_fn=<NegBackward0>) tensor(11327.6084, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11327.59375
tensor(11327.6084, grad_fn=<NegBackward0>) tensor(11327.5938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11322.796875
tensor(11327.5938, grad_fn=<NegBackward0>) tensor(11322.7969, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11322.744140625
tensor(11322.7969, grad_fn=<NegBackward0>) tensor(11322.7441, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11322.724609375
tensor(11322.7441, grad_fn=<NegBackward0>) tensor(11322.7246, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11322.72265625
tensor(11322.7246, grad_fn=<NegBackward0>) tensor(11322.7227, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11322.576171875
tensor(11322.7227, grad_fn=<NegBackward0>) tensor(11322.5762, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11318.8271484375
tensor(11322.5762, grad_fn=<NegBackward0>) tensor(11318.8271, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11318.1494140625
tensor(11318.8271, grad_fn=<NegBackward0>) tensor(11318.1494, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11315.4306640625
tensor(11318.1494, grad_fn=<NegBackward0>) tensor(11315.4307, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11315.4296875
tensor(11315.4307, grad_fn=<NegBackward0>) tensor(11315.4297, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11315.16796875
tensor(11315.4297, grad_fn=<NegBackward0>) tensor(11315.1680, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11315.1533203125
tensor(11315.1680, grad_fn=<NegBackward0>) tensor(11315.1533, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11315.1650390625
tensor(11315.1533, grad_fn=<NegBackward0>) tensor(11315.1650, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11315.1533203125
tensor(11315.1533, grad_fn=<NegBackward0>) tensor(11315.1533, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11315.15234375
tensor(11315.1533, grad_fn=<NegBackward0>) tensor(11315.1523, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11315.1513671875
tensor(11315.1523, grad_fn=<NegBackward0>) tensor(11315.1514, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11315.150390625
tensor(11315.1514, grad_fn=<NegBackward0>) tensor(11315.1504, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11315.150390625
tensor(11315.1504, grad_fn=<NegBackward0>) tensor(11315.1504, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11315.1484375
tensor(11315.1504, grad_fn=<NegBackward0>) tensor(11315.1484, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11315.052734375
tensor(11315.1484, grad_fn=<NegBackward0>) tensor(11315.0527, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11315.0478515625
tensor(11315.0527, grad_fn=<NegBackward0>) tensor(11315.0479, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11315.033203125
tensor(11315.0479, grad_fn=<NegBackward0>) tensor(11315.0332, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11315.0341796875
tensor(11315.0332, grad_fn=<NegBackward0>) tensor(11315.0342, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11315.0341796875
tensor(11315.0332, grad_fn=<NegBackward0>) tensor(11315.0342, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11315.033203125
tensor(11315.0332, grad_fn=<NegBackward0>) tensor(11315.0332, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11315.0302734375
tensor(11315.0332, grad_fn=<NegBackward0>) tensor(11315.0303, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11314.984375
tensor(11315.0303, grad_fn=<NegBackward0>) tensor(11314.9844, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11314.9775390625
tensor(11314.9844, grad_fn=<NegBackward0>) tensor(11314.9775, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11314.974609375
tensor(11314.9775, grad_fn=<NegBackward0>) tensor(11314.9746, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11314.9736328125
tensor(11314.9746, grad_fn=<NegBackward0>) tensor(11314.9736, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11314.974609375
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9746, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11314.974609375
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9746, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11314.9736328125
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9736, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11314.9765625
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9766, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11314.9755859375
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9756, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11314.9814453125
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9814, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11314.9541015625
tensor(11314.9736, grad_fn=<NegBackward0>) tensor(11314.9541, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11314.943359375
tensor(11314.9541, grad_fn=<NegBackward0>) tensor(11314.9434, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11314.943359375
tensor(11314.9434, grad_fn=<NegBackward0>) tensor(11314.9434, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11314.9443359375
tensor(11314.9434, grad_fn=<NegBackward0>) tensor(11314.9443, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11314.9453125
tensor(11314.9434, grad_fn=<NegBackward0>) tensor(11314.9453, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11314.9423828125
tensor(11314.9434, grad_fn=<NegBackward0>) tensor(11314.9424, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11314.9423828125
tensor(11314.9424, grad_fn=<NegBackward0>) tensor(11314.9424, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11314.93359375
tensor(11314.9424, grad_fn=<NegBackward0>) tensor(11314.9336, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11314.9375
tensor(11314.9336, grad_fn=<NegBackward0>) tensor(11314.9375, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11314.9091796875
tensor(11314.9336, grad_fn=<NegBackward0>) tensor(11314.9092, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11314.9091796875
tensor(11314.9092, grad_fn=<NegBackward0>) tensor(11314.9092, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11314.91015625
tensor(11314.9092, grad_fn=<NegBackward0>) tensor(11314.9102, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11314.9072265625
tensor(11314.9092, grad_fn=<NegBackward0>) tensor(11314.9072, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11314.8896484375
tensor(11314.9072, grad_fn=<NegBackward0>) tensor(11314.8896, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11314.888671875
tensor(11314.8896, grad_fn=<NegBackward0>) tensor(11314.8887, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11314.89453125
tensor(11314.8887, grad_fn=<NegBackward0>) tensor(11314.8945, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11314.8876953125
tensor(11314.8887, grad_fn=<NegBackward0>) tensor(11314.8877, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11314.8876953125
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11314.8877, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11314.8876953125
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11314.8877, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11314.888671875
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11314.8887, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11315.08203125
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11315.0820, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11314.888671875
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11314.8887, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11314.88671875
tensor(11314.8877, grad_fn=<NegBackward0>) tensor(11314.8867, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11314.888671875
tensor(11314.8867, grad_fn=<NegBackward0>) tensor(11314.8887, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11314.8876953125
tensor(11314.8867, grad_fn=<NegBackward0>) tensor(11314.8877, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11314.890625
tensor(11314.8867, grad_fn=<NegBackward0>) tensor(11314.8906, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11314.888671875
tensor(11314.8867, grad_fn=<NegBackward0>) tensor(11314.8887, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11314.8974609375
tensor(11314.8867, grad_fn=<NegBackward0>) tensor(11314.8975, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.3994, 0.6006],
        [0.5823, 0.4177]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5488, 0.4512], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2752, 0.0926],
         [0.5526, 0.2373]],

        [[0.5214, 0.0999],
         [0.6557, 0.5776]],

        [[0.5894, 0.0912],
         [0.7007, 0.5521]],

        [[0.6155, 0.1021],
         [0.7022, 0.5543]],

        [[0.6741, 0.0970],
         [0.6980, 0.6434]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7027008103753108
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721314419105764
Global Adjusted Rand Index: 0.05765523703396009
Average Adjusted Rand Index: 0.8709662623903137
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23642.625
inf tensor(23642.6250, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11570.912109375
tensor(23642.6250, grad_fn=<NegBackward0>) tensor(11570.9121, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11492.5927734375
tensor(11570.9121, grad_fn=<NegBackward0>) tensor(11492.5928, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11484.357421875
tensor(11492.5928, grad_fn=<NegBackward0>) tensor(11484.3574, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11482.845703125
tensor(11484.3574, grad_fn=<NegBackward0>) tensor(11482.8457, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11468.63671875
tensor(11482.8457, grad_fn=<NegBackward0>) tensor(11468.6367, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11363.93359375
tensor(11468.6367, grad_fn=<NegBackward0>) tensor(11363.9336, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11330.03515625
tensor(11363.9336, grad_fn=<NegBackward0>) tensor(11330.0352, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11315.63671875
tensor(11330.0352, grad_fn=<NegBackward0>) tensor(11315.6367, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11308.421875
tensor(11315.6367, grad_fn=<NegBackward0>) tensor(11308.4219, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11307.8076171875
tensor(11308.4219, grad_fn=<NegBackward0>) tensor(11307.8076, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11307.3427734375
tensor(11307.8076, grad_fn=<NegBackward0>) tensor(11307.3428, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11307.3115234375
tensor(11307.3428, grad_fn=<NegBackward0>) tensor(11307.3115, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11307.2119140625
tensor(11307.3115, grad_fn=<NegBackward0>) tensor(11307.2119, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11307.1787109375
tensor(11307.2119, grad_fn=<NegBackward0>) tensor(11307.1787, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11307.158203125
tensor(11307.1787, grad_fn=<NegBackward0>) tensor(11307.1582, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11307.1435546875
tensor(11307.1582, grad_fn=<NegBackward0>) tensor(11307.1436, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11305.3798828125
tensor(11307.1436, grad_fn=<NegBackward0>) tensor(11305.3799, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11305.375
tensor(11305.3799, grad_fn=<NegBackward0>) tensor(11305.3750, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11305.275390625
tensor(11305.3750, grad_fn=<NegBackward0>) tensor(11305.2754, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11305.26171875
tensor(11305.2754, grad_fn=<NegBackward0>) tensor(11305.2617, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11304.931640625
tensor(11305.2617, grad_fn=<NegBackward0>) tensor(11304.9316, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11304.921875
tensor(11304.9316, grad_fn=<NegBackward0>) tensor(11304.9219, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11300.83984375
tensor(11304.9219, grad_fn=<NegBackward0>) tensor(11300.8398, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11300.833984375
tensor(11300.8398, grad_fn=<NegBackward0>) tensor(11300.8340, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11300.833984375
tensor(11300.8340, grad_fn=<NegBackward0>) tensor(11300.8340, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11300.83203125
tensor(11300.8340, grad_fn=<NegBackward0>) tensor(11300.8320, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11300.830078125
tensor(11300.8320, grad_fn=<NegBackward0>) tensor(11300.8301, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11300.830078125
tensor(11300.8301, grad_fn=<NegBackward0>) tensor(11300.8301, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11300.8056640625
tensor(11300.8301, grad_fn=<NegBackward0>) tensor(11300.8057, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11300.7802734375
tensor(11300.8057, grad_fn=<NegBackward0>) tensor(11300.7803, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11300.220703125
tensor(11300.7803, grad_fn=<NegBackward0>) tensor(11300.2207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11300.1953125
tensor(11300.2207, grad_fn=<NegBackward0>) tensor(11300.1953, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11300.1796875
tensor(11300.1953, grad_fn=<NegBackward0>) tensor(11300.1797, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11300.17578125
tensor(11300.1797, grad_fn=<NegBackward0>) tensor(11300.1758, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11300.173828125
tensor(11300.1758, grad_fn=<NegBackward0>) tensor(11300.1738, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11300.173828125
tensor(11300.1738, grad_fn=<NegBackward0>) tensor(11300.1738, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11300.173828125
tensor(11300.1738, grad_fn=<NegBackward0>) tensor(11300.1738, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11300.1728515625
tensor(11300.1738, grad_fn=<NegBackward0>) tensor(11300.1729, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11300.173828125
tensor(11300.1729, grad_fn=<NegBackward0>) tensor(11300.1738, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11294.7861328125
tensor(11300.1729, grad_fn=<NegBackward0>) tensor(11294.7861, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11294.71875
tensor(11294.7861, grad_fn=<NegBackward0>) tensor(11294.7188, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11294.662109375
tensor(11294.7188, grad_fn=<NegBackward0>) tensor(11294.6621, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11294.6630859375
tensor(11294.6621, grad_fn=<NegBackward0>) tensor(11294.6631, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11294.62109375
tensor(11294.6621, grad_fn=<NegBackward0>) tensor(11294.6211, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11294.6474609375
tensor(11294.6211, grad_fn=<NegBackward0>) tensor(11294.6475, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11294.62109375
tensor(11294.6211, grad_fn=<NegBackward0>) tensor(11294.6211, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11294.6318359375
tensor(11294.6211, grad_fn=<NegBackward0>) tensor(11294.6318, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11294.619140625
tensor(11294.6211, grad_fn=<NegBackward0>) tensor(11294.6191, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11294.6240234375
tensor(11294.6191, grad_fn=<NegBackward0>) tensor(11294.6240, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11294.6025390625
tensor(11294.6191, grad_fn=<NegBackward0>) tensor(11294.6025, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11294.564453125
tensor(11294.6025, grad_fn=<NegBackward0>) tensor(11294.5645, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11294.560546875
tensor(11294.5645, grad_fn=<NegBackward0>) tensor(11294.5605, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11288.5546875
tensor(11294.5605, grad_fn=<NegBackward0>) tensor(11288.5547, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11288.5556640625
tensor(11288.5547, grad_fn=<NegBackward0>) tensor(11288.5557, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11288.55078125
tensor(11288.5547, grad_fn=<NegBackward0>) tensor(11288.5508, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11288.5517578125
tensor(11288.5508, grad_fn=<NegBackward0>) tensor(11288.5518, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11288.5419921875
tensor(11288.5508, grad_fn=<NegBackward0>) tensor(11288.5420, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11288.5390625
tensor(11288.5420, grad_fn=<NegBackward0>) tensor(11288.5391, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11288.5380859375
tensor(11288.5391, grad_fn=<NegBackward0>) tensor(11288.5381, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11288.5400390625
tensor(11288.5381, grad_fn=<NegBackward0>) tensor(11288.5400, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11288.5390625
tensor(11288.5381, grad_fn=<NegBackward0>) tensor(11288.5391, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11288.5390625
tensor(11288.5381, grad_fn=<NegBackward0>) tensor(11288.5391, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11288.5439453125
tensor(11288.5381, grad_fn=<NegBackward0>) tensor(11288.5439, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11288.5390625
tensor(11288.5381, grad_fn=<NegBackward0>) tensor(11288.5391, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[0.7177, 0.2823],
        [0.3837, 0.6163]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3490, 0.6510], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2878, 0.0938],
         [0.5569, 0.2171]],

        [[0.6677, 0.1028],
         [0.5243, 0.6777]],

        [[0.5848, 0.0919],
         [0.6670, 0.6671]],

        [[0.6828, 0.1017],
         [0.5508, 0.7218]],

        [[0.5027, 0.0983],
         [0.5103, 0.5778]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 89
Adjusted Rand Index: 0.6046210397853521
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8081003563518231
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.36840209884428177
Average Adjusted Rand Index: 0.8507045378354577
[0.05765523703396009, 0.36840209884428177] [0.8709662623903137, 0.8507045378354577] [11314.8974609375, 11288.5390625]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11031.096760534829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20823.716796875
inf tensor(20823.7168, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11309.701171875
tensor(20823.7168, grad_fn=<NegBackward0>) tensor(11309.7012, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11290.146484375
tensor(11309.7012, grad_fn=<NegBackward0>) tensor(11290.1465, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11288.5615234375
tensor(11290.1465, grad_fn=<NegBackward0>) tensor(11288.5615, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11287.9951171875
tensor(11288.5615, grad_fn=<NegBackward0>) tensor(11287.9951, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11287.7216796875
tensor(11287.9951, grad_fn=<NegBackward0>) tensor(11287.7217, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11287.564453125
tensor(11287.7217, grad_fn=<NegBackward0>) tensor(11287.5645, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11287.4658203125
tensor(11287.5645, grad_fn=<NegBackward0>) tensor(11287.4658, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11287.3994140625
tensor(11287.4658, grad_fn=<NegBackward0>) tensor(11287.3994, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11287.3525390625
tensor(11287.3994, grad_fn=<NegBackward0>) tensor(11287.3525, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11287.3173828125
tensor(11287.3525, grad_fn=<NegBackward0>) tensor(11287.3174, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11287.287109375
tensor(11287.3174, grad_fn=<NegBackward0>) tensor(11287.2871, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11287.267578125
tensor(11287.2871, grad_fn=<NegBackward0>) tensor(11287.2676, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11287.2490234375
tensor(11287.2676, grad_fn=<NegBackward0>) tensor(11287.2490, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11287.236328125
tensor(11287.2490, grad_fn=<NegBackward0>) tensor(11287.2363, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11287.2236328125
tensor(11287.2363, grad_fn=<NegBackward0>) tensor(11287.2236, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11287.2138671875
tensor(11287.2236, grad_fn=<NegBackward0>) tensor(11287.2139, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11287.2060546875
tensor(11287.2139, grad_fn=<NegBackward0>) tensor(11287.2061, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11287.1982421875
tensor(11287.2061, grad_fn=<NegBackward0>) tensor(11287.1982, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11287.1923828125
tensor(11287.1982, grad_fn=<NegBackward0>) tensor(11287.1924, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11287.1875
tensor(11287.1924, grad_fn=<NegBackward0>) tensor(11287.1875, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11287.18359375
tensor(11287.1875, grad_fn=<NegBackward0>) tensor(11287.1836, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11287.1787109375
tensor(11287.1836, grad_fn=<NegBackward0>) tensor(11287.1787, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11287.17578125
tensor(11287.1787, grad_fn=<NegBackward0>) tensor(11287.1758, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11287.171875
tensor(11287.1758, grad_fn=<NegBackward0>) tensor(11287.1719, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11287.169921875
tensor(11287.1719, grad_fn=<NegBackward0>) tensor(11287.1699, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11287.16796875
tensor(11287.1699, grad_fn=<NegBackward0>) tensor(11287.1680, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11287.1640625
tensor(11287.1680, grad_fn=<NegBackward0>) tensor(11287.1641, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11287.1630859375
tensor(11287.1641, grad_fn=<NegBackward0>) tensor(11287.1631, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11287.1611328125
tensor(11287.1631, grad_fn=<NegBackward0>) tensor(11287.1611, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11287.1572265625
tensor(11287.1611, grad_fn=<NegBackward0>) tensor(11287.1572, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11287.158203125
tensor(11287.1572, grad_fn=<NegBackward0>) tensor(11287.1582, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11287.15625
tensor(11287.1572, grad_fn=<NegBackward0>) tensor(11287.1562, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11287.1552734375
tensor(11287.1562, grad_fn=<NegBackward0>) tensor(11287.1553, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11287.154296875
tensor(11287.1553, grad_fn=<NegBackward0>) tensor(11287.1543, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11287.1533203125
tensor(11287.1543, grad_fn=<NegBackward0>) tensor(11287.1533, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11287.15234375
tensor(11287.1533, grad_fn=<NegBackward0>) tensor(11287.1523, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11287.1513671875
tensor(11287.1523, grad_fn=<NegBackward0>) tensor(11287.1514, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11287.150390625
tensor(11287.1514, grad_fn=<NegBackward0>) tensor(11287.1504, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11287.1494140625
tensor(11287.1504, grad_fn=<NegBackward0>) tensor(11287.1494, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11287.1484375
tensor(11287.1494, grad_fn=<NegBackward0>) tensor(11287.1484, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11287.1474609375
tensor(11287.1484, grad_fn=<NegBackward0>) tensor(11287.1475, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11287.146484375
tensor(11287.1475, grad_fn=<NegBackward0>) tensor(11287.1465, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11287.146484375
tensor(11287.1465, grad_fn=<NegBackward0>) tensor(11287.1465, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11287.1455078125
tensor(11287.1465, grad_fn=<NegBackward0>) tensor(11287.1455, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11287.14453125
tensor(11287.1455, grad_fn=<NegBackward0>) tensor(11287.1445, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11287.1435546875
tensor(11287.1445, grad_fn=<NegBackward0>) tensor(11287.1436, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11287.142578125
tensor(11287.1436, grad_fn=<NegBackward0>) tensor(11287.1426, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11287.142578125
tensor(11287.1426, grad_fn=<NegBackward0>) tensor(11287.1426, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11287.142578125
tensor(11287.1426, grad_fn=<NegBackward0>) tensor(11287.1426, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11287.1435546875
tensor(11287.1426, grad_fn=<NegBackward0>) tensor(11287.1436, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11287.142578125
tensor(11287.1426, grad_fn=<NegBackward0>) tensor(11287.1426, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11287.1416015625
tensor(11287.1426, grad_fn=<NegBackward0>) tensor(11287.1416, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11287.1416015625
tensor(11287.1416, grad_fn=<NegBackward0>) tensor(11287.1416, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11287.140625
tensor(11287.1416, grad_fn=<NegBackward0>) tensor(11287.1406, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11287.1416015625
tensor(11287.1406, grad_fn=<NegBackward0>) tensor(11287.1416, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11287.1396484375
tensor(11287.1406, grad_fn=<NegBackward0>) tensor(11287.1396, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11287.140625
tensor(11287.1396, grad_fn=<NegBackward0>) tensor(11287.1406, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11287.1396484375
tensor(11287.1396, grad_fn=<NegBackward0>) tensor(11287.1396, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11287.1396484375
tensor(11287.1396, grad_fn=<NegBackward0>) tensor(11287.1396, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11287.1396484375
tensor(11287.1396, grad_fn=<NegBackward0>) tensor(11287.1396, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11287.138671875
tensor(11287.1396, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11287.1396484375
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1396, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11287.138671875
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1387, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11287.1376953125
tensor(11287.1387, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11288.1416015625
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11288.1416, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11287.1376953125
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11287.140625
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1406, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11287.13671875
tensor(11287.1377, grad_fn=<NegBackward0>) tensor(11287.1367, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11287.13671875
tensor(11287.1367, grad_fn=<NegBackward0>) tensor(11287.1367, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11287.13671875
tensor(11287.1367, grad_fn=<NegBackward0>) tensor(11287.1367, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11287.1669921875
tensor(11287.1367, grad_fn=<NegBackward0>) tensor(11287.1670, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11287.1357421875
tensor(11287.1367, grad_fn=<NegBackward0>) tensor(11287.1357, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11287.1376953125
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1377, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11287.13671875
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1367, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11287.1484375
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1484, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11287.1357421875
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1357, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11287.2626953125
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.2627, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11287.13671875
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1367, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11287.150390625
tensor(11287.1357, grad_fn=<NegBackward0>) tensor(11287.1504, grad_fn=<NegBackward0>)
3
pi: tensor([[3.1387e-06, 1.0000e+00],
        [1.0000e+00, 2.2050e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.3643e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1697, 0.1625],
         [0.7084, 0.1704]],

        [[0.7069, 0.2040],
         [0.6513, 0.5824]],

        [[0.6737, 0.1870],
         [0.6282, 0.6661]],

        [[0.6895, 0.1784],
         [0.6828, 0.7074]],

        [[0.6569, 0.1923],
         [0.5167, 0.6624]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002004071831296791
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23791.796875
inf tensor(23791.7969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11263.9931640625
tensor(23791.7969, grad_fn=<NegBackward0>) tensor(11263.9932, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11035.7509765625
tensor(11263.9932, grad_fn=<NegBackward0>) tensor(11035.7510, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11014.8798828125
tensor(11035.7510, grad_fn=<NegBackward0>) tensor(11014.8799, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11008.5732421875
tensor(11014.8799, grad_fn=<NegBackward0>) tensor(11008.5732, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11008.28515625
tensor(11008.5732, grad_fn=<NegBackward0>) tensor(11008.2852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11007.05859375
tensor(11008.2852, grad_fn=<NegBackward0>) tensor(11007.0586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11007.0224609375
tensor(11007.0586, grad_fn=<NegBackward0>) tensor(11007.0225, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11007.0
tensor(11007.0225, grad_fn=<NegBackward0>) tensor(11007., grad_fn=<NegBackward0>)
Iteration 900: Loss = -11006.9736328125
tensor(11007., grad_fn=<NegBackward0>) tensor(11006.9736, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11005.3056640625
tensor(11006.9736, grad_fn=<NegBackward0>) tensor(11005.3057, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11005.1650390625
tensor(11005.3057, grad_fn=<NegBackward0>) tensor(11005.1650, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11005.1591796875
tensor(11005.1650, grad_fn=<NegBackward0>) tensor(11005.1592, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11005.1533203125
tensor(11005.1592, grad_fn=<NegBackward0>) tensor(11005.1533, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11005.1484375
tensor(11005.1533, grad_fn=<NegBackward0>) tensor(11005.1484, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11005.12890625
tensor(11005.1484, grad_fn=<NegBackward0>) tensor(11005.1289, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11004.0439453125
tensor(11005.1289, grad_fn=<NegBackward0>) tensor(11004.0439, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11004.0244140625
tensor(11004.0439, grad_fn=<NegBackward0>) tensor(11004.0244, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11003.724609375
tensor(11004.0244, grad_fn=<NegBackward0>) tensor(11003.7246, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11003.72265625
tensor(11003.7246, grad_fn=<NegBackward0>) tensor(11003.7227, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11003.720703125
tensor(11003.7227, grad_fn=<NegBackward0>) tensor(11003.7207, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11003.720703125
tensor(11003.7207, grad_fn=<NegBackward0>) tensor(11003.7207, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11003.7197265625
tensor(11003.7207, grad_fn=<NegBackward0>) tensor(11003.7197, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11003.7177734375
tensor(11003.7197, grad_fn=<NegBackward0>) tensor(11003.7178, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11003.7177734375
tensor(11003.7178, grad_fn=<NegBackward0>) tensor(11003.7178, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11003.716796875
tensor(11003.7178, grad_fn=<NegBackward0>) tensor(11003.7168, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11003.7177734375
tensor(11003.7168, grad_fn=<NegBackward0>) tensor(11003.7178, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11003.7158203125
tensor(11003.7168, grad_fn=<NegBackward0>) tensor(11003.7158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11003.71484375
tensor(11003.7158, grad_fn=<NegBackward0>) tensor(11003.7148, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11003.71484375
tensor(11003.7148, grad_fn=<NegBackward0>) tensor(11003.7148, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11003.7138671875
tensor(11003.7148, grad_fn=<NegBackward0>) tensor(11003.7139, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11003.7138671875
tensor(11003.7139, grad_fn=<NegBackward0>) tensor(11003.7139, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11003.712890625
tensor(11003.7139, grad_fn=<NegBackward0>) tensor(11003.7129, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11003.7109375
tensor(11003.7129, grad_fn=<NegBackward0>) tensor(11003.7109, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11003.7099609375
tensor(11003.7109, grad_fn=<NegBackward0>) tensor(11003.7100, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11003.7099609375
tensor(11003.7100, grad_fn=<NegBackward0>) tensor(11003.7100, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11003.7099609375
tensor(11003.7100, grad_fn=<NegBackward0>) tensor(11003.7100, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11003.708984375
tensor(11003.7100, grad_fn=<NegBackward0>) tensor(11003.7090, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11003.7080078125
tensor(11003.7090, grad_fn=<NegBackward0>) tensor(11003.7080, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11003.708984375
tensor(11003.7080, grad_fn=<NegBackward0>) tensor(11003.7090, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11003.7080078125
tensor(11003.7080, grad_fn=<NegBackward0>) tensor(11003.7080, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11003.7060546875
tensor(11003.7080, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11003.70703125
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7070, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11003.7109375
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7109, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11003.7060546875
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11003.712890625
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7129, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11003.7060546875
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11003.70703125
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7070, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11003.7060546875
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11003.70703125
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7070, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11003.7060546875
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11003.70703125
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7070, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11003.7099609375
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7100, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11003.705078125
tensor(11003.7061, grad_fn=<NegBackward0>) tensor(11003.7051, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11003.705078125
tensor(11003.7051, grad_fn=<NegBackward0>) tensor(11003.7051, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11003.7109375
tensor(11003.7051, grad_fn=<NegBackward0>) tensor(11003.7109, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11003.7041015625
tensor(11003.7051, grad_fn=<NegBackward0>) tensor(11003.7041, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11003.712890625
tensor(11003.7041, grad_fn=<NegBackward0>) tensor(11003.7129, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11003.705078125
tensor(11003.7041, grad_fn=<NegBackward0>) tensor(11003.7051, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11003.7109375
tensor(11003.7041, grad_fn=<NegBackward0>) tensor(11003.7109, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11003.705078125
tensor(11003.7041, grad_fn=<NegBackward0>) tensor(11003.7051, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -11003.7060546875
tensor(11003.7041, grad_fn=<NegBackward0>) tensor(11003.7061, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.7989, 0.2011],
        [0.2296, 0.7704]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5417, 0.4583], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.0901],
         [0.7225, 0.3021]],

        [[0.6524, 0.1050],
         [0.5178, 0.6736]],

        [[0.5011, 0.1026],
         [0.7094, 0.5835]],

        [[0.5245, 0.0973],
         [0.5688, 0.6287]],

        [[0.6584, 0.1058],
         [0.6049, 0.5322]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9137624860793706
Average Adjusted Rand Index: 0.9132897464116596
[-0.002004071831296791, 0.9137624860793706] [0.0, 0.9132897464116596] [11287.134765625, 11003.7060546875]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11379.123632236695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19893.86328125
inf tensor(19893.8633, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11675.828125
tensor(19893.8633, grad_fn=<NegBackward0>) tensor(11675.8281, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11660.5419921875
tensor(11675.8281, grad_fn=<NegBackward0>) tensor(11660.5420, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11499.5498046875
tensor(11660.5420, grad_fn=<NegBackward0>) tensor(11499.5498, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11418.4443359375
tensor(11499.5498, grad_fn=<NegBackward0>) tensor(11418.4443, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11409.689453125
tensor(11418.4443, grad_fn=<NegBackward0>) tensor(11409.6895, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11404.486328125
tensor(11409.6895, grad_fn=<NegBackward0>) tensor(11404.4863, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11401.45703125
tensor(11404.4863, grad_fn=<NegBackward0>) tensor(11401.4570, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11401.1015625
tensor(11401.4570, grad_fn=<NegBackward0>) tensor(11401.1016, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11398.0205078125
tensor(11401.1016, grad_fn=<NegBackward0>) tensor(11398.0205, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11361.84765625
tensor(11398.0205, grad_fn=<NegBackward0>) tensor(11361.8477, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11361.4736328125
tensor(11361.8477, grad_fn=<NegBackward0>) tensor(11361.4736, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11361.4326171875
tensor(11361.4736, grad_fn=<NegBackward0>) tensor(11361.4326, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11361.4111328125
tensor(11361.4326, grad_fn=<NegBackward0>) tensor(11361.4111, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11361.396484375
tensor(11361.4111, grad_fn=<NegBackward0>) tensor(11361.3965, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11361.3896484375
tensor(11361.3965, grad_fn=<NegBackward0>) tensor(11361.3896, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11361.380859375
tensor(11361.3896, grad_fn=<NegBackward0>) tensor(11361.3809, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11361.373046875
tensor(11361.3809, grad_fn=<NegBackward0>) tensor(11361.3730, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11361.3671875
tensor(11361.3730, grad_fn=<NegBackward0>) tensor(11361.3672, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11358.2431640625
tensor(11361.3672, grad_fn=<NegBackward0>) tensor(11358.2432, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11357.845703125
tensor(11358.2432, grad_fn=<NegBackward0>) tensor(11357.8457, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11357.8408203125
tensor(11357.8457, grad_fn=<NegBackward0>) tensor(11357.8408, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11357.8359375
tensor(11357.8408, grad_fn=<NegBackward0>) tensor(11357.8359, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11357.8291015625
tensor(11357.8359, grad_fn=<NegBackward0>) tensor(11357.8291, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11357.826171875
tensor(11357.8291, grad_fn=<NegBackward0>) tensor(11357.8262, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11357.8251953125
tensor(11357.8262, grad_fn=<NegBackward0>) tensor(11357.8252, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11357.818359375
tensor(11357.8252, grad_fn=<NegBackward0>) tensor(11357.8184, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11357.8017578125
tensor(11357.8184, grad_fn=<NegBackward0>) tensor(11357.8018, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11357.8125
tensor(11357.8018, grad_fn=<NegBackward0>) tensor(11357.8125, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11357.798828125
tensor(11357.8018, grad_fn=<NegBackward0>) tensor(11357.7988, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11357.7978515625
tensor(11357.7988, grad_fn=<NegBackward0>) tensor(11357.7979, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11357.7958984375
tensor(11357.7979, grad_fn=<NegBackward0>) tensor(11357.7959, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11357.7978515625
tensor(11357.7959, grad_fn=<NegBackward0>) tensor(11357.7979, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11357.7958984375
tensor(11357.7959, grad_fn=<NegBackward0>) tensor(11357.7959, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11356.791015625
tensor(11357.7959, grad_fn=<NegBackward0>) tensor(11356.7910, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11355.345703125
tensor(11356.7910, grad_fn=<NegBackward0>) tensor(11355.3457, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11355.345703125
tensor(11355.3457, grad_fn=<NegBackward0>) tensor(11355.3457, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11355.345703125
tensor(11355.3457, grad_fn=<NegBackward0>) tensor(11355.3457, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11355.3447265625
tensor(11355.3457, grad_fn=<NegBackward0>) tensor(11355.3447, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11355.34375
tensor(11355.3447, grad_fn=<NegBackward0>) tensor(11355.3438, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11355.3427734375
tensor(11355.3438, grad_fn=<NegBackward0>) tensor(11355.3428, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11355.3427734375
tensor(11355.3428, grad_fn=<NegBackward0>) tensor(11355.3428, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11355.33984375
tensor(11355.3428, grad_fn=<NegBackward0>) tensor(11355.3398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11355.33984375
tensor(11355.3398, grad_fn=<NegBackward0>) tensor(11355.3398, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11355.337890625
tensor(11355.3398, grad_fn=<NegBackward0>) tensor(11355.3379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11355.333984375
tensor(11355.3379, grad_fn=<NegBackward0>) tensor(11355.3340, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11355.34375
tensor(11355.3340, grad_fn=<NegBackward0>) tensor(11355.3438, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11355.333984375
tensor(11355.3340, grad_fn=<NegBackward0>) tensor(11355.3340, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11355.33203125
tensor(11355.3340, grad_fn=<NegBackward0>) tensor(11355.3320, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11355.33203125
tensor(11355.3320, grad_fn=<NegBackward0>) tensor(11355.3320, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11355.3330078125
tensor(11355.3320, grad_fn=<NegBackward0>) tensor(11355.3330, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11355.3251953125
tensor(11355.3320, grad_fn=<NegBackward0>) tensor(11355.3252, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11355.255859375
tensor(11355.3252, grad_fn=<NegBackward0>) tensor(11355.2559, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11355.2705078125
tensor(11355.2559, grad_fn=<NegBackward0>) tensor(11355.2705, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11355.2568359375
tensor(11355.2559, grad_fn=<NegBackward0>) tensor(11355.2568, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11355.2705078125
tensor(11355.2559, grad_fn=<NegBackward0>) tensor(11355.2705, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11355.2548828125
tensor(11355.2559, grad_fn=<NegBackward0>) tensor(11355.2549, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11355.2578125
tensor(11355.2549, grad_fn=<NegBackward0>) tensor(11355.2578, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11355.2548828125
tensor(11355.2549, grad_fn=<NegBackward0>) tensor(11355.2549, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11355.2001953125
tensor(11355.2549, grad_fn=<NegBackward0>) tensor(11355.2002, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11355.1982421875
tensor(11355.2002, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11355.2021484375
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.2021, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11355.1982421875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11355.1982421875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11355.2119140625
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.2119, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11355.19921875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11355.1982421875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11355.2021484375
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.2021, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11355.1982421875
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11355.197265625
tensor(11355.1982, grad_fn=<NegBackward0>) tensor(11355.1973, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11355.19921875
tensor(11355.1973, grad_fn=<NegBackward0>) tensor(11355.1992, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11355.203125
tensor(11355.1973, grad_fn=<NegBackward0>) tensor(11355.2031, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11355.1982421875
tensor(11355.1973, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11355.3037109375
tensor(11355.1973, grad_fn=<NegBackward0>) tensor(11355.3037, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11355.1982421875
tensor(11355.1973, grad_fn=<NegBackward0>) tensor(11355.1982, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7345, 0.2655],
        [0.2925, 0.7075]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3118, 0.1146],
         [0.6775, 0.1967]],

        [[0.7043, 0.0975],
         [0.5191, 0.5499]],

        [[0.5781, 0.1007],
         [0.5169, 0.6077]],

        [[0.6078, 0.1035],
         [0.5178, 0.5062]],

        [[0.7029, 0.1069],
         [0.6877, 0.5395]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214431211812708
Average Adjusted Rand Index: 0.9225842727842257
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20822.73828125
inf tensor(20822.7383, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11678.5537109375
tensor(20822.7383, grad_fn=<NegBackward0>) tensor(11678.5537, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11676.80859375
tensor(11678.5537, grad_fn=<NegBackward0>) tensor(11676.8086, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11662.09375
tensor(11676.8086, grad_fn=<NegBackward0>) tensor(11662.0938, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11559.0595703125
tensor(11662.0938, grad_fn=<NegBackward0>) tensor(11559.0596, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11453.6982421875
tensor(11559.0596, grad_fn=<NegBackward0>) tensor(11453.6982, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11432.2265625
tensor(11453.6982, grad_fn=<NegBackward0>) tensor(11432.2266, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11427.94921875
tensor(11432.2266, grad_fn=<NegBackward0>) tensor(11427.9492, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11415.0751953125
tensor(11427.9492, grad_fn=<NegBackward0>) tensor(11415.0752, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11412.1513671875
tensor(11415.0752, grad_fn=<NegBackward0>) tensor(11412.1514, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11412.009765625
tensor(11412.1514, grad_fn=<NegBackward0>) tensor(11412.0098, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11411.8193359375
tensor(11412.0098, grad_fn=<NegBackward0>) tensor(11411.8193, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11411.0810546875
tensor(11411.8193, grad_fn=<NegBackward0>) tensor(11411.0811, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11410.7412109375
tensor(11411.0811, grad_fn=<NegBackward0>) tensor(11410.7412, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11409.896484375
tensor(11410.7412, grad_fn=<NegBackward0>) tensor(11409.8965, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11375.34375
tensor(11409.8965, grad_fn=<NegBackward0>) tensor(11375.3438, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11369.5234375
tensor(11375.3438, grad_fn=<NegBackward0>) tensor(11369.5234, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11369.388671875
tensor(11369.5234, grad_fn=<NegBackward0>) tensor(11369.3887, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11369.3271484375
tensor(11369.3887, grad_fn=<NegBackward0>) tensor(11369.3271, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11369.05859375
tensor(11369.3271, grad_fn=<NegBackward0>) tensor(11369.0586, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11369.046875
tensor(11369.0586, grad_fn=<NegBackward0>) tensor(11369.0469, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11369.037109375
tensor(11369.0469, grad_fn=<NegBackward0>) tensor(11369.0371, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11369.02734375
tensor(11369.0371, grad_fn=<NegBackward0>) tensor(11369.0273, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11369.015625
tensor(11369.0273, grad_fn=<NegBackward0>) tensor(11369.0156, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11363.443359375
tensor(11369.0156, grad_fn=<NegBackward0>) tensor(11363.4434, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11363.4208984375
tensor(11363.4434, grad_fn=<NegBackward0>) tensor(11363.4209, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11363.408203125
tensor(11363.4209, grad_fn=<NegBackward0>) tensor(11363.4082, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11363.4052734375
tensor(11363.4082, grad_fn=<NegBackward0>) tensor(11363.4053, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11363.4033203125
tensor(11363.4053, grad_fn=<NegBackward0>) tensor(11363.4033, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11363.3994140625
tensor(11363.4033, grad_fn=<NegBackward0>) tensor(11363.3994, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11363.3916015625
tensor(11363.3994, grad_fn=<NegBackward0>) tensor(11363.3916, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11363.3837890625
tensor(11363.3916, grad_fn=<NegBackward0>) tensor(11363.3838, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11363.3876953125
tensor(11363.3838, grad_fn=<NegBackward0>) tensor(11363.3877, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11363.3818359375
tensor(11363.3838, grad_fn=<NegBackward0>) tensor(11363.3818, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11363.3623046875
tensor(11363.3818, grad_fn=<NegBackward0>) tensor(11363.3623, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11360.462890625
tensor(11363.3623, grad_fn=<NegBackward0>) tensor(11360.4629, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11360.4619140625
tensor(11360.4629, grad_fn=<NegBackward0>) tensor(11360.4619, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11360.458984375
tensor(11360.4619, grad_fn=<NegBackward0>) tensor(11360.4590, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11358.1611328125
tensor(11360.4590, grad_fn=<NegBackward0>) tensor(11358.1611, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11358.158203125
tensor(11358.1611, grad_fn=<NegBackward0>) tensor(11358.1582, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11358.1591796875
tensor(11358.1582, grad_fn=<NegBackward0>) tensor(11358.1592, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11358.15625
tensor(11358.1582, grad_fn=<NegBackward0>) tensor(11358.1562, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11358.15625
tensor(11358.1562, grad_fn=<NegBackward0>) tensor(11358.1562, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11358.158203125
tensor(11358.1562, grad_fn=<NegBackward0>) tensor(11358.1582, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11356.0595703125
tensor(11358.1562, grad_fn=<NegBackward0>) tensor(11356.0596, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11356.060546875
tensor(11356.0596, grad_fn=<NegBackward0>) tensor(11356.0605, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11356.0556640625
tensor(11356.0596, grad_fn=<NegBackward0>) tensor(11356.0557, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11356.0556640625
tensor(11356.0557, grad_fn=<NegBackward0>) tensor(11356.0557, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11356.0537109375
tensor(11356.0557, grad_fn=<NegBackward0>) tensor(11356.0537, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11356.0537109375
tensor(11356.0537, grad_fn=<NegBackward0>) tensor(11356.0537, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11356.052734375
tensor(11356.0537, grad_fn=<NegBackward0>) tensor(11356.0527, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11356.052734375
tensor(11356.0527, grad_fn=<NegBackward0>) tensor(11356.0527, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11356.046875
tensor(11356.0527, grad_fn=<NegBackward0>) tensor(11356.0469, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11355.8876953125
tensor(11356.0469, grad_fn=<NegBackward0>) tensor(11355.8877, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11355.8857421875
tensor(11355.8877, grad_fn=<NegBackward0>) tensor(11355.8857, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11355.8857421875
tensor(11355.8857, grad_fn=<NegBackward0>) tensor(11355.8857, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11355.88671875
tensor(11355.8857, grad_fn=<NegBackward0>) tensor(11355.8867, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11355.884765625
tensor(11355.8857, grad_fn=<NegBackward0>) tensor(11355.8848, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11355.890625
tensor(11355.8848, grad_fn=<NegBackward0>) tensor(11355.8906, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11355.884765625
tensor(11355.8848, grad_fn=<NegBackward0>) tensor(11355.8848, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11355.8828125
tensor(11355.8848, grad_fn=<NegBackward0>) tensor(11355.8828, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11355.890625
tensor(11355.8828, grad_fn=<NegBackward0>) tensor(11355.8906, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11355.880859375
tensor(11355.8828, grad_fn=<NegBackward0>) tensor(11355.8809, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11355.8740234375
tensor(11355.8809, grad_fn=<NegBackward0>) tensor(11355.8740, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11355.875
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8750, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11355.876953125
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8770, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11355.875
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8750, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11355.875
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8750, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11355.8740234375
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8740, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11355.8818359375
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8818, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11355.8740234375
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8740, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11355.8837890625
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8838, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11355.8310546875
tensor(11355.8740, grad_fn=<NegBackward0>) tensor(11355.8311, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11355.2236328125
tensor(11355.8311, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11355.2255859375
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2256, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11355.2236328125
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11355.2236328125
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11355.2236328125
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11355.2236328125
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11355.228515625
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2285, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11355.345703125
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.3457, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11355.22265625
tensor(11355.2236, grad_fn=<NegBackward0>) tensor(11355.2227, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11355.2236328125
tensor(11355.2227, grad_fn=<NegBackward0>) tensor(11355.2236, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11355.220703125
tensor(11355.2227, grad_fn=<NegBackward0>) tensor(11355.2207, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11355.240234375
tensor(11355.2207, grad_fn=<NegBackward0>) tensor(11355.2402, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11355.2197265625
tensor(11355.2207, grad_fn=<NegBackward0>) tensor(11355.2197, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11355.220703125
tensor(11355.2197, grad_fn=<NegBackward0>) tensor(11355.2207, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11355.220703125
tensor(11355.2197, grad_fn=<NegBackward0>) tensor(11355.2207, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11355.228515625
tensor(11355.2197, grad_fn=<NegBackward0>) tensor(11355.2285, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11355.21875
tensor(11355.2197, grad_fn=<NegBackward0>) tensor(11355.2188, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11355.2177734375
tensor(11355.2188, grad_fn=<NegBackward0>) tensor(11355.2178, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11355.2197265625
tensor(11355.2178, grad_fn=<NegBackward0>) tensor(11355.2197, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11355.216796875
tensor(11355.2178, grad_fn=<NegBackward0>) tensor(11355.2168, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11355.216796875
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2168, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11355.21875
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2188, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11355.2177734375
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2178, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11355.216796875
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2168, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11355.23046875
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2305, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11355.2177734375
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2178, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11355.21875
tensor(11355.2168, grad_fn=<NegBackward0>) tensor(11355.2188, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7076, 0.2924],
        [0.2656, 0.7344]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5087, 0.4913], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1966, 0.1147],
         [0.6925, 0.3119]],

        [[0.6333, 0.0976],
         [0.5322, 0.5691]],

        [[0.7163, 0.1007],
         [0.6933, 0.5176]],

        [[0.5550, 0.1035],
         [0.6979, 0.6401]],

        [[0.7076, 0.1069],
         [0.6598, 0.5862]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214431211812708
Average Adjusted Rand Index: 0.9225842727842257
[0.9214431211812708, 0.9214431211812708] [0.9225842727842257, 0.9225842727842257] [11355.1982421875, 11355.21875]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11201.65124279651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21989.12109375
inf tensor(21989.1211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11517.49609375
tensor(21989.1211, grad_fn=<NegBackward0>) tensor(11517.4961, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11516.6572265625
tensor(11517.4961, grad_fn=<NegBackward0>) tensor(11516.6572, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11515.6337890625
tensor(11516.6572, grad_fn=<NegBackward0>) tensor(11515.6338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11513.12890625
tensor(11515.6338, grad_fn=<NegBackward0>) tensor(11513.1289, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11509.421875
tensor(11513.1289, grad_fn=<NegBackward0>) tensor(11509.4219, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11424.1689453125
tensor(11509.4219, grad_fn=<NegBackward0>) tensor(11424.1689, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11331.6962890625
tensor(11424.1689, grad_fn=<NegBackward0>) tensor(11331.6963, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11302.201171875
tensor(11331.6963, grad_fn=<NegBackward0>) tensor(11302.2012, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11300.015625
tensor(11302.2012, grad_fn=<NegBackward0>) tensor(11300.0156, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11299.8125
tensor(11300.0156, grad_fn=<NegBackward0>) tensor(11299.8125, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11299.701171875
tensor(11299.8125, grad_fn=<NegBackward0>) tensor(11299.7012, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11299.626953125
tensor(11299.7012, grad_fn=<NegBackward0>) tensor(11299.6270, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11299.5546875
tensor(11299.6270, grad_fn=<NegBackward0>) tensor(11299.5547, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11296.1279296875
tensor(11299.5547, grad_fn=<NegBackward0>) tensor(11296.1279, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11284.091796875
tensor(11296.1279, grad_fn=<NegBackward0>) tensor(11284.0918, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11283.529296875
tensor(11284.0918, grad_fn=<NegBackward0>) tensor(11283.5293, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11283.26171875
tensor(11283.5293, grad_fn=<NegBackward0>) tensor(11283.2617, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11283.240234375
tensor(11283.2617, grad_fn=<NegBackward0>) tensor(11283.2402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11283.2236328125
tensor(11283.2402, grad_fn=<NegBackward0>) tensor(11283.2236, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11282.62109375
tensor(11283.2236, grad_fn=<NegBackward0>) tensor(11282.6211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11282.5771484375
tensor(11282.6211, grad_fn=<NegBackward0>) tensor(11282.5771, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11282.142578125
tensor(11282.5771, grad_fn=<NegBackward0>) tensor(11282.1426, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11279.3779296875
tensor(11282.1426, grad_fn=<NegBackward0>) tensor(11279.3779, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11279.361328125
tensor(11279.3779, grad_fn=<NegBackward0>) tensor(11279.3613, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11279.3525390625
tensor(11279.3613, grad_fn=<NegBackward0>) tensor(11279.3525, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11279.3408203125
tensor(11279.3525, grad_fn=<NegBackward0>) tensor(11279.3408, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11279.322265625
tensor(11279.3408, grad_fn=<NegBackward0>) tensor(11279.3223, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11279.2705078125
tensor(11279.3223, grad_fn=<NegBackward0>) tensor(11279.2705, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11279.0498046875
tensor(11279.2705, grad_fn=<NegBackward0>) tensor(11279.0498, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11278.7998046875
tensor(11279.0498, grad_fn=<NegBackward0>) tensor(11278.7998, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11278.544921875
tensor(11278.7998, grad_fn=<NegBackward0>) tensor(11278.5449, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11278.1337890625
tensor(11278.5449, grad_fn=<NegBackward0>) tensor(11278.1338, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11277.9033203125
tensor(11278.1338, grad_fn=<NegBackward0>) tensor(11277.9033, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11277.8076171875
tensor(11277.9033, grad_fn=<NegBackward0>) tensor(11277.8076, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11274.9482421875
tensor(11277.8076, grad_fn=<NegBackward0>) tensor(11274.9482, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11274.19140625
tensor(11274.9482, grad_fn=<NegBackward0>) tensor(11274.1914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11274.177734375
tensor(11274.1914, grad_fn=<NegBackward0>) tensor(11274.1777, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11274.169921875
tensor(11274.1777, grad_fn=<NegBackward0>) tensor(11274.1699, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11274.16796875
tensor(11274.1699, grad_fn=<NegBackward0>) tensor(11274.1680, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11274.1640625
tensor(11274.1680, grad_fn=<NegBackward0>) tensor(11274.1641, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11274.1630859375
tensor(11274.1641, grad_fn=<NegBackward0>) tensor(11274.1631, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11270.0380859375
tensor(11274.1631, grad_fn=<NegBackward0>) tensor(11270.0381, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11269.9931640625
tensor(11270.0381, grad_fn=<NegBackward0>) tensor(11269.9932, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11269.994140625
tensor(11269.9932, grad_fn=<NegBackward0>) tensor(11269.9941, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11269.9921875
tensor(11269.9932, grad_fn=<NegBackward0>) tensor(11269.9922, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11269.9921875
tensor(11269.9922, grad_fn=<NegBackward0>) tensor(11269.9922, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11269.9912109375
tensor(11269.9922, grad_fn=<NegBackward0>) tensor(11269.9912, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11269.99609375
tensor(11269.9912, grad_fn=<NegBackward0>) tensor(11269.9961, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11269.98828125
tensor(11269.9912, grad_fn=<NegBackward0>) tensor(11269.9883, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11269.9833984375
tensor(11269.9883, grad_fn=<NegBackward0>) tensor(11269.9834, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11269.9833984375
tensor(11269.9834, grad_fn=<NegBackward0>) tensor(11269.9834, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11269.982421875
tensor(11269.9834, grad_fn=<NegBackward0>) tensor(11269.9824, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11269.9794921875
tensor(11269.9824, grad_fn=<NegBackward0>) tensor(11269.9795, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11269.9775390625
tensor(11269.9795, grad_fn=<NegBackward0>) tensor(11269.9775, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11269.978515625
tensor(11269.9775, grad_fn=<NegBackward0>) tensor(11269.9785, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11269.974609375
tensor(11269.9775, grad_fn=<NegBackward0>) tensor(11269.9746, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11269.978515625
tensor(11269.9746, grad_fn=<NegBackward0>) tensor(11269.9785, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11269.9736328125
tensor(11269.9746, grad_fn=<NegBackward0>) tensor(11269.9736, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11269.951171875
tensor(11269.9736, grad_fn=<NegBackward0>) tensor(11269.9512, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11269.94921875
tensor(11269.9512, grad_fn=<NegBackward0>) tensor(11269.9492, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11269.94921875
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9492, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11269.9501953125
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9502, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11269.94921875
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9492, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11269.94921875
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9492, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11269.94921875
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9492, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11269.9482421875
tensor(11269.9492, grad_fn=<NegBackward0>) tensor(11269.9482, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11269.9482421875
tensor(11269.9482, grad_fn=<NegBackward0>) tensor(11269.9482, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11269.947265625
tensor(11269.9482, grad_fn=<NegBackward0>) tensor(11269.9473, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11269.953125
tensor(11269.9473, grad_fn=<NegBackward0>) tensor(11269.9531, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11269.9482421875
tensor(11269.9473, grad_fn=<NegBackward0>) tensor(11269.9482, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11269.96875
tensor(11269.9473, grad_fn=<NegBackward0>) tensor(11269.9688, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11269.9482421875
tensor(11269.9473, grad_fn=<NegBackward0>) tensor(11269.9482, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11269.9482421875
tensor(11269.9473, grad_fn=<NegBackward0>) tensor(11269.9482, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.6954, 0.3046],
        [0.2620, 0.7380]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8458, 0.1542], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.1021],
         [0.6102, 0.3166]],

        [[0.6766, 0.1098],
         [0.5581, 0.7258]],

        [[0.5975, 0.1006],
         [0.6516, 0.6626]],

        [[0.6003, 0.1052],
         [0.6686, 0.5114]],

        [[0.5974, 0.1000],
         [0.6992, 0.5006]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0027762814403513862
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.5586130610404322
Average Adjusted Rand Index: 0.7440877030078953
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20647.0625
inf tensor(20647.0625, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11517.36328125
tensor(20647.0625, grad_fn=<NegBackward0>) tensor(11517.3633, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11517.0048828125
tensor(11517.3633, grad_fn=<NegBackward0>) tensor(11517.0049, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11516.7041015625
tensor(11517.0049, grad_fn=<NegBackward0>) tensor(11516.7041, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11515.578125
tensor(11516.7041, grad_fn=<NegBackward0>) tensor(11515.5781, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11509.89453125
tensor(11515.5781, grad_fn=<NegBackward0>) tensor(11509.8945, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11359.19140625
tensor(11509.8945, grad_fn=<NegBackward0>) tensor(11359.1914, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11293.578125
tensor(11359.1914, grad_fn=<NegBackward0>) tensor(11293.5781, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11284.5185546875
tensor(11293.5781, grad_fn=<NegBackward0>) tensor(11284.5186, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11284.14453125
tensor(11284.5186, grad_fn=<NegBackward0>) tensor(11284.1445, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11280.689453125
tensor(11284.1445, grad_fn=<NegBackward0>) tensor(11280.6895, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11277.0146484375
tensor(11280.6895, grad_fn=<NegBackward0>) tensor(11277.0146, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11275.9375
tensor(11277.0146, grad_fn=<NegBackward0>) tensor(11275.9375, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11275.8876953125
tensor(11275.9375, grad_fn=<NegBackward0>) tensor(11275.8877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11275.8564453125
tensor(11275.8877, grad_fn=<NegBackward0>) tensor(11275.8564, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11275.8330078125
tensor(11275.8564, grad_fn=<NegBackward0>) tensor(11275.8330, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11275.8056640625
tensor(11275.8330, grad_fn=<NegBackward0>) tensor(11275.8057, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11275.3544921875
tensor(11275.8057, grad_fn=<NegBackward0>) tensor(11275.3545, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11275.3349609375
tensor(11275.3545, grad_fn=<NegBackward0>) tensor(11275.3350, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11275.326171875
tensor(11275.3350, grad_fn=<NegBackward0>) tensor(11275.3262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11275.3173828125
tensor(11275.3262, grad_fn=<NegBackward0>) tensor(11275.3174, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11275.310546875
tensor(11275.3174, grad_fn=<NegBackward0>) tensor(11275.3105, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11275.3037109375
tensor(11275.3105, grad_fn=<NegBackward0>) tensor(11275.3037, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11275.2978515625
tensor(11275.3037, grad_fn=<NegBackward0>) tensor(11275.2979, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11275.2880859375
tensor(11275.2979, grad_fn=<NegBackward0>) tensor(11275.2881, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11275.2470703125
tensor(11275.2881, grad_fn=<NegBackward0>) tensor(11275.2471, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11275.2373046875
tensor(11275.2471, grad_fn=<NegBackward0>) tensor(11275.2373, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11275.23046875
tensor(11275.2373, grad_fn=<NegBackward0>) tensor(11275.2305, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11275.220703125
tensor(11275.2305, grad_fn=<NegBackward0>) tensor(11275.2207, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11275.208984375
tensor(11275.2207, grad_fn=<NegBackward0>) tensor(11275.2090, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11275.1845703125
tensor(11275.2090, grad_fn=<NegBackward0>) tensor(11275.1846, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11275.060546875
tensor(11275.1846, grad_fn=<NegBackward0>) tensor(11275.0605, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11274.4013671875
tensor(11275.0605, grad_fn=<NegBackward0>) tensor(11274.4014, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11274.2451171875
tensor(11274.4014, grad_fn=<NegBackward0>) tensor(11274.2451, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11274.0185546875
tensor(11274.2451, grad_fn=<NegBackward0>) tensor(11274.0186, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11272.6572265625
tensor(11274.0186, grad_fn=<NegBackward0>) tensor(11272.6572, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11272.5966796875
tensor(11272.6572, grad_fn=<NegBackward0>) tensor(11272.5967, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11272.5546875
tensor(11272.5967, grad_fn=<NegBackward0>) tensor(11272.5547, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11272.5439453125
tensor(11272.5547, grad_fn=<NegBackward0>) tensor(11272.5439, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11272.541015625
tensor(11272.5439, grad_fn=<NegBackward0>) tensor(11272.5410, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11272.5390625
tensor(11272.5410, grad_fn=<NegBackward0>) tensor(11272.5391, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11272.533203125
tensor(11272.5391, grad_fn=<NegBackward0>) tensor(11272.5332, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11272.53125
tensor(11272.5332, grad_fn=<NegBackward0>) tensor(11272.5312, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11272.52734375
tensor(11272.5312, grad_fn=<NegBackward0>) tensor(11272.5273, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11269.921875
tensor(11272.5273, grad_fn=<NegBackward0>) tensor(11269.9219, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11269.8740234375
tensor(11269.9219, grad_fn=<NegBackward0>) tensor(11269.8740, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11269.837890625
tensor(11269.8740, grad_fn=<NegBackward0>) tensor(11269.8379, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11269.837890625
tensor(11269.8379, grad_fn=<NegBackward0>) tensor(11269.8379, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11269.83984375
tensor(11269.8379, grad_fn=<NegBackward0>) tensor(11269.8398, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11269.8349609375
tensor(11269.8379, grad_fn=<NegBackward0>) tensor(11269.8350, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11269.8359375
tensor(11269.8350, grad_fn=<NegBackward0>) tensor(11269.8359, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11269.8349609375
tensor(11269.8350, grad_fn=<NegBackward0>) tensor(11269.8350, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11269.833984375
tensor(11269.8350, grad_fn=<NegBackward0>) tensor(11269.8340, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11269.8349609375
tensor(11269.8340, grad_fn=<NegBackward0>) tensor(11269.8350, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11269.8330078125
tensor(11269.8340, grad_fn=<NegBackward0>) tensor(11269.8330, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11269.83203125
tensor(11269.8330, grad_fn=<NegBackward0>) tensor(11269.8320, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11269.83203125
tensor(11269.8320, grad_fn=<NegBackward0>) tensor(11269.8320, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11269.8310546875
tensor(11269.8320, grad_fn=<NegBackward0>) tensor(11269.8311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11269.8310546875
tensor(11269.8311, grad_fn=<NegBackward0>) tensor(11269.8311, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11269.83203125
tensor(11269.8311, grad_fn=<NegBackward0>) tensor(11269.8320, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11269.8310546875
tensor(11269.8311, grad_fn=<NegBackward0>) tensor(11269.8311, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11269.8291015625
tensor(11269.8311, grad_fn=<NegBackward0>) tensor(11269.8291, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11269.830078125
tensor(11269.8291, grad_fn=<NegBackward0>) tensor(11269.8301, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11269.8349609375
tensor(11269.8291, grad_fn=<NegBackward0>) tensor(11269.8350, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11269.8310546875
tensor(11269.8291, grad_fn=<NegBackward0>) tensor(11269.8311, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11269.830078125
tensor(11269.8291, grad_fn=<NegBackward0>) tensor(11269.8301, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11269.830078125
tensor(11269.8291, grad_fn=<NegBackward0>) tensor(11269.8301, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.6965, 0.3035],
        [0.2605, 0.7395]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8457, 0.1543], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1021],
         [0.5783, 0.3164]],

        [[0.5662, 0.1099],
         [0.5229, 0.5217]],

        [[0.5805, 0.1006],
         [0.5598, 0.6525]],

        [[0.5367, 0.1051],
         [0.7251, 0.6242]],

        [[0.5411, 0.1000],
         [0.7008, 0.7176]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0027762814403513862
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.5586130610404322
Average Adjusted Rand Index: 0.7440877030078953
[0.5586130610404322, 0.5586130610404322] [0.7440877030078953, 0.7440877030078953] [11269.9482421875, 11269.830078125]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11452.03395267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24094.1796875
inf tensor(24094.1797, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11730.0634765625
tensor(24094.1797, grad_fn=<NegBackward0>) tensor(11730.0635, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11727.796875
tensor(11730.0635, grad_fn=<NegBackward0>) tensor(11727.7969, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11719.6201171875
tensor(11727.7969, grad_fn=<NegBackward0>) tensor(11719.6201, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11699.0224609375
tensor(11719.6201, grad_fn=<NegBackward0>) tensor(11699.0225, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11665.224609375
tensor(11699.0225, grad_fn=<NegBackward0>) tensor(11665.2246, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11605.1650390625
tensor(11665.2246, grad_fn=<NegBackward0>) tensor(11605.1650, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11580.603515625
tensor(11605.1650, grad_fn=<NegBackward0>) tensor(11580.6035, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11579.736328125
tensor(11580.6035, grad_fn=<NegBackward0>) tensor(11579.7363, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11579.3603515625
tensor(11579.7363, grad_fn=<NegBackward0>) tensor(11579.3604, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11579.23046875
tensor(11579.3604, grad_fn=<NegBackward0>) tensor(11579.2305, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11579.03515625
tensor(11579.2305, grad_fn=<NegBackward0>) tensor(11579.0352, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11578.322265625
tensor(11579.0352, grad_fn=<NegBackward0>) tensor(11578.3223, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11578.271484375
tensor(11578.3223, grad_fn=<NegBackward0>) tensor(11578.2715, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11578.24609375
tensor(11578.2715, grad_fn=<NegBackward0>) tensor(11578.2461, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11578.2197265625
tensor(11578.2461, grad_fn=<NegBackward0>) tensor(11578.2197, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11578.203125
tensor(11578.2197, grad_fn=<NegBackward0>) tensor(11578.2031, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11578.185546875
tensor(11578.2031, grad_fn=<NegBackward0>) tensor(11578.1855, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11578.0400390625
tensor(11578.1855, grad_fn=<NegBackward0>) tensor(11578.0400, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11578.0283203125
tensor(11578.0400, grad_fn=<NegBackward0>) tensor(11578.0283, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11577.94921875
tensor(11578.0283, grad_fn=<NegBackward0>) tensor(11577.9492, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11577.939453125
tensor(11577.9492, grad_fn=<NegBackward0>) tensor(11577.9395, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11577.9326171875
tensor(11577.9395, grad_fn=<NegBackward0>) tensor(11577.9326, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11577.923828125
tensor(11577.9326, grad_fn=<NegBackward0>) tensor(11577.9238, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11577.9169921875
tensor(11577.9238, grad_fn=<NegBackward0>) tensor(11577.9170, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11577.9111328125
tensor(11577.9170, grad_fn=<NegBackward0>) tensor(11577.9111, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11577.9072265625
tensor(11577.9111, grad_fn=<NegBackward0>) tensor(11577.9072, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11577.9013671875
tensor(11577.9072, grad_fn=<NegBackward0>) tensor(11577.9014, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11577.8916015625
tensor(11577.9014, grad_fn=<NegBackward0>) tensor(11577.8916, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11577.8779296875
tensor(11577.8916, grad_fn=<NegBackward0>) tensor(11577.8779, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11577.8095703125
tensor(11577.8779, grad_fn=<NegBackward0>) tensor(11577.8096, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11571.0224609375
tensor(11577.8096, grad_fn=<NegBackward0>) tensor(11571.0225, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11566.2998046875
tensor(11571.0225, grad_fn=<NegBackward0>) tensor(11566.2998, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11562.048828125
tensor(11566.2998, grad_fn=<NegBackward0>) tensor(11562.0488, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11549.130859375
tensor(11562.0488, grad_fn=<NegBackward0>) tensor(11549.1309, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11548.4033203125
tensor(11549.1309, grad_fn=<NegBackward0>) tensor(11548.4033, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11548.365234375
tensor(11548.4033, grad_fn=<NegBackward0>) tensor(11548.3652, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11548.3388671875
tensor(11548.3652, grad_fn=<NegBackward0>) tensor(11548.3389, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11547.251953125
tensor(11548.3389, grad_fn=<NegBackward0>) tensor(11547.2520, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11535.4384765625
tensor(11547.2520, grad_fn=<NegBackward0>) tensor(11535.4385, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11521.2890625
tensor(11535.4385, grad_fn=<NegBackward0>) tensor(11521.2891, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11521.2373046875
tensor(11521.2891, grad_fn=<NegBackward0>) tensor(11521.2373, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11521.2255859375
tensor(11521.2373, grad_fn=<NegBackward0>) tensor(11521.2256, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11521.169921875
tensor(11521.2256, grad_fn=<NegBackward0>) tensor(11521.1699, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11521.14453125
tensor(11521.1699, grad_fn=<NegBackward0>) tensor(11521.1445, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11517.37890625
tensor(11521.1445, grad_fn=<NegBackward0>) tensor(11517.3789, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11517.0947265625
tensor(11517.3789, grad_fn=<NegBackward0>) tensor(11517.0947, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11517.0947265625
tensor(11517.0947, grad_fn=<NegBackward0>) tensor(11517.0947, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11517.0927734375
tensor(11517.0947, grad_fn=<NegBackward0>) tensor(11517.0928, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11517.0927734375
tensor(11517.0928, grad_fn=<NegBackward0>) tensor(11517.0928, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11517.0908203125
tensor(11517.0928, grad_fn=<NegBackward0>) tensor(11517.0908, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11517.0869140625
tensor(11517.0908, grad_fn=<NegBackward0>) tensor(11517.0869, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11517.0859375
tensor(11517.0869, grad_fn=<NegBackward0>) tensor(11517.0859, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11517.0859375
tensor(11517.0859, grad_fn=<NegBackward0>) tensor(11517.0859, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11517.0859375
tensor(11517.0859, grad_fn=<NegBackward0>) tensor(11517.0859, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11517.0888671875
tensor(11517.0859, grad_fn=<NegBackward0>) tensor(11517.0889, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11517.083984375
tensor(11517.0859, grad_fn=<NegBackward0>) tensor(11517.0840, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11517.08203125
tensor(11517.0840, grad_fn=<NegBackward0>) tensor(11517.0820, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11517.080078125
tensor(11517.0820, grad_fn=<NegBackward0>) tensor(11517.0801, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11517.0791015625
tensor(11517.0801, grad_fn=<NegBackward0>) tensor(11517.0791, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11517.0791015625
tensor(11517.0791, grad_fn=<NegBackward0>) tensor(11517.0791, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11515.1103515625
tensor(11517.0791, grad_fn=<NegBackward0>) tensor(11515.1104, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11515.0888671875
tensor(11515.1104, grad_fn=<NegBackward0>) tensor(11515.0889, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11515.0888671875
tensor(11515.0889, grad_fn=<NegBackward0>) tensor(11515.0889, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11515.0869140625
tensor(11515.0889, grad_fn=<NegBackward0>) tensor(11515.0869, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11515.0869140625
tensor(11515.0869, grad_fn=<NegBackward0>) tensor(11515.0869, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11515.0859375
tensor(11515.0869, grad_fn=<NegBackward0>) tensor(11515.0859, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11515.0849609375
tensor(11515.0859, grad_fn=<NegBackward0>) tensor(11515.0850, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11515.0830078125
tensor(11515.0850, grad_fn=<NegBackward0>) tensor(11515.0830, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11515.0830078125
tensor(11515.0830, grad_fn=<NegBackward0>) tensor(11515.0830, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11515.08203125
tensor(11515.0830, grad_fn=<NegBackward0>) tensor(11515.0820, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11515.080078125
tensor(11515.0820, grad_fn=<NegBackward0>) tensor(11515.0801, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11514.9853515625
tensor(11515.0801, grad_fn=<NegBackward0>) tensor(11514.9854, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11514.9833984375
tensor(11514.9854, grad_fn=<NegBackward0>) tensor(11514.9834, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11514.9775390625
tensor(11514.9834, grad_fn=<NegBackward0>) tensor(11514.9775, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11514.978515625
tensor(11514.9775, grad_fn=<NegBackward0>) tensor(11514.9785, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11514.978515625
tensor(11514.9775, grad_fn=<NegBackward0>) tensor(11514.9785, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11514.9775390625
tensor(11514.9775, grad_fn=<NegBackward0>) tensor(11514.9775, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11514.9716796875
tensor(11514.9775, grad_fn=<NegBackward0>) tensor(11514.9717, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11514.9833984375
tensor(11514.9717, grad_fn=<NegBackward0>) tensor(11514.9834, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11514.96875
tensor(11514.9717, grad_fn=<NegBackward0>) tensor(11514.9688, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11514.98046875
tensor(11514.9688, grad_fn=<NegBackward0>) tensor(11514.9805, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11514.9677734375
tensor(11514.9688, grad_fn=<NegBackward0>) tensor(11514.9678, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11514.966796875
tensor(11514.9678, grad_fn=<NegBackward0>) tensor(11514.9668, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11514.966796875
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9668, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11514.966796875
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9668, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11514.97265625
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9727, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11514.966796875
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9668, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11514.966796875
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9668, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11514.9658203125
tensor(11514.9668, grad_fn=<NegBackward0>) tensor(11514.9658, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11514.9658203125
tensor(11514.9658, grad_fn=<NegBackward0>) tensor(11514.9658, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11515.0322265625
tensor(11514.9658, grad_fn=<NegBackward0>) tensor(11515.0322, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11514.9658203125
tensor(11514.9658, grad_fn=<NegBackward0>) tensor(11514.9658, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11514.96484375
tensor(11514.9658, grad_fn=<NegBackward0>) tensor(11514.9648, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11514.96484375
tensor(11514.9648, grad_fn=<NegBackward0>) tensor(11514.9648, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11514.96484375
tensor(11514.9648, grad_fn=<NegBackward0>) tensor(11514.9648, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11514.9638671875
tensor(11514.9648, grad_fn=<NegBackward0>) tensor(11514.9639, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11514.9619140625
tensor(11514.9639, grad_fn=<NegBackward0>) tensor(11514.9619, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11514.9619140625
tensor(11514.9619, grad_fn=<NegBackward0>) tensor(11514.9619, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11514.9736328125
tensor(11514.9619, grad_fn=<NegBackward0>) tensor(11514.9736, grad_fn=<NegBackward0>)
1
pi: tensor([[0.4253, 0.5747],
        [0.5008, 0.4992]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6076, 0.3924], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2502, 0.0940],
         [0.6381, 0.2655]],

        [[0.7098, 0.0990],
         [0.5380, 0.6993]],

        [[0.7089, 0.1076],
         [0.6854, 0.6695]],

        [[0.7082, 0.1143],
         [0.6573, 0.5375]],

        [[0.5322, 0.0950],
         [0.6973, 0.6306]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448275862068966
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.0380904920385603
Average Adjusted Rand Index: 0.8472728754739303
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24890.34765625
inf tensor(24890.3477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11727.3505859375
tensor(24890.3477, grad_fn=<NegBackward0>) tensor(11727.3506, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11706.84375
tensor(11727.3506, grad_fn=<NegBackward0>) tensor(11706.8438, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11696.248046875
tensor(11706.8438, grad_fn=<NegBackward0>) tensor(11696.2480, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11665.42578125
tensor(11696.2480, grad_fn=<NegBackward0>) tensor(11665.4258, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11559.625
tensor(11665.4258, grad_fn=<NegBackward0>) tensor(11559.6250, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11543.021484375
tensor(11559.6250, grad_fn=<NegBackward0>) tensor(11543.0215, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11525.0478515625
tensor(11543.0215, grad_fn=<NegBackward0>) tensor(11525.0479, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11519.447265625
tensor(11525.0479, grad_fn=<NegBackward0>) tensor(11519.4473, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11518.041015625
tensor(11519.4473, grad_fn=<NegBackward0>) tensor(11518.0410, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11517.83984375
tensor(11518.0410, grad_fn=<NegBackward0>) tensor(11517.8398, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11517.8291015625
tensor(11517.8398, grad_fn=<NegBackward0>) tensor(11517.8291, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11517.8076171875
tensor(11517.8291, grad_fn=<NegBackward0>) tensor(11517.8076, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11517.525390625
tensor(11517.8076, grad_fn=<NegBackward0>) tensor(11517.5254, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11514.453125
tensor(11517.5254, grad_fn=<NegBackward0>) tensor(11514.4531, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11514.3486328125
tensor(11514.4531, grad_fn=<NegBackward0>) tensor(11514.3486, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11514.314453125
tensor(11514.3486, grad_fn=<NegBackward0>) tensor(11514.3145, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11514.3076171875
tensor(11514.3145, grad_fn=<NegBackward0>) tensor(11514.3076, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11514.2890625
tensor(11514.3076, grad_fn=<NegBackward0>) tensor(11514.2891, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11514.2822265625
tensor(11514.2891, grad_fn=<NegBackward0>) tensor(11514.2822, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11514.2734375
tensor(11514.2822, grad_fn=<NegBackward0>) tensor(11514.2734, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11514.271484375
tensor(11514.2734, grad_fn=<NegBackward0>) tensor(11514.2715, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11514.2705078125
tensor(11514.2715, grad_fn=<NegBackward0>) tensor(11514.2705, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11514.2685546875
tensor(11514.2705, grad_fn=<NegBackward0>) tensor(11514.2686, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11514.2666015625
tensor(11514.2686, grad_fn=<NegBackward0>) tensor(11514.2666, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11514.2548828125
tensor(11514.2666, grad_fn=<NegBackward0>) tensor(11514.2549, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11514.25390625
tensor(11514.2549, grad_fn=<NegBackward0>) tensor(11514.2539, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11514.2529296875
tensor(11514.2539, grad_fn=<NegBackward0>) tensor(11514.2529, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11514.2529296875
tensor(11514.2529, grad_fn=<NegBackward0>) tensor(11514.2529, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11514.244140625
tensor(11514.2529, grad_fn=<NegBackward0>) tensor(11514.2441, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11514.2431640625
tensor(11514.2441, grad_fn=<NegBackward0>) tensor(11514.2432, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11514.2294921875
tensor(11514.2432, grad_fn=<NegBackward0>) tensor(11514.2295, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11514.2119140625
tensor(11514.2295, grad_fn=<NegBackward0>) tensor(11514.2119, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11514.20703125
tensor(11514.2119, grad_fn=<NegBackward0>) tensor(11514.2070, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11514.208984375
tensor(11514.2070, grad_fn=<NegBackward0>) tensor(11514.2090, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11514.2080078125
tensor(11514.2070, grad_fn=<NegBackward0>) tensor(11514.2080, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -11514.1591796875
tensor(11514.2070, grad_fn=<NegBackward0>) tensor(11514.1592, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11514.15625
tensor(11514.1592, grad_fn=<NegBackward0>) tensor(11514.1562, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11514.1513671875
tensor(11514.1562, grad_fn=<NegBackward0>) tensor(11514.1514, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11514.16015625
tensor(11514.1514, grad_fn=<NegBackward0>) tensor(11514.1602, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11514.15234375
tensor(11514.1514, grad_fn=<NegBackward0>) tensor(11514.1523, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11514.1513671875
tensor(11514.1514, grad_fn=<NegBackward0>) tensor(11514.1514, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11514.150390625
tensor(11514.1514, grad_fn=<NegBackward0>) tensor(11514.1504, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11514.150390625
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1504, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11514.1513671875
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1514, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11514.1572265625
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1572, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11514.15234375
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1523, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11514.150390625
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1504, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11514.150390625
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1504, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11514.1513671875
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1514, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11514.1494140625
tensor(11514.1504, grad_fn=<NegBackward0>) tensor(11514.1494, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11514.150390625
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1504, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11514.1533203125
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1533, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11514.1533203125
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1533, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11514.1572265625
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1572, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -11514.1494140625
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1494, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11514.1494140625
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1494, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11514.1474609375
tensor(11514.1494, grad_fn=<NegBackward0>) tensor(11514.1475, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11514.1474609375
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1475, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11514.1474609375
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1475, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11514.1484375
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1484, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11514.1533203125
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1533, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11514.1484375
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1484, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11514.146484375
tensor(11514.1475, grad_fn=<NegBackward0>) tensor(11514.1465, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11514.14453125
tensor(11514.1465, grad_fn=<NegBackward0>) tensor(11514.1445, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11514.146484375
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1465, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11514.1474609375
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1475, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11514.14453125
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1445, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11514.14453125
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1445, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11514.1455078125
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1455, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11514.146484375
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1465, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11514.154296875
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1543, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11514.1474609375
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1475, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11514.1455078125
tensor(11514.1445, grad_fn=<NegBackward0>) tensor(11514.1455, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.4778, 0.5222],
        [0.5399, 0.4601]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4083, 0.5917], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2427, 0.0944],
         [0.6214, 0.2720]],

        [[0.5526, 0.0990],
         [0.7067, 0.5218]],

        [[0.5141, 0.1073],
         [0.6163, 0.5921]],

        [[0.5381, 0.1137],
         [0.6766, 0.5890]],

        [[0.5749, 0.0959],
         [0.6694, 0.5263]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824124176797128
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691544249601056
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.04475482088937634
Average Adjusted Rand Index: 0.8328980045211323
[0.0380904920385603, 0.04475482088937634] [0.8472728754739303, 0.8328980045211323] [11514.962890625, 11514.1455078125]
-------------------------------------
This iteration is 41
True Objective function: Loss = -10999.233983799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22275.681640625
inf tensor(22275.6816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11303.0810546875
tensor(22275.6816, grad_fn=<NegBackward0>) tensor(11303.0811, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11285.5361328125
tensor(11303.0811, grad_fn=<NegBackward0>) tensor(11285.5361, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11256.447265625
tensor(11285.5361, grad_fn=<NegBackward0>) tensor(11256.4473, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11112.7705078125
tensor(11256.4473, grad_fn=<NegBackward0>) tensor(11112.7705, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11097.28515625
tensor(11112.7705, grad_fn=<NegBackward0>) tensor(11097.2852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11092.2216796875
tensor(11097.2852, grad_fn=<NegBackward0>) tensor(11092.2217, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11091.3701171875
tensor(11092.2217, grad_fn=<NegBackward0>) tensor(11091.3701, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11091.341796875
tensor(11091.3701, grad_fn=<NegBackward0>) tensor(11091.3418, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11091.32421875
tensor(11091.3418, grad_fn=<NegBackward0>) tensor(11091.3242, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11091.306640625
tensor(11091.3242, grad_fn=<NegBackward0>) tensor(11091.3066, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11091.2978515625
tensor(11091.3066, grad_fn=<NegBackward0>) tensor(11091.2979, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11091.29296875
tensor(11091.2979, grad_fn=<NegBackward0>) tensor(11091.2930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11091.2890625
tensor(11091.2930, grad_fn=<NegBackward0>) tensor(11091.2891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11091.2861328125
tensor(11091.2891, grad_fn=<NegBackward0>) tensor(11091.2861, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11091.2431640625
tensor(11091.2861, grad_fn=<NegBackward0>) tensor(11091.2432, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11091.2412109375
tensor(11091.2432, grad_fn=<NegBackward0>) tensor(11091.2412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11091.05859375
tensor(11091.2412, grad_fn=<NegBackward0>) tensor(11091.0586, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11091.041015625
tensor(11091.0586, grad_fn=<NegBackward0>) tensor(11091.0410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11091.0361328125
tensor(11091.0410, grad_fn=<NegBackward0>) tensor(11091.0361, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11090.328125
tensor(11091.0361, grad_fn=<NegBackward0>) tensor(11090.3281, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11090.3193359375
tensor(11090.3281, grad_fn=<NegBackward0>) tensor(11090.3193, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11090.318359375
tensor(11090.3193, grad_fn=<NegBackward0>) tensor(11090.3184, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11090.31640625
tensor(11090.3184, grad_fn=<NegBackward0>) tensor(11090.3164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11090.31640625
tensor(11090.3164, grad_fn=<NegBackward0>) tensor(11090.3164, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11090.31640625
tensor(11090.3164, grad_fn=<NegBackward0>) tensor(11090.3164, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11090.3134765625
tensor(11090.3164, grad_fn=<NegBackward0>) tensor(11090.3135, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11090.30078125
tensor(11090.3135, grad_fn=<NegBackward0>) tensor(11090.3008, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11090.2998046875
tensor(11090.3008, grad_fn=<NegBackward0>) tensor(11090.2998, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11090.1953125
tensor(11090.2998, grad_fn=<NegBackward0>) tensor(11090.1953, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11090.193359375
tensor(11090.1953, grad_fn=<NegBackward0>) tensor(11090.1934, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11090.193359375
tensor(11090.1934, grad_fn=<NegBackward0>) tensor(11090.1934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11090.19140625
tensor(11090.1934, grad_fn=<NegBackward0>) tensor(11090.1914, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11090.1904296875
tensor(11090.1914, grad_fn=<NegBackward0>) tensor(11090.1904, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11089.0595703125
tensor(11090.1904, grad_fn=<NegBackward0>) tensor(11089.0596, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11088.8330078125
tensor(11089.0596, grad_fn=<NegBackward0>) tensor(11088.8330, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11088.3046875
tensor(11088.8330, grad_fn=<NegBackward0>) tensor(11088.3047, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11088.302734375
tensor(11088.3047, grad_fn=<NegBackward0>) tensor(11088.3027, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11088.3046875
tensor(11088.3027, grad_fn=<NegBackward0>) tensor(11088.3047, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11088.306640625
tensor(11088.3027, grad_fn=<NegBackward0>) tensor(11088.3066, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11088.30078125
tensor(11088.3027, grad_fn=<NegBackward0>) tensor(11088.3008, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11088.298828125
tensor(11088.3008, grad_fn=<NegBackward0>) tensor(11088.2988, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11088.298828125
tensor(11088.2988, grad_fn=<NegBackward0>) tensor(11088.2988, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11088.1171875
tensor(11088.2988, grad_fn=<NegBackward0>) tensor(11088.1172, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11088.10546875
tensor(11088.1172, grad_fn=<NegBackward0>) tensor(11088.1055, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11088.1044921875
tensor(11088.1055, grad_fn=<NegBackward0>) tensor(11088.1045, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11081.3017578125
tensor(11088.1045, grad_fn=<NegBackward0>) tensor(11081.3018, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11081.298828125
tensor(11081.3018, grad_fn=<NegBackward0>) tensor(11081.2988, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11081.296875
tensor(11081.2988, grad_fn=<NegBackward0>) tensor(11081.2969, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11081.2978515625
tensor(11081.2969, grad_fn=<NegBackward0>) tensor(11081.2979, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11081.2978515625
tensor(11081.2969, grad_fn=<NegBackward0>) tensor(11081.2979, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11077.8583984375
tensor(11081.2969, grad_fn=<NegBackward0>) tensor(11077.8584, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11077.8154296875
tensor(11077.8584, grad_fn=<NegBackward0>) tensor(11077.8154, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11077.8134765625
tensor(11077.8154, grad_fn=<NegBackward0>) tensor(11077.8135, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11077.81640625
tensor(11077.8135, grad_fn=<NegBackward0>) tensor(11077.8164, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11077.8115234375
tensor(11077.8135, grad_fn=<NegBackward0>) tensor(11077.8115, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11077.810546875
tensor(11077.8115, grad_fn=<NegBackward0>) tensor(11077.8105, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11077.8125
tensor(11077.8105, grad_fn=<NegBackward0>) tensor(11077.8125, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11074.6962890625
tensor(11077.8105, grad_fn=<NegBackward0>) tensor(11074.6963, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11074.69140625
tensor(11074.6963, grad_fn=<NegBackward0>) tensor(11074.6914, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11074.6923828125
tensor(11074.6914, grad_fn=<NegBackward0>) tensor(11074.6924, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11074.6923828125
tensor(11074.6914, grad_fn=<NegBackward0>) tensor(11074.6924, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11074.69140625
tensor(11074.6914, grad_fn=<NegBackward0>) tensor(11074.6914, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11074.68359375
tensor(11074.6914, grad_fn=<NegBackward0>) tensor(11074.6836, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11074.681640625
tensor(11074.6836, grad_fn=<NegBackward0>) tensor(11074.6816, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11074.25
tensor(11074.6816, grad_fn=<NegBackward0>) tensor(11074.2500, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11074.25
tensor(11074.2500, grad_fn=<NegBackward0>) tensor(11074.2500, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11074.25
tensor(11074.2500, grad_fn=<NegBackward0>) tensor(11074.2500, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11074.2724609375
tensor(11074.2500, grad_fn=<NegBackward0>) tensor(11074.2725, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11074.240234375
tensor(11074.2500, grad_fn=<NegBackward0>) tensor(11074.2402, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11072.599609375
tensor(11074.2402, grad_fn=<NegBackward0>) tensor(11072.5996, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11072.599609375
tensor(11072.5996, grad_fn=<NegBackward0>) tensor(11072.5996, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11072.5986328125
tensor(11072.5996, grad_fn=<NegBackward0>) tensor(11072.5986, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11072.599609375
tensor(11072.5986, grad_fn=<NegBackward0>) tensor(11072.5996, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11072.5966796875
tensor(11072.5986, grad_fn=<NegBackward0>) tensor(11072.5967, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11072.6005859375
tensor(11072.5967, grad_fn=<NegBackward0>) tensor(11072.6006, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11072.59765625
tensor(11072.5967, grad_fn=<NegBackward0>) tensor(11072.5977, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11072.59765625
tensor(11072.5967, grad_fn=<NegBackward0>) tensor(11072.5977, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11072.59765625
tensor(11072.5967, grad_fn=<NegBackward0>) tensor(11072.5977, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11072.59765625
tensor(11072.5967, grad_fn=<NegBackward0>) tensor(11072.5977, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.4007, 0.5993],
        [0.6863, 0.3137]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5063, 0.4937], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2494, 0.0996],
         [0.6783, 0.2442]],

        [[0.6579, 0.1004],
         [0.5017, 0.6588]],

        [[0.5054, 0.0887],
         [0.6585, 0.6453]],

        [[0.6520, 0.0835],
         [0.6099, 0.5712]],

        [[0.6891, 0.0919],
         [0.6563, 0.6337]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369954580512469
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080477173169247
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.023648575431628564
Average Adjusted Rand Index: 0.8311089257498404
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20922.52734375
inf tensor(20922.5273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11311.33984375
tensor(20922.5273, grad_fn=<NegBackward0>) tensor(11311.3398, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11309.2509765625
tensor(11311.3398, grad_fn=<NegBackward0>) tensor(11309.2510, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11303.41796875
tensor(11309.2510, grad_fn=<NegBackward0>) tensor(11303.4180, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11290.498046875
tensor(11303.4180, grad_fn=<NegBackward0>) tensor(11290.4980, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11166.1474609375
tensor(11290.4980, grad_fn=<NegBackward0>) tensor(11166.1475, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11065.36328125
tensor(11166.1475, grad_fn=<NegBackward0>) tensor(11065.3633, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11047.6162109375
tensor(11065.3633, grad_fn=<NegBackward0>) tensor(11047.6162, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11038.654296875
tensor(11047.6162, grad_fn=<NegBackward0>) tensor(11038.6543, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11038.265625
tensor(11038.6543, grad_fn=<NegBackward0>) tensor(11038.2656, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11035.810546875
tensor(11038.2656, grad_fn=<NegBackward0>) tensor(11035.8105, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11035.66015625
tensor(11035.8105, grad_fn=<NegBackward0>) tensor(11035.6602, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11035.5966796875
tensor(11035.6602, grad_fn=<NegBackward0>) tensor(11035.5967, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11035.5517578125
tensor(11035.5967, grad_fn=<NegBackward0>) tensor(11035.5518, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11035.5166015625
tensor(11035.5518, grad_fn=<NegBackward0>) tensor(11035.5166, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11035.486328125
tensor(11035.5166, grad_fn=<NegBackward0>) tensor(11035.4863, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11035.0830078125
tensor(11035.4863, grad_fn=<NegBackward0>) tensor(11035.0830, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11026.8798828125
tensor(11035.0830, grad_fn=<NegBackward0>) tensor(11026.8799, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11026.826171875
tensor(11026.8799, grad_fn=<NegBackward0>) tensor(11026.8262, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11026.7783203125
tensor(11026.8262, grad_fn=<NegBackward0>) tensor(11026.7783, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11026.744140625
tensor(11026.7783, grad_fn=<NegBackward0>) tensor(11026.7441, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11026.732421875
tensor(11026.7441, grad_fn=<NegBackward0>) tensor(11026.7324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11026.72265625
tensor(11026.7324, grad_fn=<NegBackward0>) tensor(11026.7227, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11026.7138671875
tensor(11026.7227, grad_fn=<NegBackward0>) tensor(11026.7139, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11026.7060546875
tensor(11026.7139, grad_fn=<NegBackward0>) tensor(11026.7061, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11026.7001953125
tensor(11026.7061, grad_fn=<NegBackward0>) tensor(11026.7002, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11026.6943359375
tensor(11026.7002, grad_fn=<NegBackward0>) tensor(11026.6943, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11026.6904296875
tensor(11026.6943, grad_fn=<NegBackward0>) tensor(11026.6904, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11026.685546875
tensor(11026.6904, grad_fn=<NegBackward0>) tensor(11026.6855, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11026.6787109375
tensor(11026.6855, grad_fn=<NegBackward0>) tensor(11026.6787, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11026.6748046875
tensor(11026.6787, grad_fn=<NegBackward0>) tensor(11026.6748, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11026.6689453125
tensor(11026.6748, grad_fn=<NegBackward0>) tensor(11026.6689, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11026.6611328125
tensor(11026.6689, grad_fn=<NegBackward0>) tensor(11026.6611, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11026.650390625
tensor(11026.6611, grad_fn=<NegBackward0>) tensor(11026.6504, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11026.6259765625
tensor(11026.6504, grad_fn=<NegBackward0>) tensor(11026.6260, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11026.171875
tensor(11026.6260, grad_fn=<NegBackward0>) tensor(11026.1719, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11025.7119140625
tensor(11026.1719, grad_fn=<NegBackward0>) tensor(11025.7119, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11025.6123046875
tensor(11025.7119, grad_fn=<NegBackward0>) tensor(11025.6123, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11025.5625
tensor(11025.6123, grad_fn=<NegBackward0>) tensor(11025.5625, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11025.5390625
tensor(11025.5625, grad_fn=<NegBackward0>) tensor(11025.5391, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11025.3720703125
tensor(11025.5391, grad_fn=<NegBackward0>) tensor(11025.3721, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11025.3037109375
tensor(11025.3721, grad_fn=<NegBackward0>) tensor(11025.3037, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11025.2958984375
tensor(11025.3037, grad_fn=<NegBackward0>) tensor(11025.2959, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11025.2578125
tensor(11025.2959, grad_fn=<NegBackward0>) tensor(11025.2578, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11025.25
tensor(11025.2578, grad_fn=<NegBackward0>) tensor(11025.2500, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11025.2421875
tensor(11025.2500, grad_fn=<NegBackward0>) tensor(11025.2422, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11025.228515625
tensor(11025.2422, grad_fn=<NegBackward0>) tensor(11025.2285, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11025.2236328125
tensor(11025.2285, grad_fn=<NegBackward0>) tensor(11025.2236, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11025.2236328125
tensor(11025.2236, grad_fn=<NegBackward0>) tensor(11025.2236, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11025.22265625
tensor(11025.2236, grad_fn=<NegBackward0>) tensor(11025.2227, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11025.220703125
tensor(11025.2227, grad_fn=<NegBackward0>) tensor(11025.2207, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11025.2197265625
tensor(11025.2207, grad_fn=<NegBackward0>) tensor(11025.2197, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11025.2177734375
tensor(11025.2197, grad_fn=<NegBackward0>) tensor(11025.2178, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11025.216796875
tensor(11025.2178, grad_fn=<NegBackward0>) tensor(11025.2168, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11025.208984375
tensor(11025.2168, grad_fn=<NegBackward0>) tensor(11025.2090, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11025.2080078125
tensor(11025.2090, grad_fn=<NegBackward0>) tensor(11025.2080, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11025.20703125
tensor(11025.2080, grad_fn=<NegBackward0>) tensor(11025.2070, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11025.2080078125
tensor(11025.2070, grad_fn=<NegBackward0>) tensor(11025.2080, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11025.2060546875
tensor(11025.2070, grad_fn=<NegBackward0>) tensor(11025.2061, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11025.205078125
tensor(11025.2061, grad_fn=<NegBackward0>) tensor(11025.2051, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11025.203125
tensor(11025.2051, grad_fn=<NegBackward0>) tensor(11025.2031, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11025.1982421875
tensor(11025.2031, grad_fn=<NegBackward0>) tensor(11025.1982, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11025.1962890625
tensor(11025.1982, grad_fn=<NegBackward0>) tensor(11025.1963, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11025.1962890625
tensor(11025.1963, grad_fn=<NegBackward0>) tensor(11025.1963, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11025.216796875
tensor(11025.1963, grad_fn=<NegBackward0>) tensor(11025.2168, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11025.1953125
tensor(11025.1963, grad_fn=<NegBackward0>) tensor(11025.1953, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11025.203125
tensor(11025.1953, grad_fn=<NegBackward0>) tensor(11025.2031, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11025.1943359375
tensor(11025.1953, grad_fn=<NegBackward0>) tensor(11025.1943, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11025.1982421875
tensor(11025.1943, grad_fn=<NegBackward0>) tensor(11025.1982, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11025.1943359375
tensor(11025.1943, grad_fn=<NegBackward0>) tensor(11025.1943, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11025.193359375
tensor(11025.1943, grad_fn=<NegBackward0>) tensor(11025.1934, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11025.193359375
tensor(11025.1934, grad_fn=<NegBackward0>) tensor(11025.1934, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11025.193359375
tensor(11025.1934, grad_fn=<NegBackward0>) tensor(11025.1934, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11025.19140625
tensor(11025.1934, grad_fn=<NegBackward0>) tensor(11025.1914, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11025.1923828125
tensor(11025.1914, grad_fn=<NegBackward0>) tensor(11025.1924, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11025.1943359375
tensor(11025.1914, grad_fn=<NegBackward0>) tensor(11025.1943, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11025.1923828125
tensor(11025.1914, grad_fn=<NegBackward0>) tensor(11025.1924, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11025.216796875
tensor(11025.1914, grad_fn=<NegBackward0>) tensor(11025.2168, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11025.193359375
tensor(11025.1914, grad_fn=<NegBackward0>) tensor(11025.1934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.6351, 0.3649],
        [0.2426, 0.7574]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9695, 0.0305], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1795, 0.1052],
         [0.7211, 0.3004]],

        [[0.7160, 0.1026],
         [0.5614, 0.7135]],

        [[0.6295, 0.0920],
         [0.6977, 0.5254]],

        [[0.5441, 0.0879],
         [0.6084, 0.6969]],

        [[0.5993, 0.0924],
         [0.5367, 0.5980]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6652003077572766
Average Adjusted Rand Index: 0.7831793352400034
[0.023648575431628564, 0.6652003077572766] [0.8311089257498404, 0.7831793352400034] [11072.59765625, 11025.193359375]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11355.164791528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25579.564453125
inf tensor(25579.5645, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11664.5771484375
tensor(25579.5645, grad_fn=<NegBackward0>) tensor(11664.5771, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11653.2763671875
tensor(11664.5771, grad_fn=<NegBackward0>) tensor(11653.2764, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11639.310546875
tensor(11653.2764, grad_fn=<NegBackward0>) tensor(11639.3105, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11442.3359375
tensor(11639.3105, grad_fn=<NegBackward0>) tensor(11442.3359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11417.177734375
tensor(11442.3359, grad_fn=<NegBackward0>) tensor(11417.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11412.80859375
tensor(11417.1777, grad_fn=<NegBackward0>) tensor(11412.8086, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11406.3515625
tensor(11412.8086, grad_fn=<NegBackward0>) tensor(11406.3516, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11405.2177734375
tensor(11406.3516, grad_fn=<NegBackward0>) tensor(11405.2178, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11405.142578125
tensor(11405.2178, grad_fn=<NegBackward0>) tensor(11405.1426, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11403.41015625
tensor(11405.1426, grad_fn=<NegBackward0>) tensor(11403.4102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11403.2783203125
tensor(11403.4102, grad_fn=<NegBackward0>) tensor(11403.2783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11403.23046875
tensor(11403.2783, grad_fn=<NegBackward0>) tensor(11403.2305, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11402.7822265625
tensor(11403.2305, grad_fn=<NegBackward0>) tensor(11402.7822, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11402.72265625
tensor(11402.7822, grad_fn=<NegBackward0>) tensor(11402.7227, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11402.7099609375
tensor(11402.7227, grad_fn=<NegBackward0>) tensor(11402.7100, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11402.697265625
tensor(11402.7100, grad_fn=<NegBackward0>) tensor(11402.6973, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11402.689453125
tensor(11402.6973, grad_fn=<NegBackward0>) tensor(11402.6895, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11402.6826171875
tensor(11402.6895, grad_fn=<NegBackward0>) tensor(11402.6826, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11402.673828125
tensor(11402.6826, grad_fn=<NegBackward0>) tensor(11402.6738, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11402.662109375
tensor(11402.6738, grad_fn=<NegBackward0>) tensor(11402.6621, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11402.6552734375
tensor(11402.6621, grad_fn=<NegBackward0>) tensor(11402.6553, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11396.609375
tensor(11402.6553, grad_fn=<NegBackward0>) tensor(11396.6094, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11396.603515625
tensor(11396.6094, grad_fn=<NegBackward0>) tensor(11396.6035, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11392.216796875
tensor(11396.6035, grad_fn=<NegBackward0>) tensor(11392.2168, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11392.1982421875
tensor(11392.2168, grad_fn=<NegBackward0>) tensor(11392.1982, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11392.1962890625
tensor(11392.1982, grad_fn=<NegBackward0>) tensor(11392.1963, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11392.193359375
tensor(11392.1963, grad_fn=<NegBackward0>) tensor(11392.1934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11392.19140625
tensor(11392.1934, grad_fn=<NegBackward0>) tensor(11392.1914, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11392.1904296875
tensor(11392.1914, grad_fn=<NegBackward0>) tensor(11392.1904, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11392.189453125
tensor(11392.1904, grad_fn=<NegBackward0>) tensor(11392.1895, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11392.1875
tensor(11392.1895, grad_fn=<NegBackward0>) tensor(11392.1875, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11392.0703125
tensor(11392.1875, grad_fn=<NegBackward0>) tensor(11392.0703, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11392.06640625
tensor(11392.0703, grad_fn=<NegBackward0>) tensor(11392.0664, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11392.06640625
tensor(11392.0664, grad_fn=<NegBackward0>) tensor(11392.0664, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11392.0595703125
tensor(11392.0664, grad_fn=<NegBackward0>) tensor(11392.0596, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11391.8984375
tensor(11392.0596, grad_fn=<NegBackward0>) tensor(11391.8984, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11391.896484375
tensor(11391.8984, grad_fn=<NegBackward0>) tensor(11391.8965, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11391.896484375
tensor(11391.8965, grad_fn=<NegBackward0>) tensor(11391.8965, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11391.896484375
tensor(11391.8965, grad_fn=<NegBackward0>) tensor(11391.8965, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11391.8955078125
tensor(11391.8965, grad_fn=<NegBackward0>) tensor(11391.8955, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11391.89453125
tensor(11391.8955, grad_fn=<NegBackward0>) tensor(11391.8945, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11391.8984375
tensor(11391.8945, grad_fn=<NegBackward0>) tensor(11391.8984, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11391.892578125
tensor(11391.8945, grad_fn=<NegBackward0>) tensor(11391.8926, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11383.837890625
tensor(11391.8926, grad_fn=<NegBackward0>) tensor(11383.8379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11383.8369140625
tensor(11383.8379, grad_fn=<NegBackward0>) tensor(11383.8369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11383.8359375
tensor(11383.8369, grad_fn=<NegBackward0>) tensor(11383.8359, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11383.8359375
tensor(11383.8359, grad_fn=<NegBackward0>) tensor(11383.8359, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11383.8359375
tensor(11383.8359, grad_fn=<NegBackward0>) tensor(11383.8359, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11383.8349609375
tensor(11383.8359, grad_fn=<NegBackward0>) tensor(11383.8350, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11383.8359375
tensor(11383.8350, grad_fn=<NegBackward0>) tensor(11383.8359, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11383.8349609375
tensor(11383.8350, grad_fn=<NegBackward0>) tensor(11383.8350, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11383.8369140625
tensor(11383.8350, grad_fn=<NegBackward0>) tensor(11383.8369, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11383.8349609375
tensor(11383.8350, grad_fn=<NegBackward0>) tensor(11383.8350, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11383.833984375
tensor(11383.8350, grad_fn=<NegBackward0>) tensor(11383.8340, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11383.833984375
tensor(11383.8340, grad_fn=<NegBackward0>) tensor(11383.8340, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11383.8330078125
tensor(11383.8340, grad_fn=<NegBackward0>) tensor(11383.8330, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11383.8310546875
tensor(11383.8330, grad_fn=<NegBackward0>) tensor(11383.8311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11383.7255859375
tensor(11383.8311, grad_fn=<NegBackward0>) tensor(11383.7256, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11383.7265625
tensor(11383.7256, grad_fn=<NegBackward0>) tensor(11383.7266, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11383.7265625
tensor(11383.7256, grad_fn=<NegBackward0>) tensor(11383.7266, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11383.7255859375
tensor(11383.7256, grad_fn=<NegBackward0>) tensor(11383.7256, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11383.724609375
tensor(11383.7256, grad_fn=<NegBackward0>) tensor(11383.7246, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11383.7041015625
tensor(11383.7246, grad_fn=<NegBackward0>) tensor(11383.7041, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11383.7021484375
tensor(11383.7041, grad_fn=<NegBackward0>) tensor(11383.7021, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11383.7021484375
tensor(11383.7021, grad_fn=<NegBackward0>) tensor(11383.7021, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11383.7021484375
tensor(11383.7021, grad_fn=<NegBackward0>) tensor(11383.7021, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11383.724609375
tensor(11383.7021, grad_fn=<NegBackward0>) tensor(11383.7246, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11383.701171875
tensor(11383.7021, grad_fn=<NegBackward0>) tensor(11383.7012, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11383.7041015625
tensor(11383.7012, grad_fn=<NegBackward0>) tensor(11383.7041, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11383.7001953125
tensor(11383.7012, grad_fn=<NegBackward0>) tensor(11383.7002, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11383.701171875
tensor(11383.7002, grad_fn=<NegBackward0>) tensor(11383.7012, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11383.6875
tensor(11383.7002, grad_fn=<NegBackward0>) tensor(11383.6875, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11383.6875
tensor(11383.6875, grad_fn=<NegBackward0>) tensor(11383.6875, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11383.6865234375
tensor(11383.6875, grad_fn=<NegBackward0>) tensor(11383.6865, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11383.6875
tensor(11383.6865, grad_fn=<NegBackward0>) tensor(11383.6875, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11383.6865234375
tensor(11383.6865, grad_fn=<NegBackward0>) tensor(11383.6865, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11383.685546875
tensor(11383.6865, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11383.6943359375
tensor(11383.6855, grad_fn=<NegBackward0>) tensor(11383.6943, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11383.6845703125
tensor(11383.6855, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11383.685546875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11383.685546875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11383.6904296875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6904, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11383.6845703125
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11383.6845703125
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11383.69140625
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6914, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11383.6845703125
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11383.6904296875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6904, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11383.685546875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11383.685546875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11383.685546875
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11383.68359375
tensor(11383.6846, grad_fn=<NegBackward0>) tensor(11383.6836, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11383.6904296875
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6904, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11383.6845703125
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11383.6845703125
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6846, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11383.685546875
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11383.68359375
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6836, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11383.68359375
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6836, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11383.7060546875
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.7061, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11383.685546875
tensor(11383.6836, grad_fn=<NegBackward0>) tensor(11383.6855, grad_fn=<NegBackward0>)
2
pi: tensor([[0.6243, 0.3757],
        [0.1871, 0.8129]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 4.6830e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1822, 0.1460],
         [0.6646, 0.3058]],

        [[0.5135, 0.0973],
         [0.6936, 0.6788]],

        [[0.6215, 0.1042],
         [0.6481, 0.6136]],

        [[0.5905, 0.0974],
         [0.5703, 0.6442]],

        [[0.5420, 0.1084],
         [0.5343, 0.7118]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6201678615492023
Average Adjusted Rand Index: 0.7371284774957529
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20992.6171875
inf tensor(20992.6172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11662.3173828125
tensor(20992.6172, grad_fn=<NegBackward0>) tensor(11662.3174, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11652.828125
tensor(11662.3174, grad_fn=<NegBackward0>) tensor(11652.8281, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11630.802734375
tensor(11652.8281, grad_fn=<NegBackward0>) tensor(11630.8027, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11482.69140625
tensor(11630.8027, grad_fn=<NegBackward0>) tensor(11482.6914, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11465.9365234375
tensor(11482.6914, grad_fn=<NegBackward0>) tensor(11465.9365, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11458.4580078125
tensor(11465.9365, grad_fn=<NegBackward0>) tensor(11458.4580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11457.8232421875
tensor(11458.4580, grad_fn=<NegBackward0>) tensor(11457.8232, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11455.283203125
tensor(11457.8232, grad_fn=<NegBackward0>) tensor(11455.2832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11455.0517578125
tensor(11455.2832, grad_fn=<NegBackward0>) tensor(11455.0518, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11453.419921875
tensor(11455.0518, grad_fn=<NegBackward0>) tensor(11453.4199, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11453.37109375
tensor(11453.4199, grad_fn=<NegBackward0>) tensor(11453.3711, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11453.3310546875
tensor(11453.3711, grad_fn=<NegBackward0>) tensor(11453.3311, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11453.2490234375
tensor(11453.3311, grad_fn=<NegBackward0>) tensor(11453.2490, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11452.349609375
tensor(11453.2490, grad_fn=<NegBackward0>) tensor(11452.3496, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11450.5244140625
tensor(11452.3496, grad_fn=<NegBackward0>) tensor(11450.5244, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11450.0341796875
tensor(11450.5244, grad_fn=<NegBackward0>) tensor(11450.0342, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11448.9912109375
tensor(11450.0342, grad_fn=<NegBackward0>) tensor(11448.9912, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11448.4658203125
tensor(11448.9912, grad_fn=<NegBackward0>) tensor(11448.4658, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11448.2724609375
tensor(11448.4658, grad_fn=<NegBackward0>) tensor(11448.2725, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11448.2373046875
tensor(11448.2725, grad_fn=<NegBackward0>) tensor(11448.2373, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11448.232421875
tensor(11448.2373, grad_fn=<NegBackward0>) tensor(11448.2324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11448.2275390625
tensor(11448.2324, grad_fn=<NegBackward0>) tensor(11448.2275, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11448.2255859375
tensor(11448.2275, grad_fn=<NegBackward0>) tensor(11448.2256, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11448.2236328125
tensor(11448.2256, grad_fn=<NegBackward0>) tensor(11448.2236, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11448.2197265625
tensor(11448.2236, grad_fn=<NegBackward0>) tensor(11448.2197, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11448.2177734375
tensor(11448.2197, grad_fn=<NegBackward0>) tensor(11448.2178, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11448.21484375
tensor(11448.2178, grad_fn=<NegBackward0>) tensor(11448.2148, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11448.2109375
tensor(11448.2148, grad_fn=<NegBackward0>) tensor(11448.2109, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11448.1982421875
tensor(11448.2109, grad_fn=<NegBackward0>) tensor(11448.1982, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11448.146484375
tensor(11448.1982, grad_fn=<NegBackward0>) tensor(11448.1465, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11448.1455078125
tensor(11448.1465, grad_fn=<NegBackward0>) tensor(11448.1455, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11448.0673828125
tensor(11448.1455, grad_fn=<NegBackward0>) tensor(11448.0674, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11448.06640625
tensor(11448.0674, grad_fn=<NegBackward0>) tensor(11448.0664, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11448.06640625
tensor(11448.0664, grad_fn=<NegBackward0>) tensor(11448.0664, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11448.0654296875
tensor(11448.0664, grad_fn=<NegBackward0>) tensor(11448.0654, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11448.0625
tensor(11448.0654, grad_fn=<NegBackward0>) tensor(11448.0625, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11448.05078125
tensor(11448.0625, grad_fn=<NegBackward0>) tensor(11448.0508, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11448.048828125
tensor(11448.0508, grad_fn=<NegBackward0>) tensor(11448.0488, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11448.0458984375
tensor(11448.0488, grad_fn=<NegBackward0>) tensor(11448.0459, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11448.0439453125
tensor(11448.0459, grad_fn=<NegBackward0>) tensor(11448.0439, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11448.041015625
tensor(11448.0439, grad_fn=<NegBackward0>) tensor(11448.0410, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11448.0419921875
tensor(11448.0410, grad_fn=<NegBackward0>) tensor(11448.0420, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11448.041015625
tensor(11448.0410, grad_fn=<NegBackward0>) tensor(11448.0410, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11448.03515625
tensor(11448.0410, grad_fn=<NegBackward0>) tensor(11448.0352, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11448.0341796875
tensor(11448.0352, grad_fn=<NegBackward0>) tensor(11448.0342, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11448.033203125
tensor(11448.0342, grad_fn=<NegBackward0>) tensor(11448.0332, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11448.0341796875
tensor(11448.0332, grad_fn=<NegBackward0>) tensor(11448.0342, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11448.0341796875
tensor(11448.0332, grad_fn=<NegBackward0>) tensor(11448.0342, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11448.033203125
tensor(11448.0332, grad_fn=<NegBackward0>) tensor(11448.0332, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11448.03125
tensor(11448.0332, grad_fn=<NegBackward0>) tensor(11448.0312, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11448.03125
tensor(11448.0312, grad_fn=<NegBackward0>) tensor(11448.0312, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11448.025390625
tensor(11448.0312, grad_fn=<NegBackward0>) tensor(11448.0254, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11448.0263671875
tensor(11448.0254, grad_fn=<NegBackward0>) tensor(11448.0264, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11448.0263671875
tensor(11448.0254, grad_fn=<NegBackward0>) tensor(11448.0264, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11448.0234375
tensor(11448.0254, grad_fn=<NegBackward0>) tensor(11448.0234, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11448.0244140625
tensor(11448.0234, grad_fn=<NegBackward0>) tensor(11448.0244, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11448.0234375
tensor(11448.0234, grad_fn=<NegBackward0>) tensor(11448.0234, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11448.0224609375
tensor(11448.0234, grad_fn=<NegBackward0>) tensor(11448.0225, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11448.009765625
tensor(11448.0225, grad_fn=<NegBackward0>) tensor(11448.0098, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11448.0146484375
tensor(11448.0098, grad_fn=<NegBackward0>) tensor(11448.0146, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11448.009765625
tensor(11448.0098, grad_fn=<NegBackward0>) tensor(11448.0098, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11448.013671875
tensor(11448.0098, grad_fn=<NegBackward0>) tensor(11448.0137, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11448.00390625
tensor(11448.0098, grad_fn=<NegBackward0>) tensor(11448.0039, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11448.0029296875
tensor(11448.0039, grad_fn=<NegBackward0>) tensor(11448.0029, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11448.00390625
tensor(11448.0029, grad_fn=<NegBackward0>) tensor(11448.0039, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11448.001953125
tensor(11448.0029, grad_fn=<NegBackward0>) tensor(11448.0020, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11447.90625
tensor(11448.0020, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11447.90625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11447.9140625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9141, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11447.90625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11447.90625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11447.9189453125
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9189, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11447.9072265625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11447.90625
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11447.9052734375
tensor(11447.9062, grad_fn=<NegBackward0>) tensor(11447.9053, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11447.90625
tensor(11447.9053, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11447.90625
tensor(11447.9053, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11447.90625
tensor(11447.9053, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11447.90625
tensor(11447.9053, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11447.90625
tensor(11447.9053, grad_fn=<NegBackward0>) tensor(11447.9062, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.6809, 0.3191],
        [0.4079, 0.5921]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0864, 0.9136], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2989, 0.1054],
         [0.5007, 0.1971]],

        [[0.6390, 0.0954],
         [0.5204, 0.5376]],

        [[0.5148, 0.1041],
         [0.7115, 0.6331]],

        [[0.5338, 0.0969],
         [0.7171, 0.5451]],

        [[0.5640, 0.1084],
         [0.5530, 0.5488]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 81
Adjusted Rand Index: 0.378909408604649
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.20265433193892274
Average Adjusted Rand Index: 0.6267290732659752
[0.6201678615492023, 0.20265433193892274] [0.7371284774957529, 0.6267290732659752] [11383.6884765625, 11447.90625]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11336.625811123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21300.546875
inf tensor(21300.5469, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11699.19140625
tensor(21300.5469, grad_fn=<NegBackward0>) tensor(11699.1914, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11694.447265625
tensor(11699.1914, grad_fn=<NegBackward0>) tensor(11694.4473, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11688.8701171875
tensor(11694.4473, grad_fn=<NegBackward0>) tensor(11688.8701, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11645.9140625
tensor(11688.8701, grad_fn=<NegBackward0>) tensor(11645.9141, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11580.9912109375
tensor(11645.9141, grad_fn=<NegBackward0>) tensor(11580.9912, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11529.5341796875
tensor(11580.9912, grad_fn=<NegBackward0>) tensor(11529.5342, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11505.3505859375
tensor(11529.5342, grad_fn=<NegBackward0>) tensor(11505.3506, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11498.7236328125
tensor(11505.3506, grad_fn=<NegBackward0>) tensor(11498.7236, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11480.1083984375
tensor(11498.7236, grad_fn=<NegBackward0>) tensor(11480.1084, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11451.9521484375
tensor(11480.1084, grad_fn=<NegBackward0>) tensor(11451.9521, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11443.4326171875
tensor(11451.9521, grad_fn=<NegBackward0>) tensor(11443.4326, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11439.5234375
tensor(11443.4326, grad_fn=<NegBackward0>) tensor(11439.5234, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11438.7802734375
tensor(11439.5234, grad_fn=<NegBackward0>) tensor(11438.7803, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11438.427734375
tensor(11438.7803, grad_fn=<NegBackward0>) tensor(11438.4277, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11438.3515625
tensor(11438.4277, grad_fn=<NegBackward0>) tensor(11438.3516, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11431.626953125
tensor(11438.3516, grad_fn=<NegBackward0>) tensor(11431.6270, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11431.294921875
tensor(11431.6270, grad_fn=<NegBackward0>) tensor(11431.2949, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11431.216796875
tensor(11431.2949, grad_fn=<NegBackward0>) tensor(11431.2168, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11431.1689453125
tensor(11431.2168, grad_fn=<NegBackward0>) tensor(11431.1689, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11431.125
tensor(11431.1689, grad_fn=<NegBackward0>) tensor(11431.1250, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11431.083984375
tensor(11431.1250, grad_fn=<NegBackward0>) tensor(11431.0840, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11430.8466796875
tensor(11431.0840, grad_fn=<NegBackward0>) tensor(11430.8467, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11430.818359375
tensor(11430.8467, grad_fn=<NegBackward0>) tensor(11430.8184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11430.783203125
tensor(11430.8184, grad_fn=<NegBackward0>) tensor(11430.7832, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11426.59765625
tensor(11430.7832, grad_fn=<NegBackward0>) tensor(11426.5977, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11426.583984375
tensor(11426.5977, grad_fn=<NegBackward0>) tensor(11426.5840, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11426.5458984375
tensor(11426.5840, grad_fn=<NegBackward0>) tensor(11426.5459, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11425.7109375
tensor(11426.5459, grad_fn=<NegBackward0>) tensor(11425.7109, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11425.70703125
tensor(11425.7109, grad_fn=<NegBackward0>) tensor(11425.7070, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11425.703125
tensor(11425.7070, grad_fn=<NegBackward0>) tensor(11425.7031, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11425.69921875
tensor(11425.7031, grad_fn=<NegBackward0>) tensor(11425.6992, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11425.6962890625
tensor(11425.6992, grad_fn=<NegBackward0>) tensor(11425.6963, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11425.693359375
tensor(11425.6963, grad_fn=<NegBackward0>) tensor(11425.6934, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11425.693359375
tensor(11425.6934, grad_fn=<NegBackward0>) tensor(11425.6934, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11425.69140625
tensor(11425.6934, grad_fn=<NegBackward0>) tensor(11425.6914, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11425.689453125
tensor(11425.6914, grad_fn=<NegBackward0>) tensor(11425.6895, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11425.6884765625
tensor(11425.6895, grad_fn=<NegBackward0>) tensor(11425.6885, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11425.6865234375
tensor(11425.6885, grad_fn=<NegBackward0>) tensor(11425.6865, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11425.6865234375
tensor(11425.6865, grad_fn=<NegBackward0>) tensor(11425.6865, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11425.68359375
tensor(11425.6865, grad_fn=<NegBackward0>) tensor(11425.6836, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11425.6806640625
tensor(11425.6836, grad_fn=<NegBackward0>) tensor(11425.6807, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11425.67578125
tensor(11425.6807, grad_fn=<NegBackward0>) tensor(11425.6758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11425.6728515625
tensor(11425.6758, grad_fn=<NegBackward0>) tensor(11425.6729, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11425.671875
tensor(11425.6729, grad_fn=<NegBackward0>) tensor(11425.6719, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11425.66796875
tensor(11425.6719, grad_fn=<NegBackward0>) tensor(11425.6680, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11425.6669921875
tensor(11425.6680, grad_fn=<NegBackward0>) tensor(11425.6670, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11425.666015625
tensor(11425.6670, grad_fn=<NegBackward0>) tensor(11425.6660, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11425.6728515625
tensor(11425.6660, grad_fn=<NegBackward0>) tensor(11425.6729, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11425.662109375
tensor(11425.6660, grad_fn=<NegBackward0>) tensor(11425.6621, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11425.662109375
tensor(11425.6621, grad_fn=<NegBackward0>) tensor(11425.6621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11425.6611328125
tensor(11425.6621, grad_fn=<NegBackward0>) tensor(11425.6611, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11425.6591796875
tensor(11425.6611, grad_fn=<NegBackward0>) tensor(11425.6592, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11425.6591796875
tensor(11425.6592, grad_fn=<NegBackward0>) tensor(11425.6592, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11425.6552734375
tensor(11425.6592, grad_fn=<NegBackward0>) tensor(11425.6553, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11425.650390625
tensor(11425.6553, grad_fn=<NegBackward0>) tensor(11425.6504, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11425.650390625
tensor(11425.6504, grad_fn=<NegBackward0>) tensor(11425.6504, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11425.650390625
tensor(11425.6504, grad_fn=<NegBackward0>) tensor(11425.6504, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11425.650390625
tensor(11425.6504, grad_fn=<NegBackward0>) tensor(11425.6504, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11425.6494140625
tensor(11425.6504, grad_fn=<NegBackward0>) tensor(11425.6494, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11425.6484375
tensor(11425.6494, grad_fn=<NegBackward0>) tensor(11425.6484, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11425.646484375
tensor(11425.6484, grad_fn=<NegBackward0>) tensor(11425.6465, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11425.6455078125
tensor(11425.6465, grad_fn=<NegBackward0>) tensor(11425.6455, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11425.634765625
tensor(11425.6455, grad_fn=<NegBackward0>) tensor(11425.6348, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11425.4951171875
tensor(11425.6348, grad_fn=<NegBackward0>) tensor(11425.4951, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11425.4892578125
tensor(11425.4951, grad_fn=<NegBackward0>) tensor(11425.4893, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11425.4697265625
tensor(11425.4893, grad_fn=<NegBackward0>) tensor(11425.4697, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11425.46875
tensor(11425.4697, grad_fn=<NegBackward0>) tensor(11425.4688, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11425.4130859375
tensor(11425.4688, grad_fn=<NegBackward0>) tensor(11425.4131, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11425.41015625
tensor(11425.4131, grad_fn=<NegBackward0>) tensor(11425.4102, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11425.40625
tensor(11425.4102, grad_fn=<NegBackward0>) tensor(11425.4062, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11425.4052734375
tensor(11425.4062, grad_fn=<NegBackward0>) tensor(11425.4053, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11425.40625
tensor(11425.4053, grad_fn=<NegBackward0>) tensor(11425.4062, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11425.201171875
tensor(11425.4053, grad_fn=<NegBackward0>) tensor(11425.2012, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11425.19921875
tensor(11425.2012, grad_fn=<NegBackward0>) tensor(11425.1992, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11425.068359375
tensor(11425.1992, grad_fn=<NegBackward0>) tensor(11425.0684, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11425.0498046875
tensor(11425.0684, grad_fn=<NegBackward0>) tensor(11425.0498, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11425.048828125
tensor(11425.0498, grad_fn=<NegBackward0>) tensor(11425.0488, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11425.046875
tensor(11425.0488, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11425.0478515625
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0479, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11425.046875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11425.044921875
tensor(11425.0469, grad_fn=<NegBackward0>) tensor(11425.0449, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11425.0458984375
tensor(11425.0449, grad_fn=<NegBackward0>) tensor(11425.0459, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11425.0546875
tensor(11425.0449, grad_fn=<NegBackward0>) tensor(11425.0547, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11425.046875
tensor(11425.0449, grad_fn=<NegBackward0>) tensor(11425.0469, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11425.162109375
tensor(11425.0449, grad_fn=<NegBackward0>) tensor(11425.1621, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11425.05078125
tensor(11425.0449, grad_fn=<NegBackward0>) tensor(11425.0508, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.6368, 0.3632],
        [0.3720, 0.6280]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6026, 0.3974], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2541, 0.1094],
         [0.5984, 0.2713]],

        [[0.7119, 0.0917],
         [0.5064, 0.6621]],

        [[0.5268, 0.0916],
         [0.6993, 0.7177]],

        [[0.5038, 0.0985],
         [0.6125, 0.5475]],

        [[0.7236, 0.0972],
         [0.5955, 0.6861]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721463199647421
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
Global Adjusted Rand Index: 0.038072097543678865
Average Adjusted Rand Index: 0.8915339725193322
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22299.314453125
inf tensor(22299.3145, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11699.8896484375
tensor(22299.3145, grad_fn=<NegBackward0>) tensor(11699.8896, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11690.8212890625
tensor(11699.8896, grad_fn=<NegBackward0>) tensor(11690.8213, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11646.427734375
tensor(11690.8213, grad_fn=<NegBackward0>) tensor(11646.4277, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11392.17578125
tensor(11646.4277, grad_fn=<NegBackward0>) tensor(11392.1758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11349.50390625
tensor(11392.1758, grad_fn=<NegBackward0>) tensor(11349.5039, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11337.3056640625
tensor(11349.5039, grad_fn=<NegBackward0>) tensor(11337.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11334.0478515625
tensor(11337.3057, grad_fn=<NegBackward0>) tensor(11334.0479, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11333.8251953125
tensor(11334.0479, grad_fn=<NegBackward0>) tensor(11333.8252, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11333.7578125
tensor(11333.8252, grad_fn=<NegBackward0>) tensor(11333.7578, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11333.71875
tensor(11333.7578, grad_fn=<NegBackward0>) tensor(11333.7188, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11333.693359375
tensor(11333.7188, grad_fn=<NegBackward0>) tensor(11333.6934, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11333.673828125
tensor(11333.6934, grad_fn=<NegBackward0>) tensor(11333.6738, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11333.65625
tensor(11333.6738, grad_fn=<NegBackward0>) tensor(11333.6562, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11333.6435546875
tensor(11333.6562, grad_fn=<NegBackward0>) tensor(11333.6436, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11333.6328125
tensor(11333.6436, grad_fn=<NegBackward0>) tensor(11333.6328, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11333.62109375
tensor(11333.6328, grad_fn=<NegBackward0>) tensor(11333.6211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11328.0205078125
tensor(11333.6211, grad_fn=<NegBackward0>) tensor(11328.0205, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11327.9755859375
tensor(11328.0205, grad_fn=<NegBackward0>) tensor(11327.9756, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11327.970703125
tensor(11327.9756, grad_fn=<NegBackward0>) tensor(11327.9707, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11327.966796875
tensor(11327.9707, grad_fn=<NegBackward0>) tensor(11327.9668, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11327.9638671875
tensor(11327.9668, grad_fn=<NegBackward0>) tensor(11327.9639, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11327.962890625
tensor(11327.9639, grad_fn=<NegBackward0>) tensor(11327.9629, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11327.9580078125
tensor(11327.9629, grad_fn=<NegBackward0>) tensor(11327.9580, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11327.955078125
tensor(11327.9580, grad_fn=<NegBackward0>) tensor(11327.9551, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11327.953125
tensor(11327.9551, grad_fn=<NegBackward0>) tensor(11327.9531, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11327.9521484375
tensor(11327.9531, grad_fn=<NegBackward0>) tensor(11327.9521, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11327.94921875
tensor(11327.9521, grad_fn=<NegBackward0>) tensor(11327.9492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11327.94921875
tensor(11327.9492, grad_fn=<NegBackward0>) tensor(11327.9492, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11327.947265625
tensor(11327.9492, grad_fn=<NegBackward0>) tensor(11327.9473, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11327.947265625
tensor(11327.9473, grad_fn=<NegBackward0>) tensor(11327.9473, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11327.9453125
tensor(11327.9473, grad_fn=<NegBackward0>) tensor(11327.9453, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11327.943359375
tensor(11327.9453, grad_fn=<NegBackward0>) tensor(11327.9434, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11327.943359375
tensor(11327.9434, grad_fn=<NegBackward0>) tensor(11327.9434, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11327.943359375
tensor(11327.9434, grad_fn=<NegBackward0>) tensor(11327.9434, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11327.943359375
tensor(11327.9434, grad_fn=<NegBackward0>) tensor(11327.9434, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11327.9423828125
tensor(11327.9434, grad_fn=<NegBackward0>) tensor(11327.9424, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11327.9404296875
tensor(11327.9424, grad_fn=<NegBackward0>) tensor(11327.9404, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11327.939453125
tensor(11327.9404, grad_fn=<NegBackward0>) tensor(11327.9395, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11327.9404296875
tensor(11327.9395, grad_fn=<NegBackward0>) tensor(11327.9404, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11327.9384765625
tensor(11327.9395, grad_fn=<NegBackward0>) tensor(11327.9385, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11327.9384765625
tensor(11327.9385, grad_fn=<NegBackward0>) tensor(11327.9385, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11327.9384765625
tensor(11327.9385, grad_fn=<NegBackward0>) tensor(11327.9385, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11327.9375
tensor(11327.9385, grad_fn=<NegBackward0>) tensor(11327.9375, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11327.9404296875
tensor(11327.9375, grad_fn=<NegBackward0>) tensor(11327.9404, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11327.9375
tensor(11327.9375, grad_fn=<NegBackward0>) tensor(11327.9375, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11327.94140625
tensor(11327.9375, grad_fn=<NegBackward0>) tensor(11327.9414, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11327.9365234375
tensor(11327.9375, grad_fn=<NegBackward0>) tensor(11327.9365, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11327.935546875
tensor(11327.9365, grad_fn=<NegBackward0>) tensor(11327.9355, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11327.935546875
tensor(11327.9355, grad_fn=<NegBackward0>) tensor(11327.9355, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11327.9384765625
tensor(11327.9355, grad_fn=<NegBackward0>) tensor(11327.9385, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11327.935546875
tensor(11327.9355, grad_fn=<NegBackward0>) tensor(11327.9355, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11327.9345703125
tensor(11327.9355, grad_fn=<NegBackward0>) tensor(11327.9346, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11327.927734375
tensor(11327.9346, grad_fn=<NegBackward0>) tensor(11327.9277, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11327.927734375
tensor(11327.9277, grad_fn=<NegBackward0>) tensor(11327.9277, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11324.486328125
tensor(11327.9277, grad_fn=<NegBackward0>) tensor(11324.4863, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11324.4853515625
tensor(11324.4863, grad_fn=<NegBackward0>) tensor(11324.4854, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11324.4853515625
tensor(11324.4854, grad_fn=<NegBackward0>) tensor(11324.4854, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11321.1474609375
tensor(11324.4854, grad_fn=<NegBackward0>) tensor(11321.1475, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11321.1376953125
tensor(11321.1475, grad_fn=<NegBackward0>) tensor(11321.1377, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11321.13671875
tensor(11321.1377, grad_fn=<NegBackward0>) tensor(11321.1367, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11321.13671875
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1367, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11321.13671875
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1367, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11321.13671875
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1367, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11321.140625
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1406, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11321.13671875
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1367, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11321.1337890625
tensor(11321.1367, grad_fn=<NegBackward0>) tensor(11321.1338, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11321.134765625
tensor(11321.1338, grad_fn=<NegBackward0>) tensor(11321.1348, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11321.134765625
tensor(11321.1338, grad_fn=<NegBackward0>) tensor(11321.1348, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11321.134765625
tensor(11321.1338, grad_fn=<NegBackward0>) tensor(11321.1348, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11321.1630859375
tensor(11321.1338, grad_fn=<NegBackward0>) tensor(11321.1631, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11321.1357421875
tensor(11321.1338, grad_fn=<NegBackward0>) tensor(11321.1357, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7696, 0.2304],
        [0.2493, 0.7507]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5281, 0.4719], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3081, 0.1141],
         [0.5387, 0.2027]],

        [[0.7023, 0.0940],
         [0.6756, 0.6912]],

        [[0.6907, 0.0923],
         [0.5815, 0.6757]],

        [[0.6506, 0.0992],
         [0.5099, 0.5352]],

        [[0.6924, 0.0997],
         [0.6208, 0.5737]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599252625159036
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9681923487976571
Average Adjusted Rand Index: 0.968145521699601
[0.038072097543678865, 0.9681923487976571] [0.8915339725193322, 0.968145521699601] [11425.05078125, 11321.1357421875]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11249.628843430504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22161.826171875
inf tensor(22161.8262, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11604.541015625
tensor(22161.8262, grad_fn=<NegBackward0>) tensor(11604.5410, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11604.0283203125
tensor(11604.5410, grad_fn=<NegBackward0>) tensor(11604.0283, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11600.9580078125
tensor(11604.0283, grad_fn=<NegBackward0>) tensor(11600.9580, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11595.5712890625
tensor(11600.9580, grad_fn=<NegBackward0>) tensor(11595.5713, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11522.4208984375
tensor(11595.5713, grad_fn=<NegBackward0>) tensor(11522.4209, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11346.3740234375
tensor(11522.4209, grad_fn=<NegBackward0>) tensor(11346.3740, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11303.8623046875
tensor(11346.3740, grad_fn=<NegBackward0>) tensor(11303.8623, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11284.5
tensor(11303.8623, grad_fn=<NegBackward0>) tensor(11284.5000, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11260.1240234375
tensor(11284.5000, grad_fn=<NegBackward0>) tensor(11260.1240, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11241.6455078125
tensor(11260.1240, grad_fn=<NegBackward0>) tensor(11241.6455, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11238.3203125
tensor(11241.6455, grad_fn=<NegBackward0>) tensor(11238.3203, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11238.140625
tensor(11238.3203, grad_fn=<NegBackward0>) tensor(11238.1406, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11238.046875
tensor(11238.1406, grad_fn=<NegBackward0>) tensor(11238.0469, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11238.0126953125
tensor(11238.0469, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11237.9892578125
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11237.9893, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11237.9697265625
tensor(11237.9893, grad_fn=<NegBackward0>) tensor(11237.9697, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11237.94921875
tensor(11237.9697, grad_fn=<NegBackward0>) tensor(11237.9492, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11237.9384765625
tensor(11237.9492, grad_fn=<NegBackward0>) tensor(11237.9385, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11237.9306640625
tensor(11237.9385, grad_fn=<NegBackward0>) tensor(11237.9307, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11237.923828125
tensor(11237.9307, grad_fn=<NegBackward0>) tensor(11237.9238, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11237.91796875
tensor(11237.9238, grad_fn=<NegBackward0>) tensor(11237.9180, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11237.9140625
tensor(11237.9180, grad_fn=<NegBackward0>) tensor(11237.9141, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11237.90625
tensor(11237.9141, grad_fn=<NegBackward0>) tensor(11237.9062, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11237.89453125
tensor(11237.9062, grad_fn=<NegBackward0>) tensor(11237.8945, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11237.83984375
tensor(11237.8945, grad_fn=<NegBackward0>) tensor(11237.8398, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11237.833984375
tensor(11237.8398, grad_fn=<NegBackward0>) tensor(11237.8340, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11237.828125
tensor(11237.8340, grad_fn=<NegBackward0>) tensor(11237.8281, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11237.8193359375
tensor(11237.8281, grad_fn=<NegBackward0>) tensor(11237.8193, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11237.7998046875
tensor(11237.8193, grad_fn=<NegBackward0>) tensor(11237.7998, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11237.794921875
tensor(11237.7998, grad_fn=<NegBackward0>) tensor(11237.7949, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11237.1669921875
tensor(11237.7949, grad_fn=<NegBackward0>) tensor(11237.1670, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11237.1669921875
tensor(11237.1670, grad_fn=<NegBackward0>) tensor(11237.1670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11237.1650390625
tensor(11237.1670, grad_fn=<NegBackward0>) tensor(11237.1650, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11237.166015625
tensor(11237.1650, grad_fn=<NegBackward0>) tensor(11237.1660, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11237.1630859375
tensor(11237.1650, grad_fn=<NegBackward0>) tensor(11237.1631, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11237.162109375
tensor(11237.1631, grad_fn=<NegBackward0>) tensor(11237.1621, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11237.1611328125
tensor(11237.1621, grad_fn=<NegBackward0>) tensor(11237.1611, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11237.16015625
tensor(11237.1611, grad_fn=<NegBackward0>) tensor(11237.1602, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11237.1591796875
tensor(11237.1602, grad_fn=<NegBackward0>) tensor(11237.1592, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11237.15625
tensor(11237.1592, grad_fn=<NegBackward0>) tensor(11237.1562, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11237.15625
tensor(11237.1562, grad_fn=<NegBackward0>) tensor(11237.1562, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11237.158203125
tensor(11237.1562, grad_fn=<NegBackward0>) tensor(11237.1582, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11237.1552734375
tensor(11237.1562, grad_fn=<NegBackward0>) tensor(11237.1553, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11237.1552734375
tensor(11237.1553, grad_fn=<NegBackward0>) tensor(11237.1553, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11237.1611328125
tensor(11237.1553, grad_fn=<NegBackward0>) tensor(11237.1611, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11237.1533203125
tensor(11237.1553, grad_fn=<NegBackward0>) tensor(11237.1533, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11237.1533203125
tensor(11237.1533, grad_fn=<NegBackward0>) tensor(11237.1533, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11237.15625
tensor(11237.1533, grad_fn=<NegBackward0>) tensor(11237.1562, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11237.154296875
tensor(11237.1533, grad_fn=<NegBackward0>) tensor(11237.1543, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11237.150390625
tensor(11237.1533, grad_fn=<NegBackward0>) tensor(11237.1504, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11237.1513671875
tensor(11237.1504, grad_fn=<NegBackward0>) tensor(11237.1514, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11237.1494140625
tensor(11237.1504, grad_fn=<NegBackward0>) tensor(11237.1494, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11237.1494140625
tensor(11237.1494, grad_fn=<NegBackward0>) tensor(11237.1494, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11237.1533203125
tensor(11237.1494, grad_fn=<NegBackward0>) tensor(11237.1533, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11237.154296875
tensor(11237.1494, grad_fn=<NegBackward0>) tensor(11237.1543, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11237.1484375
tensor(11237.1494, grad_fn=<NegBackward0>) tensor(11237.1484, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11237.146484375
tensor(11237.1484, grad_fn=<NegBackward0>) tensor(11237.1465, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11235.6630859375
tensor(11237.1465, grad_fn=<NegBackward0>) tensor(11235.6631, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11235.541015625
tensor(11235.6631, grad_fn=<NegBackward0>) tensor(11235.5410, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11235.5419921875
tensor(11235.5410, grad_fn=<NegBackward0>) tensor(11235.5420, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11235.541015625
tensor(11235.5410, grad_fn=<NegBackward0>) tensor(11235.5410, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11235.5390625
tensor(11235.5410, grad_fn=<NegBackward0>) tensor(11235.5391, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11235.4775390625
tensor(11235.5391, grad_fn=<NegBackward0>) tensor(11235.4775, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11235.474609375
tensor(11235.4775, grad_fn=<NegBackward0>) tensor(11235.4746, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11235.4755859375
tensor(11235.4746, grad_fn=<NegBackward0>) tensor(11235.4756, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11235.4765625
tensor(11235.4746, grad_fn=<NegBackward0>) tensor(11235.4766, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11235.4736328125
tensor(11235.4746, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11235.478515625
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4785, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11235.474609375
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4746, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11235.4853515625
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4854, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11235.486328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4863, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11235.4736328125
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4736, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11235.41796875
tensor(11235.4736, grad_fn=<NegBackward0>) tensor(11235.4180, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11235.4345703125
tensor(11235.4180, grad_fn=<NegBackward0>) tensor(11235.4346, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11235.412109375
tensor(11235.4180, grad_fn=<NegBackward0>) tensor(11235.4121, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11235.4130859375
tensor(11235.4121, grad_fn=<NegBackward0>) tensor(11235.4131, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11235.4140625
tensor(11235.4121, grad_fn=<NegBackward0>) tensor(11235.4141, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11235.4560546875
tensor(11235.4121, grad_fn=<NegBackward0>) tensor(11235.4561, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11235.4140625
tensor(11235.4121, grad_fn=<NegBackward0>) tensor(11235.4141, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11235.4462890625
tensor(11235.4121, grad_fn=<NegBackward0>) tensor(11235.4463, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7847, 0.2153],
        [0.2367, 0.7633]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5047, 0.4953], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3086, 0.0982],
         [0.5559, 0.2025]],

        [[0.5519, 0.0961],
         [0.6047, 0.6320]],

        [[0.5256, 0.1042],
         [0.5332, 0.6639]],

        [[0.6995, 0.1014],
         [0.6349, 0.6010]],

        [[0.7058, 0.0973],
         [0.7173, 0.6945]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809432979789
Average Adjusted Rand Index: 0.9523195119335511
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21924.001953125
inf tensor(21924.0020, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11602.57421875
tensor(21924.0020, grad_fn=<NegBackward0>) tensor(11602.5742, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11600.7392578125
tensor(11602.5742, grad_fn=<NegBackward0>) tensor(11600.7393, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11600.0107421875
tensor(11600.7393, grad_fn=<NegBackward0>) tensor(11600.0107, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11598.9833984375
tensor(11600.0107, grad_fn=<NegBackward0>) tensor(11598.9834, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11598.576171875
tensor(11598.9834, grad_fn=<NegBackward0>) tensor(11598.5762, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11598.3447265625
tensor(11598.5762, grad_fn=<NegBackward0>) tensor(11598.3447, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11598.1826171875
tensor(11598.3447, grad_fn=<NegBackward0>) tensor(11598.1826, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11598.064453125
tensor(11598.1826, grad_fn=<NegBackward0>) tensor(11598.0645, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11597.97265625
tensor(11598.0645, grad_fn=<NegBackward0>) tensor(11597.9727, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11597.890625
tensor(11597.9727, grad_fn=<NegBackward0>) tensor(11597.8906, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11597.7841796875
tensor(11597.8906, grad_fn=<NegBackward0>) tensor(11597.7842, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11597.5478515625
tensor(11597.7842, grad_fn=<NegBackward0>) tensor(11597.5479, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11541.2041015625
tensor(11597.5479, grad_fn=<NegBackward0>) tensor(11541.2041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11540.5
tensor(11541.2041, grad_fn=<NegBackward0>) tensor(11540.5000, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11540.203125
tensor(11540.5000, grad_fn=<NegBackward0>) tensor(11540.2031, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11540.1376953125
tensor(11540.2031, grad_fn=<NegBackward0>) tensor(11540.1377, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11540.044921875
tensor(11540.1377, grad_fn=<NegBackward0>) tensor(11540.0449, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11539.9638671875
tensor(11540.0449, grad_fn=<NegBackward0>) tensor(11539.9639, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11539.94921875
tensor(11539.9639, grad_fn=<NegBackward0>) tensor(11539.9492, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11539.9443359375
tensor(11539.9492, grad_fn=<NegBackward0>) tensor(11539.9443, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11539.9423828125
tensor(11539.9443, grad_fn=<NegBackward0>) tensor(11539.9424, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11539.94140625
tensor(11539.9424, grad_fn=<NegBackward0>) tensor(11539.9414, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11539.939453125
tensor(11539.9414, grad_fn=<NegBackward0>) tensor(11539.9395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11539.9384765625
tensor(11539.9395, grad_fn=<NegBackward0>) tensor(11539.9385, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11539.9365234375
tensor(11539.9385, grad_fn=<NegBackward0>) tensor(11539.9365, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11539.9228515625
tensor(11539.9365, grad_fn=<NegBackward0>) tensor(11539.9229, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11539.9228515625
tensor(11539.9229, grad_fn=<NegBackward0>) tensor(11539.9229, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11539.921875
tensor(11539.9229, grad_fn=<NegBackward0>) tensor(11539.9219, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11539.921875
tensor(11539.9219, grad_fn=<NegBackward0>) tensor(11539.9219, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11539.9208984375
tensor(11539.9219, grad_fn=<NegBackward0>) tensor(11539.9209, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11539.921875
tensor(11539.9209, grad_fn=<NegBackward0>) tensor(11539.9219, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11539.9208984375
tensor(11539.9209, grad_fn=<NegBackward0>) tensor(11539.9209, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11539.919921875
tensor(11539.9209, grad_fn=<NegBackward0>) tensor(11539.9199, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11539.9189453125
tensor(11539.9199, grad_fn=<NegBackward0>) tensor(11539.9189, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11539.919921875
tensor(11539.9189, grad_fn=<NegBackward0>) tensor(11539.9199, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11539.91796875
tensor(11539.9189, grad_fn=<NegBackward0>) tensor(11539.9180, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11539.9189453125
tensor(11539.9180, grad_fn=<NegBackward0>) tensor(11539.9189, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11539.9189453125
tensor(11539.9180, grad_fn=<NegBackward0>) tensor(11539.9189, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11539.9189453125
tensor(11539.9180, grad_fn=<NegBackward0>) tensor(11539.9189, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -11539.91796875
tensor(11539.9180, grad_fn=<NegBackward0>) tensor(11539.9180, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11539.9169921875
tensor(11539.9180, grad_fn=<NegBackward0>) tensor(11539.9170, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11539.9169921875
tensor(11539.9170, grad_fn=<NegBackward0>) tensor(11539.9170, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11539.9169921875
tensor(11539.9170, grad_fn=<NegBackward0>) tensor(11539.9170, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11539.9169921875
tensor(11539.9170, grad_fn=<NegBackward0>) tensor(11539.9170, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11539.9140625
tensor(11539.9170, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11539.9150390625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9150, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11539.9140625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11539.9140625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11539.9140625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11539.9140625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11539.9140625
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11539.9130859375
tensor(11539.9141, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11539.9130859375
tensor(11539.9131, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11539.9140625
tensor(11539.9131, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11539.912109375
tensor(11539.9131, grad_fn=<NegBackward0>) tensor(11539.9121, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11539.9140625
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11539.9130859375
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11539.912109375
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9121, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11539.9130859375
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11539.916015625
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9160, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11539.9140625
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9141, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11539.9130859375
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11539.9130859375
tensor(11539.9121, grad_fn=<NegBackward0>) tensor(11539.9131, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.0218, 0.9782],
        [0.0101, 0.9899]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5126, 0.4874], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3071, 0.0983],
         [0.6007, 0.1791]],

        [[0.5896, 0.3469],
         [0.5399, 0.6507]],

        [[0.6058, 0.2353],
         [0.5383, 0.6117]],

        [[0.5185, 0.0929],
         [0.5725, 0.7098]],

        [[0.5002, 0.2265],
         [0.5616, 0.6684]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.028240729971189024
Average Adjusted Rand Index: 0.2
[0.9524809432979789, 0.028240729971189024] [0.9523195119335511, 0.2] [11235.4462890625, 11539.9130859375]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11059.077044570342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22475.6328125
inf tensor(22475.6328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11308.380859375
tensor(22475.6328, grad_fn=<NegBackward0>) tensor(11308.3809, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11307.7744140625
tensor(11308.3809, grad_fn=<NegBackward0>) tensor(11307.7744, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11307.6318359375
tensor(11307.7744, grad_fn=<NegBackward0>) tensor(11307.6318, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11307.5322265625
tensor(11307.6318, grad_fn=<NegBackward0>) tensor(11307.5322, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11307.423828125
tensor(11307.5322, grad_fn=<NegBackward0>) tensor(11307.4238, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11307.263671875
tensor(11307.4238, grad_fn=<NegBackward0>) tensor(11307.2637, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11306.8486328125
tensor(11307.2637, grad_fn=<NegBackward0>) tensor(11306.8486, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11306.111328125
tensor(11306.8486, grad_fn=<NegBackward0>) tensor(11306.1113, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11303.669921875
tensor(11306.1113, grad_fn=<NegBackward0>) tensor(11303.6699, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11180.3818359375
tensor(11303.6699, grad_fn=<NegBackward0>) tensor(11180.3818, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11049.8095703125
tensor(11180.3818, grad_fn=<NegBackward0>) tensor(11049.8096, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11042.3447265625
tensor(11049.8096, grad_fn=<NegBackward0>) tensor(11042.3447, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11041.9697265625
tensor(11042.3447, grad_fn=<NegBackward0>) tensor(11041.9697, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11039.0625
tensor(11041.9697, grad_fn=<NegBackward0>) tensor(11039.0625, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11039.0283203125
tensor(11039.0625, grad_fn=<NegBackward0>) tensor(11039.0283, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11039.0029296875
tensor(11039.0283, grad_fn=<NegBackward0>) tensor(11039.0029, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11038.96875
tensor(11039.0029, grad_fn=<NegBackward0>) tensor(11038.9688, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11038.9521484375
tensor(11038.9688, grad_fn=<NegBackward0>) tensor(11038.9521, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11038.9228515625
tensor(11038.9521, grad_fn=<NegBackward0>) tensor(11038.9229, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11038.9150390625
tensor(11038.9229, grad_fn=<NegBackward0>) tensor(11038.9150, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11038.9091796875
tensor(11038.9150, grad_fn=<NegBackward0>) tensor(11038.9092, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11038.873046875
tensor(11038.9092, grad_fn=<NegBackward0>) tensor(11038.8730, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11038.8076171875
tensor(11038.8730, grad_fn=<NegBackward0>) tensor(11038.8076, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11034.892578125
tensor(11038.8076, grad_fn=<NegBackward0>) tensor(11034.8926, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11034.8671875
tensor(11034.8926, grad_fn=<NegBackward0>) tensor(11034.8672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11034.8212890625
tensor(11034.8672, grad_fn=<NegBackward0>) tensor(11034.8213, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11034.771484375
tensor(11034.8213, grad_fn=<NegBackward0>) tensor(11034.7715, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11034.76953125
tensor(11034.7715, grad_fn=<NegBackward0>) tensor(11034.7695, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11034.7666015625
tensor(11034.7695, grad_fn=<NegBackward0>) tensor(11034.7666, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11034.7646484375
tensor(11034.7666, grad_fn=<NegBackward0>) tensor(11034.7646, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11034.7607421875
tensor(11034.7646, grad_fn=<NegBackward0>) tensor(11034.7607, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11034.7529296875
tensor(11034.7607, grad_fn=<NegBackward0>) tensor(11034.7529, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11034.7412109375
tensor(11034.7529, grad_fn=<NegBackward0>) tensor(11034.7412, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11034.73828125
tensor(11034.7412, grad_fn=<NegBackward0>) tensor(11034.7383, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11034.736328125
tensor(11034.7383, grad_fn=<NegBackward0>) tensor(11034.7363, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11034.7353515625
tensor(11034.7363, grad_fn=<NegBackward0>) tensor(11034.7354, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11034.7353515625
tensor(11034.7354, grad_fn=<NegBackward0>) tensor(11034.7354, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11034.734375
tensor(11034.7354, grad_fn=<NegBackward0>) tensor(11034.7344, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11034.7333984375
tensor(11034.7344, grad_fn=<NegBackward0>) tensor(11034.7334, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11034.732421875
tensor(11034.7334, grad_fn=<NegBackward0>) tensor(11034.7324, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11034.732421875
tensor(11034.7324, grad_fn=<NegBackward0>) tensor(11034.7324, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11034.7314453125
tensor(11034.7324, grad_fn=<NegBackward0>) tensor(11034.7314, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11034.7314453125
tensor(11034.7314, grad_fn=<NegBackward0>) tensor(11034.7314, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11034.73046875
tensor(11034.7314, grad_fn=<NegBackward0>) tensor(11034.7305, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11034.7294921875
tensor(11034.7305, grad_fn=<NegBackward0>) tensor(11034.7295, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11034.73046875
tensor(11034.7295, grad_fn=<NegBackward0>) tensor(11034.7305, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11034.728515625
tensor(11034.7295, grad_fn=<NegBackward0>) tensor(11034.7285, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11034.7294921875
tensor(11034.7285, grad_fn=<NegBackward0>) tensor(11034.7295, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11034.728515625
tensor(11034.7285, grad_fn=<NegBackward0>) tensor(11034.7285, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11034.7294921875
tensor(11034.7285, grad_fn=<NegBackward0>) tensor(11034.7295, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11034.7275390625
tensor(11034.7285, grad_fn=<NegBackward0>) tensor(11034.7275, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11034.6787109375
tensor(11034.7275, grad_fn=<NegBackward0>) tensor(11034.6787, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11034.576171875
tensor(11034.6787, grad_fn=<NegBackward0>) tensor(11034.5762, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11034.568359375
tensor(11034.5762, grad_fn=<NegBackward0>) tensor(11034.5684, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11034.5673828125
tensor(11034.5684, grad_fn=<NegBackward0>) tensor(11034.5674, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11034.56640625
tensor(11034.5674, grad_fn=<NegBackward0>) tensor(11034.5664, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11034.5673828125
tensor(11034.5664, grad_fn=<NegBackward0>) tensor(11034.5674, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11034.5673828125
tensor(11034.5664, grad_fn=<NegBackward0>) tensor(11034.5674, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11034.5673828125
tensor(11034.5664, grad_fn=<NegBackward0>) tensor(11034.5674, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11034.5654296875
tensor(11034.5664, grad_fn=<NegBackward0>) tensor(11034.5654, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11034.5654296875
tensor(11034.5654, grad_fn=<NegBackward0>) tensor(11034.5654, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11034.55078125
tensor(11034.5654, grad_fn=<NegBackward0>) tensor(11034.5508, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11034.5439453125
tensor(11034.5508, grad_fn=<NegBackward0>) tensor(11034.5439, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11034.4921875
tensor(11034.5439, grad_fn=<NegBackward0>) tensor(11034.4922, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11034.490234375
tensor(11034.4922, grad_fn=<NegBackward0>) tensor(11034.4902, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11034.484375
tensor(11034.4902, grad_fn=<NegBackward0>) tensor(11034.4844, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11034.4833984375
tensor(11034.4844, grad_fn=<NegBackward0>) tensor(11034.4834, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11034.484375
tensor(11034.4834, grad_fn=<NegBackward0>) tensor(11034.4844, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11034.4833984375
tensor(11034.4834, grad_fn=<NegBackward0>) tensor(11034.4834, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11034.482421875
tensor(11034.4834, grad_fn=<NegBackward0>) tensor(11034.4824, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11034.482421875
tensor(11034.4824, grad_fn=<NegBackward0>) tensor(11034.4824, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11034.48046875
tensor(11034.4824, grad_fn=<NegBackward0>) tensor(11034.4805, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11034.4814453125
tensor(11034.4805, grad_fn=<NegBackward0>) tensor(11034.4814, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11034.4814453125
tensor(11034.4805, grad_fn=<NegBackward0>) tensor(11034.4814, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11034.482421875
tensor(11034.4805, grad_fn=<NegBackward0>) tensor(11034.4824, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11034.4814453125
tensor(11034.4805, grad_fn=<NegBackward0>) tensor(11034.4814, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11034.482421875
tensor(11034.4805, grad_fn=<NegBackward0>) tensor(11034.4824, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.8279, 0.1721],
        [0.2661, 0.7339]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4995, 0.5005], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.0960],
         [0.6492, 0.3083]],

        [[0.5477, 0.1043],
         [0.6140, 0.5344]],

        [[0.7075, 0.0957],
         [0.5074, 0.6477]],

        [[0.6705, 0.1007],
         [0.6929, 0.5840]],

        [[0.7137, 0.1081],
         [0.5683, 0.7111]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8823523358261945
Global Adjusted Rand Index: 0.9446666893006243
Average Adjusted Rand Index: 0.9446294654179498
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21465.669921875
inf tensor(21465.6699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11307.9443359375
tensor(21465.6699, grad_fn=<NegBackward0>) tensor(11307.9443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11307.6396484375
tensor(11307.9443, grad_fn=<NegBackward0>) tensor(11307.6396, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11307.5341796875
tensor(11307.6396, grad_fn=<NegBackward0>) tensor(11307.5342, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11307.4462890625
tensor(11307.5342, grad_fn=<NegBackward0>) tensor(11307.4463, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11307.3388671875
tensor(11307.4463, grad_fn=<NegBackward0>) tensor(11307.3389, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11307.201171875
tensor(11307.3389, grad_fn=<NegBackward0>) tensor(11307.2012, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11307.0224609375
tensor(11307.2012, grad_fn=<NegBackward0>) tensor(11307.0225, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11306.619140625
tensor(11307.0225, grad_fn=<NegBackward0>) tensor(11306.6191, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11305.248046875
tensor(11306.6191, grad_fn=<NegBackward0>) tensor(11305.2480, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11295.568359375
tensor(11305.2480, grad_fn=<NegBackward0>) tensor(11295.5684, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11077.80078125
tensor(11295.5684, grad_fn=<NegBackward0>) tensor(11077.8008, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11038.7451171875
tensor(11077.8008, grad_fn=<NegBackward0>) tensor(11038.7451, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11038.408203125
tensor(11038.7451, grad_fn=<NegBackward0>) tensor(11038.4082, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11038.30859375
tensor(11038.4082, grad_fn=<NegBackward0>) tensor(11038.3086, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11038.197265625
tensor(11038.3086, grad_fn=<NegBackward0>) tensor(11038.1973, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11038.1103515625
tensor(11038.1973, grad_fn=<NegBackward0>) tensor(11038.1104, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11038.087890625
tensor(11038.1104, grad_fn=<NegBackward0>) tensor(11038.0879, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11038.072265625
tensor(11038.0879, grad_fn=<NegBackward0>) tensor(11038.0723, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11038.0595703125
tensor(11038.0723, grad_fn=<NegBackward0>) tensor(11038.0596, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11038.0400390625
tensor(11038.0596, grad_fn=<NegBackward0>) tensor(11038.0400, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11038.0322265625
tensor(11038.0400, grad_fn=<NegBackward0>) tensor(11038.0322, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11038.0263671875
tensor(11038.0322, grad_fn=<NegBackward0>) tensor(11038.0264, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11038.0224609375
tensor(11038.0264, grad_fn=<NegBackward0>) tensor(11038.0225, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11038.0185546875
tensor(11038.0225, grad_fn=<NegBackward0>) tensor(11038.0186, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11038.01171875
tensor(11038.0186, grad_fn=<NegBackward0>) tensor(11038.0117, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11038.0029296875
tensor(11038.0117, grad_fn=<NegBackward0>) tensor(11038.0029, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11037.994140625
tensor(11038.0029, grad_fn=<NegBackward0>) tensor(11037.9941, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11037.95703125
tensor(11037.9941, grad_fn=<NegBackward0>) tensor(11037.9570, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11037.939453125
tensor(11037.9570, grad_fn=<NegBackward0>) tensor(11037.9395, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11037.92578125
tensor(11037.9395, grad_fn=<NegBackward0>) tensor(11037.9258, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11037.923828125
tensor(11037.9258, grad_fn=<NegBackward0>) tensor(11037.9238, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11037.9228515625
tensor(11037.9238, grad_fn=<NegBackward0>) tensor(11037.9229, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11037.919921875
tensor(11037.9229, grad_fn=<NegBackward0>) tensor(11037.9199, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11037.90625
tensor(11037.9199, grad_fn=<NegBackward0>) tensor(11037.9062, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11037.9052734375
tensor(11037.9062, grad_fn=<NegBackward0>) tensor(11037.9053, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11037.904296875
tensor(11037.9053, grad_fn=<NegBackward0>) tensor(11037.9043, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11037.9033203125
tensor(11037.9043, grad_fn=<NegBackward0>) tensor(11037.9033, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11037.90234375
tensor(11037.9033, grad_fn=<NegBackward0>) tensor(11037.9023, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11037.90234375
tensor(11037.9023, grad_fn=<NegBackward0>) tensor(11037.9023, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11037.9013671875
tensor(11037.9023, grad_fn=<NegBackward0>) tensor(11037.9014, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11037.8994140625
tensor(11037.9014, grad_fn=<NegBackward0>) tensor(11037.8994, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11037.8974609375
tensor(11037.8994, grad_fn=<NegBackward0>) tensor(11037.8975, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11037.90234375
tensor(11037.8975, grad_fn=<NegBackward0>) tensor(11037.9023, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11037.896484375
tensor(11037.8975, grad_fn=<NegBackward0>) tensor(11037.8965, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11037.896484375
tensor(11037.8965, grad_fn=<NegBackward0>) tensor(11037.8965, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11037.8955078125
tensor(11037.8965, grad_fn=<NegBackward0>) tensor(11037.8955, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11037.896484375
tensor(11037.8955, grad_fn=<NegBackward0>) tensor(11037.8965, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11037.9072265625
tensor(11037.8955, grad_fn=<NegBackward0>) tensor(11037.9072, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11037.89453125
tensor(11037.8955, grad_fn=<NegBackward0>) tensor(11037.8945, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11037.8955078125
tensor(11037.8945, grad_fn=<NegBackward0>) tensor(11037.8955, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11037.8935546875
tensor(11037.8945, grad_fn=<NegBackward0>) tensor(11037.8936, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11037.892578125
tensor(11037.8936, grad_fn=<NegBackward0>) tensor(11037.8926, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11037.8896484375
tensor(11037.8926, grad_fn=<NegBackward0>) tensor(11037.8896, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11037.888671875
tensor(11037.8896, grad_fn=<NegBackward0>) tensor(11037.8887, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11037.884765625
tensor(11037.8887, grad_fn=<NegBackward0>) tensor(11037.8848, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11037.8837890625
tensor(11037.8848, grad_fn=<NegBackward0>) tensor(11037.8838, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11037.884765625
tensor(11037.8838, grad_fn=<NegBackward0>) tensor(11037.8848, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11037.8837890625
tensor(11037.8838, grad_fn=<NegBackward0>) tensor(11037.8838, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11037.884765625
tensor(11037.8838, grad_fn=<NegBackward0>) tensor(11037.8848, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11037.8818359375
tensor(11037.8838, grad_fn=<NegBackward0>) tensor(11037.8818, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11037.88671875
tensor(11037.8818, grad_fn=<NegBackward0>) tensor(11037.8867, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11037.884765625
tensor(11037.8818, grad_fn=<NegBackward0>) tensor(11037.8848, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11037.8828125
tensor(11037.8818, grad_fn=<NegBackward0>) tensor(11037.8828, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11037.8828125
tensor(11037.8818, grad_fn=<NegBackward0>) tensor(11037.8828, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11037.8935546875
tensor(11037.8818, grad_fn=<NegBackward0>) tensor(11037.8936, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.8285, 0.1715],
        [0.2657, 0.7343]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1954, 0.0961],
         [0.5382, 0.3085]],

        [[0.5808, 0.1060],
         [0.6753, 0.5403]],

        [[0.6193, 0.0957],
         [0.6680, 0.5093]],

        [[0.6803, 0.1008],
         [0.5508, 0.5633]],

        [[0.6903, 0.1081],
         [0.6629, 0.5857]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8823523358261945
Global Adjusted Rand Index: 0.9291457838364122
Average Adjusted Rand Index: 0.9291136452075165
[0.9446666893006243, 0.9291457838364122] [0.9446294654179498, 0.9291136452075165] [11034.482421875, 11037.8935546875]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11164.243790471859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20066.634765625
inf tensor(20066.6348, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11422.599609375
tensor(20066.6348, grad_fn=<NegBackward0>) tensor(11422.5996, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11419.177734375
tensor(11422.5996, grad_fn=<NegBackward0>) tensor(11419.1777, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11417.6064453125
tensor(11419.1777, grad_fn=<NegBackward0>) tensor(11417.6064, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11415.46484375
tensor(11417.6064, grad_fn=<NegBackward0>) tensor(11415.4648, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11413.4775390625
tensor(11415.4648, grad_fn=<NegBackward0>) tensor(11413.4775, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11357.720703125
tensor(11413.4775, grad_fn=<NegBackward0>) tensor(11357.7207, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11356.2265625
tensor(11357.7207, grad_fn=<NegBackward0>) tensor(11356.2266, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11356.1416015625
tensor(11356.2266, grad_fn=<NegBackward0>) tensor(11356.1416, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11356.095703125
tensor(11356.1416, grad_fn=<NegBackward0>) tensor(11356.0957, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11356.0693359375
tensor(11356.0957, grad_fn=<NegBackward0>) tensor(11356.0693, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11356.0498046875
tensor(11356.0693, grad_fn=<NegBackward0>) tensor(11356.0498, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11356.0341796875
tensor(11356.0498, grad_fn=<NegBackward0>) tensor(11356.0342, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11356.025390625
tensor(11356.0342, grad_fn=<NegBackward0>) tensor(11356.0254, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11356.0166015625
tensor(11356.0254, grad_fn=<NegBackward0>) tensor(11356.0166, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11356.01171875
tensor(11356.0166, grad_fn=<NegBackward0>) tensor(11356.0117, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11356.005859375
tensor(11356.0117, grad_fn=<NegBackward0>) tensor(11356.0059, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11356.0009765625
tensor(11356.0059, grad_fn=<NegBackward0>) tensor(11356.0010, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11355.998046875
tensor(11356.0010, grad_fn=<NegBackward0>) tensor(11355.9980, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11355.99609375
tensor(11355.9980, grad_fn=<NegBackward0>) tensor(11355.9961, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11355.9931640625
tensor(11355.9961, grad_fn=<NegBackward0>) tensor(11355.9932, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11355.9912109375
tensor(11355.9932, grad_fn=<NegBackward0>) tensor(11355.9912, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11355.9892578125
tensor(11355.9912, grad_fn=<NegBackward0>) tensor(11355.9893, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11355.9873046875
tensor(11355.9893, grad_fn=<NegBackward0>) tensor(11355.9873, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11355.986328125
tensor(11355.9873, grad_fn=<NegBackward0>) tensor(11355.9863, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11355.984375
tensor(11355.9863, grad_fn=<NegBackward0>) tensor(11355.9844, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11355.984375
tensor(11355.9844, grad_fn=<NegBackward0>) tensor(11355.9844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11355.984375
tensor(11355.9844, grad_fn=<NegBackward0>) tensor(11355.9844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11355.9814453125
tensor(11355.9844, grad_fn=<NegBackward0>) tensor(11355.9814, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11355.9833984375
tensor(11355.9814, grad_fn=<NegBackward0>) tensor(11355.9834, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11355.9814453125
tensor(11355.9814, grad_fn=<NegBackward0>) tensor(11355.9814, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11355.9814453125
tensor(11355.9814, grad_fn=<NegBackward0>) tensor(11355.9814, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11355.98046875
tensor(11355.9814, grad_fn=<NegBackward0>) tensor(11355.9805, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11355.98046875
tensor(11355.9805, grad_fn=<NegBackward0>) tensor(11355.9805, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11355.98046875
tensor(11355.9805, grad_fn=<NegBackward0>) tensor(11355.9805, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11355.9775390625
tensor(11355.9805, grad_fn=<NegBackward0>) tensor(11355.9775, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11355.978515625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9785, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11355.978515625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9785, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11355.978515625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9785, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -11355.9775390625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9775, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11355.978515625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9785, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11355.9765625
tensor(11355.9775, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11355.9775390625
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9775, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11355.978515625
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9785, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11355.9814453125
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9814, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11355.9765625
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11355.9765625
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11355.9765625
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11355.9755859375
tensor(11355.9766, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11355.9755859375
tensor(11355.9756, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11355.9755859375
tensor(11355.9756, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11355.98046875
tensor(11355.9756, grad_fn=<NegBackward0>) tensor(11355.9805, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11355.9755859375
tensor(11355.9756, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11355.9736328125
tensor(11355.9756, grad_fn=<NegBackward0>) tensor(11355.9736, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11355.9765625
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11355.9736328125
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9736, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11355.9765625
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9766, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11355.974609375
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9746, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11355.974609375
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9746, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11355.9755859375
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11355.9755859375
tensor(11355.9736, grad_fn=<NegBackward0>) tensor(11355.9756, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9920, 0.0080],
        [0.9405, 0.0595]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4851, 0.5149], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1720, 0.0959],
         [0.5912, 0.3027]],

        [[0.5561, 0.2754],
         [0.6914, 0.5585]],

        [[0.5083, 0.2156],
         [0.7309, 0.7132]],

        [[0.7235, 0.3826],
         [0.5463, 0.5774]],

        [[0.7034, 0.2094],
         [0.6382, 0.6588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0374170371882659
Average Adjusted Rand Index: 0.19172400740504958
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23152.998046875
inf tensor(23152.9980, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11422.1171875
tensor(23152.9980, grad_fn=<NegBackward0>) tensor(11422.1172, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11420.580078125
tensor(11422.1172, grad_fn=<NegBackward0>) tensor(11420.5801, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11419.4951171875
tensor(11420.5801, grad_fn=<NegBackward0>) tensor(11419.4951, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11417.6728515625
tensor(11419.4951, grad_fn=<NegBackward0>) tensor(11417.6729, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11416.830078125
tensor(11417.6729, grad_fn=<NegBackward0>) tensor(11416.8301, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11416.44140625
tensor(11416.8301, grad_fn=<NegBackward0>) tensor(11416.4414, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11416.2265625
tensor(11416.4414, grad_fn=<NegBackward0>) tensor(11416.2266, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11416.05078125
tensor(11416.2266, grad_fn=<NegBackward0>) tensor(11416.0508, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11414.96484375
tensor(11416.0508, grad_fn=<NegBackward0>) tensor(11414.9648, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11401.396484375
tensor(11414.9648, grad_fn=<NegBackward0>) tensor(11401.3965, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11355.9921875
tensor(11401.3965, grad_fn=<NegBackward0>) tensor(11355.9922, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11355.765625
tensor(11355.9922, grad_fn=<NegBackward0>) tensor(11355.7656, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11355.7353515625
tensor(11355.7656, grad_fn=<NegBackward0>) tensor(11355.7354, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11355.7216796875
tensor(11355.7354, grad_fn=<NegBackward0>) tensor(11355.7217, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11355.7119140625
tensor(11355.7217, grad_fn=<NegBackward0>) tensor(11355.7119, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11355.703125
tensor(11355.7119, grad_fn=<NegBackward0>) tensor(11355.7031, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11355.6953125
tensor(11355.7031, grad_fn=<NegBackward0>) tensor(11355.6953, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11355.689453125
tensor(11355.6953, grad_fn=<NegBackward0>) tensor(11355.6895, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11355.685546875
tensor(11355.6895, grad_fn=<NegBackward0>) tensor(11355.6855, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11355.6796875
tensor(11355.6855, grad_fn=<NegBackward0>) tensor(11355.6797, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11355.6767578125
tensor(11355.6797, grad_fn=<NegBackward0>) tensor(11355.6768, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11355.673828125
tensor(11355.6768, grad_fn=<NegBackward0>) tensor(11355.6738, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11355.6708984375
tensor(11355.6738, grad_fn=<NegBackward0>) tensor(11355.6709, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11355.6689453125
tensor(11355.6709, grad_fn=<NegBackward0>) tensor(11355.6689, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11355.66796875
tensor(11355.6689, grad_fn=<NegBackward0>) tensor(11355.6680, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11355.6669921875
tensor(11355.6680, grad_fn=<NegBackward0>) tensor(11355.6670, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11355.6650390625
tensor(11355.6670, grad_fn=<NegBackward0>) tensor(11355.6650, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11355.6640625
tensor(11355.6650, grad_fn=<NegBackward0>) tensor(11355.6641, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11355.6630859375
tensor(11355.6641, grad_fn=<NegBackward0>) tensor(11355.6631, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11355.6630859375
tensor(11355.6631, grad_fn=<NegBackward0>) tensor(11355.6631, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11355.662109375
tensor(11355.6631, grad_fn=<NegBackward0>) tensor(11355.6621, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11355.66015625
tensor(11355.6621, grad_fn=<NegBackward0>) tensor(11355.6602, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11355.66015625
tensor(11355.6602, grad_fn=<NegBackward0>) tensor(11355.6602, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11355.6591796875
tensor(11355.6602, grad_fn=<NegBackward0>) tensor(11355.6592, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11355.658203125
tensor(11355.6592, grad_fn=<NegBackward0>) tensor(11355.6582, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11355.6572265625
tensor(11355.6582, grad_fn=<NegBackward0>) tensor(11355.6572, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11355.6572265625
tensor(11355.6572, grad_fn=<NegBackward0>) tensor(11355.6572, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11355.6572265625
tensor(11355.6572, grad_fn=<NegBackward0>) tensor(11355.6572, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11355.65625
tensor(11355.6572, grad_fn=<NegBackward0>) tensor(11355.6562, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11355.65625
tensor(11355.6562, grad_fn=<NegBackward0>) tensor(11355.6562, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11355.658203125
tensor(11355.6562, grad_fn=<NegBackward0>) tensor(11355.6582, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11355.6552734375
tensor(11355.6562, grad_fn=<NegBackward0>) tensor(11355.6553, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11355.6552734375
tensor(11355.6553, grad_fn=<NegBackward0>) tensor(11355.6553, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11355.65625
tensor(11355.6553, grad_fn=<NegBackward0>) tensor(11355.6562, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11355.65625
tensor(11355.6553, grad_fn=<NegBackward0>) tensor(11355.6562, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11355.66015625
tensor(11355.6553, grad_fn=<NegBackward0>) tensor(11355.6602, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11355.654296875
tensor(11355.6553, grad_fn=<NegBackward0>) tensor(11355.6543, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11355.654296875
tensor(11355.6543, grad_fn=<NegBackward0>) tensor(11355.6543, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11355.6533203125
tensor(11355.6543, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11355.6533203125
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11355.65625
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6562, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11355.6533203125
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11355.654296875
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6543, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11355.6591796875
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6592, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11355.65234375
tensor(11355.6533, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11355.654296875
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6543, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11355.65234375
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11355.6533203125
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11355.6513671875
tensor(11355.6523, grad_fn=<NegBackward0>) tensor(11355.6514, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11355.654296875
tensor(11355.6514, grad_fn=<NegBackward0>) tensor(11355.6543, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11355.65234375
tensor(11355.6514, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11355.65234375
tensor(11355.6514, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11355.6533203125
tensor(11355.6514, grad_fn=<NegBackward0>) tensor(11355.6533, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11355.65234375
tensor(11355.6514, grad_fn=<NegBackward0>) tensor(11355.6523, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.9893, 0.0107],
        [0.9502, 0.0498]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4847, 0.5153], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1729, 0.0959],
         [0.5203, 0.3024]],

        [[0.5640, 0.2800],
         [0.5747, 0.5009]],

        [[0.5026, 0.0927],
         [0.6815, 0.6534]],

        [[0.6461, 0.3822],
         [0.7199, 0.6469]],

        [[0.6680, 0.2069],
         [0.6035, 0.5473]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.035873055255670395
Average Adjusted Rand Index: 0.1917085987851246
[0.0374170371882659, 0.035873055255670395] [0.19172400740504958, 0.1917085987851246] [11355.9755859375, 11355.65234375]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11302.118749424195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22902.5859375
inf tensor(22902.5859, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11612.8017578125
tensor(22902.5859, grad_fn=<NegBackward0>) tensor(11612.8018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11603.951171875
tensor(11612.8018, grad_fn=<NegBackward0>) tensor(11603.9512, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11563.5771484375
tensor(11603.9512, grad_fn=<NegBackward0>) tensor(11563.5771, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11481.7421875
tensor(11563.5771, grad_fn=<NegBackward0>) tensor(11481.7422, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11310.1494140625
tensor(11481.7422, grad_fn=<NegBackward0>) tensor(11310.1494, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11292.201171875
tensor(11310.1494, grad_fn=<NegBackward0>) tensor(11292.2012, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11284.1357421875
tensor(11292.2012, grad_fn=<NegBackward0>) tensor(11284.1357, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11277.1005859375
tensor(11284.1357, grad_fn=<NegBackward0>) tensor(11277.1006, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11276.8271484375
tensor(11277.1006, grad_fn=<NegBackward0>) tensor(11276.8271, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11276.7080078125
tensor(11276.8271, grad_fn=<NegBackward0>) tensor(11276.7080, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11276.6396484375
tensor(11276.7080, grad_fn=<NegBackward0>) tensor(11276.6396, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11276.5908203125
tensor(11276.6396, grad_fn=<NegBackward0>) tensor(11276.5908, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11276.5517578125
tensor(11276.5908, grad_fn=<NegBackward0>) tensor(11276.5518, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11276.5048828125
tensor(11276.5518, grad_fn=<NegBackward0>) tensor(11276.5049, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11276.390625
tensor(11276.5049, grad_fn=<NegBackward0>) tensor(11276.3906, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11276.1826171875
tensor(11276.3906, grad_fn=<NegBackward0>) tensor(11276.1826, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11276.166015625
tensor(11276.1826, grad_fn=<NegBackward0>) tensor(11276.1660, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11276.150390625
tensor(11276.1660, grad_fn=<NegBackward0>) tensor(11276.1504, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11276.1025390625
tensor(11276.1504, grad_fn=<NegBackward0>) tensor(11276.1025, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11275.8046875
tensor(11276.1025, grad_fn=<NegBackward0>) tensor(11275.8047, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11275.7978515625
tensor(11275.8047, grad_fn=<NegBackward0>) tensor(11275.7979, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11275.7919921875
tensor(11275.7979, grad_fn=<NegBackward0>) tensor(11275.7920, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11275.7861328125
tensor(11275.7920, grad_fn=<NegBackward0>) tensor(11275.7861, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11275.7802734375
tensor(11275.7861, grad_fn=<NegBackward0>) tensor(11275.7803, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11275.7744140625
tensor(11275.7803, grad_fn=<NegBackward0>) tensor(11275.7744, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11275.73046875
tensor(11275.7744, grad_fn=<NegBackward0>) tensor(11275.7305, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11275.72265625
tensor(11275.7305, grad_fn=<NegBackward0>) tensor(11275.7227, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11275.7177734375
tensor(11275.7227, grad_fn=<NegBackward0>) tensor(11275.7178, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11275.7138671875
tensor(11275.7178, grad_fn=<NegBackward0>) tensor(11275.7139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11275.7119140625
tensor(11275.7139, grad_fn=<NegBackward0>) tensor(11275.7119, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11275.7109375
tensor(11275.7119, grad_fn=<NegBackward0>) tensor(11275.7109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11275.7080078125
tensor(11275.7109, grad_fn=<NegBackward0>) tensor(11275.7080, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11275.705078125
tensor(11275.7080, grad_fn=<NegBackward0>) tensor(11275.7051, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11275.703125
tensor(11275.7051, grad_fn=<NegBackward0>) tensor(11275.7031, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11275.701171875
tensor(11275.7031, grad_fn=<NegBackward0>) tensor(11275.7012, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11275.701171875
tensor(11275.7012, grad_fn=<NegBackward0>) tensor(11275.7012, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11275.7001953125
tensor(11275.7012, grad_fn=<NegBackward0>) tensor(11275.7002, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11275.6982421875
tensor(11275.7002, grad_fn=<NegBackward0>) tensor(11275.6982, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11275.7021484375
tensor(11275.6982, grad_fn=<NegBackward0>) tensor(11275.7021, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11275.6962890625
tensor(11275.6982, grad_fn=<NegBackward0>) tensor(11275.6963, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11275.68359375
tensor(11275.6963, grad_fn=<NegBackward0>) tensor(11275.6836, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11275.677734375
tensor(11275.6836, grad_fn=<NegBackward0>) tensor(11275.6777, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11275.67578125
tensor(11275.6777, grad_fn=<NegBackward0>) tensor(11275.6758, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11275.67578125
tensor(11275.6758, grad_fn=<NegBackward0>) tensor(11275.6758, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11275.67578125
tensor(11275.6758, grad_fn=<NegBackward0>) tensor(11275.6758, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11275.6767578125
tensor(11275.6758, grad_fn=<NegBackward0>) tensor(11275.6768, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11275.6748046875
tensor(11275.6758, grad_fn=<NegBackward0>) tensor(11275.6748, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11275.673828125
tensor(11275.6748, grad_fn=<NegBackward0>) tensor(11275.6738, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11275.6728515625
tensor(11275.6738, grad_fn=<NegBackward0>) tensor(11275.6729, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11275.671875
tensor(11275.6729, grad_fn=<NegBackward0>) tensor(11275.6719, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11275.67578125
tensor(11275.6719, grad_fn=<NegBackward0>) tensor(11275.6758, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11275.6708984375
tensor(11275.6719, grad_fn=<NegBackward0>) tensor(11275.6709, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11275.6201171875
tensor(11275.6709, grad_fn=<NegBackward0>) tensor(11275.6201, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11275.619140625
tensor(11275.6201, grad_fn=<NegBackward0>) tensor(11275.6191, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11275.5712890625
tensor(11275.6191, grad_fn=<NegBackward0>) tensor(11275.5713, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11275.5703125
tensor(11275.5713, grad_fn=<NegBackward0>) tensor(11275.5703, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11275.5693359375
tensor(11275.5703, grad_fn=<NegBackward0>) tensor(11275.5693, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11275.568359375
tensor(11275.5693, grad_fn=<NegBackward0>) tensor(11275.5684, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11275.5693359375
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5693, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11275.568359375
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5684, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11275.5693359375
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5693, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11275.568359375
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5684, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11275.568359375
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5684, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11275.56640625
tensor(11275.5684, grad_fn=<NegBackward0>) tensor(11275.5664, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11275.5673828125
tensor(11275.5664, grad_fn=<NegBackward0>) tensor(11275.5674, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11275.568359375
tensor(11275.5664, grad_fn=<NegBackward0>) tensor(11275.5684, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11275.5673828125
tensor(11275.5664, grad_fn=<NegBackward0>) tensor(11275.5674, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11275.5712890625
tensor(11275.5664, grad_fn=<NegBackward0>) tensor(11275.5713, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11275.5673828125
tensor(11275.5664, grad_fn=<NegBackward0>) tensor(11275.5674, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.7244, 0.2756],
        [0.2612, 0.7388]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5741, 0.4259], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3067, 0.0995],
         [0.7176, 0.2008]],

        [[0.6374, 0.0962],
         [0.5242, 0.6550]],

        [[0.5180, 0.1014],
         [0.5426, 0.7125]],

        [[0.6848, 0.1083],
         [0.5107, 0.6767]],

        [[0.5391, 0.0942],
         [0.6942, 0.6515]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.882296193749233
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291541577713935
Average Adjusted Rand Index: 0.9289407954862906
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22071.54296875
inf tensor(22071.5430, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11614.2138671875
tensor(22071.5430, grad_fn=<NegBackward0>) tensor(11614.2139, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11613.16796875
tensor(11614.2139, grad_fn=<NegBackward0>) tensor(11613.1680, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11607.75
tensor(11613.1680, grad_fn=<NegBackward0>) tensor(11607.7500, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11544.1396484375
tensor(11607.7500, grad_fn=<NegBackward0>) tensor(11544.1396, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11532.4150390625
tensor(11544.1396, grad_fn=<NegBackward0>) tensor(11532.4150, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11531.865234375
tensor(11532.4150, grad_fn=<NegBackward0>) tensor(11531.8652, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11531.3408203125
tensor(11531.8652, grad_fn=<NegBackward0>) tensor(11531.3408, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11530.38671875
tensor(11531.3408, grad_fn=<NegBackward0>) tensor(11530.3867, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11529.78125
tensor(11530.3867, grad_fn=<NegBackward0>) tensor(11529.7812, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11529.673828125
tensor(11529.7812, grad_fn=<NegBackward0>) tensor(11529.6738, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11529.638671875
tensor(11529.6738, grad_fn=<NegBackward0>) tensor(11529.6387, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11529.619140625
tensor(11529.6387, grad_fn=<NegBackward0>) tensor(11529.6191, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11529.609375
tensor(11529.6191, grad_fn=<NegBackward0>) tensor(11529.6094, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11529.603515625
tensor(11529.6094, grad_fn=<NegBackward0>) tensor(11529.6035, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11529.60546875
tensor(11529.6035, grad_fn=<NegBackward0>) tensor(11529.6055, grad_fn=<NegBackward0>)
1
Iteration 1600: Loss = -11529.5966796875
tensor(11529.6035, grad_fn=<NegBackward0>) tensor(11529.5967, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11529.59375
tensor(11529.5967, grad_fn=<NegBackward0>) tensor(11529.5938, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11529.58984375
tensor(11529.5938, grad_fn=<NegBackward0>) tensor(11529.5898, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11529.5888671875
tensor(11529.5898, grad_fn=<NegBackward0>) tensor(11529.5889, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11529.5859375
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11529.5849609375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11529.583984375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5840, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11529.5830078125
tensor(11529.5840, grad_fn=<NegBackward0>) tensor(11529.5830, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11529.58203125
tensor(11529.5830, grad_fn=<NegBackward0>) tensor(11529.5820, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11529.58203125
tensor(11529.5820, grad_fn=<NegBackward0>) tensor(11529.5820, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11529.58203125
tensor(11529.5820, grad_fn=<NegBackward0>) tensor(11529.5820, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11529.580078125
tensor(11529.5820, grad_fn=<NegBackward0>) tensor(11529.5801, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11529.580078125
tensor(11529.5801, grad_fn=<NegBackward0>) tensor(11529.5801, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11529.580078125
tensor(11529.5801, grad_fn=<NegBackward0>) tensor(11529.5801, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11529.580078125
tensor(11529.5801, grad_fn=<NegBackward0>) tensor(11529.5801, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11529.578125
tensor(11529.5801, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11529.5791015625
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5791, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11529.5791015625
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5791, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11529.578125
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11529.578125
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11529.578125
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11529.578125
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11529.5791015625
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5791, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11529.578125
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11529.5771484375
tensor(11529.5781, grad_fn=<NegBackward0>) tensor(11529.5771, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11529.578125
tensor(11529.5771, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11529.5771484375
tensor(11529.5771, grad_fn=<NegBackward0>) tensor(11529.5771, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11529.578125
tensor(11529.5771, grad_fn=<NegBackward0>) tensor(11529.5781, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11529.5771484375
tensor(11529.5771, grad_fn=<NegBackward0>) tensor(11529.5771, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11529.576171875
tensor(11529.5771, grad_fn=<NegBackward0>) tensor(11529.5762, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11529.57421875
tensor(11529.5762, grad_fn=<NegBackward0>) tensor(11529.5742, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11529.5771484375
tensor(11529.5742, grad_fn=<NegBackward0>) tensor(11529.5771, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11529.5751953125
tensor(11529.5742, grad_fn=<NegBackward0>) tensor(11529.5752, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11529.5751953125
tensor(11529.5742, grad_fn=<NegBackward0>) tensor(11529.5752, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11529.576171875
tensor(11529.5742, grad_fn=<NegBackward0>) tensor(11529.5762, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11529.576171875
tensor(11529.5742, grad_fn=<NegBackward0>) tensor(11529.5762, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[1.4556e-05, 9.9999e-01],
        [3.3163e-02, 9.6684e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5761, 0.4239], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3082, 0.0993],
         [0.5098, 0.1804]],

        [[0.6015, 0.1026],
         [0.5440, 0.7020]],

        [[0.6275, 0.0884],
         [0.6620, 0.5687]],

        [[0.5211, 0.2456],
         [0.6104, 0.5052]],

        [[0.6090, 0.0612],
         [0.5484, 0.5733]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.012864505300896736
Global Adjusted Rand Index: 0.03112747706562519
Average Adjusted Rand Index: 0.1856771458367816
[0.9291541577713935, 0.03112747706562519] [0.9289407954862906, 0.1856771458367816] [11275.5673828125, 11529.576171875]
-------------------------------------
This iteration is 48
True Objective function: Loss = -10955.471288486695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23072.62890625
inf tensor(23072.6289, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11154.78515625
tensor(23072.6289, grad_fn=<NegBackward0>) tensor(11154.7852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11153.291015625
tensor(11154.7852, grad_fn=<NegBackward0>) tensor(11153.2910, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11151.5986328125
tensor(11153.2910, grad_fn=<NegBackward0>) tensor(11151.5986, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11150.3359375
tensor(11151.5986, grad_fn=<NegBackward0>) tensor(11150.3359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11148.7177734375
tensor(11150.3359, grad_fn=<NegBackward0>) tensor(11148.7178, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11083.2890625
tensor(11148.7178, grad_fn=<NegBackward0>) tensor(11083.2891, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10957.123046875
tensor(11083.2891, grad_fn=<NegBackward0>) tensor(10957.1230, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10947.01171875
tensor(10957.1230, grad_fn=<NegBackward0>) tensor(10947.0117, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10941.4931640625
tensor(10947.0117, grad_fn=<NegBackward0>) tensor(10941.4932, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10935.7216796875
tensor(10941.4932, grad_fn=<NegBackward0>) tensor(10935.7217, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10934.2470703125
tensor(10935.7217, grad_fn=<NegBackward0>) tensor(10934.2471, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10934.0498046875
tensor(10934.2471, grad_fn=<NegBackward0>) tensor(10934.0498, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10928.1650390625
tensor(10934.0498, grad_fn=<NegBackward0>) tensor(10928.1650, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10928.0556640625
tensor(10928.1650, grad_fn=<NegBackward0>) tensor(10928.0557, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10928.0078125
tensor(10928.0557, grad_fn=<NegBackward0>) tensor(10928.0078, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10927.927734375
tensor(10928.0078, grad_fn=<NegBackward0>) tensor(10927.9277, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10927.890625
tensor(10927.9277, grad_fn=<NegBackward0>) tensor(10927.8906, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10927.8740234375
tensor(10927.8906, grad_fn=<NegBackward0>) tensor(10927.8740, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10927.8681640625
tensor(10927.8740, grad_fn=<NegBackward0>) tensor(10927.8682, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10927.865234375
tensor(10927.8682, grad_fn=<NegBackward0>) tensor(10927.8652, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10927.8623046875
tensor(10927.8652, grad_fn=<NegBackward0>) tensor(10927.8623, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10927.859375
tensor(10927.8623, grad_fn=<NegBackward0>) tensor(10927.8594, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10927.86328125
tensor(10927.8594, grad_fn=<NegBackward0>) tensor(10927.8633, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -10927.8564453125
tensor(10927.8594, grad_fn=<NegBackward0>) tensor(10927.8564, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10927.85546875
tensor(10927.8564, grad_fn=<NegBackward0>) tensor(10927.8555, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10927.8544921875
tensor(10927.8555, grad_fn=<NegBackward0>) tensor(10927.8545, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10927.853515625
tensor(10927.8545, grad_fn=<NegBackward0>) tensor(10927.8535, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10927.85546875
tensor(10927.8535, grad_fn=<NegBackward0>) tensor(10927.8555, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10927.8505859375
tensor(10927.8535, grad_fn=<NegBackward0>) tensor(10927.8506, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10927.8515625
tensor(10927.8506, grad_fn=<NegBackward0>) tensor(10927.8516, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10927.8505859375
tensor(10927.8506, grad_fn=<NegBackward0>) tensor(10927.8506, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10927.8505859375
tensor(10927.8506, grad_fn=<NegBackward0>) tensor(10927.8506, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10927.8486328125
tensor(10927.8506, grad_fn=<NegBackward0>) tensor(10927.8486, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10927.8603515625
tensor(10927.8486, grad_fn=<NegBackward0>) tensor(10927.8604, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10927.8505859375
tensor(10927.8486, grad_fn=<NegBackward0>) tensor(10927.8506, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -10927.8486328125
tensor(10927.8486, grad_fn=<NegBackward0>) tensor(10927.8486, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10927.84765625
tensor(10927.8486, grad_fn=<NegBackward0>) tensor(10927.8477, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10927.8466796875
tensor(10927.8477, grad_fn=<NegBackward0>) tensor(10927.8467, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10927.8466796875
tensor(10927.8467, grad_fn=<NegBackward0>) tensor(10927.8467, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10927.845703125
tensor(10927.8467, grad_fn=<NegBackward0>) tensor(10927.8457, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10927.8466796875
tensor(10927.8457, grad_fn=<NegBackward0>) tensor(10927.8467, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10927.84375
tensor(10927.8457, grad_fn=<NegBackward0>) tensor(10927.8438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10927.8427734375
tensor(10927.8438, grad_fn=<NegBackward0>) tensor(10927.8428, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10927.8427734375
tensor(10927.8428, grad_fn=<NegBackward0>) tensor(10927.8428, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10927.8359375
tensor(10927.8428, grad_fn=<NegBackward0>) tensor(10927.8359, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10927.8359375
tensor(10927.8359, grad_fn=<NegBackward0>) tensor(10927.8359, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10927.8349609375
tensor(10927.8359, grad_fn=<NegBackward0>) tensor(10927.8350, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10927.8349609375
tensor(10927.8350, grad_fn=<NegBackward0>) tensor(10927.8350, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10927.8330078125
tensor(10927.8350, grad_fn=<NegBackward0>) tensor(10927.8330, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10927.8037109375
tensor(10927.8330, grad_fn=<NegBackward0>) tensor(10927.8037, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10927.8037109375
tensor(10927.8037, grad_fn=<NegBackward0>) tensor(10927.8037, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10927.8046875
tensor(10927.8037, grad_fn=<NegBackward0>) tensor(10927.8047, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10927.8017578125
tensor(10927.8037, grad_fn=<NegBackward0>) tensor(10927.8018, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10927.802734375
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8027, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10927.802734375
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8027, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10927.802734375
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8027, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10927.8017578125
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8018, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10927.8046875
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8047, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10927.8017578125
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8018, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10927.8193359375
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8193, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10927.80078125
tensor(10927.8018, grad_fn=<NegBackward0>) tensor(10927.8008, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10927.8017578125
tensor(10927.8008, grad_fn=<NegBackward0>) tensor(10927.8018, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10927.80078125
tensor(10927.8008, grad_fn=<NegBackward0>) tensor(10927.8008, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10927.7998046875
tensor(10927.8008, grad_fn=<NegBackward0>) tensor(10927.7998, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10927.806640625
tensor(10927.7998, grad_fn=<NegBackward0>) tensor(10927.8066, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10927.806640625
tensor(10927.7998, grad_fn=<NegBackward0>) tensor(10927.8066, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10927.802734375
tensor(10927.7998, grad_fn=<NegBackward0>) tensor(10927.8027, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10927.8115234375
tensor(10927.7998, grad_fn=<NegBackward0>) tensor(10927.8115, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -10927.80078125
tensor(10927.7998, grad_fn=<NegBackward0>) tensor(10927.8008, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.7597, 0.2403],
        [0.2893, 0.7107]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5264, 0.4736], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1898, 0.1058],
         [0.6796, 0.2959]],

        [[0.6662, 0.1015],
         [0.7184, 0.6845]],

        [[0.6464, 0.1004],
         [0.6059, 0.6478]],

        [[0.5779, 0.0980],
         [0.6568, 0.5733]],

        [[0.5014, 0.0930],
         [0.6745, 0.7080]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.882389682918764
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
Global Adjusted Rand Index: 0.9214390317891062
Average Adjusted Rand Index: 0.9216062415680071
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23187.056640625
inf tensor(23187.0566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11153.75390625
tensor(23187.0566, grad_fn=<NegBackward0>) tensor(11153.7539, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11152.6298828125
tensor(11153.7539, grad_fn=<NegBackward0>) tensor(11152.6299, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11152.1591796875
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.1592, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11151.9267578125
tensor(11152.1592, grad_fn=<NegBackward0>) tensor(11151.9268, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11151.67578125
tensor(11151.9268, grad_fn=<NegBackward0>) tensor(11151.6758, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11151.296875
tensor(11151.6758, grad_fn=<NegBackward0>) tensor(11151.2969, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11149.7578125
tensor(11151.2969, grad_fn=<NegBackward0>) tensor(11149.7578, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11148.419921875
tensor(11149.7578, grad_fn=<NegBackward0>) tensor(11148.4199, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10974.099609375
tensor(11148.4199, grad_fn=<NegBackward0>) tensor(10974.0996, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10943.412109375
tensor(10974.0996, grad_fn=<NegBackward0>) tensor(10943.4121, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10937.15625
tensor(10943.4121, grad_fn=<NegBackward0>) tensor(10937.1562, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10934.79296875
tensor(10937.1562, grad_fn=<NegBackward0>) tensor(10934.7930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10934.7509765625
tensor(10934.7930, grad_fn=<NegBackward0>) tensor(10934.7510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10934.6494140625
tensor(10934.7510, grad_fn=<NegBackward0>) tensor(10934.6494, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10934.5390625
tensor(10934.6494, grad_fn=<NegBackward0>) tensor(10934.5391, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10934.4853515625
tensor(10934.5391, grad_fn=<NegBackward0>) tensor(10934.4854, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10932.767578125
tensor(10934.4854, grad_fn=<NegBackward0>) tensor(10932.7676, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10932.6318359375
tensor(10932.7676, grad_fn=<NegBackward0>) tensor(10932.6318, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10932.6220703125
tensor(10932.6318, grad_fn=<NegBackward0>) tensor(10932.6221, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10932.5908203125
tensor(10932.6221, grad_fn=<NegBackward0>) tensor(10932.5908, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10932.5712890625
tensor(10932.5908, grad_fn=<NegBackward0>) tensor(10932.5713, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10932.568359375
tensor(10932.5713, grad_fn=<NegBackward0>) tensor(10932.5684, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10932.5634765625
tensor(10932.5684, grad_fn=<NegBackward0>) tensor(10932.5635, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10932.552734375
tensor(10932.5635, grad_fn=<NegBackward0>) tensor(10932.5527, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10932.54296875
tensor(10932.5527, grad_fn=<NegBackward0>) tensor(10932.5430, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10932.5419921875
tensor(10932.5430, grad_fn=<NegBackward0>) tensor(10932.5420, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10932.541015625
tensor(10932.5420, grad_fn=<NegBackward0>) tensor(10932.5410, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10932.5390625
tensor(10932.5410, grad_fn=<NegBackward0>) tensor(10932.5391, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10932.5498046875
tensor(10932.5391, grad_fn=<NegBackward0>) tensor(10932.5498, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10932.5361328125
tensor(10932.5391, grad_fn=<NegBackward0>) tensor(10932.5361, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10932.53515625
tensor(10932.5361, grad_fn=<NegBackward0>) tensor(10932.5352, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10932.5263671875
tensor(10932.5352, grad_fn=<NegBackward0>) tensor(10932.5264, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10932.470703125
tensor(10932.5264, grad_fn=<NegBackward0>) tensor(10932.4707, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10932.4697265625
tensor(10932.4707, grad_fn=<NegBackward0>) tensor(10932.4697, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10932.46875
tensor(10932.4697, grad_fn=<NegBackward0>) tensor(10932.4688, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10932.4677734375
tensor(10932.4688, grad_fn=<NegBackward0>) tensor(10932.4678, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10932.4697265625
tensor(10932.4678, grad_fn=<NegBackward0>) tensor(10932.4697, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10932.466796875
tensor(10932.4678, grad_fn=<NegBackward0>) tensor(10932.4668, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10932.4658203125
tensor(10932.4668, grad_fn=<NegBackward0>) tensor(10932.4658, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10932.4658203125
tensor(10932.4658, grad_fn=<NegBackward0>) tensor(10932.4658, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10932.466796875
tensor(10932.4658, grad_fn=<NegBackward0>) tensor(10932.4668, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10932.46875
tensor(10932.4658, grad_fn=<NegBackward0>) tensor(10932.4688, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10932.4638671875
tensor(10932.4658, grad_fn=<NegBackward0>) tensor(10932.4639, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10932.4638671875
tensor(10932.4639, grad_fn=<NegBackward0>) tensor(10932.4639, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10929.662109375
tensor(10932.4639, grad_fn=<NegBackward0>) tensor(10929.6621, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10929.65625
tensor(10929.6621, grad_fn=<NegBackward0>) tensor(10929.6562, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10929.658203125
tensor(10929.6562, grad_fn=<NegBackward0>) tensor(10929.6582, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10929.6552734375
tensor(10929.6562, grad_fn=<NegBackward0>) tensor(10929.6553, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10929.6591796875
tensor(10929.6553, grad_fn=<NegBackward0>) tensor(10929.6592, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10929.6552734375
tensor(10929.6553, grad_fn=<NegBackward0>) tensor(10929.6553, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10929.654296875
tensor(10929.6553, grad_fn=<NegBackward0>) tensor(10929.6543, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10929.56640625
tensor(10929.6543, grad_fn=<NegBackward0>) tensor(10929.5664, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10929.56640625
tensor(10929.5664, grad_fn=<NegBackward0>) tensor(10929.5664, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10928.1982421875
tensor(10929.5664, grad_fn=<NegBackward0>) tensor(10928.1982, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10928.0302734375
tensor(10928.1982, grad_fn=<NegBackward0>) tensor(10928.0303, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10928.01953125
tensor(10928.0303, grad_fn=<NegBackward0>) tensor(10928.0195, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10928.0205078125
tensor(10928.0195, grad_fn=<NegBackward0>) tensor(10928.0205, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10928.0166015625
tensor(10928.0195, grad_fn=<NegBackward0>) tensor(10928.0166, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10928.005859375
tensor(10928.0166, grad_fn=<NegBackward0>) tensor(10928.0059, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10928.00390625
tensor(10928.0059, grad_fn=<NegBackward0>) tensor(10928.0039, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10928.0029296875
tensor(10928.0039, grad_fn=<NegBackward0>) tensor(10928.0029, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10928.001953125
tensor(10928.0029, grad_fn=<NegBackward0>) tensor(10928.0020, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10928.00390625
tensor(10928.0020, grad_fn=<NegBackward0>) tensor(10928.0039, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10928.0029296875
tensor(10928.0020, grad_fn=<NegBackward0>) tensor(10928.0029, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10928.0029296875
tensor(10928.0020, grad_fn=<NegBackward0>) tensor(10928.0029, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10928.0009765625
tensor(10928.0020, grad_fn=<NegBackward0>) tensor(10928.0010, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10927.994140625
tensor(10928.0010, grad_fn=<NegBackward0>) tensor(10927.9941, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10927.9970703125
tensor(10927.9941, grad_fn=<NegBackward0>) tensor(10927.9971, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10927.994140625
tensor(10927.9941, grad_fn=<NegBackward0>) tensor(10927.9941, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10927.9931640625
tensor(10927.9941, grad_fn=<NegBackward0>) tensor(10927.9932, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10927.998046875
tensor(10927.9932, grad_fn=<NegBackward0>) tensor(10927.9980, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10927.994140625
tensor(10927.9932, grad_fn=<NegBackward0>) tensor(10927.9941, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10928.0068359375
tensor(10927.9932, grad_fn=<NegBackward0>) tensor(10928.0068, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10927.994140625
tensor(10927.9932, grad_fn=<NegBackward0>) tensor(10927.9941, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -10927.9951171875
tensor(10927.9932, grad_fn=<NegBackward0>) tensor(10927.9951, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7602, 0.2398],
        [0.2886, 0.7114]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5300, 0.4700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1896, 0.1059],
         [0.5183, 0.2965]],

        [[0.5151, 0.1016],
         [0.6794, 0.6325]],

        [[0.6611, 0.1003],
         [0.6572, 0.5354]],

        [[0.6786, 0.0980],
         [0.5339, 0.5328]],

        [[0.6090, 0.0930],
         [0.6548, 0.6667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.882389682918764
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448420005390695
Global Adjusted Rand Index: 0.9137587205926886
Average Adjusted Rand Index: 0.9139294097811259
[0.9214390317891062, 0.9137587205926886] [0.9216062415680071, 0.9139294097811259] [10927.80078125, 10927.9951171875]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11181.983027760527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22352.140625
inf tensor(22352.1406, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11502.5498046875
tensor(22352.1406, grad_fn=<NegBackward0>) tensor(11502.5498, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11499.763671875
tensor(11502.5498, grad_fn=<NegBackward0>) tensor(11499.7637, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11461.8232421875
tensor(11499.7637, grad_fn=<NegBackward0>) tensor(11461.8232, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11345.1953125
tensor(11461.8232, grad_fn=<NegBackward0>) tensor(11345.1953, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11254.7138671875
tensor(11345.1953, grad_fn=<NegBackward0>) tensor(11254.7139, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11242.314453125
tensor(11254.7139, grad_fn=<NegBackward0>) tensor(11242.3145, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11239.24609375
tensor(11242.3145, grad_fn=<NegBackward0>) tensor(11239.2461, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11237.1171875
tensor(11239.2461, grad_fn=<NegBackward0>) tensor(11237.1172, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.07421875
tensor(11237.1172, grad_fn=<NegBackward0>) tensor(11235.0742, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11234.8076171875
tensor(11235.0742, grad_fn=<NegBackward0>) tensor(11234.8076, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11231.5546875
tensor(11234.8076, grad_fn=<NegBackward0>) tensor(11231.5547, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11231.02734375
tensor(11231.5547, grad_fn=<NegBackward0>) tensor(11231.0273, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11228.328125
tensor(11231.0273, grad_fn=<NegBackward0>) tensor(11228.3281, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11228.302734375
tensor(11228.3281, grad_fn=<NegBackward0>) tensor(11228.3027, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11228.283203125
tensor(11228.3027, grad_fn=<NegBackward0>) tensor(11228.2832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11228.259765625
tensor(11228.2832, grad_fn=<NegBackward0>) tensor(11228.2598, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11228.212890625
tensor(11228.2598, grad_fn=<NegBackward0>) tensor(11228.2129, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11228.201171875
tensor(11228.2129, grad_fn=<NegBackward0>) tensor(11228.2012, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11228.185546875
tensor(11228.2012, grad_fn=<NegBackward0>) tensor(11228.1855, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11228.16015625
tensor(11228.1855, grad_fn=<NegBackward0>) tensor(11228.1602, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11228.1494140625
tensor(11228.1602, grad_fn=<NegBackward0>) tensor(11228.1494, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11226.955078125
tensor(11228.1494, grad_fn=<NegBackward0>) tensor(11226.9551, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11226.8818359375
tensor(11226.9551, grad_fn=<NegBackward0>) tensor(11226.8818, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11226.8759765625
tensor(11226.8818, grad_fn=<NegBackward0>) tensor(11226.8760, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11226.8720703125
tensor(11226.8760, grad_fn=<NegBackward0>) tensor(11226.8721, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11226.8681640625
tensor(11226.8721, grad_fn=<NegBackward0>) tensor(11226.8682, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11226.8623046875
tensor(11226.8682, grad_fn=<NegBackward0>) tensor(11226.8623, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11226.859375
tensor(11226.8623, grad_fn=<NegBackward0>) tensor(11226.8594, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11226.8544921875
tensor(11226.8594, grad_fn=<NegBackward0>) tensor(11226.8545, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11226.8505859375
tensor(11226.8545, grad_fn=<NegBackward0>) tensor(11226.8506, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11226.845703125
tensor(11226.8506, grad_fn=<NegBackward0>) tensor(11226.8457, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11226.8369140625
tensor(11226.8457, grad_fn=<NegBackward0>) tensor(11226.8369, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11226.8251953125
tensor(11226.8369, grad_fn=<NegBackward0>) tensor(11226.8252, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11226.8017578125
tensor(11226.8252, grad_fn=<NegBackward0>) tensor(11226.8018, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11226.763671875
tensor(11226.8018, grad_fn=<NegBackward0>) tensor(11226.7637, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11226.689453125
tensor(11226.7637, grad_fn=<NegBackward0>) tensor(11226.6895, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11226.6015625
tensor(11226.6895, grad_fn=<NegBackward0>) tensor(11226.6016, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11226.4765625
tensor(11226.6016, grad_fn=<NegBackward0>) tensor(11226.4766, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11226.3408203125
tensor(11226.4766, grad_fn=<NegBackward0>) tensor(11226.3408, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11226.265625
tensor(11226.3408, grad_fn=<NegBackward0>) tensor(11226.2656, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11226.23046875
tensor(11226.2656, grad_fn=<NegBackward0>) tensor(11226.2305, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11226.197265625
tensor(11226.2305, grad_fn=<NegBackward0>) tensor(11226.1973, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11226.173828125
tensor(11226.1973, grad_fn=<NegBackward0>) tensor(11226.1738, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11226.1103515625
tensor(11226.1738, grad_fn=<NegBackward0>) tensor(11226.1104, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11168.857421875
tensor(11226.1104, grad_fn=<NegBackward0>) tensor(11168.8574, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11152.6884765625
tensor(11168.8574, grad_fn=<NegBackward0>) tensor(11152.6885, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11152.673828125
tensor(11152.6885, grad_fn=<NegBackward0>) tensor(11152.6738, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11152.6669921875
tensor(11152.6738, grad_fn=<NegBackward0>) tensor(11152.6670, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11152.6650390625
tensor(11152.6670, grad_fn=<NegBackward0>) tensor(11152.6650, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11152.6640625
tensor(11152.6650, grad_fn=<NegBackward0>) tensor(11152.6641, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11152.6630859375
tensor(11152.6641, grad_fn=<NegBackward0>) tensor(11152.6631, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11152.662109375
tensor(11152.6631, grad_fn=<NegBackward0>) tensor(11152.6621, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11152.6650390625
tensor(11152.6621, grad_fn=<NegBackward0>) tensor(11152.6650, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11152.6611328125
tensor(11152.6621, grad_fn=<NegBackward0>) tensor(11152.6611, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11152.66015625
tensor(11152.6611, grad_fn=<NegBackward0>) tensor(11152.6602, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11152.66015625
tensor(11152.6602, grad_fn=<NegBackward0>) tensor(11152.6602, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11152.66015625
tensor(11152.6602, grad_fn=<NegBackward0>) tensor(11152.6602, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11152.6591796875
tensor(11152.6602, grad_fn=<NegBackward0>) tensor(11152.6592, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11152.6591796875
tensor(11152.6592, grad_fn=<NegBackward0>) tensor(11152.6592, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11152.66796875
tensor(11152.6592, grad_fn=<NegBackward0>) tensor(11152.6680, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11152.658203125
tensor(11152.6592, grad_fn=<NegBackward0>) tensor(11152.6582, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11152.6611328125
tensor(11152.6582, grad_fn=<NegBackward0>) tensor(11152.6611, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11152.658203125
tensor(11152.6582, grad_fn=<NegBackward0>) tensor(11152.6582, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11152.658203125
tensor(11152.6582, grad_fn=<NegBackward0>) tensor(11152.6582, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11152.6572265625
tensor(11152.6582, grad_fn=<NegBackward0>) tensor(11152.6572, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11152.66015625
tensor(11152.6572, grad_fn=<NegBackward0>) tensor(11152.6602, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11152.658203125
tensor(11152.6572, grad_fn=<NegBackward0>) tensor(11152.6582, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11152.6552734375
tensor(11152.6572, grad_fn=<NegBackward0>) tensor(11152.6553, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11152.654296875
tensor(11152.6553, grad_fn=<NegBackward0>) tensor(11152.6543, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11152.666015625
tensor(11152.6543, grad_fn=<NegBackward0>) tensor(11152.6660, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11152.6533203125
tensor(11152.6543, grad_fn=<NegBackward0>) tensor(11152.6533, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11152.6533203125
tensor(11152.6533, grad_fn=<NegBackward0>) tensor(11152.6533, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11152.6494140625
tensor(11152.6533, grad_fn=<NegBackward0>) tensor(11152.6494, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11152.6474609375
tensor(11152.6494, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11152.6533203125
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6533, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11152.6455078125
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6455, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11152.650390625
tensor(11152.6455, grad_fn=<NegBackward0>) tensor(11152.6504, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11152.646484375
tensor(11152.6455, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11152.64453125
tensor(11152.6455, grad_fn=<NegBackward0>) tensor(11152.6445, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11152.681640625
tensor(11152.6445, grad_fn=<NegBackward0>) tensor(11152.6816, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11152.64453125
tensor(11152.6445, grad_fn=<NegBackward0>) tensor(11152.6445, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11152.6748046875
tensor(11152.6445, grad_fn=<NegBackward0>) tensor(11152.6748, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11152.6435546875
tensor(11152.6445, grad_fn=<NegBackward0>) tensor(11152.6436, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11152.6455078125
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6455, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11152.6435546875
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6436, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11152.64453125
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6445, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11152.6435546875
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6436, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11152.646484375
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11152.65625
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6562, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11152.64453125
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6445, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11152.650390625
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6504, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11152.6435546875
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6436, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11152.642578125
tensor(11152.6436, grad_fn=<NegBackward0>) tensor(11152.6426, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11152.646484375
tensor(11152.6426, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11152.6484375
tensor(11152.6426, grad_fn=<NegBackward0>) tensor(11152.6484, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7922, 0.2078],
        [0.2748, 0.7252]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3931, 0.6069], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3071, 0.0943],
         [0.7112, 0.1997]],

        [[0.5853, 0.0937],
         [0.6648, 0.5897]],

        [[0.6460, 0.0976],
         [0.6792, 0.7077]],

        [[0.6721, 0.1003],
         [0.5279, 0.7172]],

        [[0.5439, 0.1027],
         [0.6853, 0.7034]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8079912862954653
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9214425803768571
Average Adjusted Rand Index: 0.9217588703519946
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20278.279296875
inf tensor(20278.2793, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11503.0185546875
tensor(20278.2793, grad_fn=<NegBackward0>) tensor(11503.0186, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11488.70703125
tensor(11503.0186, grad_fn=<NegBackward0>) tensor(11488.7070, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11397.2392578125
tensor(11488.7070, grad_fn=<NegBackward0>) tensor(11397.2393, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11265.8359375
tensor(11397.2393, grad_fn=<NegBackward0>) tensor(11265.8359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11247.111328125
tensor(11265.8359, grad_fn=<NegBackward0>) tensor(11247.1113, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11239.103515625
tensor(11247.1113, grad_fn=<NegBackward0>) tensor(11239.1035, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11237.4326171875
tensor(11239.1035, grad_fn=<NegBackward0>) tensor(11237.4326, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11230.810546875
tensor(11237.4326, grad_fn=<NegBackward0>) tensor(11230.8105, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11230.607421875
tensor(11230.8105, grad_fn=<NegBackward0>) tensor(11230.6074, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11229.55078125
tensor(11230.6074, grad_fn=<NegBackward0>) tensor(11229.5508, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11229.0849609375
tensor(11229.5508, grad_fn=<NegBackward0>) tensor(11229.0850, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11229.04296875
tensor(11229.0850, grad_fn=<NegBackward0>) tensor(11229.0430, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11228.3837890625
tensor(11229.0430, grad_fn=<NegBackward0>) tensor(11228.3838, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11226.9384765625
tensor(11228.3838, grad_fn=<NegBackward0>) tensor(11226.9385, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11226.92578125
tensor(11226.9385, grad_fn=<NegBackward0>) tensor(11226.9258, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11226.9150390625
tensor(11226.9258, grad_fn=<NegBackward0>) tensor(11226.9150, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11226.90625
tensor(11226.9150, grad_fn=<NegBackward0>) tensor(11226.9062, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11226.9013671875
tensor(11226.9062, grad_fn=<NegBackward0>) tensor(11226.9014, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11226.89453125
tensor(11226.9014, grad_fn=<NegBackward0>) tensor(11226.8945, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11226.890625
tensor(11226.8945, grad_fn=<NegBackward0>) tensor(11226.8906, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11226.8857421875
tensor(11226.8906, grad_fn=<NegBackward0>) tensor(11226.8857, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11226.87890625
tensor(11226.8857, grad_fn=<NegBackward0>) tensor(11226.8789, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11226.875
tensor(11226.8789, grad_fn=<NegBackward0>) tensor(11226.8750, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11226.8701171875
tensor(11226.8750, grad_fn=<NegBackward0>) tensor(11226.8701, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11226.8662109375
tensor(11226.8701, grad_fn=<NegBackward0>) tensor(11226.8662, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11226.8623046875
tensor(11226.8662, grad_fn=<NegBackward0>) tensor(11226.8623, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11226.857421875
tensor(11226.8623, grad_fn=<NegBackward0>) tensor(11226.8574, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11226.8525390625
tensor(11226.8574, grad_fn=<NegBackward0>) tensor(11226.8525, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11226.845703125
tensor(11226.8525, grad_fn=<NegBackward0>) tensor(11226.8457, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11226.8369140625
tensor(11226.8457, grad_fn=<NegBackward0>) tensor(11226.8369, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11226.814453125
tensor(11226.8369, grad_fn=<NegBackward0>) tensor(11226.8145, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11226.7802734375
tensor(11226.8145, grad_fn=<NegBackward0>) tensor(11226.7803, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11226.7255859375
tensor(11226.7803, grad_fn=<NegBackward0>) tensor(11226.7256, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11226.6611328125
tensor(11226.7256, grad_fn=<NegBackward0>) tensor(11226.6611, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11226.6142578125
tensor(11226.6611, grad_fn=<NegBackward0>) tensor(11226.6143, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11226.5712890625
tensor(11226.6143, grad_fn=<NegBackward0>) tensor(11226.5713, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11226.50390625
tensor(11226.5713, grad_fn=<NegBackward0>) tensor(11226.5039, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11226.4921875
tensor(11226.5039, grad_fn=<NegBackward0>) tensor(11226.4922, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11226.427734375
tensor(11226.4922, grad_fn=<NegBackward0>) tensor(11226.4277, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11226.380859375
tensor(11226.4277, grad_fn=<NegBackward0>) tensor(11226.3809, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11226.376953125
tensor(11226.3809, grad_fn=<NegBackward0>) tensor(11226.3770, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11226.3671875
tensor(11226.3770, grad_fn=<NegBackward0>) tensor(11226.3672, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11226.349609375
tensor(11226.3672, grad_fn=<NegBackward0>) tensor(11226.3496, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11226.3369140625
tensor(11226.3496, grad_fn=<NegBackward0>) tensor(11226.3369, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11226.32421875
tensor(11226.3369, grad_fn=<NegBackward0>) tensor(11226.3242, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11226.3154296875
tensor(11226.3242, grad_fn=<NegBackward0>) tensor(11226.3154, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11226.3115234375
tensor(11226.3154, grad_fn=<NegBackward0>) tensor(11226.3115, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11226.2880859375
tensor(11226.3115, grad_fn=<NegBackward0>) tensor(11226.2881, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11226.2802734375
tensor(11226.2881, grad_fn=<NegBackward0>) tensor(11226.2803, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11226.279296875
tensor(11226.2803, grad_fn=<NegBackward0>) tensor(11226.2793, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11226.279296875
tensor(11226.2793, grad_fn=<NegBackward0>) tensor(11226.2793, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11226.26953125
tensor(11226.2793, grad_fn=<NegBackward0>) tensor(11226.2695, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11226.2275390625
tensor(11226.2695, grad_fn=<NegBackward0>) tensor(11226.2275, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11226.2275390625
tensor(11226.2275, grad_fn=<NegBackward0>) tensor(11226.2275, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11226.1875
tensor(11226.2275, grad_fn=<NegBackward0>) tensor(11226.1875, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11176.232421875
tensor(11226.1875, grad_fn=<NegBackward0>) tensor(11176.2324, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11162.8662109375
tensor(11176.2324, grad_fn=<NegBackward0>) tensor(11162.8662, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11162.8447265625
tensor(11162.8662, grad_fn=<NegBackward0>) tensor(11162.8447, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11162.8427734375
tensor(11162.8447, grad_fn=<NegBackward0>) tensor(11162.8428, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11162.841796875
tensor(11162.8428, grad_fn=<NegBackward0>) tensor(11162.8418, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11162.833984375
tensor(11162.8418, grad_fn=<NegBackward0>) tensor(11162.8340, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11162.830078125
tensor(11162.8340, grad_fn=<NegBackward0>) tensor(11162.8301, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11162.830078125
tensor(11162.8301, grad_fn=<NegBackward0>) tensor(11162.8301, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11162.810546875
tensor(11162.8301, grad_fn=<NegBackward0>) tensor(11162.8105, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11162.8046875
tensor(11162.8105, grad_fn=<NegBackward0>) tensor(11162.8047, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11162.7978515625
tensor(11162.8047, grad_fn=<NegBackward0>) tensor(11162.7979, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11152.6904296875
tensor(11162.7979, grad_fn=<NegBackward0>) tensor(11152.6904, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11152.6865234375
tensor(11152.6904, grad_fn=<NegBackward0>) tensor(11152.6865, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11152.6748046875
tensor(11152.6865, grad_fn=<NegBackward0>) tensor(11152.6748, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11152.650390625
tensor(11152.6748, grad_fn=<NegBackward0>) tensor(11152.6504, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11152.650390625
tensor(11152.6504, grad_fn=<NegBackward0>) tensor(11152.6504, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11152.6552734375
tensor(11152.6504, grad_fn=<NegBackward0>) tensor(11152.6553, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11152.6474609375
tensor(11152.6504, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11152.6484375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6484, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11152.6728515625
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6729, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11152.6669921875
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6670, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11152.69140625
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6914, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11152.6474609375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11152.646484375
tensor(11152.6475, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11152.6474609375
tensor(11152.6465, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11152.646484375
tensor(11152.6465, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11152.646484375
tensor(11152.6465, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11152.646484375
tensor(11152.6465, grad_fn=<NegBackward0>) tensor(11152.6465, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11152.64453125
tensor(11152.6465, grad_fn=<NegBackward0>) tensor(11152.6445, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11152.640625
tensor(11152.6445, grad_fn=<NegBackward0>) tensor(11152.6406, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11152.6396484375
tensor(11152.6406, grad_fn=<NegBackward0>) tensor(11152.6396, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11152.6484375
tensor(11152.6396, grad_fn=<NegBackward0>) tensor(11152.6484, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11152.6396484375
tensor(11152.6396, grad_fn=<NegBackward0>) tensor(11152.6396, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11152.6474609375
tensor(11152.6396, grad_fn=<NegBackward0>) tensor(11152.6475, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11152.638671875
tensor(11152.6396, grad_fn=<NegBackward0>) tensor(11152.6387, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11152.6298828125
tensor(11152.6387, grad_fn=<NegBackward0>) tensor(11152.6299, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11152.634765625
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6348, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11152.6611328125
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6611, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11152.6298828125
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6299, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11152.6337890625
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6338, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11152.6435546875
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6436, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11152.6298828125
tensor(11152.6299, grad_fn=<NegBackward0>) tensor(11152.6299, grad_fn=<NegBackward0>)
pi: tensor([[0.7928, 0.2072],
        [0.2725, 0.7275]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3069, 0.0944],
         [0.6583, 0.1999]],

        [[0.6477, 0.0935],
         [0.5932, 0.6878]],

        [[0.5500, 0.0974],
         [0.5860, 0.6219]],

        [[0.5838, 0.1002],
         [0.5873, 0.5747]],

        [[0.7253, 0.1027],
         [0.5938, 0.5938]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8079912862954653
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9214425803768571
Average Adjusted Rand Index: 0.9217588703519946
[0.9214425803768571, 0.9214425803768571] [0.9217588703519946, 0.9217588703519946] [11152.6474609375, 11152.630859375]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11117.496986969172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23845.138671875
inf tensor(23845.1387, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11405.28515625
tensor(23845.1387, grad_fn=<NegBackward0>) tensor(11405.2852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11402.7275390625
tensor(11405.2852, grad_fn=<NegBackward0>) tensor(11402.7275, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11400.1669921875
tensor(11402.7275, grad_fn=<NegBackward0>) tensor(11400.1670, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11396.8740234375
tensor(11400.1670, grad_fn=<NegBackward0>) tensor(11396.8740, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11386.134765625
tensor(11396.8740, grad_fn=<NegBackward0>) tensor(11386.1348, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11216.8837890625
tensor(11386.1348, grad_fn=<NegBackward0>) tensor(11216.8838, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11159.064453125
tensor(11216.8838, grad_fn=<NegBackward0>) tensor(11159.0645, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11114.0634765625
tensor(11159.0645, grad_fn=<NegBackward0>) tensor(11114.0635, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11113.6240234375
tensor(11114.0635, grad_fn=<NegBackward0>) tensor(11113.6240, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11113.4599609375
tensor(11113.6240, grad_fn=<NegBackward0>) tensor(11113.4600, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11108.8671875
tensor(11113.4600, grad_fn=<NegBackward0>) tensor(11108.8672, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11108.7978515625
tensor(11108.8672, grad_fn=<NegBackward0>) tensor(11108.7979, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11108.7490234375
tensor(11108.7979, grad_fn=<NegBackward0>) tensor(11108.7490, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11102.822265625
tensor(11108.7490, grad_fn=<NegBackward0>) tensor(11102.8223, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11102.671875
tensor(11102.8223, grad_fn=<NegBackward0>) tensor(11102.6719, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11098.5498046875
tensor(11102.6719, grad_fn=<NegBackward0>) tensor(11098.5498, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11098.53515625
tensor(11098.5498, grad_fn=<NegBackward0>) tensor(11098.5352, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11098.521484375
tensor(11098.5352, grad_fn=<NegBackward0>) tensor(11098.5215, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11098.5068359375
tensor(11098.5215, grad_fn=<NegBackward0>) tensor(11098.5068, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11098.4931640625
tensor(11098.5068, grad_fn=<NegBackward0>) tensor(11098.4932, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11098.4794921875
tensor(11098.4932, grad_fn=<NegBackward0>) tensor(11098.4795, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11098.46875
tensor(11098.4795, grad_fn=<NegBackward0>) tensor(11098.4688, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11098.4638671875
tensor(11098.4688, grad_fn=<NegBackward0>) tensor(11098.4639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11098.4541015625
tensor(11098.4639, grad_fn=<NegBackward0>) tensor(11098.4541, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11098.447265625
tensor(11098.4541, grad_fn=<NegBackward0>) tensor(11098.4473, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11098.4326171875
tensor(11098.4473, grad_fn=<NegBackward0>) tensor(11098.4326, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11098.37890625
tensor(11098.4326, grad_fn=<NegBackward0>) tensor(11098.3789, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11098.375
tensor(11098.3789, grad_fn=<NegBackward0>) tensor(11098.3750, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11098.3720703125
tensor(11098.3750, grad_fn=<NegBackward0>) tensor(11098.3721, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11098.3671875
tensor(11098.3721, grad_fn=<NegBackward0>) tensor(11098.3672, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11098.3515625
tensor(11098.3672, grad_fn=<NegBackward0>) tensor(11098.3516, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11098.2255859375
tensor(11098.3516, grad_fn=<NegBackward0>) tensor(11098.2256, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11098.220703125
tensor(11098.2256, grad_fn=<NegBackward0>) tensor(11098.2207, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11098.21875
tensor(11098.2207, grad_fn=<NegBackward0>) tensor(11098.2188, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11098.2158203125
tensor(11098.2188, grad_fn=<NegBackward0>) tensor(11098.2158, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11098.208984375
tensor(11098.2158, grad_fn=<NegBackward0>) tensor(11098.2090, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11098.171875
tensor(11098.2090, grad_fn=<NegBackward0>) tensor(11098.1719, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11098.16796875
tensor(11098.1719, grad_fn=<NegBackward0>) tensor(11098.1680, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11098.1669921875
tensor(11098.1680, grad_fn=<NegBackward0>) tensor(11098.1670, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11098.1669921875
tensor(11098.1670, grad_fn=<NegBackward0>) tensor(11098.1670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11098.166015625
tensor(11098.1670, grad_fn=<NegBackward0>) tensor(11098.1660, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11098.1650390625
tensor(11098.1660, grad_fn=<NegBackward0>) tensor(11098.1650, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11098.162109375
tensor(11098.1650, grad_fn=<NegBackward0>) tensor(11098.1621, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11098.1630859375
tensor(11098.1621, grad_fn=<NegBackward0>) tensor(11098.1631, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11098.1640625
tensor(11098.1621, grad_fn=<NegBackward0>) tensor(11098.1641, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11098.1611328125
tensor(11098.1621, grad_fn=<NegBackward0>) tensor(11098.1611, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11098.1611328125
tensor(11098.1611, grad_fn=<NegBackward0>) tensor(11098.1611, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11098.1611328125
tensor(11098.1611, grad_fn=<NegBackward0>) tensor(11098.1611, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11098.16015625
tensor(11098.1611, grad_fn=<NegBackward0>) tensor(11098.1602, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11098.16015625
tensor(11098.1602, grad_fn=<NegBackward0>) tensor(11098.1602, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11098.1591796875
tensor(11098.1602, grad_fn=<NegBackward0>) tensor(11098.1592, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11098.158203125
tensor(11098.1592, grad_fn=<NegBackward0>) tensor(11098.1582, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11098.1630859375
tensor(11098.1582, grad_fn=<NegBackward0>) tensor(11098.1631, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11098.1591796875
tensor(11098.1582, grad_fn=<NegBackward0>) tensor(11098.1592, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11098.158203125
tensor(11098.1582, grad_fn=<NegBackward0>) tensor(11098.1582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11098.1591796875
tensor(11098.1582, grad_fn=<NegBackward0>) tensor(11098.1592, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11098.1572265625
tensor(11098.1582, grad_fn=<NegBackward0>) tensor(11098.1572, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11098.15625
tensor(11098.1572, grad_fn=<NegBackward0>) tensor(11098.1562, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11098.15625
tensor(11098.1562, grad_fn=<NegBackward0>) tensor(11098.1562, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11098.15625
tensor(11098.1562, grad_fn=<NegBackward0>) tensor(11098.1562, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11098.1533203125
tensor(11098.1562, grad_fn=<NegBackward0>) tensor(11098.1533, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11098.1513671875
tensor(11098.1533, grad_fn=<NegBackward0>) tensor(11098.1514, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11098.150390625
tensor(11098.1514, grad_fn=<NegBackward0>) tensor(11098.1504, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11098.1513671875
tensor(11098.1504, grad_fn=<NegBackward0>) tensor(11098.1514, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11098.150390625
tensor(11098.1504, grad_fn=<NegBackward0>) tensor(11098.1504, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11098.1611328125
tensor(11098.1504, grad_fn=<NegBackward0>) tensor(11098.1611, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11098.1484375
tensor(11098.1504, grad_fn=<NegBackward0>) tensor(11098.1484, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11098.0859375
tensor(11098.1484, grad_fn=<NegBackward0>) tensor(11098.0859, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11098.0859375
tensor(11098.0859, grad_fn=<NegBackward0>) tensor(11098.0859, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11098.083984375
tensor(11098.0859, grad_fn=<NegBackward0>) tensor(11098.0840, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11098.0830078125
tensor(11098.0840, grad_fn=<NegBackward0>) tensor(11098.0830, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11098.0771484375
tensor(11098.0830, grad_fn=<NegBackward0>) tensor(11098.0771, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11098.076171875
tensor(11098.0771, grad_fn=<NegBackward0>) tensor(11098.0762, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11098.0771484375
tensor(11098.0762, grad_fn=<NegBackward0>) tensor(11098.0771, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11098.076171875
tensor(11098.0762, grad_fn=<NegBackward0>) tensor(11098.0762, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11098.0615234375
tensor(11098.0762, grad_fn=<NegBackward0>) tensor(11098.0615, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11098.078125
tensor(11098.0615, grad_fn=<NegBackward0>) tensor(11098.0781, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11098.0615234375
tensor(11098.0615, grad_fn=<NegBackward0>) tensor(11098.0615, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11098.0595703125
tensor(11098.0615, grad_fn=<NegBackward0>) tensor(11098.0596, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11098.0654296875
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0654, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11098.0595703125
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0596, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11098.0615234375
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0615, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11098.0595703125
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0596, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11098.0595703125
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0596, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11098.0576171875
tensor(11098.0596, grad_fn=<NegBackward0>) tensor(11098.0576, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11098.0693359375
tensor(11098.0576, grad_fn=<NegBackward0>) tensor(11098.0693, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11098.05859375
tensor(11098.0576, grad_fn=<NegBackward0>) tensor(11098.0586, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11095.849609375
tensor(11098.0576, grad_fn=<NegBackward0>) tensor(11095.8496, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11095.853515625
tensor(11095.8496, grad_fn=<NegBackward0>) tensor(11095.8535, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11095.849609375
tensor(11095.8496, grad_fn=<NegBackward0>) tensor(11095.8496, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11095.84765625
tensor(11095.8496, grad_fn=<NegBackward0>) tensor(11095.8477, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11095.857421875
tensor(11095.8477, grad_fn=<NegBackward0>) tensor(11095.8574, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11095.8447265625
tensor(11095.8477, grad_fn=<NegBackward0>) tensor(11095.8447, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11095.8447265625
tensor(11095.8447, grad_fn=<NegBackward0>) tensor(11095.8447, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11095.8447265625
tensor(11095.8447, grad_fn=<NegBackward0>) tensor(11095.8447, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11095.84375
tensor(11095.8447, grad_fn=<NegBackward0>) tensor(11095.8438, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11095.8447265625
tensor(11095.8438, grad_fn=<NegBackward0>) tensor(11095.8447, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11095.84375
tensor(11095.8438, grad_fn=<NegBackward0>) tensor(11095.8438, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11095.84375
tensor(11095.8438, grad_fn=<NegBackward0>) tensor(11095.8438, grad_fn=<NegBackward0>)
pi: tensor([[0.7511, 0.2489],
        [0.2255, 0.7745]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4648, 0.5352], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3074, 0.1019],
         [0.5364, 0.1946]],

        [[0.6820, 0.0960],
         [0.5066, 0.5676]],

        [[0.5266, 0.0946],
         [0.6635, 0.5683]],

        [[0.5166, 0.1026],
         [0.6620, 0.6916]],

        [[0.5776, 0.1079],
         [0.6492, 0.6850]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9214429532912113
Average Adjusted Rand Index: 0.9209675637096888
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21774.474609375
inf tensor(21774.4746, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11405.8857421875
tensor(21774.4746, grad_fn=<NegBackward0>) tensor(11405.8857, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11404.5947265625
tensor(11405.8857, grad_fn=<NegBackward0>) tensor(11404.5947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11402.74609375
tensor(11404.5947, grad_fn=<NegBackward0>) tensor(11402.7461, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11399.5322265625
tensor(11402.7461, grad_fn=<NegBackward0>) tensor(11399.5322, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11394.345703125
tensor(11399.5322, grad_fn=<NegBackward0>) tensor(11394.3457, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11374.330078125
tensor(11394.3457, grad_fn=<NegBackward0>) tensor(11374.3301, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11188.0537109375
tensor(11374.3301, grad_fn=<NegBackward0>) tensor(11188.0537, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11144.126953125
tensor(11188.0537, grad_fn=<NegBackward0>) tensor(11144.1270, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11124.544921875
tensor(11144.1270, grad_fn=<NegBackward0>) tensor(11124.5449, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11113.6435546875
tensor(11124.5449, grad_fn=<NegBackward0>) tensor(11113.6436, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11113.0126953125
tensor(11113.6436, grad_fn=<NegBackward0>) tensor(11113.0127, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11109.16015625
tensor(11113.0127, grad_fn=<NegBackward0>) tensor(11109.1602, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11103.8115234375
tensor(11109.1602, grad_fn=<NegBackward0>) tensor(11103.8115, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11103.7421875
tensor(11103.8115, grad_fn=<NegBackward0>) tensor(11103.7422, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11103.6923828125
tensor(11103.7422, grad_fn=<NegBackward0>) tensor(11103.6924, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11103.6513671875
tensor(11103.6924, grad_fn=<NegBackward0>) tensor(11103.6514, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11103.60546875
tensor(11103.6514, grad_fn=<NegBackward0>) tensor(11103.6055, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11099.4814453125
tensor(11103.6055, grad_fn=<NegBackward0>) tensor(11099.4814, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11098.3994140625
tensor(11099.4814, grad_fn=<NegBackward0>) tensor(11098.3994, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11098.365234375
tensor(11098.3994, grad_fn=<NegBackward0>) tensor(11098.3652, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11098.3408203125
tensor(11098.3652, grad_fn=<NegBackward0>) tensor(11098.3408, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11098.3271484375
tensor(11098.3408, grad_fn=<NegBackward0>) tensor(11098.3271, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11098.318359375
tensor(11098.3271, grad_fn=<NegBackward0>) tensor(11098.3184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11098.30859375
tensor(11098.3184, grad_fn=<NegBackward0>) tensor(11098.3086, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11098.3017578125
tensor(11098.3086, grad_fn=<NegBackward0>) tensor(11098.3018, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11098.2919921875
tensor(11098.3018, grad_fn=<NegBackward0>) tensor(11098.2920, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11098.2822265625
tensor(11098.2920, grad_fn=<NegBackward0>) tensor(11098.2822, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11098.275390625
tensor(11098.2822, grad_fn=<NegBackward0>) tensor(11098.2754, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11098.2685546875
tensor(11098.2754, grad_fn=<NegBackward0>) tensor(11098.2686, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11098.2666015625
tensor(11098.2686, grad_fn=<NegBackward0>) tensor(11098.2666, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11098.232421875
tensor(11098.2666, grad_fn=<NegBackward0>) tensor(11098.2324, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11096.0732421875
tensor(11098.2324, grad_fn=<NegBackward0>) tensor(11096.0732, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11095.99609375
tensor(11096.0732, grad_fn=<NegBackward0>) tensor(11095.9961, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11095.9912109375
tensor(11095.9961, grad_fn=<NegBackward0>) tensor(11095.9912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11095.98828125
tensor(11095.9912, grad_fn=<NegBackward0>) tensor(11095.9883, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11095.9814453125
tensor(11095.9883, grad_fn=<NegBackward0>) tensor(11095.9814, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11095.9677734375
tensor(11095.9814, grad_fn=<NegBackward0>) tensor(11095.9678, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11095.8310546875
tensor(11095.9678, grad_fn=<NegBackward0>) tensor(11095.8311, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11095.826171875
tensor(11095.8311, grad_fn=<NegBackward0>) tensor(11095.8262, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11095.828125
tensor(11095.8262, grad_fn=<NegBackward0>) tensor(11095.8281, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11095.8203125
tensor(11095.8262, grad_fn=<NegBackward0>) tensor(11095.8203, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11095.81640625
tensor(11095.8203, grad_fn=<NegBackward0>) tensor(11095.8164, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11095.8115234375
tensor(11095.8164, grad_fn=<NegBackward0>) tensor(11095.8115, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11095.806640625
tensor(11095.8115, grad_fn=<NegBackward0>) tensor(11095.8066, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11095.8046875
tensor(11095.8066, grad_fn=<NegBackward0>) tensor(11095.8047, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11095.8037109375
tensor(11095.8047, grad_fn=<NegBackward0>) tensor(11095.8037, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11095.8017578125
tensor(11095.8037, grad_fn=<NegBackward0>) tensor(11095.8018, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11095.7978515625
tensor(11095.8018, grad_fn=<NegBackward0>) tensor(11095.7979, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11095.7958984375
tensor(11095.7979, grad_fn=<NegBackward0>) tensor(11095.7959, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11095.7958984375
tensor(11095.7959, grad_fn=<NegBackward0>) tensor(11095.7959, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11095.7939453125
tensor(11095.7959, grad_fn=<NegBackward0>) tensor(11095.7939, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11095.7939453125
tensor(11095.7939, grad_fn=<NegBackward0>) tensor(11095.7939, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11095.7998046875
tensor(11095.7939, grad_fn=<NegBackward0>) tensor(11095.7998, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11095.7919921875
tensor(11095.7939, grad_fn=<NegBackward0>) tensor(11095.7920, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11095.7939453125
tensor(11095.7920, grad_fn=<NegBackward0>) tensor(11095.7939, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11095.791015625
tensor(11095.7920, grad_fn=<NegBackward0>) tensor(11095.7910, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11095.7900390625
tensor(11095.7910, grad_fn=<NegBackward0>) tensor(11095.7900, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11095.7890625
tensor(11095.7900, grad_fn=<NegBackward0>) tensor(11095.7891, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11095.7890625
tensor(11095.7891, grad_fn=<NegBackward0>) tensor(11095.7891, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11095.7890625
tensor(11095.7891, grad_fn=<NegBackward0>) tensor(11095.7891, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11095.78515625
tensor(11095.7891, grad_fn=<NegBackward0>) tensor(11095.7852, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11095.7841796875
tensor(11095.7852, grad_fn=<NegBackward0>) tensor(11095.7842, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11095.783203125
tensor(11095.7842, grad_fn=<NegBackward0>) tensor(11095.7832, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11095.7822265625
tensor(11095.7832, grad_fn=<NegBackward0>) tensor(11095.7822, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11095.783203125
tensor(11095.7822, grad_fn=<NegBackward0>) tensor(11095.7832, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11095.78125
tensor(11095.7822, grad_fn=<NegBackward0>) tensor(11095.7812, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11095.7822265625
tensor(11095.7812, grad_fn=<NegBackward0>) tensor(11095.7822, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11095.78125
tensor(11095.7812, grad_fn=<NegBackward0>) tensor(11095.7812, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11095.7802734375
tensor(11095.7812, grad_fn=<NegBackward0>) tensor(11095.7803, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11095.779296875
tensor(11095.7803, grad_fn=<NegBackward0>) tensor(11095.7793, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11095.779296875
tensor(11095.7793, grad_fn=<NegBackward0>) tensor(11095.7793, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11095.7783203125
tensor(11095.7793, grad_fn=<NegBackward0>) tensor(11095.7783, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11095.7783203125
tensor(11095.7783, grad_fn=<NegBackward0>) tensor(11095.7783, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11095.77734375
tensor(11095.7783, grad_fn=<NegBackward0>) tensor(11095.7773, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11095.77734375
tensor(11095.7773, grad_fn=<NegBackward0>) tensor(11095.7773, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11095.77734375
tensor(11095.7773, grad_fn=<NegBackward0>) tensor(11095.7773, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11095.775390625
tensor(11095.7773, grad_fn=<NegBackward0>) tensor(11095.7754, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11095.775390625
tensor(11095.7754, grad_fn=<NegBackward0>) tensor(11095.7754, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11095.7744140625
tensor(11095.7754, grad_fn=<NegBackward0>) tensor(11095.7744, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11095.7685546875
tensor(11095.7744, grad_fn=<NegBackward0>) tensor(11095.7686, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11095.76953125
tensor(11095.7686, grad_fn=<NegBackward0>) tensor(11095.7695, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11095.767578125
tensor(11095.7686, grad_fn=<NegBackward0>) tensor(11095.7676, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11095.7685546875
tensor(11095.7676, grad_fn=<NegBackward0>) tensor(11095.7686, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11095.7705078125
tensor(11095.7676, grad_fn=<NegBackward0>) tensor(11095.7705, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11095.7666015625
tensor(11095.7676, grad_fn=<NegBackward0>) tensor(11095.7666, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11095.78515625
tensor(11095.7666, grad_fn=<NegBackward0>) tensor(11095.7852, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11095.765625
tensor(11095.7666, grad_fn=<NegBackward0>) tensor(11095.7656, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11095.779296875
tensor(11095.7656, grad_fn=<NegBackward0>) tensor(11095.7793, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11095.765625
tensor(11095.7656, grad_fn=<NegBackward0>) tensor(11095.7656, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11095.79296875
tensor(11095.7656, grad_fn=<NegBackward0>) tensor(11095.7930, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11095.763671875
tensor(11095.7656, grad_fn=<NegBackward0>) tensor(11095.7637, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11095.7685546875
tensor(11095.7637, grad_fn=<NegBackward0>) tensor(11095.7686, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11095.7626953125
tensor(11095.7637, grad_fn=<NegBackward0>) tensor(11095.7627, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11095.7626953125
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7627, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11095.7626953125
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7627, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11095.767578125
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7676, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11095.763671875
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7637, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11095.7626953125
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7627, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11095.76171875
tensor(11095.7627, grad_fn=<NegBackward0>) tensor(11095.7617, grad_fn=<NegBackward0>)
pi: tensor([[0.7513, 0.2487],
        [0.2253, 0.7747]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4645, 0.5355], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3076, 0.1020],
         [0.6636, 0.1945]],

        [[0.5766, 0.0961],
         [0.5423, 0.6187]],

        [[0.6721, 0.0947],
         [0.5883, 0.7296]],

        [[0.7051, 0.1027],
         [0.6745, 0.5565]],

        [[0.6372, 0.1078],
         [0.5467, 0.5802]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9214429532912113
Average Adjusted Rand Index: 0.9209675637096888
[0.9214429532912113, 0.9214429532912113] [0.9209675637096888, 0.9209675637096888] [11095.84375, 11095.7626953125]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11044.622512007372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22800.474609375
inf tensor(22800.4746, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11295.4658203125
tensor(22800.4746, grad_fn=<NegBackward0>) tensor(11295.4658, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11290.8720703125
tensor(11295.4658, grad_fn=<NegBackward0>) tensor(11290.8721, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11289.298828125
tensor(11290.8721, grad_fn=<NegBackward0>) tensor(11289.2988, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11282.111328125
tensor(11289.2988, grad_fn=<NegBackward0>) tensor(11282.1113, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11097.2919921875
tensor(11282.1113, grad_fn=<NegBackward0>) tensor(11097.2920, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11084.845703125
tensor(11097.2920, grad_fn=<NegBackward0>) tensor(11084.8457, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11084.486328125
tensor(11084.8457, grad_fn=<NegBackward0>) tensor(11084.4863, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11081.7099609375
tensor(11084.4863, grad_fn=<NegBackward0>) tensor(11081.7100, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11076.1865234375
tensor(11081.7100, grad_fn=<NegBackward0>) tensor(11076.1865, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11076.095703125
tensor(11076.1865, grad_fn=<NegBackward0>) tensor(11076.0957, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11076.0517578125
tensor(11076.0957, grad_fn=<NegBackward0>) tensor(11076.0518, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11076.0107421875
tensor(11076.0518, grad_fn=<NegBackward0>) tensor(11076.0107, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11075.85546875
tensor(11076.0107, grad_fn=<NegBackward0>) tensor(11075.8555, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11073.2783203125
tensor(11075.8555, grad_fn=<NegBackward0>) tensor(11073.2783, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11072.9638671875
tensor(11073.2783, grad_fn=<NegBackward0>) tensor(11072.9639, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11072.6337890625
tensor(11072.9639, grad_fn=<NegBackward0>) tensor(11072.6338, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11072.298828125
tensor(11072.6338, grad_fn=<NegBackward0>) tensor(11072.2988, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11069.2939453125
tensor(11072.2988, grad_fn=<NegBackward0>) tensor(11069.2939, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11069.2724609375
tensor(11069.2939, grad_fn=<NegBackward0>) tensor(11069.2725, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11069.1025390625
tensor(11069.2725, grad_fn=<NegBackward0>) tensor(11069.1025, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11069.0966796875
tensor(11069.1025, grad_fn=<NegBackward0>) tensor(11069.0967, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11069.0908203125
tensor(11069.0967, grad_fn=<NegBackward0>) tensor(11069.0908, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11069.0859375
tensor(11069.0908, grad_fn=<NegBackward0>) tensor(11069.0859, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11069.05078125
tensor(11069.0859, grad_fn=<NegBackward0>) tensor(11069.0508, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11069.0126953125
tensor(11069.0508, grad_fn=<NegBackward0>) tensor(11069.0127, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11069.0078125
tensor(11069.0127, grad_fn=<NegBackward0>) tensor(11069.0078, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11068.994140625
tensor(11069.0078, grad_fn=<NegBackward0>) tensor(11068.9941, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11068.9453125
tensor(11068.9941, grad_fn=<NegBackward0>) tensor(11068.9453, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11068.9345703125
tensor(11068.9453, grad_fn=<NegBackward0>) tensor(11068.9346, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11068.8056640625
tensor(11068.9346, grad_fn=<NegBackward0>) tensor(11068.8057, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11068.8017578125
tensor(11068.8057, grad_fn=<NegBackward0>) tensor(11068.8018, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11068.796875
tensor(11068.8018, grad_fn=<NegBackward0>) tensor(11068.7969, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11068.7880859375
tensor(11068.7969, grad_fn=<NegBackward0>) tensor(11068.7881, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11068.759765625
tensor(11068.7881, grad_fn=<NegBackward0>) tensor(11068.7598, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11068.759765625
tensor(11068.7598, grad_fn=<NegBackward0>) tensor(11068.7598, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11068.7578125
tensor(11068.7598, grad_fn=<NegBackward0>) tensor(11068.7578, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11068.7587890625
tensor(11068.7578, grad_fn=<NegBackward0>) tensor(11068.7588, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11068.7646484375
tensor(11068.7578, grad_fn=<NegBackward0>) tensor(11068.7646, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11068.755859375
tensor(11068.7578, grad_fn=<NegBackward0>) tensor(11068.7559, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11068.7548828125
tensor(11068.7559, grad_fn=<NegBackward0>) tensor(11068.7549, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11068.7509765625
tensor(11068.7549, grad_fn=<NegBackward0>) tensor(11068.7510, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11068.7509765625
tensor(11068.7510, grad_fn=<NegBackward0>) tensor(11068.7510, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11068.7529296875
tensor(11068.7510, grad_fn=<NegBackward0>) tensor(11068.7529, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11068.75
tensor(11068.7510, grad_fn=<NegBackward0>) tensor(11068.7500, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11068.748046875
tensor(11068.7500, grad_fn=<NegBackward0>) tensor(11068.7480, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11068.7490234375
tensor(11068.7480, grad_fn=<NegBackward0>) tensor(11068.7490, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11068.748046875
tensor(11068.7480, grad_fn=<NegBackward0>) tensor(11068.7480, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11068.7470703125
tensor(11068.7480, grad_fn=<NegBackward0>) tensor(11068.7471, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11068.748046875
tensor(11068.7471, grad_fn=<NegBackward0>) tensor(11068.7480, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11068.7470703125
tensor(11068.7471, grad_fn=<NegBackward0>) tensor(11068.7471, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11068.74609375
tensor(11068.7471, grad_fn=<NegBackward0>) tensor(11068.7461, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11068.7451171875
tensor(11068.7461, grad_fn=<NegBackward0>) tensor(11068.7451, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11068.7490234375
tensor(11068.7451, grad_fn=<NegBackward0>) tensor(11068.7490, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11068.7353515625
tensor(11068.7451, grad_fn=<NegBackward0>) tensor(11068.7354, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11068.7353515625
tensor(11068.7354, grad_fn=<NegBackward0>) tensor(11068.7354, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11068.736328125
tensor(11068.7354, grad_fn=<NegBackward0>) tensor(11068.7363, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11068.736328125
tensor(11068.7354, grad_fn=<NegBackward0>) tensor(11068.7363, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11068.734375
tensor(11068.7354, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11068.734375
tensor(11068.7344, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11068.734375
tensor(11068.7344, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11068.734375
tensor(11068.7344, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11068.734375
tensor(11068.7344, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11068.7333984375
tensor(11068.7344, grad_fn=<NegBackward0>) tensor(11068.7334, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11068.734375
tensor(11068.7334, grad_fn=<NegBackward0>) tensor(11068.7344, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11068.7255859375
tensor(11068.7334, grad_fn=<NegBackward0>) tensor(11068.7256, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11068.7255859375
tensor(11068.7256, grad_fn=<NegBackward0>) tensor(11068.7256, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11068.724609375
tensor(11068.7256, grad_fn=<NegBackward0>) tensor(11068.7246, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11068.7333984375
tensor(11068.7246, grad_fn=<NegBackward0>) tensor(11068.7334, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11068.724609375
tensor(11068.7246, grad_fn=<NegBackward0>) tensor(11068.7246, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11068.7265625
tensor(11068.7246, grad_fn=<NegBackward0>) tensor(11068.7266, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11068.7236328125
tensor(11068.7246, grad_fn=<NegBackward0>) tensor(11068.7236, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11068.7275390625
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7275, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11068.736328125
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7363, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11068.7236328125
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7236, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11068.724609375
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7246, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11068.7236328125
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7236, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11068.7236328125
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7236, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11068.7265625
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7266, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11068.728515625
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7285, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11068.724609375
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.7246, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11068.62109375
tensor(11068.7236, grad_fn=<NegBackward0>) tensor(11068.6211, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11068.6181640625
tensor(11068.6211, grad_fn=<NegBackward0>) tensor(11068.6182, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11068.6201171875
tensor(11068.6182, grad_fn=<NegBackward0>) tensor(11068.6201, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11068.6171875
tensor(11068.6182, grad_fn=<NegBackward0>) tensor(11068.6172, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11068.6279296875
tensor(11068.6172, grad_fn=<NegBackward0>) tensor(11068.6279, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11068.615234375
tensor(11068.6172, grad_fn=<NegBackward0>) tensor(11068.6152, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11068.6201171875
tensor(11068.6152, grad_fn=<NegBackward0>) tensor(11068.6201, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11068.6005859375
tensor(11068.6152, grad_fn=<NegBackward0>) tensor(11068.6006, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11068.6005859375
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6006, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11068.603515625
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6035, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11068.6015625
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6016, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11068.6064453125
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6064, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11068.6318359375
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6318, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11068.6025390625
tensor(11068.6006, grad_fn=<NegBackward0>) tensor(11068.6025, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.6628, 0.3372],
        [0.2920, 0.7080]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9299, 0.0701], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1843, 0.0933],
         [0.5027, 0.3070]],

        [[0.7194, 0.0986],
         [0.5810, 0.7077]],

        [[0.6252, 0.0994],
         [0.6637, 0.5524]],

        [[0.6251, 0.1080],
         [0.6825, 0.6998]],

        [[0.7047, 0.0902],
         [0.5420, 0.5440]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.022328462449853457
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8447743642510657
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6201571202306192
Average Adjusted Rand Index: 0.7406500128515879
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23912.939453125
inf tensor(23912.9395, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11293.5791015625
tensor(23912.9395, grad_fn=<NegBackward0>) tensor(11293.5791, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11291.3408203125
tensor(11293.5791, grad_fn=<NegBackward0>) tensor(11291.3408, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11289.517578125
tensor(11291.3408, grad_fn=<NegBackward0>) tensor(11289.5176, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11283.2197265625
tensor(11289.5176, grad_fn=<NegBackward0>) tensor(11283.2197, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11079.65625
tensor(11283.2197, grad_fn=<NegBackward0>) tensor(11079.6562, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11073.62109375
tensor(11079.6562, grad_fn=<NegBackward0>) tensor(11073.6211, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11073.2646484375
tensor(11073.6211, grad_fn=<NegBackward0>) tensor(11073.2646, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11073.1328125
tensor(11073.2646, grad_fn=<NegBackward0>) tensor(11073.1328, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11073.025390625
tensor(11073.1328, grad_fn=<NegBackward0>) tensor(11073.0254, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11072.6142578125
tensor(11073.0254, grad_fn=<NegBackward0>) tensor(11072.6143, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11072.580078125
tensor(11072.6143, grad_fn=<NegBackward0>) tensor(11072.5801, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11072.5556640625
tensor(11072.5801, grad_fn=<NegBackward0>) tensor(11072.5557, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11072.541015625
tensor(11072.5557, grad_fn=<NegBackward0>) tensor(11072.5410, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11072.5283203125
tensor(11072.5410, grad_fn=<NegBackward0>) tensor(11072.5283, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11072.517578125
tensor(11072.5283, grad_fn=<NegBackward0>) tensor(11072.5176, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11072.5078125
tensor(11072.5176, grad_fn=<NegBackward0>) tensor(11072.5078, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11072.5009765625
tensor(11072.5078, grad_fn=<NegBackward0>) tensor(11072.5010, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11072.490234375
tensor(11072.5010, grad_fn=<NegBackward0>) tensor(11072.4902, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11072.48046875
tensor(11072.4902, grad_fn=<NegBackward0>) tensor(11072.4805, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11072.4619140625
tensor(11072.4805, grad_fn=<NegBackward0>) tensor(11072.4619, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11072.427734375
tensor(11072.4619, grad_fn=<NegBackward0>) tensor(11072.4277, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11072.263671875
tensor(11072.4277, grad_fn=<NegBackward0>) tensor(11072.2637, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11069.5263671875
tensor(11072.2637, grad_fn=<NegBackward0>) tensor(11069.5264, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11068.810546875
tensor(11069.5264, grad_fn=<NegBackward0>) tensor(11068.8105, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11068.65625
tensor(11068.8105, grad_fn=<NegBackward0>) tensor(11068.6562, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11068.6318359375
tensor(11068.6562, grad_fn=<NegBackward0>) tensor(11068.6318, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11068.625
tensor(11068.6318, grad_fn=<NegBackward0>) tensor(11068.6250, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11068.62109375
tensor(11068.6250, grad_fn=<NegBackward0>) tensor(11068.6211, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11068.6201171875
tensor(11068.6211, grad_fn=<NegBackward0>) tensor(11068.6201, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11068.6181640625
tensor(11068.6201, grad_fn=<NegBackward0>) tensor(11068.6182, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11068.6171875
tensor(11068.6182, grad_fn=<NegBackward0>) tensor(11068.6172, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11068.6171875
tensor(11068.6172, grad_fn=<NegBackward0>) tensor(11068.6172, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11068.615234375
tensor(11068.6172, grad_fn=<NegBackward0>) tensor(11068.6152, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11068.6142578125
tensor(11068.6152, grad_fn=<NegBackward0>) tensor(11068.6143, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11068.615234375
tensor(11068.6143, grad_fn=<NegBackward0>) tensor(11068.6152, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11068.6162109375
tensor(11068.6143, grad_fn=<NegBackward0>) tensor(11068.6162, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11068.615234375
tensor(11068.6143, grad_fn=<NegBackward0>) tensor(11068.6152, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -11068.61328125
tensor(11068.6143, grad_fn=<NegBackward0>) tensor(11068.6133, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11068.6142578125
tensor(11068.6133, grad_fn=<NegBackward0>) tensor(11068.6143, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11068.6123046875
tensor(11068.6133, grad_fn=<NegBackward0>) tensor(11068.6123, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11068.6181640625
tensor(11068.6123, grad_fn=<NegBackward0>) tensor(11068.6182, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11068.611328125
tensor(11068.6123, grad_fn=<NegBackward0>) tensor(11068.6113, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11068.611328125
tensor(11068.6113, grad_fn=<NegBackward0>) tensor(11068.6113, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11068.623046875
tensor(11068.6113, grad_fn=<NegBackward0>) tensor(11068.6230, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11068.6103515625
tensor(11068.6113, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11068.6259765625
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6260, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11068.6103515625
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11068.6103515625
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11068.6123046875
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6123, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11068.6103515625
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11068.609375
tensor(11068.6104, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11068.609375
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11068.6103515625
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11068.6103515625
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11068.6103515625
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11068.611328125
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6113, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -11068.609375
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11068.6083984375
tensor(11068.6094, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11068.609375
tensor(11068.6084, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11068.609375
tensor(11068.6084, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11068.609375
tensor(11068.6084, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11068.609375
tensor(11068.6084, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11068.607421875
tensor(11068.6084, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11068.6083984375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11068.607421875
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11068.6083984375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11068.607421875
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11068.607421875
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11068.6083984375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11068.607421875
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11068.6083984375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11068.6083984375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6084, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11068.607421875
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6074, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11068.609375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6094, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11068.6142578125
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.6143, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11068.599609375
tensor(11068.6074, grad_fn=<NegBackward0>) tensor(11068.5996, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11068.599609375
tensor(11068.5996, grad_fn=<NegBackward0>) tensor(11068.5996, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11068.599609375
tensor(11068.5996, grad_fn=<NegBackward0>) tensor(11068.5996, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11068.6015625
tensor(11068.5996, grad_fn=<NegBackward0>) tensor(11068.6016, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11068.5986328125
tensor(11068.5996, grad_fn=<NegBackward0>) tensor(11068.5986, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11068.5986328125
tensor(11068.5986, grad_fn=<NegBackward0>) tensor(11068.5986, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11068.599609375
tensor(11068.5986, grad_fn=<NegBackward0>) tensor(11068.5996, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11068.5986328125
tensor(11068.5986, grad_fn=<NegBackward0>) tensor(11068.5986, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11068.623046875
tensor(11068.5986, grad_fn=<NegBackward0>) tensor(11068.6230, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11068.59765625
tensor(11068.5986, grad_fn=<NegBackward0>) tensor(11068.5977, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11068.5966796875
tensor(11068.5977, grad_fn=<NegBackward0>) tensor(11068.5967, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11068.5986328125
tensor(11068.5967, grad_fn=<NegBackward0>) tensor(11068.5986, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11068.603515625
tensor(11068.5967, grad_fn=<NegBackward0>) tensor(11068.6035, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11068.63671875
tensor(11068.5967, grad_fn=<NegBackward0>) tensor(11068.6367, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11068.59765625
tensor(11068.5967, grad_fn=<NegBackward0>) tensor(11068.5977, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11068.6103515625
tensor(11068.5967, grad_fn=<NegBackward0>) tensor(11068.6104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.7126, 0.2874],
        [0.3322, 0.6678]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0705, 0.9295], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3066, 0.0933],
         [0.6952, 0.1844]],

        [[0.6809, 0.0986],
         [0.5511, 0.6201]],

        [[0.6672, 0.0994],
         [0.6886, 0.5439]],

        [[0.7106, 0.1079],
         [0.7137, 0.6641]],

        [[0.7248, 0.0902],
         [0.6455, 0.6973]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.022328462449853457
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6264912671691717
Average Adjusted Rand Index: 0.7481716015203148
[0.6201571202306192, 0.6264912671691717] [0.7406500128515879, 0.7481716015203148] [11068.6025390625, 11068.6103515625]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11310.035844230853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21512.79296875
inf tensor(21512.7930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11555.3291015625
tensor(21512.7930, grad_fn=<NegBackward0>) tensor(11555.3291, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11550.34765625
tensor(11555.3291, grad_fn=<NegBackward0>) tensor(11550.3477, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11516.5029296875
tensor(11550.3477, grad_fn=<NegBackward0>) tensor(11516.5029, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11506.7109375
tensor(11516.5029, grad_fn=<NegBackward0>) tensor(11506.7109, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11504.556640625
tensor(11506.7109, grad_fn=<NegBackward0>) tensor(11504.5566, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11500.4111328125
tensor(11504.5566, grad_fn=<NegBackward0>) tensor(11500.4111, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11498.3544921875
tensor(11500.4111, grad_fn=<NegBackward0>) tensor(11498.3545, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11494.783203125
tensor(11498.3545, grad_fn=<NegBackward0>) tensor(11494.7832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11405.2021484375
tensor(11494.7832, grad_fn=<NegBackward0>) tensor(11405.2021, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11381.7783203125
tensor(11405.2021, grad_fn=<NegBackward0>) tensor(11381.7783, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11370.7490234375
tensor(11381.7783, grad_fn=<NegBackward0>) tensor(11370.7490, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11366.822265625
tensor(11370.7490, grad_fn=<NegBackward0>) tensor(11366.8223, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11366.6923828125
tensor(11366.8223, grad_fn=<NegBackward0>) tensor(11366.6924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11366.6484375
tensor(11366.6924, grad_fn=<NegBackward0>) tensor(11366.6484, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11366.603515625
tensor(11366.6484, grad_fn=<NegBackward0>) tensor(11366.6035, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11366.578125
tensor(11366.6035, grad_fn=<NegBackward0>) tensor(11366.5781, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11366.5673828125
tensor(11366.5781, grad_fn=<NegBackward0>) tensor(11366.5674, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11366.5546875
tensor(11366.5674, grad_fn=<NegBackward0>) tensor(11366.5547, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11363.521484375
tensor(11366.5547, grad_fn=<NegBackward0>) tensor(11363.5215, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11363.51171875
tensor(11363.5215, grad_fn=<NegBackward0>) tensor(11363.5117, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11363.5068359375
tensor(11363.5117, grad_fn=<NegBackward0>) tensor(11363.5068, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11363.5029296875
tensor(11363.5068, grad_fn=<NegBackward0>) tensor(11363.5029, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11363.501953125
tensor(11363.5029, grad_fn=<NegBackward0>) tensor(11363.5020, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11363.4990234375
tensor(11363.5020, grad_fn=<NegBackward0>) tensor(11363.4990, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11363.49609375
tensor(11363.4990, grad_fn=<NegBackward0>) tensor(11363.4961, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11363.4892578125
tensor(11363.4961, grad_fn=<NegBackward0>) tensor(11363.4893, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11363.3544921875
tensor(11363.4893, grad_fn=<NegBackward0>) tensor(11363.3545, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11363.3076171875
tensor(11363.3545, grad_fn=<NegBackward0>) tensor(11363.3076, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11363.2998046875
tensor(11363.3076, grad_fn=<NegBackward0>) tensor(11363.2998, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11363.298828125
tensor(11363.2998, grad_fn=<NegBackward0>) tensor(11363.2988, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11363.29296875
tensor(11363.2988, grad_fn=<NegBackward0>) tensor(11363.2930, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11363.2685546875
tensor(11363.2930, grad_fn=<NegBackward0>) tensor(11363.2686, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11363.0654296875
tensor(11363.2686, grad_fn=<NegBackward0>) tensor(11363.0654, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11363.0576171875
tensor(11363.0654, grad_fn=<NegBackward0>) tensor(11363.0576, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11363.0546875
tensor(11363.0576, grad_fn=<NegBackward0>) tensor(11363.0547, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11363.052734375
tensor(11363.0547, grad_fn=<NegBackward0>) tensor(11363.0527, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11363.0498046875
tensor(11363.0527, grad_fn=<NegBackward0>) tensor(11363.0498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11363.0439453125
tensor(11363.0498, grad_fn=<NegBackward0>) tensor(11363.0439, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11363.04296875
tensor(11363.0439, grad_fn=<NegBackward0>) tensor(11363.0430, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11363.037109375
tensor(11363.0430, grad_fn=<NegBackward0>) tensor(11363.0371, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11362.8251953125
tensor(11363.0371, grad_fn=<NegBackward0>) tensor(11362.8252, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11362.6982421875
tensor(11362.8252, grad_fn=<NegBackward0>) tensor(11362.6982, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11362.69921875
tensor(11362.6982, grad_fn=<NegBackward0>) tensor(11362.6992, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11362.6953125
tensor(11362.6982, grad_fn=<NegBackward0>) tensor(11362.6953, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11362.6953125
tensor(11362.6953, grad_fn=<NegBackward0>) tensor(11362.6953, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11362.69140625
tensor(11362.6953, grad_fn=<NegBackward0>) tensor(11362.6914, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11362.697265625
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6973, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11362.69140625
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6914, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11362.693359375
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6934, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11362.6982421875
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6982, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11362.6943359375
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6943, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11362.6884765625
tensor(11362.6914, grad_fn=<NegBackward0>) tensor(11362.6885, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11362.6875
tensor(11362.6885, grad_fn=<NegBackward0>) tensor(11362.6875, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11362.689453125
tensor(11362.6875, grad_fn=<NegBackward0>) tensor(11362.6895, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11362.6875
tensor(11362.6875, grad_fn=<NegBackward0>) tensor(11362.6875, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11362.6884765625
tensor(11362.6875, grad_fn=<NegBackward0>) tensor(11362.6885, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11362.6865234375
tensor(11362.6875, grad_fn=<NegBackward0>) tensor(11362.6865, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11362.6884765625
tensor(11362.6865, grad_fn=<NegBackward0>) tensor(11362.6885, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11362.6865234375
tensor(11362.6865, grad_fn=<NegBackward0>) tensor(11362.6865, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11362.677734375
tensor(11362.6865, grad_fn=<NegBackward0>) tensor(11362.6777, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11362.67578125
tensor(11362.6777, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11362.67578125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11362.67578125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11362.677734375
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6777, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11362.681640625
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6816, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11362.67578125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11362.6767578125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11362.6767578125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11362.6845703125
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6846, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11362.6748046875
tensor(11362.6758, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11362.67578125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11362.6767578125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11362.6767578125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11362.6787109375
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6787, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11362.6748046875
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11362.6806640625
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6807, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11362.6748046875
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11362.67578125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6758, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11362.673828125
tensor(11362.6748, grad_fn=<NegBackward0>) tensor(11362.6738, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11362.6767578125
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11362.6748046875
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11362.6796875
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6797, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11362.6767578125
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6768, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -11362.6748046875
tensor(11362.6738, grad_fn=<NegBackward0>) tensor(11362.6748, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.4127, 0.5873],
        [0.5914, 0.4086]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4151, 0.5849], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2378, 0.1007],
         [0.5891, 0.2648]],

        [[0.7299, 0.0924],
         [0.5646, 0.5879]],

        [[0.6163, 0.1053],
         [0.7255, 0.6810]],

        [[0.5392, 0.1035],
         [0.5946, 0.6621]],

        [[0.7031, 0.1040],
         [0.7114, 0.5930]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.772135496372924
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.03045820558007483
Average Adjusted Rand Index: 0.779317416096141
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22188.384765625
inf tensor(22188.3848, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11557.0625
tensor(22188.3848, grad_fn=<NegBackward0>) tensor(11557.0625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11555.01171875
tensor(11557.0625, grad_fn=<NegBackward0>) tensor(11555.0117, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11553.0654296875
tensor(11555.0117, grad_fn=<NegBackward0>) tensor(11553.0654, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11549.3544921875
tensor(11553.0654, grad_fn=<NegBackward0>) tensor(11549.3545, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11543.291015625
tensor(11549.3545, grad_fn=<NegBackward0>) tensor(11543.2910, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11449.0498046875
tensor(11543.2910, grad_fn=<NegBackward0>) tensor(11449.0498, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11333.966796875
tensor(11449.0498, grad_fn=<NegBackward0>) tensor(11333.9668, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11311.9970703125
tensor(11333.9668, grad_fn=<NegBackward0>) tensor(11311.9971, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11302.2919921875
tensor(11311.9971, grad_fn=<NegBackward0>) tensor(11302.2920, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11296.4453125
tensor(11302.2920, grad_fn=<NegBackward0>) tensor(11296.4453, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11293.8193359375
tensor(11296.4453, grad_fn=<NegBackward0>) tensor(11293.8193, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11292.947265625
tensor(11293.8193, grad_fn=<NegBackward0>) tensor(11292.9473, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11288.9423828125
tensor(11292.9473, grad_fn=<NegBackward0>) tensor(11288.9424, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11288.8720703125
tensor(11288.9424, grad_fn=<NegBackward0>) tensor(11288.8721, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11288.7841796875
tensor(11288.8721, grad_fn=<NegBackward0>) tensor(11288.7842, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11285.3349609375
tensor(11288.7842, grad_fn=<NegBackward0>) tensor(11285.3350, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11285.224609375
tensor(11285.3350, grad_fn=<NegBackward0>) tensor(11285.2246, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11285.2060546875
tensor(11285.2246, grad_fn=<NegBackward0>) tensor(11285.2061, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11285.1943359375
tensor(11285.2061, grad_fn=<NegBackward0>) tensor(11285.1943, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11285.185546875
tensor(11285.1943, grad_fn=<NegBackward0>) tensor(11285.1855, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11285.17578125
tensor(11285.1855, grad_fn=<NegBackward0>) tensor(11285.1758, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11285.169921875
tensor(11285.1758, grad_fn=<NegBackward0>) tensor(11285.1699, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11285.1650390625
tensor(11285.1699, grad_fn=<NegBackward0>) tensor(11285.1650, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11285.1474609375
tensor(11285.1650, grad_fn=<NegBackward0>) tensor(11285.1475, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11285.130859375
tensor(11285.1475, grad_fn=<NegBackward0>) tensor(11285.1309, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11285.1298828125
tensor(11285.1309, grad_fn=<NegBackward0>) tensor(11285.1299, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11285.123046875
tensor(11285.1299, grad_fn=<NegBackward0>) tensor(11285.1230, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11285.119140625
tensor(11285.1230, grad_fn=<NegBackward0>) tensor(11285.1191, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11285.1162109375
tensor(11285.1191, grad_fn=<NegBackward0>) tensor(11285.1162, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11285.1142578125
tensor(11285.1162, grad_fn=<NegBackward0>) tensor(11285.1143, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11285.1044921875
tensor(11285.1143, grad_fn=<NegBackward0>) tensor(11285.1045, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11285.09375
tensor(11285.1045, grad_fn=<NegBackward0>) tensor(11285.0938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11285.09375
tensor(11285.0938, grad_fn=<NegBackward0>) tensor(11285.0938, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11285.083984375
tensor(11285.0938, grad_fn=<NegBackward0>) tensor(11285.0840, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11280.59765625
tensor(11285.0840, grad_fn=<NegBackward0>) tensor(11280.5977, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11280.595703125
tensor(11280.5977, grad_fn=<NegBackward0>) tensor(11280.5957, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11280.5927734375
tensor(11280.5957, grad_fn=<NegBackward0>) tensor(11280.5928, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11280.5908203125
tensor(11280.5928, grad_fn=<NegBackward0>) tensor(11280.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11280.58984375
tensor(11280.5908, grad_fn=<NegBackward0>) tensor(11280.5898, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11280.564453125
tensor(11280.5898, grad_fn=<NegBackward0>) tensor(11280.5645, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11280.5517578125
tensor(11280.5645, grad_fn=<NegBackward0>) tensor(11280.5518, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11280.5498046875
tensor(11280.5518, grad_fn=<NegBackward0>) tensor(11280.5498, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11280.5556640625
tensor(11280.5498, grad_fn=<NegBackward0>) tensor(11280.5557, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11280.5478515625
tensor(11280.5498, grad_fn=<NegBackward0>) tensor(11280.5479, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11280.5498046875
tensor(11280.5479, grad_fn=<NegBackward0>) tensor(11280.5498, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11280.529296875
tensor(11280.5479, grad_fn=<NegBackward0>) tensor(11280.5293, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11280.5263671875
tensor(11280.5293, grad_fn=<NegBackward0>) tensor(11280.5264, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11280.5263671875
tensor(11280.5264, grad_fn=<NegBackward0>) tensor(11280.5264, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11280.525390625
tensor(11280.5264, grad_fn=<NegBackward0>) tensor(11280.5254, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11280.5185546875
tensor(11280.5254, grad_fn=<NegBackward0>) tensor(11280.5186, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11280.521484375
tensor(11280.5186, grad_fn=<NegBackward0>) tensor(11280.5215, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11280.513671875
tensor(11280.5186, grad_fn=<NegBackward0>) tensor(11280.5137, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11280.513671875
tensor(11280.5137, grad_fn=<NegBackward0>) tensor(11280.5137, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11280.5126953125
tensor(11280.5137, grad_fn=<NegBackward0>) tensor(11280.5127, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11280.5146484375
tensor(11280.5127, grad_fn=<NegBackward0>) tensor(11280.5146, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11280.5048828125
tensor(11280.5127, grad_fn=<NegBackward0>) tensor(11280.5049, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11280.4853515625
tensor(11280.5049, grad_fn=<NegBackward0>) tensor(11280.4854, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11280.0869140625
tensor(11280.4854, grad_fn=<NegBackward0>) tensor(11280.0869, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11280.0712890625
tensor(11280.0869, grad_fn=<NegBackward0>) tensor(11280.0713, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11280.0712890625
tensor(11280.0713, grad_fn=<NegBackward0>) tensor(11280.0713, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11280.083984375
tensor(11280.0713, grad_fn=<NegBackward0>) tensor(11280.0840, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11280.0703125
tensor(11280.0713, grad_fn=<NegBackward0>) tensor(11280.0703, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11280.0712890625
tensor(11280.0703, grad_fn=<NegBackward0>) tensor(11280.0713, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11280.076171875
tensor(11280.0703, grad_fn=<NegBackward0>) tensor(11280.0762, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11280.0703125
tensor(11280.0703, grad_fn=<NegBackward0>) tensor(11280.0703, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11280.0693359375
tensor(11280.0703, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11280.0693359375
tensor(11280.0693, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11280.068359375
tensor(11280.0693, grad_fn=<NegBackward0>) tensor(11280.0684, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11280.0693359375
tensor(11280.0684, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11280.0693359375
tensor(11280.0684, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11280.0693359375
tensor(11280.0684, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11280.080078125
tensor(11280.0684, grad_fn=<NegBackward0>) tensor(11280.0801, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11280.0693359375
tensor(11280.0684, grad_fn=<NegBackward0>) tensor(11280.0693, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7267, 0.2733],
        [0.2608, 0.7392]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5670, 0.4330], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2979, 0.1026],
         [0.5843, 0.1984]],

        [[0.6224, 0.0956],
         [0.5362, 0.5025]],

        [[0.6804, 0.1072],
         [0.5982, 0.5425]],

        [[0.5692, 0.1052],
         [0.5661, 0.7262]],

        [[0.7182, 0.1064],
         [0.5431, 0.5821]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9061162228126606
Average Adjusted Rand Index: 0.9062599695771268
[0.03045820558007483, 0.9061162228126606] [0.779317416096141, 0.9062599695771268] [11362.6748046875, 11280.0693359375]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11246.202980085178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23822.97265625
inf tensor(23822.9727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11526.09765625
tensor(23822.9727, grad_fn=<NegBackward0>) tensor(11526.0977, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11522.576171875
tensor(11526.0977, grad_fn=<NegBackward0>) tensor(11522.5762, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11520.6474609375
tensor(11522.5762, grad_fn=<NegBackward0>) tensor(11520.6475, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11519.6708984375
tensor(11520.6475, grad_fn=<NegBackward0>) tensor(11519.6709, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11519.021484375
tensor(11519.6709, grad_fn=<NegBackward0>) tensor(11519.0215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11518.4599609375
tensor(11519.0215, grad_fn=<NegBackward0>) tensor(11518.4600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11517.9140625
tensor(11518.4600, grad_fn=<NegBackward0>) tensor(11517.9141, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11517.353515625
tensor(11517.9141, grad_fn=<NegBackward0>) tensor(11517.3535, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11516.6943359375
tensor(11517.3535, grad_fn=<NegBackward0>) tensor(11516.6943, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11516.03125
tensor(11516.6943, grad_fn=<NegBackward0>) tensor(11516.0312, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11515.30078125
tensor(11516.0312, grad_fn=<NegBackward0>) tensor(11515.3008, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11514.3486328125
tensor(11515.3008, grad_fn=<NegBackward0>) tensor(11514.3486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11513.634765625
tensor(11514.3486, grad_fn=<NegBackward0>) tensor(11513.6348, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11513.3896484375
tensor(11513.6348, grad_fn=<NegBackward0>) tensor(11513.3896, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11513.2763671875
tensor(11513.3896, grad_fn=<NegBackward0>) tensor(11513.2764, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11513.1904296875
tensor(11513.2764, grad_fn=<NegBackward0>) tensor(11513.1904, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11513.044921875
tensor(11513.1904, grad_fn=<NegBackward0>) tensor(11513.0449, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11512.7822265625
tensor(11513.0449, grad_fn=<NegBackward0>) tensor(11512.7822, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11512.5634765625
tensor(11512.7822, grad_fn=<NegBackward0>) tensor(11512.5635, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11512.4189453125
tensor(11512.5635, grad_fn=<NegBackward0>) tensor(11512.4189, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11512.3505859375
tensor(11512.4189, grad_fn=<NegBackward0>) tensor(11512.3506, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11512.2998046875
tensor(11512.3506, grad_fn=<NegBackward0>) tensor(11512.2998, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11512.26171875
tensor(11512.2998, grad_fn=<NegBackward0>) tensor(11512.2617, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11512.1884765625
tensor(11512.2617, grad_fn=<NegBackward0>) tensor(11512.1885, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11476.3671875
tensor(11512.1885, grad_fn=<NegBackward0>) tensor(11476.3672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11295.494140625
tensor(11476.3672, grad_fn=<NegBackward0>) tensor(11295.4941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11288.3603515625
tensor(11295.4941, grad_fn=<NegBackward0>) tensor(11288.3604, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11288.267578125
tensor(11288.3604, grad_fn=<NegBackward0>) tensor(11288.2676, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11288.2490234375
tensor(11288.2676, grad_fn=<NegBackward0>) tensor(11288.2490, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11288.2392578125
tensor(11288.2490, grad_fn=<NegBackward0>) tensor(11288.2393, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11288.234375
tensor(11288.2393, grad_fn=<NegBackward0>) tensor(11288.2344, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11288.2333984375
tensor(11288.2344, grad_fn=<NegBackward0>) tensor(11288.2334, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11288.2275390625
tensor(11288.2334, grad_fn=<NegBackward0>) tensor(11288.2275, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11288.2236328125
tensor(11288.2275, grad_fn=<NegBackward0>) tensor(11288.2236, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11288.2197265625
tensor(11288.2236, grad_fn=<NegBackward0>) tensor(11288.2197, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11288.2080078125
tensor(11288.2197, grad_fn=<NegBackward0>) tensor(11288.2080, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11288.205078125
tensor(11288.2080, grad_fn=<NegBackward0>) tensor(11288.2051, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11283.9912109375
tensor(11288.2051, grad_fn=<NegBackward0>) tensor(11283.9912, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11283.9658203125
tensor(11283.9912, grad_fn=<NegBackward0>) tensor(11283.9658, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11283.951171875
tensor(11283.9658, grad_fn=<NegBackward0>) tensor(11283.9512, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11283.5341796875
tensor(11283.9512, grad_fn=<NegBackward0>) tensor(11283.5342, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11283.5341796875
tensor(11283.5342, grad_fn=<NegBackward0>) tensor(11283.5342, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11283.533203125
tensor(11283.5342, grad_fn=<NegBackward0>) tensor(11283.5332, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11283.5283203125
tensor(11283.5332, grad_fn=<NegBackward0>) tensor(11283.5283, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11280.4287109375
tensor(11283.5283, grad_fn=<NegBackward0>) tensor(11280.4287, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11280.427734375
tensor(11280.4287, grad_fn=<NegBackward0>) tensor(11280.4277, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11280.427734375
tensor(11280.4277, grad_fn=<NegBackward0>) tensor(11280.4277, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11280.427734375
tensor(11280.4277, grad_fn=<NegBackward0>) tensor(11280.4277, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11280.427734375
tensor(11280.4277, grad_fn=<NegBackward0>) tensor(11280.4277, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11280.42578125
tensor(11280.4277, grad_fn=<NegBackward0>) tensor(11280.4258, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11280.4267578125
tensor(11280.4258, grad_fn=<NegBackward0>) tensor(11280.4268, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11280.4267578125
tensor(11280.4258, grad_fn=<NegBackward0>) tensor(11280.4268, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11280.3759765625
tensor(11280.4258, grad_fn=<NegBackward0>) tensor(11280.3760, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11280.2744140625
tensor(11280.3760, grad_fn=<NegBackward0>) tensor(11280.2744, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11280.2724609375
tensor(11280.2744, grad_fn=<NegBackward0>) tensor(11280.2725, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11280.2705078125
tensor(11280.2725, grad_fn=<NegBackward0>) tensor(11280.2705, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11280.2841796875
tensor(11280.2705, grad_fn=<NegBackward0>) tensor(11280.2842, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11280.271484375
tensor(11280.2705, grad_fn=<NegBackward0>) tensor(11280.2715, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11280.2685546875
tensor(11280.2705, grad_fn=<NegBackward0>) tensor(11280.2686, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11280.267578125
tensor(11280.2686, grad_fn=<NegBackward0>) tensor(11280.2676, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11280.275390625
tensor(11280.2676, grad_fn=<NegBackward0>) tensor(11280.2754, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11280.265625
tensor(11280.2676, grad_fn=<NegBackward0>) tensor(11280.2656, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11277.1552734375
tensor(11280.2656, grad_fn=<NegBackward0>) tensor(11277.1553, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11277.1533203125
tensor(11277.1553, grad_fn=<NegBackward0>) tensor(11277.1533, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11277.1513671875
tensor(11277.1533, grad_fn=<NegBackward0>) tensor(11277.1514, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11277.15234375
tensor(11277.1514, grad_fn=<NegBackward0>) tensor(11277.1523, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11277.1533203125
tensor(11277.1514, grad_fn=<NegBackward0>) tensor(11277.1533, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11277.150390625
tensor(11277.1514, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11277.150390625
tensor(11277.1504, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11277.150390625
tensor(11277.1504, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11277.1494140625
tensor(11277.1504, grad_fn=<NegBackward0>) tensor(11277.1494, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11277.150390625
tensor(11277.1494, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11277.150390625
tensor(11277.1494, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11277.150390625
tensor(11277.1494, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11277.150390625
tensor(11277.1494, grad_fn=<NegBackward0>) tensor(11277.1504, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11277.1484375
tensor(11277.1494, grad_fn=<NegBackward0>) tensor(11277.1484, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11277.1484375
tensor(11277.1484, grad_fn=<NegBackward0>) tensor(11277.1484, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11277.1474609375
tensor(11277.1484, grad_fn=<NegBackward0>) tensor(11277.1475, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11277.1455078125
tensor(11277.1475, grad_fn=<NegBackward0>) tensor(11277.1455, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11277.1435546875
tensor(11277.1455, grad_fn=<NegBackward0>) tensor(11277.1436, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11277.13671875
tensor(11277.1436, grad_fn=<NegBackward0>) tensor(11277.1367, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11277.142578125
tensor(11277.1367, grad_fn=<NegBackward0>) tensor(11277.1426, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11277.1376953125
tensor(11277.1367, grad_fn=<NegBackward0>) tensor(11277.1377, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11277.1357421875
tensor(11277.1367, grad_fn=<NegBackward0>) tensor(11277.1357, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11277.1259765625
tensor(11277.1357, grad_fn=<NegBackward0>) tensor(11277.1260, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11277.123046875
tensor(11277.1260, grad_fn=<NegBackward0>) tensor(11277.1230, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11277.1240234375
tensor(11277.1230, grad_fn=<NegBackward0>) tensor(11277.1240, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11277.111328125
tensor(11277.1230, grad_fn=<NegBackward0>) tensor(11277.1113, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11277.1171875
tensor(11277.1113, grad_fn=<NegBackward0>) tensor(11277.1172, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11277.1142578125
tensor(11277.1113, grad_fn=<NegBackward0>) tensor(11277.1143, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11277.111328125
tensor(11277.1113, grad_fn=<NegBackward0>) tensor(11277.1113, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11277.11328125
tensor(11277.1113, grad_fn=<NegBackward0>) tensor(11277.1133, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11277.1103515625
tensor(11277.1113, grad_fn=<NegBackward0>) tensor(11277.1104, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11277.1103515625
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.1104, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11277.1103515625
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.1104, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11277.2421875
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.2422, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11277.1103515625
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.1104, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11277.1279296875
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.1279, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11277.1826171875
tensor(11277.1104, grad_fn=<NegBackward0>) tensor(11277.1826, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7881, 0.2119],
        [0.3347, 0.6653]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0164, 0.9836], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3033, 0.0788],
         [0.7084, 0.1849]],

        [[0.7274, 0.1022],
         [0.6172, 0.6326]],

        [[0.5375, 0.1001],
         [0.6571, 0.6349]],

        [[0.5348, 0.1043],
         [0.7270, 0.5791]],

        [[0.6361, 0.0987],
         [0.5627, 0.6481]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6652033592853569
Average Adjusted Rand Index: 0.7547161375572674
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25326.81640625
inf tensor(25326.8164, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11517.6494140625
tensor(25326.8164, grad_fn=<NegBackward0>) tensor(11517.6494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11515.05078125
tensor(11517.6494, grad_fn=<NegBackward0>) tensor(11515.0508, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11510.373046875
tensor(11515.0508, grad_fn=<NegBackward0>) tensor(11510.3730, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11325.2431640625
tensor(11510.3730, grad_fn=<NegBackward0>) tensor(11325.2432, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11304.552734375
tensor(11325.2432, grad_fn=<NegBackward0>) tensor(11304.5527, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11291.21875
tensor(11304.5527, grad_fn=<NegBackward0>) tensor(11291.2188, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11283.08984375
tensor(11291.2188, grad_fn=<NegBackward0>) tensor(11283.0898, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11282.953125
tensor(11283.0898, grad_fn=<NegBackward0>) tensor(11282.9531, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11279.3271484375
tensor(11282.9531, grad_fn=<NegBackward0>) tensor(11279.3271, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11279.2939453125
tensor(11279.3271, grad_fn=<NegBackward0>) tensor(11279.2939, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11279.2568359375
tensor(11279.2939, grad_fn=<NegBackward0>) tensor(11279.2568, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11279.2138671875
tensor(11279.2568, grad_fn=<NegBackward0>) tensor(11279.2139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11279.203125
tensor(11279.2139, grad_fn=<NegBackward0>) tensor(11279.2031, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11279.193359375
tensor(11279.2031, grad_fn=<NegBackward0>) tensor(11279.1934, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11279.1787109375
tensor(11279.1934, grad_fn=<NegBackward0>) tensor(11279.1787, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11279.171875
tensor(11279.1787, grad_fn=<NegBackward0>) tensor(11279.1719, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11279.1669921875
tensor(11279.1719, grad_fn=<NegBackward0>) tensor(11279.1670, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11279.162109375
tensor(11279.1670, grad_fn=<NegBackward0>) tensor(11279.1621, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11279.1474609375
tensor(11279.1621, grad_fn=<NegBackward0>) tensor(11279.1475, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11279.12890625
tensor(11279.1475, grad_fn=<NegBackward0>) tensor(11279.1289, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11279.1240234375
tensor(11279.1289, grad_fn=<NegBackward0>) tensor(11279.1240, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11279.119140625
tensor(11279.1240, grad_fn=<NegBackward0>) tensor(11279.1191, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11279.115234375
tensor(11279.1191, grad_fn=<NegBackward0>) tensor(11279.1152, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11279.111328125
tensor(11279.1152, grad_fn=<NegBackward0>) tensor(11279.1113, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11279.1044921875
tensor(11279.1113, grad_fn=<NegBackward0>) tensor(11279.1045, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11279.0859375
tensor(11279.1045, grad_fn=<NegBackward0>) tensor(11279.0859, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11278.837890625
tensor(11279.0859, grad_fn=<NegBackward0>) tensor(11278.8379, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11277.0830078125
tensor(11278.8379, grad_fn=<NegBackward0>) tensor(11277.0830, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11277.0322265625
tensor(11277.0830, grad_fn=<NegBackward0>) tensor(11277.0322, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11277.0224609375
tensor(11277.0322, grad_fn=<NegBackward0>) tensor(11277.0225, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11277.021484375
tensor(11277.0225, grad_fn=<NegBackward0>) tensor(11277.0215, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11277.0185546875
tensor(11277.0215, grad_fn=<NegBackward0>) tensor(11277.0186, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11277.0185546875
tensor(11277.0186, grad_fn=<NegBackward0>) tensor(11277.0186, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11277.017578125
tensor(11277.0186, grad_fn=<NegBackward0>) tensor(11277.0176, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11277.0166015625
tensor(11277.0176, grad_fn=<NegBackward0>) tensor(11277.0166, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11277.0166015625
tensor(11277.0166, grad_fn=<NegBackward0>) tensor(11277.0166, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11277.015625
tensor(11277.0166, grad_fn=<NegBackward0>) tensor(11277.0156, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11277.0146484375
tensor(11277.0156, grad_fn=<NegBackward0>) tensor(11277.0146, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11277.0146484375
tensor(11277.0146, grad_fn=<NegBackward0>) tensor(11277.0146, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11276.984375
tensor(11277.0146, grad_fn=<NegBackward0>) tensor(11276.9844, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11276.982421875
tensor(11276.9844, grad_fn=<NegBackward0>) tensor(11276.9824, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11276.9873046875
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9873, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11276.982421875
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9824, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11276.98828125
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9883, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11276.984375
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9844, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11276.982421875
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9824, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11276.98046875
tensor(11276.9824, grad_fn=<NegBackward0>) tensor(11276.9805, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11276.986328125
tensor(11276.9805, grad_fn=<NegBackward0>) tensor(11276.9863, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11276.9814453125
tensor(11276.9805, grad_fn=<NegBackward0>) tensor(11276.9814, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11276.982421875
tensor(11276.9805, grad_fn=<NegBackward0>) tensor(11276.9824, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11276.982421875
tensor(11276.9805, grad_fn=<NegBackward0>) tensor(11276.9824, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11276.9814453125
tensor(11276.9805, grad_fn=<NegBackward0>) tensor(11276.9814, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.7863, 0.2137],
        [0.3360, 0.6640]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0164, 0.9836], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.0790],
         [0.6639, 0.1851]],

        [[0.6357, 0.1021],
         [0.5776, 0.6378]],

        [[0.6988, 0.1000],
         [0.6329, 0.6531]],

        [[0.5958, 0.1043],
         [0.6798, 0.5893]],

        [[0.5167, 0.0987],
         [0.7137, 0.6251]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6652033592853569
Average Adjusted Rand Index: 0.7547161375572674
[0.6652033592853569, 0.6652033592853569] [0.7547161375572674, 0.7547161375572674] [11277.1103515625, 11276.9814453125]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11224.027703349544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21735.599609375
inf tensor(21735.5996, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11513.611328125
tensor(21735.5996, grad_fn=<NegBackward0>) tensor(11513.6113, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11507.6669921875
tensor(11513.6113, grad_fn=<NegBackward0>) tensor(11507.6670, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11500.3544921875
tensor(11507.6670, grad_fn=<NegBackward0>) tensor(11500.3545, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11498.97265625
tensor(11500.3545, grad_fn=<NegBackward0>) tensor(11498.9727, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11496.455078125
tensor(11498.9727, grad_fn=<NegBackward0>) tensor(11496.4551, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11495.8896484375
tensor(11496.4551, grad_fn=<NegBackward0>) tensor(11495.8896, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11342.3271484375
tensor(11495.8896, grad_fn=<NegBackward0>) tensor(11342.3271, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11211.0498046875
tensor(11342.3271, grad_fn=<NegBackward0>) tensor(11211.0498, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11208.3525390625
tensor(11211.0498, grad_fn=<NegBackward0>) tensor(11208.3525, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11208.15234375
tensor(11208.3525, grad_fn=<NegBackward0>) tensor(11208.1523, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11208.015625
tensor(11208.1523, grad_fn=<NegBackward0>) tensor(11208.0156, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11207.9521484375
tensor(11208.0156, grad_fn=<NegBackward0>) tensor(11207.9521, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11204.3876953125
tensor(11207.9521, grad_fn=<NegBackward0>) tensor(11204.3877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11199.427734375
tensor(11204.3877, grad_fn=<NegBackward0>) tensor(11199.4277, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11199.3916015625
tensor(11199.4277, grad_fn=<NegBackward0>) tensor(11199.3916, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11199.376953125
tensor(11199.3916, grad_fn=<NegBackward0>) tensor(11199.3770, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11199.3740234375
tensor(11199.3770, grad_fn=<NegBackward0>) tensor(11199.3740, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11199.369140625
tensor(11199.3740, grad_fn=<NegBackward0>) tensor(11199.3691, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11199.3662109375
tensor(11199.3691, grad_fn=<NegBackward0>) tensor(11199.3662, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11199.361328125
tensor(11199.3662, grad_fn=<NegBackward0>) tensor(11199.3613, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11199.357421875
tensor(11199.3613, grad_fn=<NegBackward0>) tensor(11199.3574, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11199.3544921875
tensor(11199.3574, grad_fn=<NegBackward0>) tensor(11199.3545, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11199.3505859375
tensor(11199.3545, grad_fn=<NegBackward0>) tensor(11199.3506, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11199.3291015625
tensor(11199.3506, grad_fn=<NegBackward0>) tensor(11199.3291, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11199.251953125
tensor(11199.3291, grad_fn=<NegBackward0>) tensor(11199.2520, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11198.7822265625
tensor(11199.2520, grad_fn=<NegBackward0>) tensor(11198.7822, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11198.7373046875
tensor(11198.7822, grad_fn=<NegBackward0>) tensor(11198.7373, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11198.724609375
tensor(11198.7373, grad_fn=<NegBackward0>) tensor(11198.7246, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11197.716796875
tensor(11198.7246, grad_fn=<NegBackward0>) tensor(11197.7168, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11197.716796875
tensor(11197.7168, grad_fn=<NegBackward0>) tensor(11197.7168, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11197.7080078125
tensor(11197.7168, grad_fn=<NegBackward0>) tensor(11197.7080, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11197.7021484375
tensor(11197.7080, grad_fn=<NegBackward0>) tensor(11197.7021, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11197.701171875
tensor(11197.7021, grad_fn=<NegBackward0>) tensor(11197.7012, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11197.673828125
tensor(11197.7012, grad_fn=<NegBackward0>) tensor(11197.6738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11197.673828125
tensor(11197.6738, grad_fn=<NegBackward0>) tensor(11197.6738, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11197.6728515625
tensor(11197.6738, grad_fn=<NegBackward0>) tensor(11197.6729, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11197.66796875
tensor(11197.6729, grad_fn=<NegBackward0>) tensor(11197.6680, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11197.666015625
tensor(11197.6680, grad_fn=<NegBackward0>) tensor(11197.6660, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11197.6669921875
tensor(11197.6660, grad_fn=<NegBackward0>) tensor(11197.6670, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11197.689453125
tensor(11197.6660, grad_fn=<NegBackward0>) tensor(11197.6895, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11197.6650390625
tensor(11197.6660, grad_fn=<NegBackward0>) tensor(11197.6650, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11197.6650390625
tensor(11197.6650, grad_fn=<NegBackward0>) tensor(11197.6650, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11197.6728515625
tensor(11197.6650, grad_fn=<NegBackward0>) tensor(11197.6729, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11197.6708984375
tensor(11197.6650, grad_fn=<NegBackward0>) tensor(11197.6709, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11197.6640625
tensor(11197.6650, grad_fn=<NegBackward0>) tensor(11197.6641, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11197.662109375
tensor(11197.6641, grad_fn=<NegBackward0>) tensor(11197.6621, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11197.6630859375
tensor(11197.6621, grad_fn=<NegBackward0>) tensor(11197.6631, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11197.662109375
tensor(11197.6621, grad_fn=<NegBackward0>) tensor(11197.6621, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11197.6611328125
tensor(11197.6621, grad_fn=<NegBackward0>) tensor(11197.6611, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11197.658203125
tensor(11197.6611, grad_fn=<NegBackward0>) tensor(11197.6582, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11197.6669921875
tensor(11197.6582, grad_fn=<NegBackward0>) tensor(11197.6670, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11197.6552734375
tensor(11197.6582, grad_fn=<NegBackward0>) tensor(11197.6553, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11197.662109375
tensor(11197.6553, grad_fn=<NegBackward0>) tensor(11197.6621, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11197.6591796875
tensor(11197.6553, grad_fn=<NegBackward0>) tensor(11197.6592, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11197.6552734375
tensor(11197.6553, grad_fn=<NegBackward0>) tensor(11197.6553, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11197.6728515625
tensor(11197.6553, grad_fn=<NegBackward0>) tensor(11197.6729, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11197.6220703125
tensor(11197.6553, grad_fn=<NegBackward0>) tensor(11197.6221, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11197.623046875
tensor(11197.6221, grad_fn=<NegBackward0>) tensor(11197.6230, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11197.6220703125
tensor(11197.6221, grad_fn=<NegBackward0>) tensor(11197.6221, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11197.62109375
tensor(11197.6221, grad_fn=<NegBackward0>) tensor(11197.6211, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11197.619140625
tensor(11197.6211, grad_fn=<NegBackward0>) tensor(11197.6191, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11197.640625
tensor(11197.6191, grad_fn=<NegBackward0>) tensor(11197.6406, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11195.1357421875
tensor(11197.6191, grad_fn=<NegBackward0>) tensor(11195.1357, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11195.11328125
tensor(11195.1357, grad_fn=<NegBackward0>) tensor(11195.1133, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11195.1123046875
tensor(11195.1133, grad_fn=<NegBackward0>) tensor(11195.1123, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11195.1142578125
tensor(11195.1123, grad_fn=<NegBackward0>) tensor(11195.1143, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11195.111328125
tensor(11195.1123, grad_fn=<NegBackward0>) tensor(11195.1113, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11195.1123046875
tensor(11195.1113, grad_fn=<NegBackward0>) tensor(11195.1123, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11195.1103515625
tensor(11195.1113, grad_fn=<NegBackward0>) tensor(11195.1104, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11195.111328125
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1113, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11195.1103515625
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1104, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11195.1103515625
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1104, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11195.1103515625
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1104, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11195.1103515625
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1104, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11195.109375
tensor(11195.1104, grad_fn=<NegBackward0>) tensor(11195.1094, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11195.1083984375
tensor(11195.1094, grad_fn=<NegBackward0>) tensor(11195.1084, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11195.1171875
tensor(11195.1084, grad_fn=<NegBackward0>) tensor(11195.1172, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11195.109375
tensor(11195.1084, grad_fn=<NegBackward0>) tensor(11195.1094, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11195.109375
tensor(11195.1084, grad_fn=<NegBackward0>) tensor(11195.1094, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11195.11328125
tensor(11195.1084, grad_fn=<NegBackward0>) tensor(11195.1133, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11195.109375
tensor(11195.1084, grad_fn=<NegBackward0>) tensor(11195.1094, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.6950, 0.3050],
        [0.3044, 0.6956]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5547, 0.4453], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2992, 0.0974],
         [0.6082, 0.1945]],

        [[0.6196, 0.1075],
         [0.6823, 0.6228]],

        [[0.6535, 0.0958],
         [0.6788, 0.7097]],

        [[0.7117, 0.0907],
         [0.5641, 0.7033]],

        [[0.5483, 0.1010],
         [0.6400, 0.6276]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080514782056689
Global Adjusted Rand Index: 0.9291542740196213
Average Adjusted Rand Index: 0.9297684360007941
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22048.91796875
inf tensor(22048.9180, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11515.693359375
tensor(22048.9180, grad_fn=<NegBackward0>) tensor(11515.6934, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11511.51953125
tensor(11515.6934, grad_fn=<NegBackward0>) tensor(11511.5195, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11505.056640625
tensor(11511.5195, grad_fn=<NegBackward0>) tensor(11505.0566, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11496.091796875
tensor(11505.0566, grad_fn=<NegBackward0>) tensor(11496.0918, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11355.8349609375
tensor(11496.0918, grad_fn=<NegBackward0>) tensor(11355.8350, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11234.4482421875
tensor(11355.8350, grad_fn=<NegBackward0>) tensor(11234.4482, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11210.2763671875
tensor(11234.4482, grad_fn=<NegBackward0>) tensor(11210.2764, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11209.79296875
tensor(11210.2764, grad_fn=<NegBackward0>) tensor(11209.7930, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11208.7080078125
tensor(11209.7930, grad_fn=<NegBackward0>) tensor(11208.7080, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11208.49609375
tensor(11208.7080, grad_fn=<NegBackward0>) tensor(11208.4961, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11208.3740234375
tensor(11208.4961, grad_fn=<NegBackward0>) tensor(11208.3740, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11205.330078125
tensor(11208.3740, grad_fn=<NegBackward0>) tensor(11205.3301, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11205.232421875
tensor(11205.3301, grad_fn=<NegBackward0>) tensor(11205.2324, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11205.1943359375
tensor(11205.2324, grad_fn=<NegBackward0>) tensor(11205.1943, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11205.1435546875
tensor(11205.1943, grad_fn=<NegBackward0>) tensor(11205.1436, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11203.9736328125
tensor(11205.1436, grad_fn=<NegBackward0>) tensor(11203.9736, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11203.8447265625
tensor(11203.9736, grad_fn=<NegBackward0>) tensor(11203.8447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11203.8291015625
tensor(11203.8447, grad_fn=<NegBackward0>) tensor(11203.8291, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11199.1015625
tensor(11203.8291, grad_fn=<NegBackward0>) tensor(11199.1016, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11199.087890625
tensor(11199.1016, grad_fn=<NegBackward0>) tensor(11199.0879, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11195.859375
tensor(11199.0879, grad_fn=<NegBackward0>) tensor(11195.8594, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11195.759765625
tensor(11195.8594, grad_fn=<NegBackward0>) tensor(11195.7598, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11195.7509765625
tensor(11195.7598, grad_fn=<NegBackward0>) tensor(11195.7510, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11195.7431640625
tensor(11195.7510, grad_fn=<NegBackward0>) tensor(11195.7432, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11195.7265625
tensor(11195.7432, grad_fn=<NegBackward0>) tensor(11195.7266, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11195.6484375
tensor(11195.7266, grad_fn=<NegBackward0>) tensor(11195.6484, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11195.6435546875
tensor(11195.6484, grad_fn=<NegBackward0>) tensor(11195.6436, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11195.640625
tensor(11195.6436, grad_fn=<NegBackward0>) tensor(11195.6406, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11195.638671875
tensor(11195.6406, grad_fn=<NegBackward0>) tensor(11195.6387, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11195.6376953125
tensor(11195.6387, grad_fn=<NegBackward0>) tensor(11195.6377, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11195.6337890625
tensor(11195.6377, grad_fn=<NegBackward0>) tensor(11195.6338, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11195.6318359375
tensor(11195.6338, grad_fn=<NegBackward0>) tensor(11195.6318, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11195.6298828125
tensor(11195.6318, grad_fn=<NegBackward0>) tensor(11195.6299, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11195.62890625
tensor(11195.6299, grad_fn=<NegBackward0>) tensor(11195.6289, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11195.6279296875
tensor(11195.6289, grad_fn=<NegBackward0>) tensor(11195.6279, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11195.630859375
tensor(11195.6279, grad_fn=<NegBackward0>) tensor(11195.6309, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11195.6298828125
tensor(11195.6279, grad_fn=<NegBackward0>) tensor(11195.6299, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11195.623046875
tensor(11195.6279, grad_fn=<NegBackward0>) tensor(11195.6230, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11195.62109375
tensor(11195.6230, grad_fn=<NegBackward0>) tensor(11195.6211, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11195.6181640625
tensor(11195.6211, grad_fn=<NegBackward0>) tensor(11195.6182, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11195.2109375
tensor(11195.6182, grad_fn=<NegBackward0>) tensor(11195.2109, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11195.2041015625
tensor(11195.2109, grad_fn=<NegBackward0>) tensor(11195.2041, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11195.1923828125
tensor(11195.2041, grad_fn=<NegBackward0>) tensor(11195.1924, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11195.1845703125
tensor(11195.1924, grad_fn=<NegBackward0>) tensor(11195.1846, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11195.185546875
tensor(11195.1846, grad_fn=<NegBackward0>) tensor(11195.1855, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11195.1787109375
tensor(11195.1846, grad_fn=<NegBackward0>) tensor(11195.1787, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11195.1787109375
tensor(11195.1787, grad_fn=<NegBackward0>) tensor(11195.1787, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11195.177734375
tensor(11195.1787, grad_fn=<NegBackward0>) tensor(11195.1777, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11194.912109375
tensor(11195.1777, grad_fn=<NegBackward0>) tensor(11194.9121, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11194.900390625
tensor(11194.9121, grad_fn=<NegBackward0>) tensor(11194.9004, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11194.8984375
tensor(11194.9004, grad_fn=<NegBackward0>) tensor(11194.8984, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11194.8994140625
tensor(11194.8984, grad_fn=<NegBackward0>) tensor(11194.8994, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11194.90234375
tensor(11194.8984, grad_fn=<NegBackward0>) tensor(11194.9023, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11194.8994140625
tensor(11194.8984, grad_fn=<NegBackward0>) tensor(11194.8994, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -11194.8974609375
tensor(11194.8984, grad_fn=<NegBackward0>) tensor(11194.8975, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11194.896484375
tensor(11194.8975, grad_fn=<NegBackward0>) tensor(11194.8965, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11194.8955078125
tensor(11194.8965, grad_fn=<NegBackward0>) tensor(11194.8955, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11194.8974609375
tensor(11194.8955, grad_fn=<NegBackward0>) tensor(11194.8975, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11194.8955078125
tensor(11194.8955, grad_fn=<NegBackward0>) tensor(11194.8955, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11194.8955078125
tensor(11194.8955, grad_fn=<NegBackward0>) tensor(11194.8955, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11194.8955078125
tensor(11194.8955, grad_fn=<NegBackward0>) tensor(11194.8955, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11194.8935546875
tensor(11194.8955, grad_fn=<NegBackward0>) tensor(11194.8936, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11194.8896484375
tensor(11194.8936, grad_fn=<NegBackward0>) tensor(11194.8896, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11194.890625
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8906, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11194.8916015625
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8916, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11194.8896484375
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8896, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11194.8896484375
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8896, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11194.890625
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8906, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11194.892578125
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8926, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11194.8896484375
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8896, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11194.888671875
tensor(11194.8896, grad_fn=<NegBackward0>) tensor(11194.8887, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11194.888671875
tensor(11194.8887, grad_fn=<NegBackward0>) tensor(11194.8887, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11194.888671875
tensor(11194.8887, grad_fn=<NegBackward0>) tensor(11194.8887, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11194.8896484375
tensor(11194.8887, grad_fn=<NegBackward0>) tensor(11194.8896, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11194.888671875
tensor(11194.8887, grad_fn=<NegBackward0>) tensor(11194.8887, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11194.88671875
tensor(11194.8887, grad_fn=<NegBackward0>) tensor(11194.8867, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11194.8876953125
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8877, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11194.88671875
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8867, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11194.88671875
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8867, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11194.90234375
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.9023, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11194.88671875
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8867, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11194.8876953125
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8877, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11194.884765625
tensor(11194.8867, grad_fn=<NegBackward0>) tensor(11194.8848, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11194.8828125
tensor(11194.8848, grad_fn=<NegBackward0>) tensor(11194.8828, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11194.884765625
tensor(11194.8828, grad_fn=<NegBackward0>) tensor(11194.8848, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11194.8837890625
tensor(11194.8828, grad_fn=<NegBackward0>) tensor(11194.8838, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11194.8837890625
tensor(11194.8828, grad_fn=<NegBackward0>) tensor(11194.8838, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11194.8740234375
tensor(11194.8828, grad_fn=<NegBackward0>) tensor(11194.8740, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11194.873046875
tensor(11194.8740, grad_fn=<NegBackward0>) tensor(11194.8730, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11194.8974609375
tensor(11194.8730, grad_fn=<NegBackward0>) tensor(11194.8975, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11194.873046875
tensor(11194.8730, grad_fn=<NegBackward0>) tensor(11194.8730, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11194.873046875
tensor(11194.8730, grad_fn=<NegBackward0>) tensor(11194.8730, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11194.8720703125
tensor(11194.8730, grad_fn=<NegBackward0>) tensor(11194.8721, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11194.8701171875
tensor(11194.8721, grad_fn=<NegBackward0>) tensor(11194.8701, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11194.87109375
tensor(11194.8701, grad_fn=<NegBackward0>) tensor(11194.8711, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11194.869140625
tensor(11194.8701, grad_fn=<NegBackward0>) tensor(11194.8691, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11194.8701171875
tensor(11194.8691, grad_fn=<NegBackward0>) tensor(11194.8701, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11194.8916015625
tensor(11194.8691, grad_fn=<NegBackward0>) tensor(11194.8916, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11194.8701171875
tensor(11194.8691, grad_fn=<NegBackward0>) tensor(11194.8701, grad_fn=<NegBackward0>)
3
pi: tensor([[0.6961, 0.3039],
        [0.3032, 0.6968]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5556, 0.4444], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2984, 0.0977],
         [0.6061, 0.1942]],

        [[0.7145, 0.1069],
         [0.7118, 0.5317]],

        [[0.5060, 0.0959],
         [0.6549, 0.5824]],

        [[0.6072, 0.0908],
         [0.5646, 0.6178]],

        [[0.6741, 0.1015],
         [0.7232, 0.5522]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080514782056689
Global Adjusted Rand Index: 0.9291542740196213
Average Adjusted Rand Index: 0.9297684360007941
[0.9291542740196213, 0.9291542740196213] [0.9297684360007941, 0.9297684360007941] [11195.109375, 11194.908203125]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10974.103370710178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22818.4921875
inf tensor(22818.4922, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11249.0927734375
tensor(22818.4922, grad_fn=<NegBackward0>) tensor(11249.0928, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11248.15234375
tensor(11249.0928, grad_fn=<NegBackward0>) tensor(11248.1523, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11247.19140625
tensor(11248.1523, grad_fn=<NegBackward0>) tensor(11247.1914, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11246.3974609375
tensor(11247.1914, grad_fn=<NegBackward0>) tensor(11246.3975, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11245.6552734375
tensor(11246.3975, grad_fn=<NegBackward0>) tensor(11245.6553, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11245.41015625
tensor(11245.6553, grad_fn=<NegBackward0>) tensor(11245.4102, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11245.1943359375
tensor(11245.4102, grad_fn=<NegBackward0>) tensor(11245.1943, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11245.04296875
tensor(11245.1943, grad_fn=<NegBackward0>) tensor(11245.0430, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11244.9736328125
tensor(11245.0430, grad_fn=<NegBackward0>) tensor(11244.9736, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11244.9345703125
tensor(11244.9736, grad_fn=<NegBackward0>) tensor(11244.9346, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11244.91015625
tensor(11244.9346, grad_fn=<NegBackward0>) tensor(11244.9102, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11244.892578125
tensor(11244.9102, grad_fn=<NegBackward0>) tensor(11244.8926, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11244.8779296875
tensor(11244.8926, grad_fn=<NegBackward0>) tensor(11244.8779, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11244.8671875
tensor(11244.8779, grad_fn=<NegBackward0>) tensor(11244.8672, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11244.857421875
tensor(11244.8672, grad_fn=<NegBackward0>) tensor(11244.8574, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11244.8486328125
tensor(11244.8574, grad_fn=<NegBackward0>) tensor(11244.8486, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11244.84375
tensor(11244.8486, grad_fn=<NegBackward0>) tensor(11244.8438, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11244.8369140625
tensor(11244.8438, grad_fn=<NegBackward0>) tensor(11244.8369, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11244.83203125
tensor(11244.8369, grad_fn=<NegBackward0>) tensor(11244.8320, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11244.8291015625
tensor(11244.8320, grad_fn=<NegBackward0>) tensor(11244.8291, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11244.826171875
tensor(11244.8291, grad_fn=<NegBackward0>) tensor(11244.8262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11244.8232421875
tensor(11244.8262, grad_fn=<NegBackward0>) tensor(11244.8232, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11244.8203125
tensor(11244.8232, grad_fn=<NegBackward0>) tensor(11244.8203, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11244.8173828125
tensor(11244.8203, grad_fn=<NegBackward0>) tensor(11244.8174, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11244.8154296875
tensor(11244.8174, grad_fn=<NegBackward0>) tensor(11244.8154, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11244.8125
tensor(11244.8154, grad_fn=<NegBackward0>) tensor(11244.8125, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11244.8115234375
tensor(11244.8125, grad_fn=<NegBackward0>) tensor(11244.8115, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11244.810546875
tensor(11244.8115, grad_fn=<NegBackward0>) tensor(11244.8105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11244.80859375
tensor(11244.8105, grad_fn=<NegBackward0>) tensor(11244.8086, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11244.8076171875
tensor(11244.8086, grad_fn=<NegBackward0>) tensor(11244.8076, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11244.8076171875
tensor(11244.8076, grad_fn=<NegBackward0>) tensor(11244.8076, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11244.8056640625
tensor(11244.8076, grad_fn=<NegBackward0>) tensor(11244.8057, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11244.8056640625
tensor(11244.8057, grad_fn=<NegBackward0>) tensor(11244.8057, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11244.802734375
tensor(11244.8057, grad_fn=<NegBackward0>) tensor(11244.8027, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11244.8017578125
tensor(11244.8027, grad_fn=<NegBackward0>) tensor(11244.8018, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11244.802734375
tensor(11244.8018, grad_fn=<NegBackward0>) tensor(11244.8027, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11244.8017578125
tensor(11244.8018, grad_fn=<NegBackward0>) tensor(11244.8018, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11244.80078125
tensor(11244.8018, grad_fn=<NegBackward0>) tensor(11244.8008, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11244.80078125
tensor(11244.8008, grad_fn=<NegBackward0>) tensor(11244.8008, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11244.7998046875
tensor(11244.8008, grad_fn=<NegBackward0>) tensor(11244.7998, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11244.7998046875
tensor(11244.7998, grad_fn=<NegBackward0>) tensor(11244.7998, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11244.7998046875
tensor(11244.7998, grad_fn=<NegBackward0>) tensor(11244.7998, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11244.796875
tensor(11244.7998, grad_fn=<NegBackward0>) tensor(11244.7969, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11244.7978515625
tensor(11244.7969, grad_fn=<NegBackward0>) tensor(11244.7979, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11244.796875
tensor(11244.7969, grad_fn=<NegBackward0>) tensor(11244.7969, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11244.796875
tensor(11244.7969, grad_fn=<NegBackward0>) tensor(11244.7969, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11244.7958984375
tensor(11244.7969, grad_fn=<NegBackward0>) tensor(11244.7959, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11244.794921875
tensor(11244.7959, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11244.794921875
tensor(11244.7949, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11244.794921875
tensor(11244.7949, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11244.794921875
tensor(11244.7949, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11244.7939453125
tensor(11244.7949, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11244.796875
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7969, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11244.794921875
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11244.7939453125
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11244.796875
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7969, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11244.7958984375
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7959, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11244.7939453125
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11244.794921875
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11244.7939453125
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11244.802734375
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.8027, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11244.79296875
tensor(11244.7939, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11244.79296875
tensor(11244.7930, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11244.7939453125
tensor(11244.7930, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11244.794921875
tensor(11244.7930, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11244.79296875
tensor(11244.7930, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11244.7919921875
tensor(11244.7930, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11244.79296875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11244.79296875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11244.794921875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7949, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11244.7919921875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11244.79296875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11244.7919921875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11244.7939453125
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11244.7919921875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11244.7919921875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11244.79296875
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7930, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11244.791015625
tensor(11244.7920, grad_fn=<NegBackward0>) tensor(11244.7910, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11244.8037109375
tensor(11244.7910, grad_fn=<NegBackward0>) tensor(11244.8037, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11244.7919921875
tensor(11244.7910, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11244.7939453125
tensor(11244.7910, grad_fn=<NegBackward0>) tensor(11244.7939, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11244.8486328125
tensor(11244.7910, grad_fn=<NegBackward0>) tensor(11244.8486, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11244.7919921875
tensor(11244.7910, grad_fn=<NegBackward0>) tensor(11244.7920, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[9.2572e-01, 7.4283e-02],
        [9.9990e-01, 1.0165e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9446, 0.0554], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1665, 0.2049],
         [0.5660, 0.3030]],

        [[0.7206, 0.2152],
         [0.5704, 0.7186]],

        [[0.6704, 0.1868],
         [0.7163, 0.7111]],

        [[0.7289, 0.0881],
         [0.6312, 0.6495]],

        [[0.6533, 0.2102],
         [0.6809, 0.7003]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002504260136592525
Average Adjusted Rand Index: 0.0008333839518921218
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22224.35546875
inf tensor(22224.3555, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11249.7744140625
tensor(22224.3555, grad_fn=<NegBackward0>) tensor(11249.7744, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11248.818359375
tensor(11249.7744, grad_fn=<NegBackward0>) tensor(11248.8184, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11248.470703125
tensor(11248.8184, grad_fn=<NegBackward0>) tensor(11248.4707, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11247.9873046875
tensor(11248.4707, grad_fn=<NegBackward0>) tensor(11247.9873, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11247.267578125
tensor(11247.9873, grad_fn=<NegBackward0>) tensor(11247.2676, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11246.298828125
tensor(11247.2676, grad_fn=<NegBackward0>) tensor(11246.2988, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11244.748046875
tensor(11246.2988, grad_fn=<NegBackward0>) tensor(11244.7480, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11239.03515625
tensor(11244.7480, grad_fn=<NegBackward0>) tensor(11239.0352, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11069.267578125
tensor(11239.0352, grad_fn=<NegBackward0>) tensor(11069.2676, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11002.3955078125
tensor(11069.2676, grad_fn=<NegBackward0>) tensor(11002.3955, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10964.6279296875
tensor(11002.3955, grad_fn=<NegBackward0>) tensor(10964.6279, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10962.611328125
tensor(10964.6279, grad_fn=<NegBackward0>) tensor(10962.6113, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10962.4423828125
tensor(10962.6113, grad_fn=<NegBackward0>) tensor(10962.4424, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10962.3525390625
tensor(10962.4424, grad_fn=<NegBackward0>) tensor(10962.3525, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10962.3017578125
tensor(10962.3525, grad_fn=<NegBackward0>) tensor(10962.3018, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10962.26171875
tensor(10962.3018, grad_fn=<NegBackward0>) tensor(10962.2617, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10962.220703125
tensor(10962.2617, grad_fn=<NegBackward0>) tensor(10962.2207, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10961.2353515625
tensor(10962.2207, grad_fn=<NegBackward0>) tensor(10961.2354, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10960.9033203125
tensor(10961.2354, grad_fn=<NegBackward0>) tensor(10960.9033, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10960.8837890625
tensor(10960.9033, grad_fn=<NegBackward0>) tensor(10960.8838, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10960.8564453125
tensor(10960.8838, grad_fn=<NegBackward0>) tensor(10960.8564, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10960.8173828125
tensor(10960.8564, grad_fn=<NegBackward0>) tensor(10960.8174, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10960.8095703125
tensor(10960.8174, grad_fn=<NegBackward0>) tensor(10960.8096, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10960.8037109375
tensor(10960.8096, grad_fn=<NegBackward0>) tensor(10960.8037, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10960.7978515625
tensor(10960.8037, grad_fn=<NegBackward0>) tensor(10960.7979, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10960.7939453125
tensor(10960.7979, grad_fn=<NegBackward0>) tensor(10960.7939, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10960.7900390625
tensor(10960.7939, grad_fn=<NegBackward0>) tensor(10960.7900, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10960.7822265625
tensor(10960.7900, grad_fn=<NegBackward0>) tensor(10960.7822, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10960.7724609375
tensor(10960.7822, grad_fn=<NegBackward0>) tensor(10960.7725, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10960.765625
tensor(10960.7725, grad_fn=<NegBackward0>) tensor(10960.7656, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10960.763671875
tensor(10960.7656, grad_fn=<NegBackward0>) tensor(10960.7637, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10960.755859375
tensor(10960.7637, grad_fn=<NegBackward0>) tensor(10960.7559, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10960.6357421875
tensor(10960.7559, grad_fn=<NegBackward0>) tensor(10960.6357, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10960.49609375
tensor(10960.6357, grad_fn=<NegBackward0>) tensor(10960.4961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10960.490234375
tensor(10960.4961, grad_fn=<NegBackward0>) tensor(10960.4902, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10958.158203125
tensor(10960.4902, grad_fn=<NegBackward0>) tensor(10958.1582, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10958.1513671875
tensor(10958.1582, grad_fn=<NegBackward0>) tensor(10958.1514, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10958.1494140625
tensor(10958.1514, grad_fn=<NegBackward0>) tensor(10958.1494, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10958.1494140625
tensor(10958.1494, grad_fn=<NegBackward0>) tensor(10958.1494, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10958.142578125
tensor(10958.1494, grad_fn=<NegBackward0>) tensor(10958.1426, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10958.140625
tensor(10958.1426, grad_fn=<NegBackward0>) tensor(10958.1406, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10958.1435546875
tensor(10958.1406, grad_fn=<NegBackward0>) tensor(10958.1436, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10958.138671875
tensor(10958.1406, grad_fn=<NegBackward0>) tensor(10958.1387, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10958.1357421875
tensor(10958.1387, grad_fn=<NegBackward0>) tensor(10958.1357, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10958.1328125
tensor(10958.1357, grad_fn=<NegBackward0>) tensor(10958.1328, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10958.1298828125
tensor(10958.1328, grad_fn=<NegBackward0>) tensor(10958.1299, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10958.125
tensor(10958.1299, grad_fn=<NegBackward0>) tensor(10958.1250, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10958.0869140625
tensor(10958.1250, grad_fn=<NegBackward0>) tensor(10958.0869, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10958.0869140625
tensor(10958.0869, grad_fn=<NegBackward0>) tensor(10958.0869, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10958.0869140625
tensor(10958.0869, grad_fn=<NegBackward0>) tensor(10958.0869, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10958.083984375
tensor(10958.0869, grad_fn=<NegBackward0>) tensor(10958.0840, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10958.08203125
tensor(10958.0840, grad_fn=<NegBackward0>) tensor(10958.0820, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10958.080078125
tensor(10958.0820, grad_fn=<NegBackward0>) tensor(10958.0801, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10958.09765625
tensor(10958.0801, grad_fn=<NegBackward0>) tensor(10958.0977, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10958.080078125
tensor(10958.0801, grad_fn=<NegBackward0>) tensor(10958.0801, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10958.0791015625
tensor(10958.0801, grad_fn=<NegBackward0>) tensor(10958.0791, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10958.0791015625
tensor(10958.0791, grad_fn=<NegBackward0>) tensor(10958.0791, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10958.078125
tensor(10958.0791, grad_fn=<NegBackward0>) tensor(10958.0781, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10958.068359375
tensor(10958.0781, grad_fn=<NegBackward0>) tensor(10958.0684, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10958.0517578125
tensor(10958.0684, grad_fn=<NegBackward0>) tensor(10958.0518, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10958.0498046875
tensor(10958.0518, grad_fn=<NegBackward0>) tensor(10958.0498, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10957.9482421875
tensor(10958.0498, grad_fn=<NegBackward0>) tensor(10957.9482, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10957.9482421875
tensor(10957.9482, grad_fn=<NegBackward0>) tensor(10957.9482, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10957.9521484375
tensor(10957.9482, grad_fn=<NegBackward0>) tensor(10957.9521, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10957.9482421875
tensor(10957.9482, grad_fn=<NegBackward0>) tensor(10957.9482, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10957.9462890625
tensor(10957.9482, grad_fn=<NegBackward0>) tensor(10957.9463, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10957.947265625
tensor(10957.9463, grad_fn=<NegBackward0>) tensor(10957.9473, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10957.9501953125
tensor(10957.9463, grad_fn=<NegBackward0>) tensor(10957.9502, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10957.9462890625
tensor(10957.9463, grad_fn=<NegBackward0>) tensor(10957.9463, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10957.8701171875
tensor(10957.9463, grad_fn=<NegBackward0>) tensor(10957.8701, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10957.8701171875
tensor(10957.8701, grad_fn=<NegBackward0>) tensor(10957.8701, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10957.87109375
tensor(10957.8701, grad_fn=<NegBackward0>) tensor(10957.8711, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10957.875
tensor(10957.8701, grad_fn=<NegBackward0>) tensor(10957.8750, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10957.8046875
tensor(10957.8701, grad_fn=<NegBackward0>) tensor(10957.8047, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10957.802734375
tensor(10957.8047, grad_fn=<NegBackward0>) tensor(10957.8027, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10957.8046875
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.8047, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10957.802734375
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.8027, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10957.8037109375
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.8037, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10957.8037109375
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.8037, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10957.8037109375
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.8037, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10957.7978515625
tensor(10957.8027, grad_fn=<NegBackward0>) tensor(10957.7979, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10957.7978515625
tensor(10957.7979, grad_fn=<NegBackward0>) tensor(10957.7979, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10957.802734375
tensor(10957.7979, grad_fn=<NegBackward0>) tensor(10957.8027, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10957.8115234375
tensor(10957.7979, grad_fn=<NegBackward0>) tensor(10957.8115, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10957.7958984375
tensor(10957.7979, grad_fn=<NegBackward0>) tensor(10957.7959, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10957.796875
tensor(10957.7959, grad_fn=<NegBackward0>) tensor(10957.7969, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10957.796875
tensor(10957.7959, grad_fn=<NegBackward0>) tensor(10957.7969, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10957.796875
tensor(10957.7959, grad_fn=<NegBackward0>) tensor(10957.7969, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10957.796875
tensor(10957.7959, grad_fn=<NegBackward0>) tensor(10957.7969, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -10957.8046875
tensor(10957.7959, grad_fn=<NegBackward0>) tensor(10957.8047, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7774, 0.2226],
        [0.2585, 0.7415]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5264, 0.4736], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1966, 0.1013],
         [0.5068, 0.3025]],

        [[0.6969, 0.0989],
         [0.6772, 0.6479]],

        [[0.5432, 0.0999],
         [0.5909, 0.6400]],

        [[0.6639, 0.0918],
         [0.6415, 0.5240]],

        [[0.5509, 0.0936],
         [0.6697, 0.7199]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9603199793778593
Average Adjusted Rand Index: 0.9604819104933195
[-0.002504260136592525, 0.9603199793778593] [0.0008333839518921218, 0.9604819104933195] [11244.7919921875, 10957.8046875]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11043.867875480853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22129.30078125
inf tensor(22129.3008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11308.6220703125
tensor(22129.3008, grad_fn=<NegBackward0>) tensor(11308.6221, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11306.1787109375
tensor(11308.6221, grad_fn=<NegBackward0>) tensor(11306.1787, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11305.66796875
tensor(11306.1787, grad_fn=<NegBackward0>) tensor(11305.6680, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11305.4306640625
tensor(11305.6680, grad_fn=<NegBackward0>) tensor(11305.4307, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11305.2099609375
tensor(11305.4307, grad_fn=<NegBackward0>) tensor(11305.2100, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11304.828125
tensor(11305.2100, grad_fn=<NegBackward0>) tensor(11304.8281, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11303.4345703125
tensor(11304.8281, grad_fn=<NegBackward0>) tensor(11303.4346, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11297.365234375
tensor(11303.4346, grad_fn=<NegBackward0>) tensor(11297.3652, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11117.169921875
tensor(11297.3652, grad_fn=<NegBackward0>) tensor(11117.1699, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11039.4833984375
tensor(11117.1699, grad_fn=<NegBackward0>) tensor(11039.4834, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11020.6552734375
tensor(11039.4834, grad_fn=<NegBackward0>) tensor(11020.6553, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11018.4326171875
tensor(11020.6553, grad_fn=<NegBackward0>) tensor(11018.4326, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11017.9453125
tensor(11018.4326, grad_fn=<NegBackward0>) tensor(11017.9453, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11016.7705078125
tensor(11017.9453, grad_fn=<NegBackward0>) tensor(11016.7705, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11016.708984375
tensor(11016.7705, grad_fn=<NegBackward0>) tensor(11016.7090, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11016.67578125
tensor(11016.7090, grad_fn=<NegBackward0>) tensor(11016.6758, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11016.64453125
tensor(11016.6758, grad_fn=<NegBackward0>) tensor(11016.6445, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11016.5947265625
tensor(11016.6445, grad_fn=<NegBackward0>) tensor(11016.5947, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11016.578125
tensor(11016.5947, grad_fn=<NegBackward0>) tensor(11016.5781, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11016.5361328125
tensor(11016.5781, grad_fn=<NegBackward0>) tensor(11016.5361, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11016.4833984375
tensor(11016.5361, grad_fn=<NegBackward0>) tensor(11016.4834, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11016.4736328125
tensor(11016.4834, grad_fn=<NegBackward0>) tensor(11016.4736, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11016.4638671875
tensor(11016.4736, grad_fn=<NegBackward0>) tensor(11016.4639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11016.455078125
tensor(11016.4639, grad_fn=<NegBackward0>) tensor(11016.4551, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11016.4453125
tensor(11016.4551, grad_fn=<NegBackward0>) tensor(11016.4453, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11016.4365234375
tensor(11016.4453, grad_fn=<NegBackward0>) tensor(11016.4365, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11016.4208984375
tensor(11016.4365, grad_fn=<NegBackward0>) tensor(11016.4209, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11013.4697265625
tensor(11016.4209, grad_fn=<NegBackward0>) tensor(11013.4697, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11013.4560546875
tensor(11013.4697, grad_fn=<NegBackward0>) tensor(11013.4561, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11013.443359375
tensor(11013.4561, grad_fn=<NegBackward0>) tensor(11013.4434, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11013.43359375
tensor(11013.4434, grad_fn=<NegBackward0>) tensor(11013.4336, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11013.228515625
tensor(11013.4336, grad_fn=<NegBackward0>) tensor(11013.2285, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11013.2265625
tensor(11013.2285, grad_fn=<NegBackward0>) tensor(11013.2266, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11013.2255859375
tensor(11013.2266, grad_fn=<NegBackward0>) tensor(11013.2256, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11013.2236328125
tensor(11013.2256, grad_fn=<NegBackward0>) tensor(11013.2236, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11013.22265625
tensor(11013.2236, grad_fn=<NegBackward0>) tensor(11013.2227, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11013.220703125
tensor(11013.2227, grad_fn=<NegBackward0>) tensor(11013.2207, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11013.220703125
tensor(11013.2207, grad_fn=<NegBackward0>) tensor(11013.2207, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11013.220703125
tensor(11013.2207, grad_fn=<NegBackward0>) tensor(11013.2207, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11013.21875
tensor(11013.2207, grad_fn=<NegBackward0>) tensor(11013.2188, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11013.2177734375
tensor(11013.2188, grad_fn=<NegBackward0>) tensor(11013.2178, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11013.21875
tensor(11013.2178, grad_fn=<NegBackward0>) tensor(11013.2188, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11013.2177734375
tensor(11013.2178, grad_fn=<NegBackward0>) tensor(11013.2178, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11013.220703125
tensor(11013.2178, grad_fn=<NegBackward0>) tensor(11013.2207, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11013.21484375
tensor(11013.2178, grad_fn=<NegBackward0>) tensor(11013.2148, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11013.185546875
tensor(11013.2148, grad_fn=<NegBackward0>) tensor(11013.1855, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11013.1845703125
tensor(11013.1855, grad_fn=<NegBackward0>) tensor(11013.1846, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11013.18359375
tensor(11013.1846, grad_fn=<NegBackward0>) tensor(11013.1836, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11013.18359375
tensor(11013.1836, grad_fn=<NegBackward0>) tensor(11013.1836, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11013.162109375
tensor(11013.1836, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11013.162109375
tensor(11013.1621, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11013.1611328125
tensor(11013.1621, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11013.1640625
tensor(11013.1611, grad_fn=<NegBackward0>) tensor(11013.1641, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11013.162109375
tensor(11013.1611, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11013.162109375
tensor(11013.1611, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11013.1591796875
tensor(11013.1611, grad_fn=<NegBackward0>) tensor(11013.1592, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11013.162109375
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11013.1591796875
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1592, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11013.1611328125
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11013.1591796875
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1592, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11013.1591796875
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1592, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11013.16015625
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1602, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11013.1611328125
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11013.1572265625
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11013.1611328125
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11013.158203125
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11013.158203125
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11013.158203125
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11013.1572265625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11013.1787109375
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1787, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11013.1611328125
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11013.15625
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1562, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11013.154296875
tensor(11013.1562, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11013.1513671875
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1514, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11013.154296875
tensor(11013.1514, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11013.1494140625
tensor(11013.1514, grad_fn=<NegBackward0>) tensor(11013.1494, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11013.158203125
tensor(11013.1494, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11013.1572265625
tensor(11013.1494, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11013.1552734375
tensor(11013.1494, grad_fn=<NegBackward0>) tensor(11013.1553, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11013.1416015625
tensor(11013.1494, grad_fn=<NegBackward0>) tensor(11013.1416, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11013.138671875
tensor(11013.1416, grad_fn=<NegBackward0>) tensor(11013.1387, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11013.15234375
tensor(11013.1387, grad_fn=<NegBackward0>) tensor(11013.1523, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11013.140625
tensor(11013.1387, grad_fn=<NegBackward0>) tensor(11013.1406, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11013.1376953125
tensor(11013.1387, grad_fn=<NegBackward0>) tensor(11013.1377, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11013.154296875
tensor(11013.1377, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11013.13671875
tensor(11013.1377, grad_fn=<NegBackward0>) tensor(11013.1367, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11013.14453125
tensor(11013.1367, grad_fn=<NegBackward0>) tensor(11013.1445, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11013.13671875
tensor(11013.1367, grad_fn=<NegBackward0>) tensor(11013.1367, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11013.138671875
tensor(11013.1367, grad_fn=<NegBackward0>) tensor(11013.1387, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11013.13671875
tensor(11013.1367, grad_fn=<NegBackward0>) tensor(11013.1367, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11013.2119140625
tensor(11013.1367, grad_fn=<NegBackward0>) tensor(11013.2119, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7788, 0.2212],
        [0.2812, 0.7188]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5173, 0.4827], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.0984],
         [0.6401, 0.3063]],

        [[0.5784, 0.1024],
         [0.7118, 0.6160]],

        [[0.5506, 0.0938],
         [0.6325, 0.5763]],

        [[0.7208, 0.0949],
         [0.6637, 0.6408]],

        [[0.6965, 0.1019],
         [0.5468, 0.5997]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
Global Adjusted Rand Index: 0.9061144075474898
Average Adjusted Rand Index: 0.9073832489123866
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22022.99609375
inf tensor(22022.9961, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11307.9169921875
tensor(22022.9961, grad_fn=<NegBackward0>) tensor(11307.9170, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11305.8251953125
tensor(11307.9170, grad_fn=<NegBackward0>) tensor(11305.8252, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11305.4482421875
tensor(11305.8252, grad_fn=<NegBackward0>) tensor(11305.4482, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11304.841796875
tensor(11305.4482, grad_fn=<NegBackward0>) tensor(11304.8418, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11303.2451171875
tensor(11304.8418, grad_fn=<NegBackward0>) tensor(11303.2451, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11296.880859375
tensor(11303.2451, grad_fn=<NegBackward0>) tensor(11296.8809, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11093.431640625
tensor(11296.8809, grad_fn=<NegBackward0>) tensor(11093.4316, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11049.8095703125
tensor(11093.4316, grad_fn=<NegBackward0>) tensor(11049.8096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11018.7421875
tensor(11049.8096, grad_fn=<NegBackward0>) tensor(11018.7422, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11018.318359375
tensor(11018.7422, grad_fn=<NegBackward0>) tensor(11018.3184, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11018.1552734375
tensor(11018.3184, grad_fn=<NegBackward0>) tensor(11018.1553, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11018.033203125
tensor(11018.1553, grad_fn=<NegBackward0>) tensor(11018.0332, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11017.9580078125
tensor(11018.0332, grad_fn=<NegBackward0>) tensor(11017.9580, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11017.90234375
tensor(11017.9580, grad_fn=<NegBackward0>) tensor(11017.9023, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11016.5380859375
tensor(11017.9023, grad_fn=<NegBackward0>) tensor(11016.5381, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11016.5234375
tensor(11016.5381, grad_fn=<NegBackward0>) tensor(11016.5234, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11016.51171875
tensor(11016.5234, grad_fn=<NegBackward0>) tensor(11016.5117, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11016.5048828125
tensor(11016.5117, grad_fn=<NegBackward0>) tensor(11016.5049, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11016.4951171875
tensor(11016.5049, grad_fn=<NegBackward0>) tensor(11016.4951, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11016.484375
tensor(11016.4951, grad_fn=<NegBackward0>) tensor(11016.4844, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11016.4697265625
tensor(11016.4844, grad_fn=<NegBackward0>) tensor(11016.4697, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11013.51953125
tensor(11016.4697, grad_fn=<NegBackward0>) tensor(11013.5195, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11013.51171875
tensor(11013.5195, grad_fn=<NegBackward0>) tensor(11013.5117, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11013.5048828125
tensor(11013.5117, grad_fn=<NegBackward0>) tensor(11013.5049, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11013.4990234375
tensor(11013.5049, grad_fn=<NegBackward0>) tensor(11013.4990, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11013.4892578125
tensor(11013.4990, grad_fn=<NegBackward0>) tensor(11013.4893, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11013.484375
tensor(11013.4893, grad_fn=<NegBackward0>) tensor(11013.4844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11013.4814453125
tensor(11013.4844, grad_fn=<NegBackward0>) tensor(11013.4814, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11013.4658203125
tensor(11013.4814, grad_fn=<NegBackward0>) tensor(11013.4658, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11013.44921875
tensor(11013.4658, grad_fn=<NegBackward0>) tensor(11013.4492, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11013.4482421875
tensor(11013.4492, grad_fn=<NegBackward0>) tensor(11013.4482, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11013.4482421875
tensor(11013.4482, grad_fn=<NegBackward0>) tensor(11013.4482, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11013.4423828125
tensor(11013.4482, grad_fn=<NegBackward0>) tensor(11013.4424, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11013.4384765625
tensor(11013.4424, grad_fn=<NegBackward0>) tensor(11013.4385, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11013.4326171875
tensor(11013.4385, grad_fn=<NegBackward0>) tensor(11013.4326, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11013.43359375
tensor(11013.4326, grad_fn=<NegBackward0>) tensor(11013.4336, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11013.431640625
tensor(11013.4326, grad_fn=<NegBackward0>) tensor(11013.4316, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11013.4453125
tensor(11013.4316, grad_fn=<NegBackward0>) tensor(11013.4453, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11013.431640625
tensor(11013.4316, grad_fn=<NegBackward0>) tensor(11013.4316, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11013.4296875
tensor(11013.4316, grad_fn=<NegBackward0>) tensor(11013.4297, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11013.4296875
tensor(11013.4297, grad_fn=<NegBackward0>) tensor(11013.4297, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11013.4287109375
tensor(11013.4297, grad_fn=<NegBackward0>) tensor(11013.4287, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11013.427734375
tensor(11013.4287, grad_fn=<NegBackward0>) tensor(11013.4277, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11013.400390625
tensor(11013.4277, grad_fn=<NegBackward0>) tensor(11013.4004, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11013.3994140625
tensor(11013.4004, grad_fn=<NegBackward0>) tensor(11013.3994, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11013.3994140625
tensor(11013.3994, grad_fn=<NegBackward0>) tensor(11013.3994, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11013.3994140625
tensor(11013.3994, grad_fn=<NegBackward0>) tensor(11013.3994, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11013.3984375
tensor(11013.3994, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11013.3984375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11013.3984375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11013.3984375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11013.3984375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11013.3984375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11013.396484375
tensor(11013.3984, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11013.396484375
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11013.396484375
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11013.4013671875
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.4014, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11013.396484375
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11013.396484375
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11013.396484375
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3965, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11013.416015625
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.4160, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11013.3955078125
tensor(11013.3965, grad_fn=<NegBackward0>) tensor(11013.3955, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11013.3984375
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3984, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11013.3955078125
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3955, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11013.3974609375
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3975, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11013.4033203125
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.4033, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11013.3955078125
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3955, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11013.3955078125
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3955, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11013.3876953125
tensor(11013.3955, grad_fn=<NegBackward0>) tensor(11013.3877, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11013.3857421875
tensor(11013.3877, grad_fn=<NegBackward0>) tensor(11013.3857, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11013.3837890625
tensor(11013.3857, grad_fn=<NegBackward0>) tensor(11013.3838, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11013.38671875
tensor(11013.3838, grad_fn=<NegBackward0>) tensor(11013.3867, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11013.384765625
tensor(11013.3838, grad_fn=<NegBackward0>) tensor(11013.3848, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11013.384765625
tensor(11013.3838, grad_fn=<NegBackward0>) tensor(11013.3848, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11013.171875
tensor(11013.3838, grad_fn=<NegBackward0>) tensor(11013.1719, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11013.1728515625
tensor(11013.1719, grad_fn=<NegBackward0>) tensor(11013.1729, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11013.1787109375
tensor(11013.1719, grad_fn=<NegBackward0>) tensor(11013.1787, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11013.1767578125
tensor(11013.1719, grad_fn=<NegBackward0>) tensor(11013.1768, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11013.1708984375
tensor(11013.1719, grad_fn=<NegBackward0>) tensor(11013.1709, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11013.171875
tensor(11013.1709, grad_fn=<NegBackward0>) tensor(11013.1719, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11013.1708984375
tensor(11013.1709, grad_fn=<NegBackward0>) tensor(11013.1709, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11013.1708984375
tensor(11013.1709, grad_fn=<NegBackward0>) tensor(11013.1709, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11013.271484375
tensor(11013.1709, grad_fn=<NegBackward0>) tensor(11013.2715, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11013.169921875
tensor(11013.1709, grad_fn=<NegBackward0>) tensor(11013.1699, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11013.1806640625
tensor(11013.1699, grad_fn=<NegBackward0>) tensor(11013.1807, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11013.1591796875
tensor(11013.1699, grad_fn=<NegBackward0>) tensor(11013.1592, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11013.1650390625
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1650, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11013.158203125
tensor(11013.1592, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11013.158203125
tensor(11013.1582, grad_fn=<NegBackward0>) tensor(11013.1582, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11013.1572265625
tensor(11013.1582, grad_fn=<NegBackward0>) tensor(11013.1572, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11013.1552734375
tensor(11013.1572, grad_fn=<NegBackward0>) tensor(11013.1553, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11013.1611328125
tensor(11013.1553, grad_fn=<NegBackward0>) tensor(11013.1611, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11013.154296875
tensor(11013.1553, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11013.1630859375
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1631, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11013.154296875
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11013.1767578125
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1768, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11013.154296875
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1543, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11013.162109375
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1621, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11013.1533203125
tensor(11013.1543, grad_fn=<NegBackward0>) tensor(11013.1533, grad_fn=<NegBackward0>)
pi: tensor([[0.7185, 0.2815],
        [0.2209, 0.7791]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4827, 0.5173], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3065, 0.0984],
         [0.7201, 0.1963]],

        [[0.6702, 0.1025],
         [0.6989, 0.5664]],

        [[0.6515, 0.0938],
         [0.5458, 0.5702]],

        [[0.5783, 0.0949],
         [0.6776, 0.6566]],

        [[0.7209, 0.1020],
         [0.5798, 0.5578]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
Global Adjusted Rand Index: 0.9061144075474898
Average Adjusted Rand Index: 0.9073832489123866
[0.9061144075474898, 0.9061144075474898] [0.9073832489123866, 0.9073832489123866] [11013.1357421875, 11013.1572265625]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11255.983212475014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21479.390625
inf tensor(21479.3906, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11586.4189453125
tensor(21479.3906, grad_fn=<NegBackward0>) tensor(11586.4189, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11546.1181640625
tensor(11586.4189, grad_fn=<NegBackward0>) tensor(11546.1182, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11259.7802734375
tensor(11546.1182, grad_fn=<NegBackward0>) tensor(11259.7803, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11241.056640625
tensor(11259.7803, grad_fn=<NegBackward0>) tensor(11241.0566, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11235.57421875
tensor(11241.0566, grad_fn=<NegBackward0>) tensor(11235.5742, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11235.4296875
tensor(11235.5742, grad_fn=<NegBackward0>) tensor(11235.4297, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11235.3642578125
tensor(11235.4297, grad_fn=<NegBackward0>) tensor(11235.3643, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11235.30078125
tensor(11235.3643, grad_fn=<NegBackward0>) tensor(11235.3008, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.2412109375
tensor(11235.3008, grad_fn=<NegBackward0>) tensor(11235.2412, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11234.9345703125
tensor(11235.2412, grad_fn=<NegBackward0>) tensor(11234.9346, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11234.9189453125
tensor(11234.9346, grad_fn=<NegBackward0>) tensor(11234.9189, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11234.9091796875
tensor(11234.9189, grad_fn=<NegBackward0>) tensor(11234.9092, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11234.9013671875
tensor(11234.9092, grad_fn=<NegBackward0>) tensor(11234.9014, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11234.89453125
tensor(11234.9014, grad_fn=<NegBackward0>) tensor(11234.8945, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11234.888671875
tensor(11234.8945, grad_fn=<NegBackward0>) tensor(11234.8887, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11234.7392578125
tensor(11234.8887, grad_fn=<NegBackward0>) tensor(11234.7393, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11234.734375
tensor(11234.7393, grad_fn=<NegBackward0>) tensor(11234.7344, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11234.7294921875
tensor(11234.7344, grad_fn=<NegBackward0>) tensor(11234.7295, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11234.7255859375
tensor(11234.7295, grad_fn=<NegBackward0>) tensor(11234.7256, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11234.7216796875
tensor(11234.7256, grad_fn=<NegBackward0>) tensor(11234.7217, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11234.71875
tensor(11234.7217, grad_fn=<NegBackward0>) tensor(11234.7188, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11234.716796875
tensor(11234.7188, grad_fn=<NegBackward0>) tensor(11234.7168, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11234.7158203125
tensor(11234.7168, grad_fn=<NegBackward0>) tensor(11234.7158, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11234.7197265625
tensor(11234.7158, grad_fn=<NegBackward0>) tensor(11234.7197, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11234.708984375
tensor(11234.7158, grad_fn=<NegBackward0>) tensor(11234.7090, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11234.6669921875
tensor(11234.7090, grad_fn=<NegBackward0>) tensor(11234.6670, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11234.6650390625
tensor(11234.6670, grad_fn=<NegBackward0>) tensor(11234.6650, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11234.6650390625
tensor(11234.6650, grad_fn=<NegBackward0>) tensor(11234.6650, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11234.6630859375
tensor(11234.6650, grad_fn=<NegBackward0>) tensor(11234.6631, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11234.677734375
tensor(11234.6631, grad_fn=<NegBackward0>) tensor(11234.6777, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11234.6630859375
tensor(11234.6631, grad_fn=<NegBackward0>) tensor(11234.6631, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11234.662109375
tensor(11234.6631, grad_fn=<NegBackward0>) tensor(11234.6621, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11234.6611328125
tensor(11234.6621, grad_fn=<NegBackward0>) tensor(11234.6611, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11234.6611328125
tensor(11234.6611, grad_fn=<NegBackward0>) tensor(11234.6611, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11234.6728515625
tensor(11234.6611, grad_fn=<NegBackward0>) tensor(11234.6729, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11234.66015625
tensor(11234.6611, grad_fn=<NegBackward0>) tensor(11234.6602, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11234.658203125
tensor(11234.6602, grad_fn=<NegBackward0>) tensor(11234.6582, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11234.6591796875
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6592, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11234.6591796875
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6592, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11234.6650390625
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6650, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -11234.658203125
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6582, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11234.658203125
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6582, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11234.6572265625
tensor(11234.6582, grad_fn=<NegBackward0>) tensor(11234.6572, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11234.6572265625
tensor(11234.6572, grad_fn=<NegBackward0>) tensor(11234.6572, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11234.66015625
tensor(11234.6572, grad_fn=<NegBackward0>) tensor(11234.6602, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11234.6572265625
tensor(11234.6572, grad_fn=<NegBackward0>) tensor(11234.6572, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11234.6552734375
tensor(11234.6572, grad_fn=<NegBackward0>) tensor(11234.6553, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11234.654296875
tensor(11234.6553, grad_fn=<NegBackward0>) tensor(11234.6543, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11234.6640625
tensor(11234.6543, grad_fn=<NegBackward0>) tensor(11234.6641, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11234.6552734375
tensor(11234.6543, grad_fn=<NegBackward0>) tensor(11234.6553, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11234.65625
tensor(11234.6543, grad_fn=<NegBackward0>) tensor(11234.6562, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11234.6552734375
tensor(11234.6543, grad_fn=<NegBackward0>) tensor(11234.6553, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11234.65625
tensor(11234.6543, grad_fn=<NegBackward0>) tensor(11234.6562, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.7649, 0.2351],
        [0.2551, 0.7449]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4104, 0.5896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.0929],
         [0.5146, 0.3085]],

        [[0.6010, 0.0992],
         [0.6330, 0.6367]],

        [[0.7234, 0.1016],
         [0.7306, 0.6086]],

        [[0.7262, 0.0974],
         [0.6421, 0.5601]],

        [[0.6746, 0.1056],
         [0.5652, 0.7156]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.929154224198999
Average Adjusted Rand Index: 0.9296034544181598
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22517.87890625
inf tensor(22517.8789, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11597.591796875
tensor(22517.8789, grad_fn=<NegBackward0>) tensor(11597.5918, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11580.76953125
tensor(11597.5918, grad_fn=<NegBackward0>) tensor(11580.7695, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11505.6357421875
tensor(11580.7695, grad_fn=<NegBackward0>) tensor(11505.6357, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11420.7783203125
tensor(11505.6357, grad_fn=<NegBackward0>) tensor(11420.7783, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11269.41796875
tensor(11420.7783, grad_fn=<NegBackward0>) tensor(11269.4180, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11247.400390625
tensor(11269.4180, grad_fn=<NegBackward0>) tensor(11247.4004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11237.3330078125
tensor(11247.4004, grad_fn=<NegBackward0>) tensor(11237.3330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11236.736328125
tensor(11237.3330, grad_fn=<NegBackward0>) tensor(11236.7363, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.3544921875
tensor(11236.7363, grad_fn=<NegBackward0>) tensor(11235.3545, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11234.2001953125
tensor(11235.3545, grad_fn=<NegBackward0>) tensor(11234.2002, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11234.126953125
tensor(11234.2002, grad_fn=<NegBackward0>) tensor(11234.1270, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11234.0830078125
tensor(11234.1270, grad_fn=<NegBackward0>) tensor(11234.0830, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11234.048828125
tensor(11234.0830, grad_fn=<NegBackward0>) tensor(11234.0488, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11234.0146484375
tensor(11234.0488, grad_fn=<NegBackward0>) tensor(11234.0146, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11233.9736328125
tensor(11234.0146, grad_fn=<NegBackward0>) tensor(11233.9736, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11233.927734375
tensor(11233.9736, grad_fn=<NegBackward0>) tensor(11233.9277, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11233.88671875
tensor(11233.9277, grad_fn=<NegBackward0>) tensor(11233.8867, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11233.8740234375
tensor(11233.8867, grad_fn=<NegBackward0>) tensor(11233.8740, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11233.8642578125
tensor(11233.8740, grad_fn=<NegBackward0>) tensor(11233.8643, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11233.85546875
tensor(11233.8643, grad_fn=<NegBackward0>) tensor(11233.8555, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11233.8486328125
tensor(11233.8555, grad_fn=<NegBackward0>) tensor(11233.8486, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11233.837890625
tensor(11233.8486, grad_fn=<NegBackward0>) tensor(11233.8379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11233.7802734375
tensor(11233.8379, grad_fn=<NegBackward0>) tensor(11233.7803, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11233.7626953125
tensor(11233.7803, grad_fn=<NegBackward0>) tensor(11233.7627, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11233.7314453125
tensor(11233.7627, grad_fn=<NegBackward0>) tensor(11233.7314, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11233.7236328125
tensor(11233.7314, grad_fn=<NegBackward0>) tensor(11233.7236, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11233.7080078125
tensor(11233.7236, grad_fn=<NegBackward0>) tensor(11233.7080, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11233.7001953125
tensor(11233.7080, grad_fn=<NegBackward0>) tensor(11233.7002, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11233.6982421875
tensor(11233.7002, grad_fn=<NegBackward0>) tensor(11233.6982, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11233.6943359375
tensor(11233.6982, grad_fn=<NegBackward0>) tensor(11233.6943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11233.6923828125
tensor(11233.6943, grad_fn=<NegBackward0>) tensor(11233.6924, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11233.689453125
tensor(11233.6924, grad_fn=<NegBackward0>) tensor(11233.6895, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11233.6875
tensor(11233.6895, grad_fn=<NegBackward0>) tensor(11233.6875, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11233.685546875
tensor(11233.6875, grad_fn=<NegBackward0>) tensor(11233.6855, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11233.6845703125
tensor(11233.6855, grad_fn=<NegBackward0>) tensor(11233.6846, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11233.68359375
tensor(11233.6846, grad_fn=<NegBackward0>) tensor(11233.6836, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11233.6796875
tensor(11233.6836, grad_fn=<NegBackward0>) tensor(11233.6797, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11233.6787109375
tensor(11233.6797, grad_fn=<NegBackward0>) tensor(11233.6787, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11233.67578125
tensor(11233.6787, grad_fn=<NegBackward0>) tensor(11233.6758, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11233.6728515625
tensor(11233.6758, grad_fn=<NegBackward0>) tensor(11233.6729, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11233.671875
tensor(11233.6729, grad_fn=<NegBackward0>) tensor(11233.6719, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11233.6708984375
tensor(11233.6719, grad_fn=<NegBackward0>) tensor(11233.6709, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11233.666015625
tensor(11233.6709, grad_fn=<NegBackward0>) tensor(11233.6660, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11233.6572265625
tensor(11233.6660, grad_fn=<NegBackward0>) tensor(11233.6572, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11233.6572265625
tensor(11233.6572, grad_fn=<NegBackward0>) tensor(11233.6572, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11233.65625
tensor(11233.6572, grad_fn=<NegBackward0>) tensor(11233.6562, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11233.6552734375
tensor(11233.6562, grad_fn=<NegBackward0>) tensor(11233.6553, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11233.654296875
tensor(11233.6553, grad_fn=<NegBackward0>) tensor(11233.6543, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11233.6552734375
tensor(11233.6543, grad_fn=<NegBackward0>) tensor(11233.6553, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11233.6533203125
tensor(11233.6543, grad_fn=<NegBackward0>) tensor(11233.6533, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11233.654296875
tensor(11233.6533, grad_fn=<NegBackward0>) tensor(11233.6543, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11233.6533203125
tensor(11233.6533, grad_fn=<NegBackward0>) tensor(11233.6533, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11233.662109375
tensor(11233.6533, grad_fn=<NegBackward0>) tensor(11233.6621, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11233.6484375
tensor(11233.6533, grad_fn=<NegBackward0>) tensor(11233.6484, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11233.6455078125
tensor(11233.6484, grad_fn=<NegBackward0>) tensor(11233.6455, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11233.6455078125
tensor(11233.6455, grad_fn=<NegBackward0>) tensor(11233.6455, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11233.6435546875
tensor(11233.6455, grad_fn=<NegBackward0>) tensor(11233.6436, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11233.6435546875
tensor(11233.6436, grad_fn=<NegBackward0>) tensor(11233.6436, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11233.6435546875
tensor(11233.6436, grad_fn=<NegBackward0>) tensor(11233.6436, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11233.642578125
tensor(11233.6436, grad_fn=<NegBackward0>) tensor(11233.6426, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11233.625
tensor(11233.6426, grad_fn=<NegBackward0>) tensor(11233.6250, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11233.623046875
tensor(11233.6250, grad_fn=<NegBackward0>) tensor(11233.6230, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11233.62109375
tensor(11233.6230, grad_fn=<NegBackward0>) tensor(11233.6211, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11233.6220703125
tensor(11233.6211, grad_fn=<NegBackward0>) tensor(11233.6221, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11233.6201171875
tensor(11233.6211, grad_fn=<NegBackward0>) tensor(11233.6201, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11233.6181640625
tensor(11233.6201, grad_fn=<NegBackward0>) tensor(11233.6182, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11233.62109375
tensor(11233.6182, grad_fn=<NegBackward0>) tensor(11233.6211, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11233.619140625
tensor(11233.6182, grad_fn=<NegBackward0>) tensor(11233.6191, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11233.6123046875
tensor(11233.6182, grad_fn=<NegBackward0>) tensor(11233.6123, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11233.6123046875
tensor(11233.6123, grad_fn=<NegBackward0>) tensor(11233.6123, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11233.61328125
tensor(11233.6123, grad_fn=<NegBackward0>) tensor(11233.6133, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11233.611328125
tensor(11233.6123, grad_fn=<NegBackward0>) tensor(11233.6113, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11233.6103515625
tensor(11233.6113, grad_fn=<NegBackward0>) tensor(11233.6104, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11233.6142578125
tensor(11233.6104, grad_fn=<NegBackward0>) tensor(11233.6143, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11233.609375
tensor(11233.6104, grad_fn=<NegBackward0>) tensor(11233.6094, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11233.609375
tensor(11233.6094, grad_fn=<NegBackward0>) tensor(11233.6094, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11233.609375
tensor(11233.6094, grad_fn=<NegBackward0>) tensor(11233.6094, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11233.6201171875
tensor(11233.6094, grad_fn=<NegBackward0>) tensor(11233.6201, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11233.6083984375
tensor(11233.6094, grad_fn=<NegBackward0>) tensor(11233.6084, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11233.6064453125
tensor(11233.6084, grad_fn=<NegBackward0>) tensor(11233.6064, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11233.6083984375
tensor(11233.6064, grad_fn=<NegBackward0>) tensor(11233.6084, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11233.607421875
tensor(11233.6064, grad_fn=<NegBackward0>) tensor(11233.6074, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11233.6064453125
tensor(11233.6064, grad_fn=<NegBackward0>) tensor(11233.6064, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11233.603515625
tensor(11233.6064, grad_fn=<NegBackward0>) tensor(11233.6035, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11233.6435546875
tensor(11233.6035, grad_fn=<NegBackward0>) tensor(11233.6436, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11233.6083984375
tensor(11233.6035, grad_fn=<NegBackward0>) tensor(11233.6084, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11233.599609375
tensor(11233.6035, grad_fn=<NegBackward0>) tensor(11233.5996, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11233.59765625
tensor(11233.5996, grad_fn=<NegBackward0>) tensor(11233.5977, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11233.59765625
tensor(11233.5977, grad_fn=<NegBackward0>) tensor(11233.5977, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11233.599609375
tensor(11233.5977, grad_fn=<NegBackward0>) tensor(11233.5996, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11233.5947265625
tensor(11233.5977, grad_fn=<NegBackward0>) tensor(11233.5947, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11233.595703125
tensor(11233.5947, grad_fn=<NegBackward0>) tensor(11233.5957, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11233.5947265625
tensor(11233.5947, grad_fn=<NegBackward0>) tensor(11233.5947, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11233.5986328125
tensor(11233.5947, grad_fn=<NegBackward0>) tensor(11233.5986, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11233.59375
tensor(11233.5947, grad_fn=<NegBackward0>) tensor(11233.5938, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11233.6748046875
tensor(11233.5938, grad_fn=<NegBackward0>) tensor(11233.6748, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11233.59375
tensor(11233.5938, grad_fn=<NegBackward0>) tensor(11233.5938, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11233.595703125
tensor(11233.5938, grad_fn=<NegBackward0>) tensor(11233.5957, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11233.59375
tensor(11233.5938, grad_fn=<NegBackward0>) tensor(11233.5938, grad_fn=<NegBackward0>)
pi: tensor([[0.7674, 0.2326],
        [0.2563, 0.7437]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4118, 0.5882], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.0931],
         [0.5208, 0.3095]],

        [[0.5696, 0.1003],
         [0.7225, 0.5994]],

        [[0.5025, 0.1016],
         [0.5120, 0.5581]],

        [[0.6618, 0.0974],
         [0.5133, 0.5446]],

        [[0.5968, 0.1057],
         [0.6661, 0.7142]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.9368977019102417
Average Adjusted Rand Index: 0.9371189621145032
[0.929154224198999, 0.9368977019102417] [0.9296034544181598, 0.9371189621145032] [11234.65625, 11233.59375]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11254.820836290199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19848.556640625
inf tensor(19848.5566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11460.5986328125
tensor(19848.5566, grad_fn=<NegBackward0>) tensor(11460.5986, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11458.884765625
tensor(11460.5986, grad_fn=<NegBackward0>) tensor(11458.8848, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11458.1845703125
tensor(11458.8848, grad_fn=<NegBackward0>) tensor(11458.1846, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11457.67578125
tensor(11458.1846, grad_fn=<NegBackward0>) tensor(11457.6758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11457.29296875
tensor(11457.6758, grad_fn=<NegBackward0>) tensor(11457.2930, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11457.123046875
tensor(11457.2930, grad_fn=<NegBackward0>) tensor(11457.1230, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11456.990234375
tensor(11457.1230, grad_fn=<NegBackward0>) tensor(11456.9902, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11456.8125
tensor(11456.9902, grad_fn=<NegBackward0>) tensor(11456.8125, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11456.4912109375
tensor(11456.8125, grad_fn=<NegBackward0>) tensor(11456.4912, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11455.44921875
tensor(11456.4912, grad_fn=<NegBackward0>) tensor(11455.4492, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11400.240234375
tensor(11455.4492, grad_fn=<NegBackward0>) tensor(11400.2402, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11396.3037109375
tensor(11400.2402, grad_fn=<NegBackward0>) tensor(11396.3037, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11394.7431640625
tensor(11396.3037, grad_fn=<NegBackward0>) tensor(11394.7432, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11394.7060546875
tensor(11394.7432, grad_fn=<NegBackward0>) tensor(11394.7061, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11394.6884765625
tensor(11394.7061, grad_fn=<NegBackward0>) tensor(11394.6885, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11394.6767578125
tensor(11394.6885, grad_fn=<NegBackward0>) tensor(11394.6768, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11394.21875
tensor(11394.6768, grad_fn=<NegBackward0>) tensor(11394.2188, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11314.2490234375
tensor(11394.2188, grad_fn=<NegBackward0>) tensor(11314.2490, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11296.005859375
tensor(11314.2490, grad_fn=<NegBackward0>) tensor(11296.0059, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11295.68359375
tensor(11296.0059, grad_fn=<NegBackward0>) tensor(11295.6836, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11292.6767578125
tensor(11295.6836, grad_fn=<NegBackward0>) tensor(11292.6768, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11291.8505859375
tensor(11292.6768, grad_fn=<NegBackward0>) tensor(11291.8506, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11290.2060546875
tensor(11291.8506, grad_fn=<NegBackward0>) tensor(11290.2061, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11288.9345703125
tensor(11290.2061, grad_fn=<NegBackward0>) tensor(11288.9346, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11288.8896484375
tensor(11288.9346, grad_fn=<NegBackward0>) tensor(11288.8896, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11288.884765625
tensor(11288.8896, grad_fn=<NegBackward0>) tensor(11288.8848, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11288.8818359375
tensor(11288.8848, grad_fn=<NegBackward0>) tensor(11288.8818, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11288.8486328125
tensor(11288.8818, grad_fn=<NegBackward0>) tensor(11288.8486, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11288.853515625
tensor(11288.8486, grad_fn=<NegBackward0>) tensor(11288.8535, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11288.83984375
tensor(11288.8486, grad_fn=<NegBackward0>) tensor(11288.8398, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11288.8232421875
tensor(11288.8398, grad_fn=<NegBackward0>) tensor(11288.8232, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11286.3525390625
tensor(11288.8232, grad_fn=<NegBackward0>) tensor(11286.3525, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11286.33984375
tensor(11286.3525, grad_fn=<NegBackward0>) tensor(11286.3398, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11286.3173828125
tensor(11286.3398, grad_fn=<NegBackward0>) tensor(11286.3174, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11286.3115234375
tensor(11286.3174, grad_fn=<NegBackward0>) tensor(11286.3115, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11286.30078125
tensor(11286.3115, grad_fn=<NegBackward0>) tensor(11286.3008, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11286.2939453125
tensor(11286.3008, grad_fn=<NegBackward0>) tensor(11286.2939, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11285.6201171875
tensor(11286.2939, grad_fn=<NegBackward0>) tensor(11285.6201, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11282.0224609375
tensor(11285.6201, grad_fn=<NegBackward0>) tensor(11282.0225, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11281.9482421875
tensor(11282.0225, grad_fn=<NegBackward0>) tensor(11281.9482, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11281.9462890625
tensor(11281.9482, grad_fn=<NegBackward0>) tensor(11281.9463, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11281.9453125
tensor(11281.9463, grad_fn=<NegBackward0>) tensor(11281.9453, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11281.9443359375
tensor(11281.9453, grad_fn=<NegBackward0>) tensor(11281.9443, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11281.9345703125
tensor(11281.9443, grad_fn=<NegBackward0>) tensor(11281.9346, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11281.9345703125
tensor(11281.9346, grad_fn=<NegBackward0>) tensor(11281.9346, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11281.919921875
tensor(11281.9346, grad_fn=<NegBackward0>) tensor(11281.9199, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11281.919921875
tensor(11281.9199, grad_fn=<NegBackward0>) tensor(11281.9199, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11281.8857421875
tensor(11281.9199, grad_fn=<NegBackward0>) tensor(11281.8857, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11281.88671875
tensor(11281.8857, grad_fn=<NegBackward0>) tensor(11281.8867, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11281.8837890625
tensor(11281.8857, grad_fn=<NegBackward0>) tensor(11281.8838, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11281.884765625
tensor(11281.8838, grad_fn=<NegBackward0>) tensor(11281.8848, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11281.880859375
tensor(11281.8838, grad_fn=<NegBackward0>) tensor(11281.8809, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11281.830078125
tensor(11281.8809, grad_fn=<NegBackward0>) tensor(11281.8301, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11281.828125
tensor(11281.8301, grad_fn=<NegBackward0>) tensor(11281.8281, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11280.1142578125
tensor(11281.8281, grad_fn=<NegBackward0>) tensor(11280.1143, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11279.13671875
tensor(11280.1143, grad_fn=<NegBackward0>) tensor(11279.1367, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11279.140625
tensor(11279.1367, grad_fn=<NegBackward0>) tensor(11279.1406, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11278.775390625
tensor(11279.1367, grad_fn=<NegBackward0>) tensor(11278.7754, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11278.74609375
tensor(11278.7754, grad_fn=<NegBackward0>) tensor(11278.7461, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11278.7431640625
tensor(11278.7461, grad_fn=<NegBackward0>) tensor(11278.7432, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11278.7431640625
tensor(11278.7432, grad_fn=<NegBackward0>) tensor(11278.7432, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11277.453125
tensor(11278.7432, grad_fn=<NegBackward0>) tensor(11277.4531, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11275.69140625
tensor(11277.4531, grad_fn=<NegBackward0>) tensor(11275.6914, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11275.681640625
tensor(11275.6914, grad_fn=<NegBackward0>) tensor(11275.6816, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11275.6748046875
tensor(11275.6816, grad_fn=<NegBackward0>) tensor(11275.6748, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11275.6728515625
tensor(11275.6748, grad_fn=<NegBackward0>) tensor(11275.6729, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11275.6357421875
tensor(11275.6729, grad_fn=<NegBackward0>) tensor(11275.6357, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11275.6357421875
tensor(11275.6357, grad_fn=<NegBackward0>) tensor(11275.6357, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11275.638671875
tensor(11275.6357, grad_fn=<NegBackward0>) tensor(11275.6387, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11275.634765625
tensor(11275.6357, grad_fn=<NegBackward0>) tensor(11275.6348, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11275.634765625
tensor(11275.6348, grad_fn=<NegBackward0>) tensor(11275.6348, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11275.634765625
tensor(11275.6348, grad_fn=<NegBackward0>) tensor(11275.6348, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11275.6318359375
tensor(11275.6348, grad_fn=<NegBackward0>) tensor(11275.6318, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11275.6826171875
tensor(11275.6318, grad_fn=<NegBackward0>) tensor(11275.6826, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11275.625
tensor(11275.6318, grad_fn=<NegBackward0>) tensor(11275.6250, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11275.6328125
tensor(11275.6250, grad_fn=<NegBackward0>) tensor(11275.6328, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11275.6240234375
tensor(11275.6250, grad_fn=<NegBackward0>) tensor(11275.6240, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11275.6279296875
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6279, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11275.6240234375
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6240, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11275.625
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6250, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11275.6240234375
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6240, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11275.6396484375
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6396, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11275.6240234375
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6240, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11275.6396484375
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6396, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11275.6201171875
tensor(11275.6240, grad_fn=<NegBackward0>) tensor(11275.6201, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11275.681640625
tensor(11275.6201, grad_fn=<NegBackward0>) tensor(11275.6816, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11273.197265625
tensor(11275.6201, grad_fn=<NegBackward0>) tensor(11273.1973, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11273.0986328125
tensor(11273.1973, grad_fn=<NegBackward0>) tensor(11273.0986, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11273.099609375
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0996, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11273.0986328125
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0986, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11273.099609375
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0996, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11273.0986328125
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0986, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11273.1025390625
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.1025, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11273.0986328125
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0986, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11273.09765625
tensor(11273.0986, grad_fn=<NegBackward0>) tensor(11273.0977, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11273.0869140625
tensor(11273.0977, grad_fn=<NegBackward0>) tensor(11273.0869, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11273.09375
tensor(11273.0869, grad_fn=<NegBackward0>) tensor(11273.0938, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11273.0869140625
tensor(11273.0869, grad_fn=<NegBackward0>) tensor(11273.0869, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11273.0908203125
tensor(11273.0869, grad_fn=<NegBackward0>) tensor(11273.0908, grad_fn=<NegBackward0>)
1
pi: tensor([[0.2163, 0.7837],
        [0.6901, 0.3099]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5637, 0.4363], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2633, 0.0944],
         [0.5571, 0.2288]],

        [[0.5331, 0.1045],
         [0.5305, 0.5737]],

        [[0.7190, 0.1049],
         [0.5917, 0.6215]],

        [[0.7098, 0.1025],
         [0.5096, 0.5886]],

        [[0.5240, 0.1123],
         [0.5919, 0.5822]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8081003563518231
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369635135591801
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8081003563518231
Global Adjusted Rand Index: 0.03969334608479265
Average Adjusted Rand Index: 0.8237633023220271
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23877.5390625
inf tensor(23877.5391, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11226.5625
tensor(23877.5391, grad_fn=<NegBackward0>) tensor(11226.5625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11224.9072265625
tensor(11226.5625, grad_fn=<NegBackward0>) tensor(11224.9072, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11224.76953125
tensor(11224.9072, grad_fn=<NegBackward0>) tensor(11224.7695, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11224.716796875
tensor(11224.7695, grad_fn=<NegBackward0>) tensor(11224.7168, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11224.6904296875
tensor(11224.7168, grad_fn=<NegBackward0>) tensor(11224.6904, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11224.6748046875
tensor(11224.6904, grad_fn=<NegBackward0>) tensor(11224.6748, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11224.666015625
tensor(11224.6748, grad_fn=<NegBackward0>) tensor(11224.6660, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11224.6591796875
tensor(11224.6660, grad_fn=<NegBackward0>) tensor(11224.6592, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11224.654296875
tensor(11224.6592, grad_fn=<NegBackward0>) tensor(11224.6543, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11224.650390625
tensor(11224.6543, grad_fn=<NegBackward0>) tensor(11224.6504, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11224.6474609375
tensor(11224.6504, grad_fn=<NegBackward0>) tensor(11224.6475, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11224.64453125
tensor(11224.6475, grad_fn=<NegBackward0>) tensor(11224.6445, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11224.6435546875
tensor(11224.6445, grad_fn=<NegBackward0>) tensor(11224.6436, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11224.642578125
tensor(11224.6436, grad_fn=<NegBackward0>) tensor(11224.6426, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11224.640625
tensor(11224.6426, grad_fn=<NegBackward0>) tensor(11224.6406, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11224.6396484375
tensor(11224.6406, grad_fn=<NegBackward0>) tensor(11224.6396, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11224.638671875
tensor(11224.6396, grad_fn=<NegBackward0>) tensor(11224.6387, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11224.638671875
tensor(11224.6387, grad_fn=<NegBackward0>) tensor(11224.6387, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11224.638671875
tensor(11224.6387, grad_fn=<NegBackward0>) tensor(11224.6387, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11224.638671875
tensor(11224.6387, grad_fn=<NegBackward0>) tensor(11224.6387, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11224.6376953125
tensor(11224.6387, grad_fn=<NegBackward0>) tensor(11224.6377, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11224.63671875
tensor(11224.6377, grad_fn=<NegBackward0>) tensor(11224.6367, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11224.6357421875
tensor(11224.6367, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11224.6357421875
tensor(11224.6357, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11224.6357421875
tensor(11224.6357, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11224.634765625
tensor(11224.6357, grad_fn=<NegBackward0>) tensor(11224.6348, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11224.6357421875
tensor(11224.6348, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11224.634765625
tensor(11224.6348, grad_fn=<NegBackward0>) tensor(11224.6348, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11224.6337890625
tensor(11224.6348, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11224.634765625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6348, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11224.634765625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6348, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -11224.6357421875
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -11224.6337890625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11224.634765625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6348, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11224.6337890625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11224.6337890625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11224.642578125
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6426, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11224.6572265625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6572, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11224.6337890625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11224.650390625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6504, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11224.6337890625
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11224.6328125
tensor(11224.6338, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11224.6328125
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11224.6328125
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11224.63671875
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6367, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11224.6328125
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11224.6337890625
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11224.6328125
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11224.6357421875
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11224.63671875
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6367, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11224.6318359375
tensor(11224.6328, grad_fn=<NegBackward0>) tensor(11224.6318, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11224.6337890625
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11224.642578125
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6426, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11224.6318359375
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6318, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11224.6357421875
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11224.6337890625
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6338, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11224.6357421875
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6357, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11224.6328125
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11224.6328125
tensor(11224.6318, grad_fn=<NegBackward0>) tensor(11224.6328, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.7543, 0.2457],
        [0.2945, 0.7055]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4630, 0.5370], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.0959],
         [0.5114, 0.2969]],

        [[0.5189, 0.1044],
         [0.6973, 0.5422]],

        [[0.6139, 0.1073],
         [0.5634, 0.6423]],

        [[0.5669, 0.1045],
         [0.5075, 0.6915]],

        [[0.5896, 0.1148],
         [0.7185, 0.5277]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369635135591801
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.8909175636830465
Average Adjusted Rand Index: 0.891871151361937
[0.03969334608479265, 0.8909175636830465] [0.8237633023220271, 0.891871151361937] [11273.087890625, 11224.6328125]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11148.918492540199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20512.755859375
inf tensor(20512.7559, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11372.357421875
tensor(20512.7559, grad_fn=<NegBackward0>) tensor(11372.3574, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11369.3447265625
tensor(11372.3574, grad_fn=<NegBackward0>) tensor(11369.3447, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11354.482421875
tensor(11369.3447, grad_fn=<NegBackward0>) tensor(11354.4824, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11306.82421875
tensor(11354.4824, grad_fn=<NegBackward0>) tensor(11306.8242, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11234.216796875
tensor(11306.8242, grad_fn=<NegBackward0>) tensor(11234.2168, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11142.138671875
tensor(11234.2168, grad_fn=<NegBackward0>) tensor(11142.1387, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11138.1318359375
tensor(11142.1387, grad_fn=<NegBackward0>) tensor(11138.1318, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11123.0419921875
tensor(11138.1318, grad_fn=<NegBackward0>) tensor(11123.0420, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11122.79296875
tensor(11123.0420, grad_fn=<NegBackward0>) tensor(11122.7930, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11122.580078125
tensor(11122.7930, grad_fn=<NegBackward0>) tensor(11122.5801, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11122.2509765625
tensor(11122.5801, grad_fn=<NegBackward0>) tensor(11122.2510, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11122.22265625
tensor(11122.2510, grad_fn=<NegBackward0>) tensor(11122.2227, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11122.193359375
tensor(11122.2227, grad_fn=<NegBackward0>) tensor(11122.1934, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11122.1728515625
tensor(11122.1934, grad_fn=<NegBackward0>) tensor(11122.1729, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11122.154296875
tensor(11122.1729, grad_fn=<NegBackward0>) tensor(11122.1543, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11122.1455078125
tensor(11122.1543, grad_fn=<NegBackward0>) tensor(11122.1455, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11122.1279296875
tensor(11122.1455, grad_fn=<NegBackward0>) tensor(11122.1279, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11122.1181640625
tensor(11122.1279, grad_fn=<NegBackward0>) tensor(11122.1182, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11122.095703125
tensor(11122.1182, grad_fn=<NegBackward0>) tensor(11122.0957, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11121.6796875
tensor(11122.0957, grad_fn=<NegBackward0>) tensor(11121.6797, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11121.6640625
tensor(11121.6797, grad_fn=<NegBackward0>) tensor(11121.6641, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11121.6572265625
tensor(11121.6641, grad_fn=<NegBackward0>) tensor(11121.6572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11121.650390625
tensor(11121.6572, grad_fn=<NegBackward0>) tensor(11121.6504, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11121.5859375
tensor(11121.6504, grad_fn=<NegBackward0>) tensor(11121.5859, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11121.4921875
tensor(11121.5859, grad_fn=<NegBackward0>) tensor(11121.4922, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11121.4833984375
tensor(11121.4922, grad_fn=<NegBackward0>) tensor(11121.4834, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11121.4794921875
tensor(11121.4834, grad_fn=<NegBackward0>) tensor(11121.4795, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11121.4736328125
tensor(11121.4795, grad_fn=<NegBackward0>) tensor(11121.4736, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11121.4580078125
tensor(11121.4736, grad_fn=<NegBackward0>) tensor(11121.4580, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11121.4345703125
tensor(11121.4580, grad_fn=<NegBackward0>) tensor(11121.4346, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11121.4287109375
tensor(11121.4346, grad_fn=<NegBackward0>) tensor(11121.4287, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11121.42578125
tensor(11121.4287, grad_fn=<NegBackward0>) tensor(11121.4258, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11121.427734375
tensor(11121.4258, grad_fn=<NegBackward0>) tensor(11121.4277, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11121.419921875
tensor(11121.4258, grad_fn=<NegBackward0>) tensor(11121.4199, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11121.41796875
tensor(11121.4199, grad_fn=<NegBackward0>) tensor(11121.4180, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11121.4140625
tensor(11121.4180, grad_fn=<NegBackward0>) tensor(11121.4141, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11121.4091796875
tensor(11121.4141, grad_fn=<NegBackward0>) tensor(11121.4092, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11121.400390625
tensor(11121.4092, grad_fn=<NegBackward0>) tensor(11121.4004, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11121.3974609375
tensor(11121.4004, grad_fn=<NegBackward0>) tensor(11121.3975, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11121.3974609375
tensor(11121.3975, grad_fn=<NegBackward0>) tensor(11121.3975, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11121.39453125
tensor(11121.3975, grad_fn=<NegBackward0>) tensor(11121.3945, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11121.3935546875
tensor(11121.3945, grad_fn=<NegBackward0>) tensor(11121.3936, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11121.390625
tensor(11121.3936, grad_fn=<NegBackward0>) tensor(11121.3906, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11121.3896484375
tensor(11121.3906, grad_fn=<NegBackward0>) tensor(11121.3896, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11121.3876953125
tensor(11121.3896, grad_fn=<NegBackward0>) tensor(11121.3877, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11121.384765625
tensor(11121.3877, grad_fn=<NegBackward0>) tensor(11121.3848, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11121.3828125
tensor(11121.3848, grad_fn=<NegBackward0>) tensor(11121.3828, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11121.380859375
tensor(11121.3828, grad_fn=<NegBackward0>) tensor(11121.3809, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11121.37890625
tensor(11121.3809, grad_fn=<NegBackward0>) tensor(11121.3789, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11121.3779296875
tensor(11121.3789, grad_fn=<NegBackward0>) tensor(11121.3779, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11121.3818359375
tensor(11121.3779, grad_fn=<NegBackward0>) tensor(11121.3818, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11121.376953125
tensor(11121.3779, grad_fn=<NegBackward0>) tensor(11121.3770, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11121.3759765625
tensor(11121.3770, grad_fn=<NegBackward0>) tensor(11121.3760, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11121.376953125
tensor(11121.3760, grad_fn=<NegBackward0>) tensor(11121.3770, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11121.375
tensor(11121.3760, grad_fn=<NegBackward0>) tensor(11121.3750, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11121.2109375
tensor(11121.3750, grad_fn=<NegBackward0>) tensor(11121.2109, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11121.20703125
tensor(11121.2109, grad_fn=<NegBackward0>) tensor(11121.2070, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11121.2060546875
tensor(11121.2070, grad_fn=<NegBackward0>) tensor(11121.2061, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11121.205078125
tensor(11121.2061, grad_fn=<NegBackward0>) tensor(11121.2051, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11121.2041015625
tensor(11121.2051, grad_fn=<NegBackward0>) tensor(11121.2041, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11121.173828125
tensor(11121.2041, grad_fn=<NegBackward0>) tensor(11121.1738, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11121.1728515625
tensor(11121.1738, grad_fn=<NegBackward0>) tensor(11121.1729, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11121.1748046875
tensor(11121.1729, grad_fn=<NegBackward0>) tensor(11121.1748, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11121.171875
tensor(11121.1729, grad_fn=<NegBackward0>) tensor(11121.1719, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11121.1708984375
tensor(11121.1719, grad_fn=<NegBackward0>) tensor(11121.1709, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11121.169921875
tensor(11121.1709, grad_fn=<NegBackward0>) tensor(11121.1699, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11121.1708984375
tensor(11121.1699, grad_fn=<NegBackward0>) tensor(11121.1709, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11121.1708984375
tensor(11121.1699, grad_fn=<NegBackward0>) tensor(11121.1709, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11121.1689453125
tensor(11121.1699, grad_fn=<NegBackward0>) tensor(11121.1689, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11121.1689453125
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1689, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11121.1767578125
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1768, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11121.169921875
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1699, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11121.1689453125
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1689, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11121.1708984375
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1709, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11121.169921875
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1699, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11121.1669921875
tensor(11121.1689, grad_fn=<NegBackward0>) tensor(11121.1670, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11121.1904296875
tensor(11121.1670, grad_fn=<NegBackward0>) tensor(11121.1904, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11121.1689453125
tensor(11121.1670, grad_fn=<NegBackward0>) tensor(11121.1689, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11121.169921875
tensor(11121.1670, grad_fn=<NegBackward0>) tensor(11121.1699, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11121.16796875
tensor(11121.1670, grad_fn=<NegBackward0>) tensor(11121.1680, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11121.1748046875
tensor(11121.1670, grad_fn=<NegBackward0>) tensor(11121.1748, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.7512, 0.2488],
        [0.2907, 0.7093]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4863, 0.5137], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2882, 0.1018],
         [0.7194, 0.1948]],

        [[0.7066, 0.0936],
         [0.5784, 0.5594]],

        [[0.6789, 0.1054],
         [0.5081, 0.6929]],

        [[0.6577, 0.1082],
         [0.6103, 0.6421]],

        [[0.6018, 0.0965],
         [0.6571, 0.7080]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9137632854537449
Average Adjusted Rand Index: 0.9136093118688464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23147.359375
inf tensor(23147.3594, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11372.8291015625
tensor(23147.3594, grad_fn=<NegBackward0>) tensor(11372.8291, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11369.6494140625
tensor(11372.8291, grad_fn=<NegBackward0>) tensor(11369.6494, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11366.705078125
tensor(11369.6494, grad_fn=<NegBackward0>) tensor(11366.7051, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11360.642578125
tensor(11366.7051, grad_fn=<NegBackward0>) tensor(11360.6426, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11323.5126953125
tensor(11360.6426, grad_fn=<NegBackward0>) tensor(11323.5127, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11257.974609375
tensor(11323.5127, grad_fn=<NegBackward0>) tensor(11257.9746, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11151.6123046875
tensor(11257.9746, grad_fn=<NegBackward0>) tensor(11151.6123, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11146.6201171875
tensor(11151.6123, grad_fn=<NegBackward0>) tensor(11146.6201, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11144.4765625
tensor(11146.6201, grad_fn=<NegBackward0>) tensor(11144.4766, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11134.7099609375
tensor(11144.4766, grad_fn=<NegBackward0>) tensor(11134.7100, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11128.4150390625
tensor(11134.7100, grad_fn=<NegBackward0>) tensor(11128.4150, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11128.33203125
tensor(11128.4150, grad_fn=<NegBackward0>) tensor(11128.3320, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11128.248046875
tensor(11128.3320, grad_fn=<NegBackward0>) tensor(11128.2480, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11122.654296875
tensor(11128.2480, grad_fn=<NegBackward0>) tensor(11122.6543, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11122.615234375
tensor(11122.6543, grad_fn=<NegBackward0>) tensor(11122.6152, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11122.5791015625
tensor(11122.6152, grad_fn=<NegBackward0>) tensor(11122.5791, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11122.4365234375
tensor(11122.5791, grad_fn=<NegBackward0>) tensor(11122.4365, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11122.41796875
tensor(11122.4365, grad_fn=<NegBackward0>) tensor(11122.4180, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11122.4033203125
tensor(11122.4180, grad_fn=<NegBackward0>) tensor(11122.4033, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11122.326171875
tensor(11122.4033, grad_fn=<NegBackward0>) tensor(11122.3262, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11122.287109375
tensor(11122.3262, grad_fn=<NegBackward0>) tensor(11122.2871, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11122.2626953125
tensor(11122.2871, grad_fn=<NegBackward0>) tensor(11122.2627, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11122.2509765625
tensor(11122.2627, grad_fn=<NegBackward0>) tensor(11122.2510, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11122.2451171875
tensor(11122.2510, grad_fn=<NegBackward0>) tensor(11122.2451, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11122.11328125
tensor(11122.2451, grad_fn=<NegBackward0>) tensor(11122.1133, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11122.0791015625
tensor(11122.1133, grad_fn=<NegBackward0>) tensor(11122.0791, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11122.07421875
tensor(11122.0791, grad_fn=<NegBackward0>) tensor(11122.0742, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11122.0703125
tensor(11122.0742, grad_fn=<NegBackward0>) tensor(11122.0703, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11122.0625
tensor(11122.0703, grad_fn=<NegBackward0>) tensor(11122.0625, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11121.966796875
tensor(11122.0625, grad_fn=<NegBackward0>) tensor(11121.9668, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11121.2919921875
tensor(11121.9668, grad_fn=<NegBackward0>) tensor(11121.2920, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11121.287109375
tensor(11121.2920, grad_fn=<NegBackward0>) tensor(11121.2871, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11121.283203125
tensor(11121.2871, grad_fn=<NegBackward0>) tensor(11121.2832, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11121.28125
tensor(11121.2832, grad_fn=<NegBackward0>) tensor(11121.2812, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11121.2783203125
tensor(11121.2812, grad_fn=<NegBackward0>) tensor(11121.2783, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11121.2763671875
tensor(11121.2783, grad_fn=<NegBackward0>) tensor(11121.2764, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11121.2724609375
tensor(11121.2764, grad_fn=<NegBackward0>) tensor(11121.2725, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11121.2705078125
tensor(11121.2725, grad_fn=<NegBackward0>) tensor(11121.2705, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11121.2666015625
tensor(11121.2705, grad_fn=<NegBackward0>) tensor(11121.2666, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11121.265625
tensor(11121.2666, grad_fn=<NegBackward0>) tensor(11121.2656, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11121.265625
tensor(11121.2656, grad_fn=<NegBackward0>) tensor(11121.2656, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11121.2685546875
tensor(11121.2656, grad_fn=<NegBackward0>) tensor(11121.2686, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11121.26171875
tensor(11121.2656, grad_fn=<NegBackward0>) tensor(11121.2617, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11121.259765625
tensor(11121.2617, grad_fn=<NegBackward0>) tensor(11121.2598, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11121.263671875
tensor(11121.2598, grad_fn=<NegBackward0>) tensor(11121.2637, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11121.2578125
tensor(11121.2598, grad_fn=<NegBackward0>) tensor(11121.2578, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11121.2568359375
tensor(11121.2578, grad_fn=<NegBackward0>) tensor(11121.2568, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11121.2578125
tensor(11121.2568, grad_fn=<NegBackward0>) tensor(11121.2578, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11121.2568359375
tensor(11121.2568, grad_fn=<NegBackward0>) tensor(11121.2568, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11121.255859375
tensor(11121.2568, grad_fn=<NegBackward0>) tensor(11121.2559, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11121.2431640625
tensor(11121.2559, grad_fn=<NegBackward0>) tensor(11121.2432, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11121.234375
tensor(11121.2432, grad_fn=<NegBackward0>) tensor(11121.2344, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11121.2314453125
tensor(11121.2344, grad_fn=<NegBackward0>) tensor(11121.2314, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11121.232421875
tensor(11121.2314, grad_fn=<NegBackward0>) tensor(11121.2324, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11121.23046875
tensor(11121.2314, grad_fn=<NegBackward0>) tensor(11121.2305, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11121.2294921875
tensor(11121.2305, grad_fn=<NegBackward0>) tensor(11121.2295, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11121.23046875
tensor(11121.2295, grad_fn=<NegBackward0>) tensor(11121.2305, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11121.2294921875
tensor(11121.2295, grad_fn=<NegBackward0>) tensor(11121.2295, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11121.23046875
tensor(11121.2295, grad_fn=<NegBackward0>) tensor(11121.2305, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11121.2294921875
tensor(11121.2295, grad_fn=<NegBackward0>) tensor(11121.2295, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11121.224609375
tensor(11121.2295, grad_fn=<NegBackward0>) tensor(11121.2246, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11121.2392578125
tensor(11121.2246, grad_fn=<NegBackward0>) tensor(11121.2393, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11121.2197265625
tensor(11121.2246, grad_fn=<NegBackward0>) tensor(11121.2197, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11121.2177734375
tensor(11121.2197, grad_fn=<NegBackward0>) tensor(11121.2178, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11121.2197265625
tensor(11121.2178, grad_fn=<NegBackward0>) tensor(11121.2197, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11121.21875
tensor(11121.2178, grad_fn=<NegBackward0>) tensor(11121.2188, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11121.21875
tensor(11121.2178, grad_fn=<NegBackward0>) tensor(11121.2188, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11121.2392578125
tensor(11121.2178, grad_fn=<NegBackward0>) tensor(11121.2393, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11121.216796875
tensor(11121.2178, grad_fn=<NegBackward0>) tensor(11121.2168, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11121.2177734375
tensor(11121.2168, grad_fn=<NegBackward0>) tensor(11121.2178, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11121.21484375
tensor(11121.2168, grad_fn=<NegBackward0>) tensor(11121.2148, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11121.2138671875
tensor(11121.2148, grad_fn=<NegBackward0>) tensor(11121.2139, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11121.2099609375
tensor(11121.2139, grad_fn=<NegBackward0>) tensor(11121.2100, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11121.2109375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2109, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11121.2109375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2109, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11121.2109375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2109, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11121.2109375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2109, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11121.2099609375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2100, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11121.212890625
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2129, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11121.208984375
tensor(11121.2100, grad_fn=<NegBackward0>) tensor(11121.2090, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11121.208984375
tensor(11121.2090, grad_fn=<NegBackward0>) tensor(11121.2090, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11121.2080078125
tensor(11121.2090, grad_fn=<NegBackward0>) tensor(11121.2080, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11121.203125
tensor(11121.2080, grad_fn=<NegBackward0>) tensor(11121.2031, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11121.216796875
tensor(11121.2031, grad_fn=<NegBackward0>) tensor(11121.2168, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11121.201171875
tensor(11121.2031, grad_fn=<NegBackward0>) tensor(11121.2012, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11121.2138671875
tensor(11121.2012, grad_fn=<NegBackward0>) tensor(11121.2139, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11121.2001953125
tensor(11121.2012, grad_fn=<NegBackward0>) tensor(11121.2002, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11121.1953125
tensor(11121.2002, grad_fn=<NegBackward0>) tensor(11121.1953, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11121.1953125
tensor(11121.1953, grad_fn=<NegBackward0>) tensor(11121.1953, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11121.1943359375
tensor(11121.1953, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11121.203125
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.2031, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11121.1953125
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1953, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11121.19921875
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1992, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11121.1943359375
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11121.197265625
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1973, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11121.1943359375
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11121.1943359375
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11121.1943359375
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11121.1943359375
tensor(11121.1943, grad_fn=<NegBackward0>) tensor(11121.1943, grad_fn=<NegBackward0>)
pi: tensor([[0.7512, 0.2488],
        [0.2915, 0.7085]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4874, 0.5126], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2882, 0.1017],
         [0.5962, 0.1946]],

        [[0.5628, 0.0937],
         [0.5308, 0.5896]],

        [[0.6836, 0.1054],
         [0.6908, 0.5362]],

        [[0.6213, 0.1085],
         [0.5613, 0.5976]],

        [[0.7030, 0.0966],
         [0.5609, 0.5317]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9137632854537449
Average Adjusted Rand Index: 0.9136093118688464
[0.9137632854537449, 0.9137632854537449] [0.9136093118688464, 0.9136093118688464] [11121.1748046875, 11121.2001953125]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11234.038958632842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20866.55859375
inf tensor(20866.5586, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11561.880859375
tensor(20866.5586, grad_fn=<NegBackward0>) tensor(11561.8809, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11560.8154296875
tensor(11561.8809, grad_fn=<NegBackward0>) tensor(11560.8154, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11556.712890625
tensor(11560.8154, grad_fn=<NegBackward0>) tensor(11556.7129, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11547.6513671875
tensor(11556.7129, grad_fn=<NegBackward0>) tensor(11547.6514, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11499.064453125
tensor(11547.6514, grad_fn=<NegBackward0>) tensor(11499.0645, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11408.080078125
tensor(11499.0645, grad_fn=<NegBackward0>) tensor(11408.0801, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11384.1064453125
tensor(11408.0801, grad_fn=<NegBackward0>) tensor(11384.1064, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11376.2822265625
tensor(11384.1064, grad_fn=<NegBackward0>) tensor(11376.2822, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11370.4384765625
tensor(11376.2822, grad_fn=<NegBackward0>) tensor(11370.4385, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11369.9951171875
tensor(11370.4385, grad_fn=<NegBackward0>) tensor(11369.9951, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11367.6884765625
tensor(11369.9951, grad_fn=<NegBackward0>) tensor(11367.6885, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11367.5634765625
tensor(11367.6885, grad_fn=<NegBackward0>) tensor(11367.5635, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11367.369140625
tensor(11367.5635, grad_fn=<NegBackward0>) tensor(11367.3691, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11357.9091796875
tensor(11367.3691, grad_fn=<NegBackward0>) tensor(11357.9092, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11354.24609375
tensor(11357.9092, grad_fn=<NegBackward0>) tensor(11354.2461, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11350.2333984375
tensor(11354.2461, grad_fn=<NegBackward0>) tensor(11350.2334, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11348.75
tensor(11350.2334, grad_fn=<NegBackward0>) tensor(11348.7500, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11335.0947265625
tensor(11348.7500, grad_fn=<NegBackward0>) tensor(11335.0947, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11327.0869140625
tensor(11335.0947, grad_fn=<NegBackward0>) tensor(11327.0869, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11326.763671875
tensor(11327.0869, grad_fn=<NegBackward0>) tensor(11326.7637, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11326.7138671875
tensor(11326.7637, grad_fn=<NegBackward0>) tensor(11326.7139, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11319.626953125
tensor(11326.7139, grad_fn=<NegBackward0>) tensor(11319.6270, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11318.5439453125
tensor(11319.6270, grad_fn=<NegBackward0>) tensor(11318.5439, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11317.8134765625
tensor(11318.5439, grad_fn=<NegBackward0>) tensor(11317.8135, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11314.9326171875
tensor(11317.8135, grad_fn=<NegBackward0>) tensor(11314.9326, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11314.91015625
tensor(11314.9326, grad_fn=<NegBackward0>) tensor(11314.9102, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11314.892578125
tensor(11314.9102, grad_fn=<NegBackward0>) tensor(11314.8926, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11314.8583984375
tensor(11314.8926, grad_fn=<NegBackward0>) tensor(11314.8584, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11314.8525390625
tensor(11314.8584, grad_fn=<NegBackward0>) tensor(11314.8525, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11314.84765625
tensor(11314.8525, grad_fn=<NegBackward0>) tensor(11314.8477, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11314.8447265625
tensor(11314.8477, grad_fn=<NegBackward0>) tensor(11314.8447, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11314.83984375
tensor(11314.8447, grad_fn=<NegBackward0>) tensor(11314.8398, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11314.8388671875
tensor(11314.8398, grad_fn=<NegBackward0>) tensor(11314.8389, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11314.8359375
tensor(11314.8389, grad_fn=<NegBackward0>) tensor(11314.8359, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11314.8330078125
tensor(11314.8359, grad_fn=<NegBackward0>) tensor(11314.8330, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11313.9404296875
tensor(11314.8330, grad_fn=<NegBackward0>) tensor(11313.9404, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11313.921875
tensor(11313.9404, grad_fn=<NegBackward0>) tensor(11313.9219, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11312.609375
tensor(11313.9219, grad_fn=<NegBackward0>) tensor(11312.6094, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11311.96875
tensor(11312.6094, grad_fn=<NegBackward0>) tensor(11311.9688, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11311.9638671875
tensor(11311.9688, grad_fn=<NegBackward0>) tensor(11311.9639, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11311.9619140625
tensor(11311.9639, grad_fn=<NegBackward0>) tensor(11311.9619, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11311.958984375
tensor(11311.9619, grad_fn=<NegBackward0>) tensor(11311.9590, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11311.955078125
tensor(11311.9590, grad_fn=<NegBackward0>) tensor(11311.9551, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11311.94140625
tensor(11311.9551, grad_fn=<NegBackward0>) tensor(11311.9414, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11311.9404296875
tensor(11311.9414, grad_fn=<NegBackward0>) tensor(11311.9404, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11311.94140625
tensor(11311.9404, grad_fn=<NegBackward0>) tensor(11311.9414, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11311.9443359375
tensor(11311.9404, grad_fn=<NegBackward0>) tensor(11311.9443, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11311.9375
tensor(11311.9404, grad_fn=<NegBackward0>) tensor(11311.9375, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11311.9384765625
tensor(11311.9375, grad_fn=<NegBackward0>) tensor(11311.9385, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11311.9375
tensor(11311.9375, grad_fn=<NegBackward0>) tensor(11311.9375, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11311.9365234375
tensor(11311.9375, grad_fn=<NegBackward0>) tensor(11311.9365, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11311.9365234375
tensor(11311.9365, grad_fn=<NegBackward0>) tensor(11311.9365, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11311.9375
tensor(11311.9365, grad_fn=<NegBackward0>) tensor(11311.9375, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11311.935546875
tensor(11311.9365, grad_fn=<NegBackward0>) tensor(11311.9355, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11311.94921875
tensor(11311.9355, grad_fn=<NegBackward0>) tensor(11311.9492, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11311.93359375
tensor(11311.9355, grad_fn=<NegBackward0>) tensor(11311.9336, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11311.93359375
tensor(11311.9336, grad_fn=<NegBackward0>) tensor(11311.9336, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11311.931640625
tensor(11311.9336, grad_fn=<NegBackward0>) tensor(11311.9316, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11311.9326171875
tensor(11311.9316, grad_fn=<NegBackward0>) tensor(11311.9326, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11311.939453125
tensor(11311.9316, grad_fn=<NegBackward0>) tensor(11311.9395, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11311.9306640625
tensor(11311.9316, grad_fn=<NegBackward0>) tensor(11311.9307, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11311.927734375
tensor(11311.9307, grad_fn=<NegBackward0>) tensor(11311.9277, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11311.9296875
tensor(11311.9277, grad_fn=<NegBackward0>) tensor(11311.9297, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11311.9267578125
tensor(11311.9277, grad_fn=<NegBackward0>) tensor(11311.9268, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11311.931640625
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9316, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11311.9287109375
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9287, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11311.9267578125
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9268, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11311.9267578125
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9268, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11311.9267578125
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9268, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11311.923828125
tensor(11311.9268, grad_fn=<NegBackward0>) tensor(11311.9238, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11311.9169921875
tensor(11311.9238, grad_fn=<NegBackward0>) tensor(11311.9170, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11311.9169921875
tensor(11311.9170, grad_fn=<NegBackward0>) tensor(11311.9170, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11311.9140625
tensor(11311.9170, grad_fn=<NegBackward0>) tensor(11311.9141, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11311.9140625
tensor(11311.9141, grad_fn=<NegBackward0>) tensor(11311.9141, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11311.916015625
tensor(11311.9141, grad_fn=<NegBackward0>) tensor(11311.9160, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11311.9150390625
tensor(11311.9141, grad_fn=<NegBackward0>) tensor(11311.9150, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11311.869140625
tensor(11311.9141, grad_fn=<NegBackward0>) tensor(11311.8691, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11311.8671875
tensor(11311.8691, grad_fn=<NegBackward0>) tensor(11311.8672, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11311.8662109375
tensor(11311.8672, grad_fn=<NegBackward0>) tensor(11311.8662, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11311.8779296875
tensor(11311.8662, grad_fn=<NegBackward0>) tensor(11311.8779, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11311.865234375
tensor(11311.8662, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11311.8720703125
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8721, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11311.865234375
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11311.865234375
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11311.865234375
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11311.865234375
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11312.0166015625
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11312.0166, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11311.865234375
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11311.8642578125
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8643, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11311.8642578125
tensor(11311.8643, grad_fn=<NegBackward0>) tensor(11311.8643, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11312.033203125
tensor(11311.8643, grad_fn=<NegBackward0>) tensor(11312.0332, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11311.8603515625
tensor(11311.8643, grad_fn=<NegBackward0>) tensor(11311.8604, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11311.861328125
tensor(11311.8604, grad_fn=<NegBackward0>) tensor(11311.8613, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11311.8603515625
tensor(11311.8604, grad_fn=<NegBackward0>) tensor(11311.8604, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11311.8701171875
tensor(11311.8604, grad_fn=<NegBackward0>) tensor(11311.8701, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11311.8447265625
tensor(11311.8604, grad_fn=<NegBackward0>) tensor(11311.8447, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11311.8212890625
tensor(11311.8447, grad_fn=<NegBackward0>) tensor(11311.8213, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11311.9560546875
tensor(11311.8213, grad_fn=<NegBackward0>) tensor(11311.9561, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11311.8203125
tensor(11311.8213, grad_fn=<NegBackward0>) tensor(11311.8203, grad_fn=<NegBackward0>)
pi: tensor([[0.6141, 0.3859],
        [0.4045, 0.5955]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6700, 0.3300], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2223, 0.1033],
         [0.6119, 0.2946]],

        [[0.6265, 0.0880],
         [0.5486, 0.6220]],

        [[0.6802, 0.1005],
         [0.7130, 0.6434]],

        [[0.7065, 0.1008],
         [0.7302, 0.5627]],

        [[0.7035, 0.1003],
         [0.5172, 0.5974]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 20
Adjusted Rand Index: 0.353980823997459
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.702534919813761
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.08344352555302904
Average Adjusted Rand Index: 0.7719456937314482
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22598.05859375
inf tensor(22598.0586, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11559.439453125
tensor(22598.0586, grad_fn=<NegBackward0>) tensor(11559.4395, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11541.1953125
tensor(11559.4395, grad_fn=<NegBackward0>) tensor(11541.1953, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11286.7373046875
tensor(11541.1953, grad_fn=<NegBackward0>) tensor(11286.7373, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11230.580078125
tensor(11286.7373, grad_fn=<NegBackward0>) tensor(11230.5801, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11223.603515625
tensor(11230.5801, grad_fn=<NegBackward0>) tensor(11223.6035, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11213.681640625
tensor(11223.6035, grad_fn=<NegBackward0>) tensor(11213.6816, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11213.6298828125
tensor(11213.6816, grad_fn=<NegBackward0>) tensor(11213.6299, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11213.59765625
tensor(11213.6299, grad_fn=<NegBackward0>) tensor(11213.5977, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11213.5771484375
tensor(11213.5977, grad_fn=<NegBackward0>) tensor(11213.5771, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11213.5576171875
tensor(11213.5771, grad_fn=<NegBackward0>) tensor(11213.5576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11213.5419921875
tensor(11213.5576, grad_fn=<NegBackward0>) tensor(11213.5420, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11213.515625
tensor(11213.5420, grad_fn=<NegBackward0>) tensor(11213.5156, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11213.509765625
tensor(11213.5156, grad_fn=<NegBackward0>) tensor(11213.5098, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11213.501953125
tensor(11213.5098, grad_fn=<NegBackward0>) tensor(11213.5020, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11213.494140625
tensor(11213.5020, grad_fn=<NegBackward0>) tensor(11213.4941, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11213.4111328125
tensor(11213.4941, grad_fn=<NegBackward0>) tensor(11213.4111, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11213.4033203125
tensor(11213.4111, grad_fn=<NegBackward0>) tensor(11213.4033, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11213.400390625
tensor(11213.4033, grad_fn=<NegBackward0>) tensor(11213.4004, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11213.3974609375
tensor(11213.4004, grad_fn=<NegBackward0>) tensor(11213.3975, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11213.3935546875
tensor(11213.3975, grad_fn=<NegBackward0>) tensor(11213.3936, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11209.640625
tensor(11213.3936, grad_fn=<NegBackward0>) tensor(11209.6406, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11209.638671875
tensor(11209.6406, grad_fn=<NegBackward0>) tensor(11209.6387, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11209.634765625
tensor(11209.6387, grad_fn=<NegBackward0>) tensor(11209.6348, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11209.6298828125
tensor(11209.6348, grad_fn=<NegBackward0>) tensor(11209.6299, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11209.6298828125
tensor(11209.6299, grad_fn=<NegBackward0>) tensor(11209.6299, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11209.626953125
tensor(11209.6299, grad_fn=<NegBackward0>) tensor(11209.6270, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11209.6220703125
tensor(11209.6270, grad_fn=<NegBackward0>) tensor(11209.6221, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11209.6201171875
tensor(11209.6221, grad_fn=<NegBackward0>) tensor(11209.6201, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11209.6123046875
tensor(11209.6201, grad_fn=<NegBackward0>) tensor(11209.6123, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11209.611328125
tensor(11209.6123, grad_fn=<NegBackward0>) tensor(11209.6113, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11209.611328125
tensor(11209.6113, grad_fn=<NegBackward0>) tensor(11209.6113, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11209.6103515625
tensor(11209.6113, grad_fn=<NegBackward0>) tensor(11209.6104, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11209.607421875
tensor(11209.6104, grad_fn=<NegBackward0>) tensor(11209.6074, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11209.607421875
tensor(11209.6074, grad_fn=<NegBackward0>) tensor(11209.6074, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11209.6064453125
tensor(11209.6074, grad_fn=<NegBackward0>) tensor(11209.6064, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11209.6064453125
tensor(11209.6064, grad_fn=<NegBackward0>) tensor(11209.6064, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11209.5986328125
tensor(11209.6064, grad_fn=<NegBackward0>) tensor(11209.5986, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11209.5146484375
tensor(11209.5986, grad_fn=<NegBackward0>) tensor(11209.5146, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11209.513671875
tensor(11209.5146, grad_fn=<NegBackward0>) tensor(11209.5137, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11209.365234375
tensor(11209.5137, grad_fn=<NegBackward0>) tensor(11209.3652, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11206.8994140625
tensor(11209.3652, grad_fn=<NegBackward0>) tensor(11206.8994, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11206.8984375
tensor(11206.8994, grad_fn=<NegBackward0>) tensor(11206.8984, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11206.90234375
tensor(11206.8984, grad_fn=<NegBackward0>) tensor(11206.9023, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11206.8994140625
tensor(11206.8984, grad_fn=<NegBackward0>) tensor(11206.8994, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11206.896484375
tensor(11206.8984, grad_fn=<NegBackward0>) tensor(11206.8965, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11206.896484375
tensor(11206.8965, grad_fn=<NegBackward0>) tensor(11206.8965, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11206.8974609375
tensor(11206.8965, grad_fn=<NegBackward0>) tensor(11206.8975, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11206.8974609375
tensor(11206.8965, grad_fn=<NegBackward0>) tensor(11206.8975, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11206.8955078125
tensor(11206.8965, grad_fn=<NegBackward0>) tensor(11206.8955, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11206.900390625
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.9004, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11206.896484375
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8965, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11206.8955078125
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8955, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11206.896484375
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8965, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11206.8974609375
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8975, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11206.8955078125
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8955, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11206.869140625
tensor(11206.8955, grad_fn=<NegBackward0>) tensor(11206.8691, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11206.8701171875
tensor(11206.8691, grad_fn=<NegBackward0>) tensor(11206.8701, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11206.869140625
tensor(11206.8691, grad_fn=<NegBackward0>) tensor(11206.8691, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11206.869140625
tensor(11206.8691, grad_fn=<NegBackward0>) tensor(11206.8691, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11206.8671875
tensor(11206.8691, grad_fn=<NegBackward0>) tensor(11206.8672, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11206.8134765625
tensor(11206.8672, grad_fn=<NegBackward0>) tensor(11206.8135, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11206.8076171875
tensor(11206.8135, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11206.80859375
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8086, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11206.80859375
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8086, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11206.80859375
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8086, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11206.80859375
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8086, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11206.814453125
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8145, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11206.82421875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8242, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11206.8076171875
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11206.806640625
tensor(11206.8076, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11206.806640625
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11206.806640625
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11206.8076171875
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11206.80859375
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8086, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11206.806640625
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11206.8076171875
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11206.8076171875
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8076, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11206.8056640625
tensor(11206.8066, grad_fn=<NegBackward0>) tensor(11206.8057, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11206.806640625
tensor(11206.8057, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11206.8203125
tensor(11206.8057, grad_fn=<NegBackward0>) tensor(11206.8203, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11206.8046875
tensor(11206.8057, grad_fn=<NegBackward0>) tensor(11206.8047, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11206.8056640625
tensor(11206.8047, grad_fn=<NegBackward0>) tensor(11206.8057, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11206.8056640625
tensor(11206.8047, grad_fn=<NegBackward0>) tensor(11206.8057, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11206.806640625
tensor(11206.8047, grad_fn=<NegBackward0>) tensor(11206.8066, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11206.8828125
tensor(11206.8047, grad_fn=<NegBackward0>) tensor(11206.8828, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11206.80078125
tensor(11206.8047, grad_fn=<NegBackward0>) tensor(11206.8008, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11206.7998046875
tensor(11206.8008, grad_fn=<NegBackward0>) tensor(11206.7998, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11206.7998046875
tensor(11206.7998, grad_fn=<NegBackward0>) tensor(11206.7998, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11206.80078125
tensor(11206.7998, grad_fn=<NegBackward0>) tensor(11206.8008, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7608, 0.2392],
        [0.2234, 0.7766]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5182, 0.4818], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1952, 0.1043],
         [0.6097, 0.3097]],

        [[0.6800, 0.0939],
         [0.5650, 0.6651]],

        [[0.5640, 0.1006],
         [0.6157, 0.6876]],

        [[0.5521, 0.1024],
         [0.5144, 0.6830]],

        [[0.7137, 0.1027],
         [0.5645, 0.6658]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9368976382505101
Average Adjusted Rand Index: 0.9368036680392615
[0.08344352555302904, 0.9368976382505101] [0.7719456937314482, 0.9368036680392615] [11311.833984375, 11207.0234375]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11177.785885278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21786.66015625
inf tensor(21786.6602, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11531.810546875
tensor(21786.6602, grad_fn=<NegBackward0>) tensor(11531.8105, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11524.1279296875
tensor(11531.8105, grad_fn=<NegBackward0>) tensor(11524.1279, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11517.716796875
tensor(11524.1279, grad_fn=<NegBackward0>) tensor(11517.7168, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11443.5830078125
tensor(11517.7168, grad_fn=<NegBackward0>) tensor(11443.5830, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11313.578125
tensor(11443.5830, grad_fn=<NegBackward0>) tensor(11313.5781, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11282.7705078125
tensor(11313.5781, grad_fn=<NegBackward0>) tensor(11282.7705, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11271.2587890625
tensor(11282.7705, grad_fn=<NegBackward0>) tensor(11271.2588, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11263.9296875
tensor(11271.2588, grad_fn=<NegBackward0>) tensor(11263.9297, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11256.7724609375
tensor(11263.9297, grad_fn=<NegBackward0>) tensor(11256.7725, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11251.564453125
tensor(11256.7725, grad_fn=<NegBackward0>) tensor(11251.5645, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11240.3232421875
tensor(11251.5645, grad_fn=<NegBackward0>) tensor(11240.3232, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11237.333984375
tensor(11240.3232, grad_fn=<NegBackward0>) tensor(11237.3340, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11237.2900390625
tensor(11237.3340, grad_fn=<NegBackward0>) tensor(11237.2900, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11237.2490234375
tensor(11237.2900, grad_fn=<NegBackward0>) tensor(11237.2490, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11237.228515625
tensor(11237.2490, grad_fn=<NegBackward0>) tensor(11237.2285, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11237.2109375
tensor(11237.2285, grad_fn=<NegBackward0>) tensor(11237.2109, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11237.1796875
tensor(11237.2109, grad_fn=<NegBackward0>) tensor(11237.1797, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11237.1455078125
tensor(11237.1797, grad_fn=<NegBackward0>) tensor(11237.1455, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11237.13671875
tensor(11237.1455, grad_fn=<NegBackward0>) tensor(11237.1367, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11237.1279296875
tensor(11237.1367, grad_fn=<NegBackward0>) tensor(11237.1279, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11237.1201171875
tensor(11237.1279, grad_fn=<NegBackward0>) tensor(11237.1201, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11237.091796875
tensor(11237.1201, grad_fn=<NegBackward0>) tensor(11237.0918, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11237.0712890625
tensor(11237.0918, grad_fn=<NegBackward0>) tensor(11237.0713, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11237.064453125
tensor(11237.0713, grad_fn=<NegBackward0>) tensor(11237.0645, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11237.05859375
tensor(11237.0645, grad_fn=<NegBackward0>) tensor(11237.0586, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11237.0537109375
tensor(11237.0586, grad_fn=<NegBackward0>) tensor(11237.0537, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11237.0126953125
tensor(11237.0537, grad_fn=<NegBackward0>) tensor(11237.0127, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11237.0087890625
tensor(11237.0127, grad_fn=<NegBackward0>) tensor(11237.0088, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11237.0048828125
tensor(11237.0088, grad_fn=<NegBackward0>) tensor(11237.0049, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11237.0048828125
tensor(11237.0049, grad_fn=<NegBackward0>) tensor(11237.0049, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11237.0
tensor(11237.0049, grad_fn=<NegBackward0>) tensor(11237., grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11236.99609375
tensor(11237., grad_fn=<NegBackward0>) tensor(11236.9961, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11236.98828125
tensor(11236.9961, grad_fn=<NegBackward0>) tensor(11236.9883, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11236.947265625
tensor(11236.9883, grad_fn=<NegBackward0>) tensor(11236.9473, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11234.88671875
tensor(11236.9473, grad_fn=<NegBackward0>) tensor(11234.8867, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11234.2177734375
tensor(11234.8867, grad_fn=<NegBackward0>) tensor(11234.2178, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11234.1875
tensor(11234.2178, grad_fn=<NegBackward0>) tensor(11234.1875, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11234.142578125
tensor(11234.1875, grad_fn=<NegBackward0>) tensor(11234.1426, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11234.1337890625
tensor(11234.1426, grad_fn=<NegBackward0>) tensor(11234.1338, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11234.130859375
tensor(11234.1338, grad_fn=<NegBackward0>) tensor(11234.1309, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11234.1357421875
tensor(11234.1309, grad_fn=<NegBackward0>) tensor(11234.1357, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11234.125
tensor(11234.1309, grad_fn=<NegBackward0>) tensor(11234.1250, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11234.1142578125
tensor(11234.1250, grad_fn=<NegBackward0>) tensor(11234.1143, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11234.0830078125
tensor(11234.1143, grad_fn=<NegBackward0>) tensor(11234.0830, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11231.212890625
tensor(11234.0830, grad_fn=<NegBackward0>) tensor(11231.2129, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11231.12109375
tensor(11231.2129, grad_fn=<NegBackward0>) tensor(11231.1211, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11231.111328125
tensor(11231.1211, grad_fn=<NegBackward0>) tensor(11231.1113, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11231.10546875
tensor(11231.1113, grad_fn=<NegBackward0>) tensor(11231.1055, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11231.0947265625
tensor(11231.1055, grad_fn=<NegBackward0>) tensor(11231.0947, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11231.00390625
tensor(11231.0947, grad_fn=<NegBackward0>) tensor(11231.0039, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11231.0009765625
tensor(11231.0039, grad_fn=<NegBackward0>) tensor(11231.0010, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11230.998046875
tensor(11231.0010, grad_fn=<NegBackward0>) tensor(11230.9980, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11230.9951171875
tensor(11230.9980, grad_fn=<NegBackward0>) tensor(11230.9951, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11230.9638671875
tensor(11230.9951, grad_fn=<NegBackward0>) tensor(11230.9639, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11230.9287109375
tensor(11230.9639, grad_fn=<NegBackward0>) tensor(11230.9287, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11230.927734375
tensor(11230.9287, grad_fn=<NegBackward0>) tensor(11230.9277, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11230.87890625
tensor(11230.9277, grad_fn=<NegBackward0>) tensor(11230.8789, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11230.876953125
tensor(11230.8789, grad_fn=<NegBackward0>) tensor(11230.8770, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11230.857421875
tensor(11230.8770, grad_fn=<NegBackward0>) tensor(11230.8574, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11230.8544921875
tensor(11230.8574, grad_fn=<NegBackward0>) tensor(11230.8545, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11230.8544921875
tensor(11230.8545, grad_fn=<NegBackward0>) tensor(11230.8545, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11230.8544921875
tensor(11230.8545, grad_fn=<NegBackward0>) tensor(11230.8545, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11230.8544921875
tensor(11230.8545, grad_fn=<NegBackward0>) tensor(11230.8545, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11230.8525390625
tensor(11230.8545, grad_fn=<NegBackward0>) tensor(11230.8525, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11230.8515625
tensor(11230.8525, grad_fn=<NegBackward0>) tensor(11230.8516, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11230.849609375
tensor(11230.8516, grad_fn=<NegBackward0>) tensor(11230.8496, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11230.84765625
tensor(11230.8496, grad_fn=<NegBackward0>) tensor(11230.8477, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11230.845703125
tensor(11230.8477, grad_fn=<NegBackward0>) tensor(11230.8457, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11230.8447265625
tensor(11230.8457, grad_fn=<NegBackward0>) tensor(11230.8447, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11230.8427734375
tensor(11230.8447, grad_fn=<NegBackward0>) tensor(11230.8428, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11230.763671875
tensor(11230.8428, grad_fn=<NegBackward0>) tensor(11230.7637, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11230.7490234375
tensor(11230.7637, grad_fn=<NegBackward0>) tensor(11230.7490, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11230.748046875
tensor(11230.7490, grad_fn=<NegBackward0>) tensor(11230.7480, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11230.75
tensor(11230.7480, grad_fn=<NegBackward0>) tensor(11230.7500, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11230.748046875
tensor(11230.7480, grad_fn=<NegBackward0>) tensor(11230.7480, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11230.7861328125
tensor(11230.7480, grad_fn=<NegBackward0>) tensor(11230.7861, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11230.640625
tensor(11230.7480, grad_fn=<NegBackward0>) tensor(11230.6406, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11230.6337890625
tensor(11230.6406, grad_fn=<NegBackward0>) tensor(11230.6338, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11230.6337890625
tensor(11230.6338, grad_fn=<NegBackward0>) tensor(11230.6338, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11230.6328125
tensor(11230.6338, grad_fn=<NegBackward0>) tensor(11230.6328, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11230.6337890625
tensor(11230.6328, grad_fn=<NegBackward0>) tensor(11230.6338, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11230.6357421875
tensor(11230.6328, grad_fn=<NegBackward0>) tensor(11230.6357, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11230.6318359375
tensor(11230.6328, grad_fn=<NegBackward0>) tensor(11230.6318, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11230.6357421875
tensor(11230.6318, grad_fn=<NegBackward0>) tensor(11230.6357, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11230.6318359375
tensor(11230.6318, grad_fn=<NegBackward0>) tensor(11230.6318, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11230.6318359375
tensor(11230.6318, grad_fn=<NegBackward0>) tensor(11230.6318, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11230.6279296875
tensor(11230.6318, grad_fn=<NegBackward0>) tensor(11230.6279, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11230.61328125
tensor(11230.6279, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11230.61328125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11230.61328125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11230.61328125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11230.6142578125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6143, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11230.61328125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11230.61328125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.6133, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11230.5830078125
tensor(11230.6133, grad_fn=<NegBackward0>) tensor(11230.5830, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11230.5830078125
tensor(11230.5830, grad_fn=<NegBackward0>) tensor(11230.5830, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11230.62890625
tensor(11230.5830, grad_fn=<NegBackward0>) tensor(11230.6289, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11230.5771484375
tensor(11230.5830, grad_fn=<NegBackward0>) tensor(11230.5771, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11230.6376953125
tensor(11230.5771, grad_fn=<NegBackward0>) tensor(11230.6377, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6173, 0.3827],
        [0.2459, 0.7541]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9196, 0.0804], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1876, 0.0850],
         [0.6251, 0.3085]],

        [[0.6152, 0.0965],
         [0.7168, 0.6000]],

        [[0.6798, 0.0881],
         [0.5412, 0.6723]],

        [[0.5554, 0.0967],
         [0.7012, 0.6984]],

        [[0.6949, 0.1131],
         [0.5021, 0.5473]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.029240383366564728
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5646173151267517
Average Adjusted Rand Index: 0.782008147214472
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24757.470703125
inf tensor(24757.4707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11534.205078125
tensor(24757.4707, grad_fn=<NegBackward0>) tensor(11534.2051, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11531.8935546875
tensor(11534.2051, grad_fn=<NegBackward0>) tensor(11531.8936, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11529.6484375
tensor(11531.8936, grad_fn=<NegBackward0>) tensor(11529.6484, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11524.84765625
tensor(11529.6484, grad_fn=<NegBackward0>) tensor(11524.8477, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11521.4130859375
tensor(11524.8477, grad_fn=<NegBackward0>) tensor(11521.4131, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11517.134765625
tensor(11521.4131, grad_fn=<NegBackward0>) tensor(11517.1348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11378.75
tensor(11517.1348, grad_fn=<NegBackward0>) tensor(11378.7500, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11293.23828125
tensor(11378.7500, grad_fn=<NegBackward0>) tensor(11293.2383, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11268.693359375
tensor(11293.2383, grad_fn=<NegBackward0>) tensor(11268.6934, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11259.935546875
tensor(11268.6934, grad_fn=<NegBackward0>) tensor(11259.9355, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11258.7333984375
tensor(11259.9355, grad_fn=<NegBackward0>) tensor(11258.7334, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11258.6220703125
tensor(11258.7334, grad_fn=<NegBackward0>) tensor(11258.6221, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11258.4794921875
tensor(11258.6221, grad_fn=<NegBackward0>) tensor(11258.4795, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11256.4091796875
tensor(11258.4795, grad_fn=<NegBackward0>) tensor(11256.4092, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11252.708984375
tensor(11256.4092, grad_fn=<NegBackward0>) tensor(11252.7090, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11252.69921875
tensor(11252.7090, grad_fn=<NegBackward0>) tensor(11252.6992, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11250.6005859375
tensor(11252.6992, grad_fn=<NegBackward0>) tensor(11250.6006, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11250.55078125
tensor(11250.6006, grad_fn=<NegBackward0>) tensor(11250.5508, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11250.4609375
tensor(11250.5508, grad_fn=<NegBackward0>) tensor(11250.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11250.4501953125
tensor(11250.4609, grad_fn=<NegBackward0>) tensor(11250.4502, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11250.3798828125
tensor(11250.4502, grad_fn=<NegBackward0>) tensor(11250.3799, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11250.3603515625
tensor(11250.3799, grad_fn=<NegBackward0>) tensor(11250.3604, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11250.353515625
tensor(11250.3604, grad_fn=<NegBackward0>) tensor(11250.3535, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11250.3447265625
tensor(11250.3535, grad_fn=<NegBackward0>) tensor(11250.3447, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11250.341796875
tensor(11250.3447, grad_fn=<NegBackward0>) tensor(11250.3418, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11250.337890625
tensor(11250.3418, grad_fn=<NegBackward0>) tensor(11250.3379, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11250.3251953125
tensor(11250.3379, grad_fn=<NegBackward0>) tensor(11250.3252, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11250.32421875
tensor(11250.3252, grad_fn=<NegBackward0>) tensor(11250.3242, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11250.3232421875
tensor(11250.3242, grad_fn=<NegBackward0>) tensor(11250.3232, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11250.3232421875
tensor(11250.3232, grad_fn=<NegBackward0>) tensor(11250.3232, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11250.322265625
tensor(11250.3232, grad_fn=<NegBackward0>) tensor(11250.3223, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11250.318359375
tensor(11250.3223, grad_fn=<NegBackward0>) tensor(11250.3184, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11250.318359375
tensor(11250.3184, grad_fn=<NegBackward0>) tensor(11250.3184, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11250.3173828125
tensor(11250.3184, grad_fn=<NegBackward0>) tensor(11250.3174, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11250.3369140625
tensor(11250.3174, grad_fn=<NegBackward0>) tensor(11250.3369, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11250.31640625
tensor(11250.3174, grad_fn=<NegBackward0>) tensor(11250.3164, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11250.314453125
tensor(11250.3164, grad_fn=<NegBackward0>) tensor(11250.3145, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11250.310546875
tensor(11250.3145, grad_fn=<NegBackward0>) tensor(11250.3105, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11250.30859375
tensor(11250.3105, grad_fn=<NegBackward0>) tensor(11250.3086, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11250.322265625
tensor(11250.3086, grad_fn=<NegBackward0>) tensor(11250.3223, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11250.3076171875
tensor(11250.3086, grad_fn=<NegBackward0>) tensor(11250.3076, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11250.3076171875
tensor(11250.3076, grad_fn=<NegBackward0>) tensor(11250.3076, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11250.2783203125
tensor(11250.3076, grad_fn=<NegBackward0>) tensor(11250.2783, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11250.1533203125
tensor(11250.2783, grad_fn=<NegBackward0>) tensor(11250.1533, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11250.1142578125
tensor(11250.1533, grad_fn=<NegBackward0>) tensor(11250.1143, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11250.11328125
tensor(11250.1143, grad_fn=<NegBackward0>) tensor(11250.1133, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11249.2177734375
tensor(11250.1133, grad_fn=<NegBackward0>) tensor(11249.2178, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11249.2158203125
tensor(11249.2178, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11249.2158203125
tensor(11249.2158, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11249.2158203125
tensor(11249.2158, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11249.21484375
tensor(11249.2158, grad_fn=<NegBackward0>) tensor(11249.2148, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11249.21875
tensor(11249.2148, grad_fn=<NegBackward0>) tensor(11249.2188, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11249.21484375
tensor(11249.2148, grad_fn=<NegBackward0>) tensor(11249.2148, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11249.2197265625
tensor(11249.2148, grad_fn=<NegBackward0>) tensor(11249.2197, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11249.2138671875
tensor(11249.2148, grad_fn=<NegBackward0>) tensor(11249.2139, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11249.2158203125
tensor(11249.2139, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11249.21875
tensor(11249.2139, grad_fn=<NegBackward0>) tensor(11249.2188, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11249.2158203125
tensor(11249.2139, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11249.2158203125
tensor(11249.2139, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11249.2158203125
tensor(11249.2139, grad_fn=<NegBackward0>) tensor(11249.2158, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.4822, 0.5178],
        [0.4911, 0.5089]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4808, 0.5192], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2232, 0.0862],
         [0.6816, 0.2870]],

        [[0.7230, 0.0947],
         [0.5670, 0.5427]],

        [[0.5852, 0.0888],
         [0.5698, 0.5533]],

        [[0.6414, 0.0958],
         [0.5034, 0.6299]],

        [[0.5917, 0.1101],
         [0.6085, 0.5085]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721545392564556
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
Global Adjusted Rand Index: 0.34443529142698565
Average Adjusted Rand Index: 0.8835526527830098
[0.5646173151267517, 0.34443529142698565] [0.782008147214472, 0.8835526527830098] [11230.55859375, 11249.2158203125]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11322.30749279651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24511.66015625
inf tensor(24511.6602, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11614.025390625
tensor(24511.6602, grad_fn=<NegBackward0>) tensor(11614.0254, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11604.62109375
tensor(11614.0254, grad_fn=<NegBackward0>) tensor(11604.6211, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11603.3876953125
tensor(11604.6211, grad_fn=<NegBackward0>) tensor(11603.3877, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11601.888671875
tensor(11603.3877, grad_fn=<NegBackward0>) tensor(11601.8887, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11539.794921875
tensor(11601.8887, grad_fn=<NegBackward0>) tensor(11539.7949, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11385.92578125
tensor(11539.7949, grad_fn=<NegBackward0>) tensor(11385.9258, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11379.5673828125
tensor(11385.9258, grad_fn=<NegBackward0>) tensor(11379.5674, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11379.45703125
tensor(11379.5674, grad_fn=<NegBackward0>) tensor(11379.4570, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11379.4091796875
tensor(11379.4570, grad_fn=<NegBackward0>) tensor(11379.4092, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11379.3818359375
tensor(11379.4092, grad_fn=<NegBackward0>) tensor(11379.3818, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11379.36328125
tensor(11379.3818, grad_fn=<NegBackward0>) tensor(11379.3633, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11379.341796875
tensor(11379.3633, grad_fn=<NegBackward0>) tensor(11379.3418, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11379.32421875
tensor(11379.3418, grad_fn=<NegBackward0>) tensor(11379.3242, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11379.31640625
tensor(11379.3242, grad_fn=<NegBackward0>) tensor(11379.3164, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11379.2998046875
tensor(11379.3164, grad_fn=<NegBackward0>) tensor(11379.2998, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11379.287109375
tensor(11379.2998, grad_fn=<NegBackward0>) tensor(11379.2871, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11379.2861328125
tensor(11379.2871, grad_fn=<NegBackward0>) tensor(11379.2861, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11379.2822265625
tensor(11379.2861, grad_fn=<NegBackward0>) tensor(11379.2822, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11379.279296875
tensor(11379.2822, grad_fn=<NegBackward0>) tensor(11379.2793, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11379.275390625
tensor(11379.2793, grad_fn=<NegBackward0>) tensor(11379.2754, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11379.2705078125
tensor(11379.2754, grad_fn=<NegBackward0>) tensor(11379.2705, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11379.2685546875
tensor(11379.2705, grad_fn=<NegBackward0>) tensor(11379.2686, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11379.2685546875
tensor(11379.2686, grad_fn=<NegBackward0>) tensor(11379.2686, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11379.265625
tensor(11379.2686, grad_fn=<NegBackward0>) tensor(11379.2656, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11379.263671875
tensor(11379.2656, grad_fn=<NegBackward0>) tensor(11379.2637, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11379.26171875
tensor(11379.2637, grad_fn=<NegBackward0>) tensor(11379.2617, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11379.259765625
tensor(11379.2617, grad_fn=<NegBackward0>) tensor(11379.2598, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11379.2421875
tensor(11379.2598, grad_fn=<NegBackward0>) tensor(11379.2422, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11379.2412109375
tensor(11379.2422, grad_fn=<NegBackward0>) tensor(11379.2412, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11379.2412109375
tensor(11379.2412, grad_fn=<NegBackward0>) tensor(11379.2412, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11379.2412109375
tensor(11379.2412, grad_fn=<NegBackward0>) tensor(11379.2412, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11379.240234375
tensor(11379.2412, grad_fn=<NegBackward0>) tensor(11379.2402, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11379.2392578125
tensor(11379.2402, grad_fn=<NegBackward0>) tensor(11379.2393, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11379.23828125
tensor(11379.2393, grad_fn=<NegBackward0>) tensor(11379.2383, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11379.236328125
tensor(11379.2383, grad_fn=<NegBackward0>) tensor(11379.2363, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11379.197265625
tensor(11379.2363, grad_fn=<NegBackward0>) tensor(11379.1973, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11379.1904296875
tensor(11379.1973, grad_fn=<NegBackward0>) tensor(11379.1904, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11379.189453125
tensor(11379.1904, grad_fn=<NegBackward0>) tensor(11379.1895, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11379.1865234375
tensor(11379.1895, grad_fn=<NegBackward0>) tensor(11379.1865, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11379.185546875
tensor(11379.1865, grad_fn=<NegBackward0>) tensor(11379.1855, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11379.185546875
tensor(11379.1855, grad_fn=<NegBackward0>) tensor(11379.1855, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11379.18359375
tensor(11379.1855, grad_fn=<NegBackward0>) tensor(11379.1836, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11379.18359375
tensor(11379.1836, grad_fn=<NegBackward0>) tensor(11379.1836, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11379.1982421875
tensor(11379.1836, grad_fn=<NegBackward0>) tensor(11379.1982, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11379.1826171875
tensor(11379.1836, grad_fn=<NegBackward0>) tensor(11379.1826, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11379.1826171875
tensor(11379.1826, grad_fn=<NegBackward0>) tensor(11379.1826, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11379.1826171875
tensor(11379.1826, grad_fn=<NegBackward0>) tensor(11379.1826, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11379.19140625
tensor(11379.1826, grad_fn=<NegBackward0>) tensor(11379.1914, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11379.1806640625
tensor(11379.1826, grad_fn=<NegBackward0>) tensor(11379.1807, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11379.1806640625
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1807, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11379.1865234375
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1865, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11379.1806640625
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1807, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11379.1826171875
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1826, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11379.181640625
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1816, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11379.1826171875
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1826, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11379.1875
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1875, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -11379.1806640625
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1807, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11379.1669921875
tensor(11379.1807, grad_fn=<NegBackward0>) tensor(11379.1670, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11378.724609375
tensor(11379.1670, grad_fn=<NegBackward0>) tensor(11378.7246, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11378.72265625
tensor(11378.7246, grad_fn=<NegBackward0>) tensor(11378.7227, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11378.7177734375
tensor(11378.7227, grad_fn=<NegBackward0>) tensor(11378.7178, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11378.7177734375
tensor(11378.7178, grad_fn=<NegBackward0>) tensor(11378.7178, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11378.7158203125
tensor(11378.7178, grad_fn=<NegBackward0>) tensor(11378.7158, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11378.71484375
tensor(11378.7158, grad_fn=<NegBackward0>) tensor(11378.7148, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11378.6748046875
tensor(11378.7148, grad_fn=<NegBackward0>) tensor(11378.6748, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11378.6708984375
tensor(11378.6748, grad_fn=<NegBackward0>) tensor(11378.6709, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11378.6669921875
tensor(11378.6709, grad_fn=<NegBackward0>) tensor(11378.6670, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11378.6513671875
tensor(11378.6670, grad_fn=<NegBackward0>) tensor(11378.6514, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11378.650390625
tensor(11378.6514, grad_fn=<NegBackward0>) tensor(11378.6504, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11378.6533203125
tensor(11378.6504, grad_fn=<NegBackward0>) tensor(11378.6533, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11378.66015625
tensor(11378.6504, grad_fn=<NegBackward0>) tensor(11378.6602, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11378.6513671875
tensor(11378.6504, grad_fn=<NegBackward0>) tensor(11378.6514, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11378.6533203125
tensor(11378.6504, grad_fn=<NegBackward0>) tensor(11378.6533, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11378.6552734375
tensor(11378.6504, grad_fn=<NegBackward0>) tensor(11378.6553, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.6419, 0.3581],
        [0.3299, 0.6701]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5855, 0.4145], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2943, 0.1013],
         [0.5837, 0.2128]],

        [[0.5195, 0.1029],
         [0.6363, 0.6543]],

        [[0.5608, 0.1016],
         [0.5935, 0.5009]],

        [[0.6714, 0.0954],
         [0.6840, 0.6061]],

        [[0.6726, 0.1010],
         [0.5911, 0.7192]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 88
Adjusted Rand Index: 0.5735404496948732
Global Adjusted Rand Index: 0.3881514829171605
Average Adjusted Rand Index: 0.867190573474917
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22289.556640625
inf tensor(22289.5566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11655.1513671875
tensor(22289.5566, grad_fn=<NegBackward0>) tensor(11655.1514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11652.880859375
tensor(11655.1514, grad_fn=<NegBackward0>) tensor(11652.8809, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11642.4365234375
tensor(11652.8809, grad_fn=<NegBackward0>) tensor(11642.4365, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11603.4482421875
tensor(11642.4365, grad_fn=<NegBackward0>) tensor(11603.4482, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11521.83984375
tensor(11603.4482, grad_fn=<NegBackward0>) tensor(11521.8398, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11425.2392578125
tensor(11521.8398, grad_fn=<NegBackward0>) tensor(11425.2393, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11367.85546875
tensor(11425.2393, grad_fn=<NegBackward0>) tensor(11367.8555, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11354.8037109375
tensor(11367.8555, grad_fn=<NegBackward0>) tensor(11354.8037, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11344.03125
tensor(11354.8037, grad_fn=<NegBackward0>) tensor(11344.0312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11341.8115234375
tensor(11344.0312, grad_fn=<NegBackward0>) tensor(11341.8115, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11341.486328125
tensor(11341.8115, grad_fn=<NegBackward0>) tensor(11341.4863, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11331.6953125
tensor(11341.4863, grad_fn=<NegBackward0>) tensor(11331.6953, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11331.6376953125
tensor(11331.6953, grad_fn=<NegBackward0>) tensor(11331.6377, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11331.5595703125
tensor(11331.6377, grad_fn=<NegBackward0>) tensor(11331.5596, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11325.0087890625
tensor(11331.5596, grad_fn=<NegBackward0>) tensor(11325.0088, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11312.1162109375
tensor(11325.0088, grad_fn=<NegBackward0>) tensor(11312.1162, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11312.078125
tensor(11312.1162, grad_fn=<NegBackward0>) tensor(11312.0781, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11312.0283203125
tensor(11312.0781, grad_fn=<NegBackward0>) tensor(11312.0283, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11312.01171875
tensor(11312.0283, grad_fn=<NegBackward0>) tensor(11312.0117, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11312.0
tensor(11312.0117, grad_fn=<NegBackward0>) tensor(11312., grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11311.986328125
tensor(11312., grad_fn=<NegBackward0>) tensor(11311.9863, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11311.9697265625
tensor(11311.9863, grad_fn=<NegBackward0>) tensor(11311.9697, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11311.919921875
tensor(11311.9697, grad_fn=<NegBackward0>) tensor(11311.9199, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11311.9111328125
tensor(11311.9199, grad_fn=<NegBackward0>) tensor(11311.9111, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11311.90625
tensor(11311.9111, grad_fn=<NegBackward0>) tensor(11311.9062, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11311.900390625
tensor(11311.9062, grad_fn=<NegBackward0>) tensor(11311.9004, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11311.8984375
tensor(11311.9004, grad_fn=<NegBackward0>) tensor(11311.8984, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11311.8955078125
tensor(11311.8984, grad_fn=<NegBackward0>) tensor(11311.8955, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11311.890625
tensor(11311.8955, grad_fn=<NegBackward0>) tensor(11311.8906, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11311.8876953125
tensor(11311.8906, grad_fn=<NegBackward0>) tensor(11311.8877, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11311.8798828125
tensor(11311.8877, grad_fn=<NegBackward0>) tensor(11311.8799, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11311.875
tensor(11311.8799, grad_fn=<NegBackward0>) tensor(11311.8750, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11311.8671875
tensor(11311.8750, grad_fn=<NegBackward0>) tensor(11311.8672, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11311.8544921875
tensor(11311.8672, grad_fn=<NegBackward0>) tensor(11311.8545, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11311.8505859375
tensor(11311.8545, grad_fn=<NegBackward0>) tensor(11311.8506, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11311.8486328125
tensor(11311.8506, grad_fn=<NegBackward0>) tensor(11311.8486, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11311.8466796875
tensor(11311.8486, grad_fn=<NegBackward0>) tensor(11311.8467, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11311.8447265625
tensor(11311.8467, grad_fn=<NegBackward0>) tensor(11311.8447, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11311.841796875
tensor(11311.8447, grad_fn=<NegBackward0>) tensor(11311.8418, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11311.837890625
tensor(11311.8418, grad_fn=<NegBackward0>) tensor(11311.8379, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11311.822265625
tensor(11311.8379, grad_fn=<NegBackward0>) tensor(11311.8223, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11311.0712890625
tensor(11311.8223, grad_fn=<NegBackward0>) tensor(11311.0713, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11311.0673828125
tensor(11311.0713, grad_fn=<NegBackward0>) tensor(11311.0674, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11311.0791015625
tensor(11311.0674, grad_fn=<NegBackward0>) tensor(11311.0791, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11311.0537109375
tensor(11311.0674, grad_fn=<NegBackward0>) tensor(11311.0537, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11311.044921875
tensor(11311.0537, grad_fn=<NegBackward0>) tensor(11311.0449, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11311.0419921875
tensor(11311.0449, grad_fn=<NegBackward0>) tensor(11311.0420, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11311.052734375
tensor(11311.0420, grad_fn=<NegBackward0>) tensor(11311.0527, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11311.0400390625
tensor(11311.0420, grad_fn=<NegBackward0>) tensor(11311.0400, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11311.0400390625
tensor(11311.0400, grad_fn=<NegBackward0>) tensor(11311.0400, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11311.0390625
tensor(11311.0400, grad_fn=<NegBackward0>) tensor(11311.0391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11311.0380859375
tensor(11311.0391, grad_fn=<NegBackward0>) tensor(11311.0381, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11311.0380859375
tensor(11311.0381, grad_fn=<NegBackward0>) tensor(11311.0381, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11311.037109375
tensor(11311.0381, grad_fn=<NegBackward0>) tensor(11311.0371, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11311.0361328125
tensor(11311.0371, grad_fn=<NegBackward0>) tensor(11311.0361, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11311.0361328125
tensor(11311.0361, grad_fn=<NegBackward0>) tensor(11311.0361, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11311.033203125
tensor(11311.0361, grad_fn=<NegBackward0>) tensor(11311.0332, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11311.0302734375
tensor(11311.0332, grad_fn=<NegBackward0>) tensor(11311.0303, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11311.029296875
tensor(11311.0303, grad_fn=<NegBackward0>) tensor(11311.0293, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11311.0302734375
tensor(11311.0293, grad_fn=<NegBackward0>) tensor(11311.0303, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11311.029296875
tensor(11311.0293, grad_fn=<NegBackward0>) tensor(11311.0293, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11311.0146484375
tensor(11311.0293, grad_fn=<NegBackward0>) tensor(11311.0146, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11311.0146484375
tensor(11311.0146, grad_fn=<NegBackward0>) tensor(11311.0146, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11311.013671875
tensor(11311.0146, grad_fn=<NegBackward0>) tensor(11311.0137, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11311.0146484375
tensor(11311.0137, grad_fn=<NegBackward0>) tensor(11311.0146, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11311.013671875
tensor(11311.0137, grad_fn=<NegBackward0>) tensor(11311.0137, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11311.0126953125
tensor(11311.0137, grad_fn=<NegBackward0>) tensor(11311.0127, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11311.0126953125
tensor(11311.0127, grad_fn=<NegBackward0>) tensor(11311.0127, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11311.0126953125
tensor(11311.0127, grad_fn=<NegBackward0>) tensor(11311.0127, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11311.01171875
tensor(11311.0127, grad_fn=<NegBackward0>) tensor(11311.0117, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11311.01171875
tensor(11311.0117, grad_fn=<NegBackward0>) tensor(11311.0117, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11311.009765625
tensor(11311.0117, grad_fn=<NegBackward0>) tensor(11311.0098, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11311.009765625
tensor(11311.0098, grad_fn=<NegBackward0>) tensor(11311.0098, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11311.0087890625
tensor(11311.0098, grad_fn=<NegBackward0>) tensor(11311.0088, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11311.00390625
tensor(11311.0088, grad_fn=<NegBackward0>) tensor(11311.0039, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11311.0029296875
tensor(11311.0039, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11311.005859375
tensor(11311.0029, grad_fn=<NegBackward0>) tensor(11311.0059, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11311.0029296875
tensor(11311.0029, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11311.00390625
tensor(11311.0029, grad_fn=<NegBackward0>) tensor(11311.0039, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11311.001953125
tensor(11311.0029, grad_fn=<NegBackward0>) tensor(11311.0020, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11311.0029296875
tensor(11311.0020, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11311.00390625
tensor(11311.0020, grad_fn=<NegBackward0>) tensor(11311.0039, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11311.0029296875
tensor(11311.0020, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11311.0029296875
tensor(11311.0020, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11311.0029296875
tensor(11311.0020, grad_fn=<NegBackward0>) tensor(11311.0029, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7753, 0.2247],
        [0.2330, 0.7670]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5649, 0.4351], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3030, 0.1034],
         [0.7184, 0.1960]],

        [[0.6201, 0.1063],
         [0.5394, 0.6694]],

        [[0.5530, 0.1022],
         [0.5749, 0.5747]],

        [[0.5118, 0.0964],
         [0.6769, 0.6755]],

        [[0.5812, 0.1041],
         [0.5921, 0.6994]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9524805219219655
Average Adjusted Rand Index: 0.9524835374650543
[0.3881514829171605, 0.9524805219219655] [0.867190573474917, 0.9524835374650543] [11378.6552734375, 11311.0029296875]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11297.732030674195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22388.287109375
inf tensor(22388.2871, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11488.0947265625
tensor(22388.2871, grad_fn=<NegBackward0>) tensor(11488.0947, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11487.619140625
tensor(11488.0947, grad_fn=<NegBackward0>) tensor(11487.6191, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11487.3349609375
tensor(11487.6191, grad_fn=<NegBackward0>) tensor(11487.3350, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11483.4375
tensor(11487.3350, grad_fn=<NegBackward0>) tensor(11483.4375, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11481.283203125
tensor(11483.4375, grad_fn=<NegBackward0>) tensor(11481.2832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11478.3876953125
tensor(11481.2832, grad_fn=<NegBackward0>) tensor(11478.3877, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11443.6767578125
tensor(11478.3877, grad_fn=<NegBackward0>) tensor(11443.6768, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11357.4189453125
tensor(11443.6768, grad_fn=<NegBackward0>) tensor(11357.4189, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11341.31640625
tensor(11357.4189, grad_fn=<NegBackward0>) tensor(11341.3164, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11340.8037109375
tensor(11341.3164, grad_fn=<NegBackward0>) tensor(11340.8037, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11335.994140625
tensor(11340.8037, grad_fn=<NegBackward0>) tensor(11335.9941, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11331.0693359375
tensor(11335.9941, grad_fn=<NegBackward0>) tensor(11331.0693, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11329.6767578125
tensor(11331.0693, grad_fn=<NegBackward0>) tensor(11329.6768, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11329.6162109375
tensor(11329.6768, grad_fn=<NegBackward0>) tensor(11329.6162, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11325.3857421875
tensor(11329.6162, grad_fn=<NegBackward0>) tensor(11325.3857, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11325.3173828125
tensor(11325.3857, grad_fn=<NegBackward0>) tensor(11325.3174, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11324.333984375
tensor(11325.3174, grad_fn=<NegBackward0>) tensor(11324.3340, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11324.294921875
tensor(11324.3340, grad_fn=<NegBackward0>) tensor(11324.2949, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11324.28125
tensor(11324.2949, grad_fn=<NegBackward0>) tensor(11324.2812, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11324.275390625
tensor(11324.2812, grad_fn=<NegBackward0>) tensor(11324.2754, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11324.263671875
tensor(11324.2754, grad_fn=<NegBackward0>) tensor(11324.2637, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11324.2578125
tensor(11324.2637, grad_fn=<NegBackward0>) tensor(11324.2578, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11324.25
tensor(11324.2578, grad_fn=<NegBackward0>) tensor(11324.2500, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11324.2470703125
tensor(11324.2500, grad_fn=<NegBackward0>) tensor(11324.2471, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11324.240234375
tensor(11324.2471, grad_fn=<NegBackward0>) tensor(11324.2402, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11324.236328125
tensor(11324.2402, grad_fn=<NegBackward0>) tensor(11324.2363, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11324.2412109375
tensor(11324.2363, grad_fn=<NegBackward0>) tensor(11324.2412, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11324.2255859375
tensor(11324.2363, grad_fn=<NegBackward0>) tensor(11324.2256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11320.228515625
tensor(11324.2256, grad_fn=<NegBackward0>) tensor(11320.2285, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11320.2138671875
tensor(11320.2285, grad_fn=<NegBackward0>) tensor(11320.2139, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11320.2109375
tensor(11320.2139, grad_fn=<NegBackward0>) tensor(11320.2109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11320.2080078125
tensor(11320.2109, grad_fn=<NegBackward0>) tensor(11320.2080, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11320.205078125
tensor(11320.2080, grad_fn=<NegBackward0>) tensor(11320.2051, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11320.2041015625
tensor(11320.2051, grad_fn=<NegBackward0>) tensor(11320.2041, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11320.203125
tensor(11320.2041, grad_fn=<NegBackward0>) tensor(11320.2031, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11320.201171875
tensor(11320.2031, grad_fn=<NegBackward0>) tensor(11320.2012, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11320.1953125
tensor(11320.2012, grad_fn=<NegBackward0>) tensor(11320.1953, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11320.189453125
tensor(11320.1953, grad_fn=<NegBackward0>) tensor(11320.1895, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11320.1884765625
tensor(11320.1895, grad_fn=<NegBackward0>) tensor(11320.1885, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11320.1845703125
tensor(11320.1885, grad_fn=<NegBackward0>) tensor(11320.1846, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11320.1806640625
tensor(11320.1846, grad_fn=<NegBackward0>) tensor(11320.1807, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11320.1748046875
tensor(11320.1807, grad_fn=<NegBackward0>) tensor(11320.1748, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11320.173828125
tensor(11320.1748, grad_fn=<NegBackward0>) tensor(11320.1738, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11320.171875
tensor(11320.1738, grad_fn=<NegBackward0>) tensor(11320.1719, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11320.171875
tensor(11320.1719, grad_fn=<NegBackward0>) tensor(11320.1719, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11320.166015625
tensor(11320.1719, grad_fn=<NegBackward0>) tensor(11320.1660, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11320.166015625
tensor(11320.1660, grad_fn=<NegBackward0>) tensor(11320.1660, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11320.1650390625
tensor(11320.1660, grad_fn=<NegBackward0>) tensor(11320.1650, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11320.1640625
tensor(11320.1650, grad_fn=<NegBackward0>) tensor(11320.1641, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11320.1630859375
tensor(11320.1641, grad_fn=<NegBackward0>) tensor(11320.1631, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11320.1611328125
tensor(11320.1631, grad_fn=<NegBackward0>) tensor(11320.1611, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11320.1533203125
tensor(11320.1611, grad_fn=<NegBackward0>) tensor(11320.1533, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11320.154296875
tensor(11320.1533, grad_fn=<NegBackward0>) tensor(11320.1543, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11320.1552734375
tensor(11320.1533, grad_fn=<NegBackward0>) tensor(11320.1553, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11320.1474609375
tensor(11320.1533, grad_fn=<NegBackward0>) tensor(11320.1475, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11320.1484375
tensor(11320.1475, grad_fn=<NegBackward0>) tensor(11320.1484, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11320.1455078125
tensor(11320.1475, grad_fn=<NegBackward0>) tensor(11320.1455, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11320.146484375
tensor(11320.1455, grad_fn=<NegBackward0>) tensor(11320.1465, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11320.1455078125
tensor(11320.1455, grad_fn=<NegBackward0>) tensor(11320.1455, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11320.109375
tensor(11320.1455, grad_fn=<NegBackward0>) tensor(11320.1094, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11320.109375
tensor(11320.1094, grad_fn=<NegBackward0>) tensor(11320.1094, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11320.1083984375
tensor(11320.1094, grad_fn=<NegBackward0>) tensor(11320.1084, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11320.107421875
tensor(11320.1084, grad_fn=<NegBackward0>) tensor(11320.1074, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11319.9755859375
tensor(11320.1074, grad_fn=<NegBackward0>) tensor(11319.9756, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11319.9755859375
tensor(11319.9756, grad_fn=<NegBackward0>) tensor(11319.9756, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11319.974609375
tensor(11319.9756, grad_fn=<NegBackward0>) tensor(11319.9746, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11319.9736328125
tensor(11319.9746, grad_fn=<NegBackward0>) tensor(11319.9736, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11319.943359375
tensor(11319.9736, grad_fn=<NegBackward0>) tensor(11319.9434, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11319.943359375
tensor(11319.9434, grad_fn=<NegBackward0>) tensor(11319.9434, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11319.9423828125
tensor(11319.9434, grad_fn=<NegBackward0>) tensor(11319.9424, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11319.94140625
tensor(11319.9424, grad_fn=<NegBackward0>) tensor(11319.9414, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11319.939453125
tensor(11319.9414, grad_fn=<NegBackward0>) tensor(11319.9395, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11319.9248046875
tensor(11319.9395, grad_fn=<NegBackward0>) tensor(11319.9248, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11319.9267578125
tensor(11319.9248, grad_fn=<NegBackward0>) tensor(11319.9268, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11319.923828125
tensor(11319.9248, grad_fn=<NegBackward0>) tensor(11319.9238, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11319.92578125
tensor(11319.9238, grad_fn=<NegBackward0>) tensor(11319.9258, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11319.9345703125
tensor(11319.9238, grad_fn=<NegBackward0>) tensor(11319.9346, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11319.9228515625
tensor(11319.9238, grad_fn=<NegBackward0>) tensor(11319.9229, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11319.9248046875
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9248, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11319.923828125
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9238, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11319.923828125
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9238, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11319.9248046875
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9248, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11319.9453125
tensor(11319.9229, grad_fn=<NegBackward0>) tensor(11319.9453, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.6335, 0.3665],
        [0.3606, 0.6394]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 2.2213e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1849, 0.1560],
         [0.6806, 0.3111]],

        [[0.5557, 0.1006],
         [0.6722, 0.7289]],

        [[0.5821, 0.1148],
         [0.7273, 0.6553]],

        [[0.5876, 0.1106],
         [0.6582, 0.5134]],

        [[0.6140, 0.1055],
         [0.5938, 0.5242]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 86
Adjusted Rand Index: 0.5135536976383382
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721116882917585
Global Adjusted Rand Index: 0.5117371670477326
Average Adjusted Rand Index: 0.6332928969576052
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23677.154296875
inf tensor(23677.1543, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11488.609375
tensor(23677.1543, grad_fn=<NegBackward0>) tensor(11488.6094, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11487.7373046875
tensor(11488.6094, grad_fn=<NegBackward0>) tensor(11487.7373, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11486.9951171875
tensor(11487.7373, grad_fn=<NegBackward0>) tensor(11486.9951, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11482.533203125
tensor(11486.9951, grad_fn=<NegBackward0>) tensor(11482.5332, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11480.06640625
tensor(11482.5332, grad_fn=<NegBackward0>) tensor(11480.0664, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11476.1533203125
tensor(11480.0664, grad_fn=<NegBackward0>) tensor(11476.1533, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11461.0712890625
tensor(11476.1533, grad_fn=<NegBackward0>) tensor(11461.0713, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11393.91015625
tensor(11461.0713, grad_fn=<NegBackward0>) tensor(11393.9102, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11345.2890625
tensor(11393.9102, grad_fn=<NegBackward0>) tensor(11345.2891, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11343.0
tensor(11345.2891, grad_fn=<NegBackward0>) tensor(11343., grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11341.0859375
tensor(11343., grad_fn=<NegBackward0>) tensor(11341.0859, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11336.2392578125
tensor(11341.0859, grad_fn=<NegBackward0>) tensor(11336.2393, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11331.802734375
tensor(11336.2393, grad_fn=<NegBackward0>) tensor(11331.8027, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11331.2666015625
tensor(11331.8027, grad_fn=<NegBackward0>) tensor(11331.2666, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11331.1005859375
tensor(11331.2666, grad_fn=<NegBackward0>) tensor(11331.1006, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11329.78515625
tensor(11331.1006, grad_fn=<NegBackward0>) tensor(11329.7852, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11329.6474609375
tensor(11329.7852, grad_fn=<NegBackward0>) tensor(11329.6475, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11325.4248046875
tensor(11329.6475, grad_fn=<NegBackward0>) tensor(11325.4248, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11325.345703125
tensor(11325.4248, grad_fn=<NegBackward0>) tensor(11325.3457, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11321.486328125
tensor(11325.3457, grad_fn=<NegBackward0>) tensor(11321.4863, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11321.4140625
tensor(11321.4863, grad_fn=<NegBackward0>) tensor(11321.4141, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11320.3564453125
tensor(11321.4141, grad_fn=<NegBackward0>) tensor(11320.3564, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11320.3388671875
tensor(11320.3564, grad_fn=<NegBackward0>) tensor(11320.3389, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11320.3271484375
tensor(11320.3389, grad_fn=<NegBackward0>) tensor(11320.3271, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11320.31640625
tensor(11320.3271, grad_fn=<NegBackward0>) tensor(11320.3164, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11320.30859375
tensor(11320.3164, grad_fn=<NegBackward0>) tensor(11320.3086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11320.3017578125
tensor(11320.3086, grad_fn=<NegBackward0>) tensor(11320.3018, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11320.294921875
tensor(11320.3018, grad_fn=<NegBackward0>) tensor(11320.2949, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11320.283203125
tensor(11320.2949, grad_fn=<NegBackward0>) tensor(11320.2832, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11320.275390625
tensor(11320.2832, grad_fn=<NegBackward0>) tensor(11320.2754, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11320.267578125
tensor(11320.2754, grad_fn=<NegBackward0>) tensor(11320.2676, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11320.2509765625
tensor(11320.2676, grad_fn=<NegBackward0>) tensor(11320.2510, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11320.2216796875
tensor(11320.2510, grad_fn=<NegBackward0>) tensor(11320.2217, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11320.20703125
tensor(11320.2217, grad_fn=<NegBackward0>) tensor(11320.2070, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11320.197265625
tensor(11320.2070, grad_fn=<NegBackward0>) tensor(11320.1973, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11320.1923828125
tensor(11320.1973, grad_fn=<NegBackward0>) tensor(11320.1924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11320.1884765625
tensor(11320.1924, grad_fn=<NegBackward0>) tensor(11320.1885, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11320.1865234375
tensor(11320.1885, grad_fn=<NegBackward0>) tensor(11320.1865, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11320.1826171875
tensor(11320.1865, grad_fn=<NegBackward0>) tensor(11320.1826, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11320.1796875
tensor(11320.1826, grad_fn=<NegBackward0>) tensor(11320.1797, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11320.1787109375
tensor(11320.1797, grad_fn=<NegBackward0>) tensor(11320.1787, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11320.17578125
tensor(11320.1787, grad_fn=<NegBackward0>) tensor(11320.1758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11320.173828125
tensor(11320.1758, grad_fn=<NegBackward0>) tensor(11320.1738, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11320.171875
tensor(11320.1738, grad_fn=<NegBackward0>) tensor(11320.1719, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11320.1611328125
tensor(11320.1719, grad_fn=<NegBackward0>) tensor(11320.1611, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11320.138671875
tensor(11320.1611, grad_fn=<NegBackward0>) tensor(11320.1387, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11320.1337890625
tensor(11320.1387, grad_fn=<NegBackward0>) tensor(11320.1338, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11320.1328125
tensor(11320.1338, grad_fn=<NegBackward0>) tensor(11320.1328, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11320.130859375
tensor(11320.1328, grad_fn=<NegBackward0>) tensor(11320.1309, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11320.1298828125
tensor(11320.1309, grad_fn=<NegBackward0>) tensor(11320.1299, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11320.12890625
tensor(11320.1299, grad_fn=<NegBackward0>) tensor(11320.1289, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11320.126953125
tensor(11320.1289, grad_fn=<NegBackward0>) tensor(11320.1270, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11320.1259765625
tensor(11320.1270, grad_fn=<NegBackward0>) tensor(11320.1260, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11320.1259765625
tensor(11320.1260, grad_fn=<NegBackward0>) tensor(11320.1260, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11320.1259765625
tensor(11320.1260, grad_fn=<NegBackward0>) tensor(11320.1260, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11320.125
tensor(11320.1260, grad_fn=<NegBackward0>) tensor(11320.1250, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11320.1240234375
tensor(11320.1250, grad_fn=<NegBackward0>) tensor(11320.1240, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11320.1240234375
tensor(11320.1240, grad_fn=<NegBackward0>) tensor(11320.1240, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11320.09765625
tensor(11320.1240, grad_fn=<NegBackward0>) tensor(11320.0977, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11320.0869140625
tensor(11320.0977, grad_fn=<NegBackward0>) tensor(11320.0869, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11320.087890625
tensor(11320.0869, grad_fn=<NegBackward0>) tensor(11320.0879, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11320.0859375
tensor(11320.0869, grad_fn=<NegBackward0>) tensor(11320.0859, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11320.08984375
tensor(11320.0859, grad_fn=<NegBackward0>) tensor(11320.0898, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11320.0849609375
tensor(11320.0859, grad_fn=<NegBackward0>) tensor(11320.0850, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11320.087890625
tensor(11320.0850, grad_fn=<NegBackward0>) tensor(11320.0879, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11320.0830078125
tensor(11320.0850, grad_fn=<NegBackward0>) tensor(11320.0830, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11320.0771484375
tensor(11320.0830, grad_fn=<NegBackward0>) tensor(11320.0771, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11320.078125
tensor(11320.0771, grad_fn=<NegBackward0>) tensor(11320.0781, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11320.078125
tensor(11320.0771, grad_fn=<NegBackward0>) tensor(11320.0781, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11320.076171875
tensor(11320.0771, grad_fn=<NegBackward0>) tensor(11320.0762, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11320.0712890625
tensor(11320.0762, grad_fn=<NegBackward0>) tensor(11320.0713, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11319.9951171875
tensor(11320.0713, grad_fn=<NegBackward0>) tensor(11319.9951, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11319.98046875
tensor(11319.9951, grad_fn=<NegBackward0>) tensor(11319.9805, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11319.978515625
tensor(11319.9805, grad_fn=<NegBackward0>) tensor(11319.9785, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11319.984375
tensor(11319.9785, grad_fn=<NegBackward0>) tensor(11319.9844, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11319.9365234375
tensor(11319.9785, grad_fn=<NegBackward0>) tensor(11319.9365, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11319.9375
tensor(11319.9365, grad_fn=<NegBackward0>) tensor(11319.9375, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11319.935546875
tensor(11319.9365, grad_fn=<NegBackward0>) tensor(11319.9355, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11319.9423828125
tensor(11319.9355, grad_fn=<NegBackward0>) tensor(11319.9424, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11319.927734375
tensor(11319.9355, grad_fn=<NegBackward0>) tensor(11319.9277, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11319.9208984375
tensor(11319.9277, grad_fn=<NegBackward0>) tensor(11319.9209, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11319.9013671875
tensor(11319.9209, grad_fn=<NegBackward0>) tensor(11319.9014, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11316.326171875
tensor(11319.9014, grad_fn=<NegBackward0>) tensor(11316.3262, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11315.708984375
tensor(11316.3262, grad_fn=<NegBackward0>) tensor(11315.7090, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11315.6708984375
tensor(11315.7090, grad_fn=<NegBackward0>) tensor(11315.6709, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11315.6630859375
tensor(11315.6709, grad_fn=<NegBackward0>) tensor(11315.6631, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11315.6640625
tensor(11315.6631, grad_fn=<NegBackward0>) tensor(11315.6641, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11315.66796875
tensor(11315.6631, grad_fn=<NegBackward0>) tensor(11315.6680, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11315.6611328125
tensor(11315.6631, grad_fn=<NegBackward0>) tensor(11315.6611, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11315.666015625
tensor(11315.6611, grad_fn=<NegBackward0>) tensor(11315.6660, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11315.66015625
tensor(11315.6611, grad_fn=<NegBackward0>) tensor(11315.6602, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11315.66015625
tensor(11315.6602, grad_fn=<NegBackward0>) tensor(11315.6602, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11315.6708984375
tensor(11315.6602, grad_fn=<NegBackward0>) tensor(11315.6709, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11315.658203125
tensor(11315.6602, grad_fn=<NegBackward0>) tensor(11315.6582, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11315.66015625
tensor(11315.6582, grad_fn=<NegBackward0>) tensor(11315.6602, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11315.6591796875
tensor(11315.6582, grad_fn=<NegBackward0>) tensor(11315.6592, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11315.65234375
tensor(11315.6582, grad_fn=<NegBackward0>) tensor(11315.6523, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11315.65234375
tensor(11315.6523, grad_fn=<NegBackward0>) tensor(11315.6523, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11315.6494140625
tensor(11315.6523, grad_fn=<NegBackward0>) tensor(11315.6494, grad_fn=<NegBackward0>)
pi: tensor([[0.6232, 0.3768],
        [0.3751, 0.6249]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0731, 0.9269], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3086, 0.0925],
         [0.6320, 0.1914]],

        [[0.7277, 0.1003],
         [0.6851, 0.5690]],

        [[0.5454, 0.1140],
         [0.7142, 0.5607]],

        [[0.5936, 0.1104],
         [0.5931, 0.6353]],

        [[0.5864, 0.1056],
         [0.5604, 0.6232]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.5135536976383382
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721116882917585
Global Adjusted Rand Index: 0.4834344528804254
Average Adjusted Rand Index: 0.6333707227009018
[0.5117371670477326, 0.4834344528804254] [0.6332928969576052, 0.6333707227009018] [11319.9453125, 11315.6572265625]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11141.438003266367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22079.68359375
inf tensor(22079.6836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11346.73046875
tensor(22079.6836, grad_fn=<NegBackward0>) tensor(11346.7305, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11344.326171875
tensor(11346.7305, grad_fn=<NegBackward0>) tensor(11344.3262, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11342.1328125
tensor(11344.3262, grad_fn=<NegBackward0>) tensor(11342.1328, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11340.240234375
tensor(11342.1328, grad_fn=<NegBackward0>) tensor(11340.2402, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11336.310546875
tensor(11340.2402, grad_fn=<NegBackward0>) tensor(11336.3105, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11285.0029296875
tensor(11336.3105, grad_fn=<NegBackward0>) tensor(11285.0029, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11225.603515625
tensor(11285.0029, grad_fn=<NegBackward0>) tensor(11225.6035, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11196.546875
tensor(11225.6035, grad_fn=<NegBackward0>) tensor(11196.5469, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11185.376953125
tensor(11196.5469, grad_fn=<NegBackward0>) tensor(11185.3770, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11175.3125
tensor(11185.3770, grad_fn=<NegBackward0>) tensor(11175.3125, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11170.7275390625
tensor(11175.3125, grad_fn=<NegBackward0>) tensor(11170.7275, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11163.5595703125
tensor(11170.7275, grad_fn=<NegBackward0>) tensor(11163.5596, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11163.2841796875
tensor(11163.5596, grad_fn=<NegBackward0>) tensor(11163.2842, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11157.29296875
tensor(11163.2842, grad_fn=<NegBackward0>) tensor(11157.2930, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11126.4189453125
tensor(11157.2930, grad_fn=<NegBackward0>) tensor(11126.4189, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11122.2236328125
tensor(11126.4189, grad_fn=<NegBackward0>) tensor(11122.2236, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11114.521484375
tensor(11122.2236, grad_fn=<NegBackward0>) tensor(11114.5215, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11113.5107421875
tensor(11114.5215, grad_fn=<NegBackward0>) tensor(11113.5107, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11113.4619140625
tensor(11113.5107, grad_fn=<NegBackward0>) tensor(11113.4619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11107.7294921875
tensor(11113.4619, grad_fn=<NegBackward0>) tensor(11107.7295, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11107.6845703125
tensor(11107.7295, grad_fn=<NegBackward0>) tensor(11107.6846, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11107.6572265625
tensor(11107.6846, grad_fn=<NegBackward0>) tensor(11107.6572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11107.634765625
tensor(11107.6572, grad_fn=<NegBackward0>) tensor(11107.6348, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11107.5498046875
tensor(11107.6348, grad_fn=<NegBackward0>) tensor(11107.5498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11107.306640625
tensor(11107.5498, grad_fn=<NegBackward0>) tensor(11107.3066, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11103.2099609375
tensor(11107.3066, grad_fn=<NegBackward0>) tensor(11103.2100, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11103.18359375
tensor(11103.2100, grad_fn=<NegBackward0>) tensor(11103.1836, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11103.150390625
tensor(11103.1836, grad_fn=<NegBackward0>) tensor(11103.1504, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11103.1455078125
tensor(11103.1504, grad_fn=<NegBackward0>) tensor(11103.1455, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11103.1396484375
tensor(11103.1455, grad_fn=<NegBackward0>) tensor(11103.1396, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11103.1298828125
tensor(11103.1396, grad_fn=<NegBackward0>) tensor(11103.1299, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11099.0947265625
tensor(11103.1299, grad_fn=<NegBackward0>) tensor(11099.0947, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11097.3681640625
tensor(11099.0947, grad_fn=<NegBackward0>) tensor(11097.3682, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11097.359375
tensor(11097.3682, grad_fn=<NegBackward0>) tensor(11097.3594, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11097.35546875
tensor(11097.3594, grad_fn=<NegBackward0>) tensor(11097.3555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11097.3525390625
tensor(11097.3555, grad_fn=<NegBackward0>) tensor(11097.3525, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11097.345703125
tensor(11097.3525, grad_fn=<NegBackward0>) tensor(11097.3457, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11097.3447265625
tensor(11097.3457, grad_fn=<NegBackward0>) tensor(11097.3447, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11097.3427734375
tensor(11097.3447, grad_fn=<NegBackward0>) tensor(11097.3428, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11097.341796875
tensor(11097.3428, grad_fn=<NegBackward0>) tensor(11097.3418, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11097.3408203125
tensor(11097.3418, grad_fn=<NegBackward0>) tensor(11097.3408, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11097.33984375
tensor(11097.3408, grad_fn=<NegBackward0>) tensor(11097.3398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11097.33984375
tensor(11097.3398, grad_fn=<NegBackward0>) tensor(11097.3398, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11097.337890625
tensor(11097.3398, grad_fn=<NegBackward0>) tensor(11097.3379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11097.3369140625
tensor(11097.3379, grad_fn=<NegBackward0>) tensor(11097.3369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11097.3359375
tensor(11097.3369, grad_fn=<NegBackward0>) tensor(11097.3359, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11097.3349609375
tensor(11097.3359, grad_fn=<NegBackward0>) tensor(11097.3350, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11097.333984375
tensor(11097.3350, grad_fn=<NegBackward0>) tensor(11097.3340, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11097.3330078125
tensor(11097.3340, grad_fn=<NegBackward0>) tensor(11097.3330, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11097.3330078125
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3330, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11097.333984375
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3340, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11097.3330078125
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3330, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11097.3330078125
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3330, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11097.333984375
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3340, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11097.3359375
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3359, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11097.33203125
tensor(11097.3330, grad_fn=<NegBackward0>) tensor(11097.3320, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11097.33203125
tensor(11097.3320, grad_fn=<NegBackward0>) tensor(11097.3320, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11097.326171875
tensor(11097.3320, grad_fn=<NegBackward0>) tensor(11097.3262, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11097.33203125
tensor(11097.3262, grad_fn=<NegBackward0>) tensor(11097.3320, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11097.326171875
tensor(11097.3262, grad_fn=<NegBackward0>) tensor(11097.3262, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11097.3251953125
tensor(11097.3262, grad_fn=<NegBackward0>) tensor(11097.3252, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11097.3232421875
tensor(11097.3252, grad_fn=<NegBackward0>) tensor(11097.3232, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11097.3232421875
tensor(11097.3232, grad_fn=<NegBackward0>) tensor(11097.3232, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11097.322265625
tensor(11097.3232, grad_fn=<NegBackward0>) tensor(11097.3223, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11097.3251953125
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3252, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11097.322265625
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3223, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11097.32421875
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3242, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11097.322265625
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3223, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11097.322265625
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3223, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11097.3359375
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3359, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11097.322265625
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3223, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11097.3193359375
tensor(11097.3223, grad_fn=<NegBackward0>) tensor(11097.3193, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11097.3193359375
tensor(11097.3193, grad_fn=<NegBackward0>) tensor(11097.3193, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11097.3310546875
tensor(11097.3193, grad_fn=<NegBackward0>) tensor(11097.3311, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11097.31640625
tensor(11097.3193, grad_fn=<NegBackward0>) tensor(11097.3164, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11097.263671875
tensor(11097.3164, grad_fn=<NegBackward0>) tensor(11097.2637, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11097.26171875
tensor(11097.2637, grad_fn=<NegBackward0>) tensor(11097.2617, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11097.26171875
tensor(11097.2617, grad_fn=<NegBackward0>) tensor(11097.2617, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11097.2939453125
tensor(11097.2617, grad_fn=<NegBackward0>) tensor(11097.2939, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11097.2373046875
tensor(11097.2617, grad_fn=<NegBackward0>) tensor(11097.2373, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11097.23828125
tensor(11097.2373, grad_fn=<NegBackward0>) tensor(11097.2383, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11097.236328125
tensor(11097.2373, grad_fn=<NegBackward0>) tensor(11097.2363, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11097.20703125
tensor(11097.2363, grad_fn=<NegBackward0>) tensor(11097.2070, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11097.205078125
tensor(11097.2070, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11097.283203125
tensor(11097.2051, grad_fn=<NegBackward0>) tensor(11097.2832, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11097.205078125
tensor(11097.2051, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11097.205078125
tensor(11097.2051, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11097.205078125
tensor(11097.2051, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11097.2041015625
tensor(11097.2051, grad_fn=<NegBackward0>) tensor(11097.2041, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11097.205078125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11097.205078125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11097.2509765625
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2510, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11097.205078125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11097.2041015625
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2041, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11097.2041015625
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2041, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11097.205078125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2051, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11097.2080078125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2080, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11097.2041015625
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2041, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11097.20703125
tensor(11097.2041, grad_fn=<NegBackward0>) tensor(11097.2070, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7181, 0.2819],
        [0.2391, 0.7609]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4752, 0.5248], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2992, 0.1056],
         [0.7045, 0.1941]],

        [[0.5766, 0.1086],
         [0.5929, 0.5823]],

        [[0.7122, 0.0978],
         [0.5706, 0.6363]],

        [[0.6784, 0.1070],
         [0.6766, 0.6435]],

        [[0.7232, 0.0924],
         [0.5872, 0.5550]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6362838347881163
Global Adjusted Rand Index: 0.8387333374443819
Average Adjusted Rand Index: 0.8408685634084275
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20227.298828125
inf tensor(20227.2988, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11346.0595703125
tensor(20227.2988, grad_fn=<NegBackward0>) tensor(11346.0596, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11339.0986328125
tensor(11346.0596, grad_fn=<NegBackward0>) tensor(11339.0986, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11336.099609375
tensor(11339.0986, grad_fn=<NegBackward0>) tensor(11336.0996, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11252.38671875
tensor(11336.0996, grad_fn=<NegBackward0>) tensor(11252.3867, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11154.341796875
tensor(11252.3867, grad_fn=<NegBackward0>) tensor(11154.3418, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11145.6796875
tensor(11154.3418, grad_fn=<NegBackward0>) tensor(11145.6797, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11145.0478515625
tensor(11145.6797, grad_fn=<NegBackward0>) tensor(11145.0479, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11144.869140625
tensor(11145.0479, grad_fn=<NegBackward0>) tensor(11144.8691, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11144.75
tensor(11144.8691, grad_fn=<NegBackward0>) tensor(11144.7500, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11144.5927734375
tensor(11144.7500, grad_fn=<NegBackward0>) tensor(11144.5928, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11144.275390625
tensor(11144.5928, grad_fn=<NegBackward0>) tensor(11144.2754, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11126.3642578125
tensor(11144.2754, grad_fn=<NegBackward0>) tensor(11126.3643, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11098.7548828125
tensor(11126.3643, grad_fn=<NegBackward0>) tensor(11098.7549, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11097.408203125
tensor(11098.7549, grad_fn=<NegBackward0>) tensor(11097.4082, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11097.3505859375
tensor(11097.4082, grad_fn=<NegBackward0>) tensor(11097.3506, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11097.2373046875
tensor(11097.3506, grad_fn=<NegBackward0>) tensor(11097.2373, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11097.1982421875
tensor(11097.2373, grad_fn=<NegBackward0>) tensor(11097.1982, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11097.1826171875
tensor(11097.1982, grad_fn=<NegBackward0>) tensor(11097.1826, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11097.1611328125
tensor(11097.1826, grad_fn=<NegBackward0>) tensor(11097.1611, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11097.1533203125
tensor(11097.1611, grad_fn=<NegBackward0>) tensor(11097.1533, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11097.1484375
tensor(11097.1533, grad_fn=<NegBackward0>) tensor(11097.1484, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11097.1435546875
tensor(11097.1484, grad_fn=<NegBackward0>) tensor(11097.1436, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11097.140625
tensor(11097.1436, grad_fn=<NegBackward0>) tensor(11097.1406, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11097.13671875
tensor(11097.1406, grad_fn=<NegBackward0>) tensor(11097.1367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11097.1318359375
tensor(11097.1367, grad_fn=<NegBackward0>) tensor(11097.1318, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11097.1328125
tensor(11097.1318, grad_fn=<NegBackward0>) tensor(11097.1328, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11097.1279296875
tensor(11097.1318, grad_fn=<NegBackward0>) tensor(11097.1279, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11097.126953125
tensor(11097.1279, grad_fn=<NegBackward0>) tensor(11097.1270, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11097.1240234375
tensor(11097.1270, grad_fn=<NegBackward0>) tensor(11097.1240, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11097.1240234375
tensor(11097.1240, grad_fn=<NegBackward0>) tensor(11097.1240, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11097.1220703125
tensor(11097.1240, grad_fn=<NegBackward0>) tensor(11097.1221, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11097.1201171875
tensor(11097.1221, grad_fn=<NegBackward0>) tensor(11097.1201, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11097.119140625
tensor(11097.1201, grad_fn=<NegBackward0>) tensor(11097.1191, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11097.12109375
tensor(11097.1191, grad_fn=<NegBackward0>) tensor(11097.1211, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11097.115234375
tensor(11097.1191, grad_fn=<NegBackward0>) tensor(11097.1152, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11097.1162109375
tensor(11097.1152, grad_fn=<NegBackward0>) tensor(11097.1162, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11097.115234375
tensor(11097.1152, grad_fn=<NegBackward0>) tensor(11097.1152, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11097.1142578125
tensor(11097.1152, grad_fn=<NegBackward0>) tensor(11097.1143, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11097.11328125
tensor(11097.1143, grad_fn=<NegBackward0>) tensor(11097.1133, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11097.11328125
tensor(11097.1133, grad_fn=<NegBackward0>) tensor(11097.1133, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11097.115234375
tensor(11097.1133, grad_fn=<NegBackward0>) tensor(11097.1152, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11097.11328125
tensor(11097.1133, grad_fn=<NegBackward0>) tensor(11097.1133, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11097.1123046875
tensor(11097.1133, grad_fn=<NegBackward0>) tensor(11097.1123, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11097.11328125
tensor(11097.1123, grad_fn=<NegBackward0>) tensor(11097.1133, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11097.1123046875
tensor(11097.1123, grad_fn=<NegBackward0>) tensor(11097.1123, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11097.1181640625
tensor(11097.1123, grad_fn=<NegBackward0>) tensor(11097.1182, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11097.111328125
tensor(11097.1123, grad_fn=<NegBackward0>) tensor(11097.1113, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11097.111328125
tensor(11097.1113, grad_fn=<NegBackward0>) tensor(11097.1113, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11097.1103515625
tensor(11097.1113, grad_fn=<NegBackward0>) tensor(11097.1104, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11097.111328125
tensor(11097.1104, grad_fn=<NegBackward0>) tensor(11097.1113, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11097.1103515625
tensor(11097.1104, grad_fn=<NegBackward0>) tensor(11097.1104, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11097.115234375
tensor(11097.1104, grad_fn=<NegBackward0>) tensor(11097.1152, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11097.109375
tensor(11097.1104, grad_fn=<NegBackward0>) tensor(11097.1094, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11097.1103515625
tensor(11097.1094, grad_fn=<NegBackward0>) tensor(11097.1104, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11097.11328125
tensor(11097.1094, grad_fn=<NegBackward0>) tensor(11097.1133, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11097.1171875
tensor(11097.1094, grad_fn=<NegBackward0>) tensor(11097.1172, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11097.109375
tensor(11097.1094, grad_fn=<NegBackward0>) tensor(11097.1094, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11097.1083984375
tensor(11097.1094, grad_fn=<NegBackward0>) tensor(11097.1084, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11097.1083984375
tensor(11097.1084, grad_fn=<NegBackward0>) tensor(11097.1084, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11097.1171875
tensor(11097.1084, grad_fn=<NegBackward0>) tensor(11097.1172, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11097.107421875
tensor(11097.1084, grad_fn=<NegBackward0>) tensor(11097.1074, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11097.1064453125
tensor(11097.1074, grad_fn=<NegBackward0>) tensor(11097.1064, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11097.109375
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1094, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11097.107421875
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1074, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11097.107421875
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1074, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11097.107421875
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1074, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11097.1064453125
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1064, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11097.107421875
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1074, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11097.1025390625
tensor(11097.1064, grad_fn=<NegBackward0>) tensor(11097.1025, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11097.1015625
tensor(11097.1025, grad_fn=<NegBackward0>) tensor(11097.1016, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11097.099609375
tensor(11097.1016, grad_fn=<NegBackward0>) tensor(11097.0996, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11097.1005859375
tensor(11097.0996, grad_fn=<NegBackward0>) tensor(11097.1006, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11097.1005859375
tensor(11097.0996, grad_fn=<NegBackward0>) tensor(11097.1006, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11097.103515625
tensor(11097.0996, grad_fn=<NegBackward0>) tensor(11097.1035, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11097.1005859375
tensor(11097.0996, grad_fn=<NegBackward0>) tensor(11097.1006, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11097.1005859375
tensor(11097.0996, grad_fn=<NegBackward0>) tensor(11097.1006, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7175, 0.2825],
        [0.2398, 0.7602]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4741, 0.5259], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.1057],
         [0.6714, 0.1940]],

        [[0.5668, 0.1086],
         [0.7305, 0.5901]],

        [[0.6604, 0.0978],
         [0.6297, 0.5984]],

        [[0.6120, 0.1070],
         [0.7144, 0.6807]],

        [[0.7177, 0.0924],
         [0.5901, 0.6823]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6362838347881163
Global Adjusted Rand Index: 0.8460920103112441
Average Adjusted Rand Index: 0.8485461256786296
[0.8387333374443819, 0.8460920103112441] [0.8408685634084275, 0.8485461256786296] [11097.1982421875, 11097.1005859375]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11133.915562852699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20028.162109375
inf tensor(20028.1621, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11305.939453125
tensor(20028.1621, grad_fn=<NegBackward0>) tensor(11305.9395, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11305.1123046875
tensor(11305.9395, grad_fn=<NegBackward0>) tensor(11305.1123, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11302.7939453125
tensor(11305.1123, grad_fn=<NegBackward0>) tensor(11302.7939, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11301.41015625
tensor(11302.7939, grad_fn=<NegBackward0>) tensor(11301.4102, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11301.048828125
tensor(11301.4102, grad_fn=<NegBackward0>) tensor(11301.0488, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11300.9580078125
tensor(11301.0488, grad_fn=<NegBackward0>) tensor(11300.9580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11300.9326171875
tensor(11300.9580, grad_fn=<NegBackward0>) tensor(11300.9326, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11300.921875
tensor(11300.9326, grad_fn=<NegBackward0>) tensor(11300.9219, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11300.9150390625
tensor(11300.9219, grad_fn=<NegBackward0>) tensor(11300.9150, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11300.908203125
tensor(11300.9150, grad_fn=<NegBackward0>) tensor(11300.9082, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11300.904296875
tensor(11300.9082, grad_fn=<NegBackward0>) tensor(11300.9043, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11300.904296875
tensor(11300.9043, grad_fn=<NegBackward0>) tensor(11300.9043, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11300.900390625
tensor(11300.9043, grad_fn=<NegBackward0>) tensor(11300.9004, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11300.8974609375
tensor(11300.9004, grad_fn=<NegBackward0>) tensor(11300.8975, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11300.8955078125
tensor(11300.8975, grad_fn=<NegBackward0>) tensor(11300.8955, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11300.89453125
tensor(11300.8955, grad_fn=<NegBackward0>) tensor(11300.8945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11300.8935546875
tensor(11300.8945, grad_fn=<NegBackward0>) tensor(11300.8936, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11300.890625
tensor(11300.8936, grad_fn=<NegBackward0>) tensor(11300.8906, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11300.8896484375
tensor(11300.8906, grad_fn=<NegBackward0>) tensor(11300.8896, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11300.888671875
tensor(11300.8896, grad_fn=<NegBackward0>) tensor(11300.8887, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11300.88671875
tensor(11300.8887, grad_fn=<NegBackward0>) tensor(11300.8867, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11300.8876953125
tensor(11300.8867, grad_fn=<NegBackward0>) tensor(11300.8877, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11300.8857421875
tensor(11300.8867, grad_fn=<NegBackward0>) tensor(11300.8857, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11300.8837890625
tensor(11300.8857, grad_fn=<NegBackward0>) tensor(11300.8838, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11300.8837890625
tensor(11300.8838, grad_fn=<NegBackward0>) tensor(11300.8838, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11300.8837890625
tensor(11300.8838, grad_fn=<NegBackward0>) tensor(11300.8838, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11300.8828125
tensor(11300.8838, grad_fn=<NegBackward0>) tensor(11300.8828, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11300.880859375
tensor(11300.8828, grad_fn=<NegBackward0>) tensor(11300.8809, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11300.880859375
tensor(11300.8809, grad_fn=<NegBackward0>) tensor(11300.8809, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11300.87890625
tensor(11300.8809, grad_fn=<NegBackward0>) tensor(11300.8789, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11300.8798828125
tensor(11300.8789, grad_fn=<NegBackward0>) tensor(11300.8799, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11300.8779296875
tensor(11300.8789, grad_fn=<NegBackward0>) tensor(11300.8779, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11300.8779296875
tensor(11300.8779, grad_fn=<NegBackward0>) tensor(11300.8779, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11300.876953125
tensor(11300.8779, grad_fn=<NegBackward0>) tensor(11300.8770, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11300.8759765625
tensor(11300.8770, grad_fn=<NegBackward0>) tensor(11300.8760, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11300.8759765625
tensor(11300.8760, grad_fn=<NegBackward0>) tensor(11300.8760, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11300.8759765625
tensor(11300.8760, grad_fn=<NegBackward0>) tensor(11300.8760, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11300.8740234375
tensor(11300.8760, grad_fn=<NegBackward0>) tensor(11300.8740, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11300.875
tensor(11300.8740, grad_fn=<NegBackward0>) tensor(11300.8750, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11300.873046875
tensor(11300.8740, grad_fn=<NegBackward0>) tensor(11300.8730, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11300.8740234375
tensor(11300.8730, grad_fn=<NegBackward0>) tensor(11300.8740, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11300.873046875
tensor(11300.8730, grad_fn=<NegBackward0>) tensor(11300.8730, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11300.8720703125
tensor(11300.8730, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11300.873046875
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8730, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11300.8720703125
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11300.8720703125
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11300.8720703125
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11300.8720703125
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11300.8720703125
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8721, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11300.87109375
tensor(11300.8721, grad_fn=<NegBackward0>) tensor(11300.8711, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11300.869140625
tensor(11300.8711, grad_fn=<NegBackward0>) tensor(11300.8691, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11300.8701171875
tensor(11300.8691, grad_fn=<NegBackward0>) tensor(11300.8701, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11300.8701171875
tensor(11300.8691, grad_fn=<NegBackward0>) tensor(11300.8701, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11300.87109375
tensor(11300.8691, grad_fn=<NegBackward0>) tensor(11300.8711, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -11300.87109375
tensor(11300.8691, grad_fn=<NegBackward0>) tensor(11300.8711, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -11300.8701171875
tensor(11300.8691, grad_fn=<NegBackward0>) tensor(11300.8701, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.0025, 0.9975],
        [0.0749, 0.9251]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0072, 0.9928], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3020, 0.1841],
         [0.5820, 0.1637]],

        [[0.6799, 0.2176],
         [0.5552, 0.7082]],

        [[0.7277, 0.2356],
         [0.5268, 0.6060]],

        [[0.7273, 0.2281],
         [0.6721, 0.6462]],

        [[0.5386, 0.2118],
         [0.7115, 0.6084]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.012598425196850394
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
Global Adjusted Rand Index: 0.006729803955004279
Average Adjusted Rand Index: 0.005515765010131432
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20814.859375
inf tensor(20814.8594, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11305.7509765625
tensor(20814.8594, grad_fn=<NegBackward0>) tensor(11305.7510, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11304.7412109375
tensor(11305.7510, grad_fn=<NegBackward0>) tensor(11304.7412, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11303.642578125
tensor(11304.7412, grad_fn=<NegBackward0>) tensor(11303.6426, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11302.2158203125
tensor(11303.6426, grad_fn=<NegBackward0>) tensor(11302.2158, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11301.73046875
tensor(11302.2158, grad_fn=<NegBackward0>) tensor(11301.7305, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11301.560546875
tensor(11301.7305, grad_fn=<NegBackward0>) tensor(11301.5605, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11301.4453125
tensor(11301.5605, grad_fn=<NegBackward0>) tensor(11301.4453, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11301.3291015625
tensor(11301.4453, grad_fn=<NegBackward0>) tensor(11301.3291, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11301.20703125
tensor(11301.3291, grad_fn=<NegBackward0>) tensor(11301.2070, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11301.115234375
tensor(11301.2070, grad_fn=<NegBackward0>) tensor(11301.1152, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11301.0126953125
tensor(11301.1152, grad_fn=<NegBackward0>) tensor(11301.0127, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11300.669921875
tensor(11301.0127, grad_fn=<NegBackward0>) tensor(11300.6699, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11300.310546875
tensor(11300.6699, grad_fn=<NegBackward0>) tensor(11300.3105, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11300.095703125
tensor(11300.3105, grad_fn=<NegBackward0>) tensor(11300.0957, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11299.953125
tensor(11300.0957, grad_fn=<NegBackward0>) tensor(11299.9531, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11299.8408203125
tensor(11299.9531, grad_fn=<NegBackward0>) tensor(11299.8408, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11299.7646484375
tensor(11299.8408, grad_fn=<NegBackward0>) tensor(11299.7646, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11299.720703125
tensor(11299.7646, grad_fn=<NegBackward0>) tensor(11299.7207, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11299.6904296875
tensor(11299.7207, grad_fn=<NegBackward0>) tensor(11299.6904, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11299.666015625
tensor(11299.6904, grad_fn=<NegBackward0>) tensor(11299.6660, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11299.6533203125
tensor(11299.6660, grad_fn=<NegBackward0>) tensor(11299.6533, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11299.6435546875
tensor(11299.6533, grad_fn=<NegBackward0>) tensor(11299.6436, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11299.6357421875
tensor(11299.6436, grad_fn=<NegBackward0>) tensor(11299.6357, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11299.630859375
tensor(11299.6357, grad_fn=<NegBackward0>) tensor(11299.6309, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11299.6240234375
tensor(11299.6309, grad_fn=<NegBackward0>) tensor(11299.6240, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11299.6005859375
tensor(11299.6240, grad_fn=<NegBackward0>) tensor(11299.6006, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11299.58984375
tensor(11299.6006, grad_fn=<NegBackward0>) tensor(11299.5898, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11299.58203125
tensor(11299.5898, grad_fn=<NegBackward0>) tensor(11299.5820, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11299.576171875
tensor(11299.5820, grad_fn=<NegBackward0>) tensor(11299.5762, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11299.57421875
tensor(11299.5762, grad_fn=<NegBackward0>) tensor(11299.5742, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11299.5732421875
tensor(11299.5742, grad_fn=<NegBackward0>) tensor(11299.5732, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11299.5732421875
tensor(11299.5732, grad_fn=<NegBackward0>) tensor(11299.5732, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11299.5712890625
tensor(11299.5732, grad_fn=<NegBackward0>) tensor(11299.5713, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11299.564453125
tensor(11299.5713, grad_fn=<NegBackward0>) tensor(11299.5645, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11299.564453125
tensor(11299.5645, grad_fn=<NegBackward0>) tensor(11299.5645, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11299.5634765625
tensor(11299.5645, grad_fn=<NegBackward0>) tensor(11299.5635, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11299.5615234375
tensor(11299.5635, grad_fn=<NegBackward0>) tensor(11299.5615, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11299.5625
tensor(11299.5615, grad_fn=<NegBackward0>) tensor(11299.5625, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11299.5615234375
tensor(11299.5615, grad_fn=<NegBackward0>) tensor(11299.5615, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11299.5615234375
tensor(11299.5615, grad_fn=<NegBackward0>) tensor(11299.5615, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11299.5595703125
tensor(11299.5615, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11299.560546875
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5605, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11299.5595703125
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11299.5595703125
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11299.560546875
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5605, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11299.560546875
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5605, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11299.5595703125
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11299.560546875
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5605, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11299.5595703125
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11299.55859375
tensor(11299.5596, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11299.55859375
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11299.5595703125
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11299.55859375
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11299.5615234375
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5615, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11299.5595703125
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11299.55859375
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11299.5595703125
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11299.55859375
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11299.5576171875
tensor(11299.5586, grad_fn=<NegBackward0>) tensor(11299.5576, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11299.5595703125
tensor(11299.5576, grad_fn=<NegBackward0>) tensor(11299.5596, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11299.5576171875
tensor(11299.5576, grad_fn=<NegBackward0>) tensor(11299.5576, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11299.556640625
tensor(11299.5576, grad_fn=<NegBackward0>) tensor(11299.5566, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11299.5576171875
tensor(11299.5566, grad_fn=<NegBackward0>) tensor(11299.5576, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11299.55859375
tensor(11299.5566, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11299.5576171875
tensor(11299.5566, grad_fn=<NegBackward0>) tensor(11299.5576, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11299.560546875
tensor(11299.5566, grad_fn=<NegBackward0>) tensor(11299.5605, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11299.55859375
tensor(11299.5566, grad_fn=<NegBackward0>) tensor(11299.5586, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.4195, 0.5805],
        [0.0065, 0.9935]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1048, 0.8952], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1281, 0.1219],
         [0.6400, 0.1737]],

        [[0.5486, 0.0958],
         [0.5154, 0.6034]],

        [[0.6462, 0.1467],
         [0.7215, 0.5239]],

        [[0.5381, 0.3052],
         [0.6055, 0.6012]],

        [[0.5773, 0.2032],
         [0.6150, 0.6157]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.014778186472389411
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0041166116796702715
Average Adjusted Rand Index: -0.0032873311152028365
[0.006729803955004279, -0.0041166116796702715] [0.005515765010131432, -0.0032873311152028365] [11300.8701171875, 11299.55859375]
-------------------------------------
This iteration is 66
True Objective function: Loss = -10927.004573707021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23871.970703125
inf tensor(23871.9707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11149.173828125
tensor(23871.9707, grad_fn=<NegBackward0>) tensor(11149.1738, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11145.455078125
tensor(11149.1738, grad_fn=<NegBackward0>) tensor(11145.4551, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11143.009765625
tensor(11145.4551, grad_fn=<NegBackward0>) tensor(11143.0098, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11117.060546875
tensor(11143.0098, grad_fn=<NegBackward0>) tensor(11117.0605, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11061.865234375
tensor(11117.0605, grad_fn=<NegBackward0>) tensor(11061.8652, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10974.865234375
tensor(11061.8652, grad_fn=<NegBackward0>) tensor(10974.8652, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10924.0498046875
tensor(10974.8652, grad_fn=<NegBackward0>) tensor(10924.0498, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10912.2431640625
tensor(10924.0498, grad_fn=<NegBackward0>) tensor(10912.2432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10907.728515625
tensor(10912.2432, grad_fn=<NegBackward0>) tensor(10907.7285, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10906.923828125
tensor(10907.7285, grad_fn=<NegBackward0>) tensor(10906.9238, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10906.83203125
tensor(10906.9238, grad_fn=<NegBackward0>) tensor(10906.8320, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10906.7685546875
tensor(10906.8320, grad_fn=<NegBackward0>) tensor(10906.7686, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10906.705078125
tensor(10906.7686, grad_fn=<NegBackward0>) tensor(10906.7051, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10906.6845703125
tensor(10906.7051, grad_fn=<NegBackward0>) tensor(10906.6846, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10906.6728515625
tensor(10906.6846, grad_fn=<NegBackward0>) tensor(10906.6729, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10906.6611328125
tensor(10906.6729, grad_fn=<NegBackward0>) tensor(10906.6611, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10906.6484375
tensor(10906.6611, grad_fn=<NegBackward0>) tensor(10906.6484, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10906.6298828125
tensor(10906.6484, grad_fn=<NegBackward0>) tensor(10906.6299, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10906.6220703125
tensor(10906.6299, grad_fn=<NegBackward0>) tensor(10906.6221, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10906.615234375
tensor(10906.6221, grad_fn=<NegBackward0>) tensor(10906.6152, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10906.607421875
tensor(10906.6152, grad_fn=<NegBackward0>) tensor(10906.6074, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10906.60546875
tensor(10906.6074, grad_fn=<NegBackward0>) tensor(10906.6055, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10906.6025390625
tensor(10906.6055, grad_fn=<NegBackward0>) tensor(10906.6025, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10906.5986328125
tensor(10906.6025, grad_fn=<NegBackward0>) tensor(10906.5986, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10906.59375
tensor(10906.5986, grad_fn=<NegBackward0>) tensor(10906.5938, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10906.5908203125
tensor(10906.5938, grad_fn=<NegBackward0>) tensor(10906.5908, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10906.5859375
tensor(10906.5908, grad_fn=<NegBackward0>) tensor(10906.5859, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10906.4658203125
tensor(10906.5859, grad_fn=<NegBackward0>) tensor(10906.4658, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10906.4638671875
tensor(10906.4658, grad_fn=<NegBackward0>) tensor(10906.4639, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10906.4619140625
tensor(10906.4639, grad_fn=<NegBackward0>) tensor(10906.4619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10906.462890625
tensor(10906.4619, grad_fn=<NegBackward0>) tensor(10906.4629, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10906.4599609375
tensor(10906.4619, grad_fn=<NegBackward0>) tensor(10906.4600, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10906.455078125
tensor(10906.4600, grad_fn=<NegBackward0>) tensor(10906.4551, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10906.4541015625
tensor(10906.4551, grad_fn=<NegBackward0>) tensor(10906.4541, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10906.4521484375
tensor(10906.4541, grad_fn=<NegBackward0>) tensor(10906.4521, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10906.4521484375
tensor(10906.4521, grad_fn=<NegBackward0>) tensor(10906.4521, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10906.447265625
tensor(10906.4521, grad_fn=<NegBackward0>) tensor(10906.4473, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10906.3486328125
tensor(10906.4473, grad_fn=<NegBackward0>) tensor(10906.3486, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10906.34375
tensor(10906.3486, grad_fn=<NegBackward0>) tensor(10906.3438, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10906.33984375
tensor(10906.3438, grad_fn=<NegBackward0>) tensor(10906.3398, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10906.3359375
tensor(10906.3398, grad_fn=<NegBackward0>) tensor(10906.3359, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10906.3349609375
tensor(10906.3359, grad_fn=<NegBackward0>) tensor(10906.3350, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10906.3359375
tensor(10906.3350, grad_fn=<NegBackward0>) tensor(10906.3359, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10906.3349609375
tensor(10906.3350, grad_fn=<NegBackward0>) tensor(10906.3350, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10906.333984375
tensor(10906.3350, grad_fn=<NegBackward0>) tensor(10906.3340, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10906.345703125
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3457, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10906.3330078125
tensor(10906.3340, grad_fn=<NegBackward0>) tensor(10906.3330, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10906.3388671875
tensor(10906.3330, grad_fn=<NegBackward0>) tensor(10906.3389, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10906.33203125
tensor(10906.3330, grad_fn=<NegBackward0>) tensor(10906.3320, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10906.33203125
tensor(10906.3320, grad_fn=<NegBackward0>) tensor(10906.3320, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10906.330078125
tensor(10906.3320, grad_fn=<NegBackward0>) tensor(10906.3301, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10906.33203125
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3320, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10906.3310546875
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3311, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10906.330078125
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3301, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10906.330078125
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3301, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10906.3310546875
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3311, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10906.330078125
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3301, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10906.328125
tensor(10906.3301, grad_fn=<NegBackward0>) tensor(10906.3281, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10906.3291015625
tensor(10906.3281, grad_fn=<NegBackward0>) tensor(10906.3291, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10906.3291015625
tensor(10906.3281, grad_fn=<NegBackward0>) tensor(10906.3291, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10906.3310546875
tensor(10906.3281, grad_fn=<NegBackward0>) tensor(10906.3311, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10906.3291015625
tensor(10906.3281, grad_fn=<NegBackward0>) tensor(10906.3291, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10906.330078125
tensor(10906.3281, grad_fn=<NegBackward0>) tensor(10906.3301, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.7829, 0.2171],
        [0.3267, 0.6733]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5056, 0.4944], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1980, 0.1000],
         [0.6146, 0.2946]],

        [[0.6560, 0.1011],
         [0.5586, 0.5286]],

        [[0.7262, 0.0884],
         [0.6703, 0.6511]],

        [[0.5157, 0.0997],
         [0.6242, 0.7050]],

        [[0.6561, 0.0901],
         [0.6108, 0.6308]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8070032874081188
Global Adjusted Rand Index: 0.9446666360464014
Average Adjusted Rand Index: 0.9453994348889336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20415.189453125
inf tensor(20415.1895, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11149.724609375
tensor(20415.1895, grad_fn=<NegBackward0>) tensor(11149.7246, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11148.01171875
tensor(11149.7246, grad_fn=<NegBackward0>) tensor(11148.0117, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11147.255859375
tensor(11148.0117, grad_fn=<NegBackward0>) tensor(11147.2559, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11146.4990234375
tensor(11147.2559, grad_fn=<NegBackward0>) tensor(11146.4990, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11144.2607421875
tensor(11146.4990, grad_fn=<NegBackward0>) tensor(11144.2607, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11110.9873046875
tensor(11144.2607, grad_fn=<NegBackward0>) tensor(11110.9873, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11058.275390625
tensor(11110.9873, grad_fn=<NegBackward0>) tensor(11058.2754, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10928.103515625
tensor(11058.2754, grad_fn=<NegBackward0>) tensor(10928.1035, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10914.314453125
tensor(10928.1035, grad_fn=<NegBackward0>) tensor(10914.3145, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10911.353515625
tensor(10914.3145, grad_fn=<NegBackward0>) tensor(10911.3535, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10907.3056640625
tensor(10911.3535, grad_fn=<NegBackward0>) tensor(10907.3057, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10907.23046875
tensor(10907.3057, grad_fn=<NegBackward0>) tensor(10907.2305, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10907.1494140625
tensor(10907.2305, grad_fn=<NegBackward0>) tensor(10907.1494, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10906.716796875
tensor(10907.1494, grad_fn=<NegBackward0>) tensor(10906.7168, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10906.658203125
tensor(10906.7168, grad_fn=<NegBackward0>) tensor(10906.6582, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10906.6376953125
tensor(10906.6582, grad_fn=<NegBackward0>) tensor(10906.6377, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10906.625
tensor(10906.6377, grad_fn=<NegBackward0>) tensor(10906.6250, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10906.6142578125
tensor(10906.6250, grad_fn=<NegBackward0>) tensor(10906.6143, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10906.6025390625
tensor(10906.6143, grad_fn=<NegBackward0>) tensor(10906.6025, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10906.5966796875
tensor(10906.6025, grad_fn=<NegBackward0>) tensor(10906.5967, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10906.591796875
tensor(10906.5967, grad_fn=<NegBackward0>) tensor(10906.5918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10906.587890625
tensor(10906.5918, grad_fn=<NegBackward0>) tensor(10906.5879, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10906.5869140625
tensor(10906.5879, grad_fn=<NegBackward0>) tensor(10906.5869, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10906.5810546875
tensor(10906.5869, grad_fn=<NegBackward0>) tensor(10906.5811, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10906.572265625
tensor(10906.5811, grad_fn=<NegBackward0>) tensor(10906.5723, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10906.529296875
tensor(10906.5723, grad_fn=<NegBackward0>) tensor(10906.5293, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10906.5390625
tensor(10906.5293, grad_fn=<NegBackward0>) tensor(10906.5391, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10906.5244140625
tensor(10906.5293, grad_fn=<NegBackward0>) tensor(10906.5244, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10906.5263671875
tensor(10906.5244, grad_fn=<NegBackward0>) tensor(10906.5264, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10906.521484375
tensor(10906.5244, grad_fn=<NegBackward0>) tensor(10906.5215, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10906.5205078125
tensor(10906.5215, grad_fn=<NegBackward0>) tensor(10906.5205, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10906.5185546875
tensor(10906.5205, grad_fn=<NegBackward0>) tensor(10906.5186, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10906.5166015625
tensor(10906.5186, grad_fn=<NegBackward0>) tensor(10906.5166, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10906.4853515625
tensor(10906.5166, grad_fn=<NegBackward0>) tensor(10906.4854, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10906.484375
tensor(10906.4854, grad_fn=<NegBackward0>) tensor(10906.4844, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10906.4833984375
tensor(10906.4844, grad_fn=<NegBackward0>) tensor(10906.4834, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10906.4814453125
tensor(10906.4834, grad_fn=<NegBackward0>) tensor(10906.4814, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10906.48046875
tensor(10906.4814, grad_fn=<NegBackward0>) tensor(10906.4805, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10906.490234375
tensor(10906.4805, grad_fn=<NegBackward0>) tensor(10906.4902, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10906.478515625
tensor(10906.4805, grad_fn=<NegBackward0>) tensor(10906.4785, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10906.478515625
tensor(10906.4785, grad_fn=<NegBackward0>) tensor(10906.4785, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10906.474609375
tensor(10906.4785, grad_fn=<NegBackward0>) tensor(10906.4746, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10906.474609375
tensor(10906.4746, grad_fn=<NegBackward0>) tensor(10906.4746, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10906.455078125
tensor(10906.4746, grad_fn=<NegBackward0>) tensor(10906.4551, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10906.44921875
tensor(10906.4551, grad_fn=<NegBackward0>) tensor(10906.4492, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10906.4462890625
tensor(10906.4492, grad_fn=<NegBackward0>) tensor(10906.4463, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10906.4453125
tensor(10906.4463, grad_fn=<NegBackward0>) tensor(10906.4453, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10906.41015625
tensor(10906.4453, grad_fn=<NegBackward0>) tensor(10906.4102, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10906.4072265625
tensor(10906.4102, grad_fn=<NegBackward0>) tensor(10906.4072, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10906.4052734375
tensor(10906.4072, grad_fn=<NegBackward0>) tensor(10906.4053, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10906.408203125
tensor(10906.4053, grad_fn=<NegBackward0>) tensor(10906.4082, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10906.4033203125
tensor(10906.4053, grad_fn=<NegBackward0>) tensor(10906.4033, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10906.404296875
tensor(10906.4033, grad_fn=<NegBackward0>) tensor(10906.4043, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10906.404296875
tensor(10906.4033, grad_fn=<NegBackward0>) tensor(10906.4043, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10906.421875
tensor(10906.4033, grad_fn=<NegBackward0>) tensor(10906.4219, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -10906.4013671875
tensor(10906.4033, grad_fn=<NegBackward0>) tensor(10906.4014, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10906.4013671875
tensor(10906.4014, grad_fn=<NegBackward0>) tensor(10906.4014, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10906.400390625
tensor(10906.4014, grad_fn=<NegBackward0>) tensor(10906.4004, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10906.3828125
tensor(10906.4004, grad_fn=<NegBackward0>) tensor(10906.3828, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10906.3818359375
tensor(10906.3828, grad_fn=<NegBackward0>) tensor(10906.3818, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10906.3828125
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3828, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10906.3828125
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3828, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10906.3828125
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3828, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10906.3818359375
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3818, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10906.3828125
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3828, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10906.3818359375
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3818, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10906.380859375
tensor(10906.3818, grad_fn=<NegBackward0>) tensor(10906.3809, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10906.3798828125
tensor(10906.3809, grad_fn=<NegBackward0>) tensor(10906.3799, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10906.380859375
tensor(10906.3799, grad_fn=<NegBackward0>) tensor(10906.3809, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10906.380859375
tensor(10906.3799, grad_fn=<NegBackward0>) tensor(10906.3809, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10906.375
tensor(10906.3799, grad_fn=<NegBackward0>) tensor(10906.3750, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10906.3740234375
tensor(10906.3750, grad_fn=<NegBackward0>) tensor(10906.3740, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10906.376953125
tensor(10906.3740, grad_fn=<NegBackward0>) tensor(10906.3770, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10906.375
tensor(10906.3740, grad_fn=<NegBackward0>) tensor(10906.3750, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10906.373046875
tensor(10906.3740, grad_fn=<NegBackward0>) tensor(10906.3730, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10906.3935546875
tensor(10906.3730, grad_fn=<NegBackward0>) tensor(10906.3936, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10906.3740234375
tensor(10906.3730, grad_fn=<NegBackward0>) tensor(10906.3740, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10906.38671875
tensor(10906.3730, grad_fn=<NegBackward0>) tensor(10906.3867, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10906.384765625
tensor(10906.3730, grad_fn=<NegBackward0>) tensor(10906.3848, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -10906.3837890625
tensor(10906.3730, grad_fn=<NegBackward0>) tensor(10906.3838, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.7845, 0.2155],
        [0.3265, 0.6735]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5059, 0.4941], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1981, 0.1000],
         [0.5060, 0.2955]],

        [[0.6930, 0.1011],
         [0.5556, 0.7266]],

        [[0.5155, 0.0882],
         [0.5336, 0.5826]],

        [[0.5836, 0.0997],
         [0.5244, 0.6991]],

        [[0.6438, 0.0901],
         [0.5393, 0.5202]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8070032874081188
Global Adjusted Rand Index: 0.9446666360464014
Average Adjusted Rand Index: 0.9453994348889336
[0.9446666360464014, 0.9446666360464014] [0.9453994348889336, 0.9453994348889336] [10906.330078125, 10906.3837890625]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10996.83233785948
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23467.603515625
inf tensor(23467.6035, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11291.685546875
tensor(23467.6035, grad_fn=<NegBackward0>) tensor(11291.6855, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11241.912109375
tensor(11291.6855, grad_fn=<NegBackward0>) tensor(11241.9121, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11240.873046875
tensor(11241.9121, grad_fn=<NegBackward0>) tensor(11240.8730, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11240.1640625
tensor(11240.8730, grad_fn=<NegBackward0>) tensor(11240.1641, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11205.4853515625
tensor(11240.1641, grad_fn=<NegBackward0>) tensor(11205.4854, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11047.29296875
tensor(11205.4854, grad_fn=<NegBackward0>) tensor(11047.2930, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10998.0859375
tensor(11047.2930, grad_fn=<NegBackward0>) tensor(10998.0859, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10987.6171875
tensor(10998.0859, grad_fn=<NegBackward0>) tensor(10987.6172, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10982.2548828125
tensor(10987.6172, grad_fn=<NegBackward0>) tensor(10982.2549, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10982.044921875
tensor(10982.2549, grad_fn=<NegBackward0>) tensor(10982.0449, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10972.408203125
tensor(10982.0449, grad_fn=<NegBackward0>) tensor(10972.4082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10972.259765625
tensor(10972.4082, grad_fn=<NegBackward0>) tensor(10972.2598, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10972.23828125
tensor(10972.2598, grad_fn=<NegBackward0>) tensor(10972.2383, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10972.2265625
tensor(10972.2383, grad_fn=<NegBackward0>) tensor(10972.2266, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10972.21484375
tensor(10972.2266, grad_fn=<NegBackward0>) tensor(10972.2148, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10972.20703125
tensor(10972.2148, grad_fn=<NegBackward0>) tensor(10972.2070, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10972.19921875
tensor(10972.2070, grad_fn=<NegBackward0>) tensor(10972.1992, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10972.189453125
tensor(10972.1992, grad_fn=<NegBackward0>) tensor(10972.1895, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10972.166015625
tensor(10972.1895, grad_fn=<NegBackward0>) tensor(10972.1660, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10972.1513671875
tensor(10972.1660, grad_fn=<NegBackward0>) tensor(10972.1514, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10972.13671875
tensor(10972.1514, grad_fn=<NegBackward0>) tensor(10972.1367, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10972.1318359375
tensor(10972.1367, grad_fn=<NegBackward0>) tensor(10972.1318, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10972.126953125
tensor(10972.1318, grad_fn=<NegBackward0>) tensor(10972.1270, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10972.123046875
tensor(10972.1270, grad_fn=<NegBackward0>) tensor(10972.1230, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10972.1240234375
tensor(10972.1230, grad_fn=<NegBackward0>) tensor(10972.1240, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -10972.12109375
tensor(10972.1230, grad_fn=<NegBackward0>) tensor(10972.1211, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10972.119140625
tensor(10972.1211, grad_fn=<NegBackward0>) tensor(10972.1191, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10972.1162109375
tensor(10972.1191, grad_fn=<NegBackward0>) tensor(10972.1162, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10972.115234375
tensor(10972.1162, grad_fn=<NegBackward0>) tensor(10972.1152, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10972.1142578125
tensor(10972.1152, grad_fn=<NegBackward0>) tensor(10972.1143, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10972.11328125
tensor(10972.1143, grad_fn=<NegBackward0>) tensor(10972.1133, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10972.1142578125
tensor(10972.1133, grad_fn=<NegBackward0>) tensor(10972.1143, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10972.1103515625
tensor(10972.1133, grad_fn=<NegBackward0>) tensor(10972.1104, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10972.109375
tensor(10972.1104, grad_fn=<NegBackward0>) tensor(10972.1094, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10972.109375
tensor(10972.1094, grad_fn=<NegBackward0>) tensor(10972.1094, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10972.107421875
tensor(10972.1094, grad_fn=<NegBackward0>) tensor(10972.1074, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10972.107421875
tensor(10972.1074, grad_fn=<NegBackward0>) tensor(10972.1074, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10972.1064453125
tensor(10972.1074, grad_fn=<NegBackward0>) tensor(10972.1064, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10970.3037109375
tensor(10972.1064, grad_fn=<NegBackward0>) tensor(10970.3037, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10970.2978515625
tensor(10970.3037, grad_fn=<NegBackward0>) tensor(10970.2979, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10970.296875
tensor(10970.2979, grad_fn=<NegBackward0>) tensor(10970.2969, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10970.296875
tensor(10970.2969, grad_fn=<NegBackward0>) tensor(10970.2969, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10970.296875
tensor(10970.2969, grad_fn=<NegBackward0>) tensor(10970.2969, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10970.2900390625
tensor(10970.2969, grad_fn=<NegBackward0>) tensor(10970.2900, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10970.224609375
tensor(10970.2900, grad_fn=<NegBackward0>) tensor(10970.2246, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10970.1015625
tensor(10970.2246, grad_fn=<NegBackward0>) tensor(10970.1016, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10970.0888671875
tensor(10970.1016, grad_fn=<NegBackward0>) tensor(10970.0889, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10970.09375
tensor(10970.0889, grad_fn=<NegBackward0>) tensor(10970.0938, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10970.087890625
tensor(10970.0889, grad_fn=<NegBackward0>) tensor(10970.0879, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10970.087890625
tensor(10970.0879, grad_fn=<NegBackward0>) tensor(10970.0879, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10970.0888671875
tensor(10970.0879, grad_fn=<NegBackward0>) tensor(10970.0889, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10970.0869140625
tensor(10970.0879, grad_fn=<NegBackward0>) tensor(10970.0869, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10970.0869140625
tensor(10970.0869, grad_fn=<NegBackward0>) tensor(10970.0869, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10970.0859375
tensor(10970.0869, grad_fn=<NegBackward0>) tensor(10970.0859, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10970.0849609375
tensor(10970.0859, grad_fn=<NegBackward0>) tensor(10970.0850, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10970.0869140625
tensor(10970.0850, grad_fn=<NegBackward0>) tensor(10970.0869, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10970.08203125
tensor(10970.0850, grad_fn=<NegBackward0>) tensor(10970.0820, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10970.08203125
tensor(10970.0820, grad_fn=<NegBackward0>) tensor(10970.0820, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10970.08203125
tensor(10970.0820, grad_fn=<NegBackward0>) tensor(10970.0820, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10970.0810546875
tensor(10970.0820, grad_fn=<NegBackward0>) tensor(10970.0811, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10970.08203125
tensor(10970.0811, grad_fn=<NegBackward0>) tensor(10970.0820, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10970.0810546875
tensor(10970.0811, grad_fn=<NegBackward0>) tensor(10970.0811, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10970.0810546875
tensor(10970.0811, grad_fn=<NegBackward0>) tensor(10970.0811, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10970.080078125
tensor(10970.0811, grad_fn=<NegBackward0>) tensor(10970.0801, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10970.0810546875
tensor(10970.0801, grad_fn=<NegBackward0>) tensor(10970.0811, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10970.076171875
tensor(10970.0801, grad_fn=<NegBackward0>) tensor(10970.0762, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10970.078125
tensor(10970.0762, grad_fn=<NegBackward0>) tensor(10970.0781, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10970.07421875
tensor(10970.0762, grad_fn=<NegBackward0>) tensor(10970.0742, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10970.0732421875
tensor(10970.0742, grad_fn=<NegBackward0>) tensor(10970.0732, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10970.0732421875
tensor(10970.0732, grad_fn=<NegBackward0>) tensor(10970.0732, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10970.07421875
tensor(10970.0732, grad_fn=<NegBackward0>) tensor(10970.0742, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10970.0732421875
tensor(10970.0732, grad_fn=<NegBackward0>) tensor(10970.0732, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10970.072265625
tensor(10970.0732, grad_fn=<NegBackward0>) tensor(10970.0723, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10970.0712890625
tensor(10970.0723, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10970.0712890625
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10970.0712890625
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10970.072265625
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0723, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10970.0712890625
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10970.076171875
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0762, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10970.0712890625
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0713, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10970.0703125
tensor(10970.0713, grad_fn=<NegBackward0>) tensor(10970.0703, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10970.07421875
tensor(10970.0703, grad_fn=<NegBackward0>) tensor(10970.0742, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10970.0703125
tensor(10970.0703, grad_fn=<NegBackward0>) tensor(10970.0703, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10970.0693359375
tensor(10970.0703, grad_fn=<NegBackward0>) tensor(10970.0693, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10970.072265625
tensor(10970.0693, grad_fn=<NegBackward0>) tensor(10970.0723, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10970.068359375
tensor(10970.0693, grad_fn=<NegBackward0>) tensor(10970.0684, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10970.07421875
tensor(10970.0684, grad_fn=<NegBackward0>) tensor(10970.0742, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10970.095703125
tensor(10970.0684, grad_fn=<NegBackward0>) tensor(10970.0957, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10970.068359375
tensor(10970.0684, grad_fn=<NegBackward0>) tensor(10970.0684, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10970.03515625
tensor(10970.0684, grad_fn=<NegBackward0>) tensor(10970.0352, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10970.03515625
tensor(10970.0352, grad_fn=<NegBackward0>) tensor(10970.0352, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10969.8671875
tensor(10970.0352, grad_fn=<NegBackward0>) tensor(10969.8672, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10969.861328125
tensor(10969.8672, grad_fn=<NegBackward0>) tensor(10969.8613, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10969.8798828125
tensor(10969.8613, grad_fn=<NegBackward0>) tensor(10969.8799, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10969.880859375
tensor(10969.8613, grad_fn=<NegBackward0>) tensor(10969.8809, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10969.8935546875
tensor(10969.8613, grad_fn=<NegBackward0>) tensor(10969.8936, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -10969.8359375
tensor(10969.8613, grad_fn=<NegBackward0>) tensor(10969.8359, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10969.8388671875
tensor(10969.8359, grad_fn=<NegBackward0>) tensor(10969.8389, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10969.8359375
tensor(10969.8359, grad_fn=<NegBackward0>) tensor(10969.8359, grad_fn=<NegBackward0>)
pi: tensor([[0.7917, 0.2083],
        [0.1562, 0.8438]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5080, 0.4920], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3002, 0.1075],
         [0.7194, 0.1963]],

        [[0.6956, 0.0848],
         [0.6564, 0.5183]],

        [[0.5455, 0.1055],
         [0.6070, 0.5225]],

        [[0.5202, 0.0952],
         [0.6710, 0.5722]],

        [[0.6614, 0.0945],
         [0.6146, 0.7126]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.944673345271342
Average Adjusted Rand Index: 0.9446418293262768
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21772.142578125
inf tensor(21772.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11298.7421875
tensor(21772.1426, grad_fn=<NegBackward0>) tensor(11298.7422, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11293.0576171875
tensor(11298.7422, grad_fn=<NegBackward0>) tensor(11293.0576, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11290.0390625
tensor(11293.0576, grad_fn=<NegBackward0>) tensor(11290.0391, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11250.28515625
tensor(11290.0391, grad_fn=<NegBackward0>) tensor(11250.2852, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11245.052734375
tensor(11250.2852, grad_fn=<NegBackward0>) tensor(11245.0527, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11244.5556640625
tensor(11245.0527, grad_fn=<NegBackward0>) tensor(11244.5557, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11239.6416015625
tensor(11244.5557, grad_fn=<NegBackward0>) tensor(11239.6416, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11238.775390625
tensor(11239.6416, grad_fn=<NegBackward0>) tensor(11238.7754, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11238.53125
tensor(11238.7754, grad_fn=<NegBackward0>) tensor(11238.5312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11238.390625
tensor(11238.5312, grad_fn=<NegBackward0>) tensor(11238.3906, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11238.2958984375
tensor(11238.3906, grad_fn=<NegBackward0>) tensor(11238.2959, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11238.228515625
tensor(11238.2959, grad_fn=<NegBackward0>) tensor(11238.2285, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11238.181640625
tensor(11238.2285, grad_fn=<NegBackward0>) tensor(11238.1816, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11238.1484375
tensor(11238.1816, grad_fn=<NegBackward0>) tensor(11238.1484, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11238.1259765625
tensor(11238.1484, grad_fn=<NegBackward0>) tensor(11238.1260, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11238.1044921875
tensor(11238.1260, grad_fn=<NegBackward0>) tensor(11238.1045, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11238.0888671875
tensor(11238.1045, grad_fn=<NegBackward0>) tensor(11238.0889, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11238.0791015625
tensor(11238.0889, grad_fn=<NegBackward0>) tensor(11238.0791, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11238.0712890625
tensor(11238.0791, grad_fn=<NegBackward0>) tensor(11238.0713, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11238.0634765625
tensor(11238.0713, grad_fn=<NegBackward0>) tensor(11238.0635, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11238.0576171875
tensor(11238.0635, grad_fn=<NegBackward0>) tensor(11238.0576, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11238.0546875
tensor(11238.0576, grad_fn=<NegBackward0>) tensor(11238.0547, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11238.048828125
tensor(11238.0547, grad_fn=<NegBackward0>) tensor(11238.0488, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11238.0458984375
tensor(11238.0488, grad_fn=<NegBackward0>) tensor(11238.0459, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11238.0419921875
tensor(11238.0459, grad_fn=<NegBackward0>) tensor(11238.0420, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11238.041015625
tensor(11238.0420, grad_fn=<NegBackward0>) tensor(11238.0410, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11238.0380859375
tensor(11238.0410, grad_fn=<NegBackward0>) tensor(11238.0381, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11238.0478515625
tensor(11238.0381, grad_fn=<NegBackward0>) tensor(11238.0479, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11238.03515625
tensor(11238.0381, grad_fn=<NegBackward0>) tensor(11238.0352, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11238.0341796875
tensor(11238.0352, grad_fn=<NegBackward0>) tensor(11238.0342, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11238.0322265625
tensor(11238.0342, grad_fn=<NegBackward0>) tensor(11238.0322, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11238.029296875
tensor(11238.0322, grad_fn=<NegBackward0>) tensor(11238.0293, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11238.029296875
tensor(11238.0293, grad_fn=<NegBackward0>) tensor(11238.0293, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11238.0283203125
tensor(11238.0293, grad_fn=<NegBackward0>) tensor(11238.0283, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11238.02734375
tensor(11238.0283, grad_fn=<NegBackward0>) tensor(11238.0273, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11238.0263671875
tensor(11238.0273, grad_fn=<NegBackward0>) tensor(11238.0264, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11238.025390625
tensor(11238.0264, grad_fn=<NegBackward0>) tensor(11238.0254, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11238.0234375
tensor(11238.0254, grad_fn=<NegBackward0>) tensor(11238.0234, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11238.0244140625
tensor(11238.0234, grad_fn=<NegBackward0>) tensor(11238.0244, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11238.0224609375
tensor(11238.0234, grad_fn=<NegBackward0>) tensor(11238.0225, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11238.021484375
tensor(11238.0225, grad_fn=<NegBackward0>) tensor(11238.0215, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11238.021484375
tensor(11238.0215, grad_fn=<NegBackward0>) tensor(11238.0215, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11238.0205078125
tensor(11238.0215, grad_fn=<NegBackward0>) tensor(11238.0205, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11238.021484375
tensor(11238.0205, grad_fn=<NegBackward0>) tensor(11238.0215, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11238.01953125
tensor(11238.0205, grad_fn=<NegBackward0>) tensor(11238.0195, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11238.0205078125
tensor(11238.0195, grad_fn=<NegBackward0>) tensor(11238.0205, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11238.0224609375
tensor(11238.0195, grad_fn=<NegBackward0>) tensor(11238.0225, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11238.017578125
tensor(11238.0195, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11238.01953125
tensor(11238.0176, grad_fn=<NegBackward0>) tensor(11238.0195, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11238.017578125
tensor(11238.0176, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11238.017578125
tensor(11238.0176, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11238.017578125
tensor(11238.0176, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11238.0166015625
tensor(11238.0176, grad_fn=<NegBackward0>) tensor(11238.0166, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11238.0166015625
tensor(11238.0166, grad_fn=<NegBackward0>) tensor(11238.0166, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11238.015625
tensor(11238.0166, grad_fn=<NegBackward0>) tensor(11238.0156, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11238.017578125
tensor(11238.0156, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11238.0166015625
tensor(11238.0156, grad_fn=<NegBackward0>) tensor(11238.0166, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11238.0146484375
tensor(11238.0156, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11238.015625
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0156, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11238.015625
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0156, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11238.0146484375
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11238.013671875
tensor(11238.0146, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11238.013671875
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11238.013671875
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11238.013671875
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11238.0146484375
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11238.013671875
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11238.0185546875
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0186, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11238.0126953125
tensor(11238.0137, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11238.013671875
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11238.013671875
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11238.0849609375
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0850, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11238.0126953125
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11238.02734375
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0273, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11238.0126953125
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11238.0146484375
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11238.0126953125
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11238.0146484375
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0146, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11238.275390625
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.2754, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11238.01171875
tensor(11238.0127, grad_fn=<NegBackward0>) tensor(11238.0117, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11238.0498046875
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0498, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11238.0126953125
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11238.08203125
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0820, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11238.01171875
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0117, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11238.0625
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0625, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11238.0126953125
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11238.017578125
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0176, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11238.013671875
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0137, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11238.0126953125
tensor(11238.0117, grad_fn=<NegBackward0>) tensor(11238.0127, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[9.3473e-01, 6.5270e-02],
        [1.0000e+00, 4.8063e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5034, 0.4966], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1700, 0.1088],
         [0.6799, 0.3193]],

        [[0.7163, 0.0772],
         [0.5311, 0.5725]],

        [[0.6247, 0.2175],
         [0.6242, 0.5451]],

        [[0.6185, 0.2068],
         [0.5007, 0.5544]],

        [[0.6470, 0.1151],
         [0.5398, 0.5850]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.044781949260518664
Average Adjusted Rand Index: 0.193177739216676
[0.944673345271342, 0.044781949260518664] [0.9446418293262768, 0.193177739216676] [10969.833984375, 11238.0126953125]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11188.077939037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19887.419921875
inf tensor(19887.4199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11316.81640625
tensor(19887.4199, grad_fn=<NegBackward0>) tensor(11316.8164, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11294.744140625
tensor(11316.8164, grad_fn=<NegBackward0>) tensor(11294.7441, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11294.412109375
tensor(11294.7441, grad_fn=<NegBackward0>) tensor(11294.4121, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11294.099609375
tensor(11294.4121, grad_fn=<NegBackward0>) tensor(11294.0996, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11293.107421875
tensor(11294.0996, grad_fn=<NegBackward0>) tensor(11293.1074, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11292.0966796875
tensor(11293.1074, grad_fn=<NegBackward0>) tensor(11292.0967, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11291.244140625
tensor(11292.0967, grad_fn=<NegBackward0>) tensor(11291.2441, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11291.0439453125
tensor(11291.2441, grad_fn=<NegBackward0>) tensor(11291.0439, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11291.009765625
tensor(11291.0439, grad_fn=<NegBackward0>) tensor(11291.0098, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11290.9951171875
tensor(11291.0098, grad_fn=<NegBackward0>) tensor(11290.9951, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11290.986328125
tensor(11290.9951, grad_fn=<NegBackward0>) tensor(11290.9863, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11290.9814453125
tensor(11290.9863, grad_fn=<NegBackward0>) tensor(11290.9814, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11290.978515625
tensor(11290.9814, grad_fn=<NegBackward0>) tensor(11290.9785, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11290.974609375
tensor(11290.9785, grad_fn=<NegBackward0>) tensor(11290.9746, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11290.9736328125
tensor(11290.9746, grad_fn=<NegBackward0>) tensor(11290.9736, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11290.970703125
tensor(11290.9736, grad_fn=<NegBackward0>) tensor(11290.9707, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11290.9697265625
tensor(11290.9707, grad_fn=<NegBackward0>) tensor(11290.9697, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11290.96875
tensor(11290.9697, grad_fn=<NegBackward0>) tensor(11290.9688, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11290.9677734375
tensor(11290.9688, grad_fn=<NegBackward0>) tensor(11290.9678, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11290.966796875
tensor(11290.9678, grad_fn=<NegBackward0>) tensor(11290.9668, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11290.96484375
tensor(11290.9668, grad_fn=<NegBackward0>) tensor(11290.9648, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11290.9658203125
tensor(11290.9648, grad_fn=<NegBackward0>) tensor(11290.9658, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11290.9638671875
tensor(11290.9648, grad_fn=<NegBackward0>) tensor(11290.9639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11290.9638671875
tensor(11290.9639, grad_fn=<NegBackward0>) tensor(11290.9639, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11290.96484375
tensor(11290.9639, grad_fn=<NegBackward0>) tensor(11290.9648, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11290.9638671875
tensor(11290.9639, grad_fn=<NegBackward0>) tensor(11290.9639, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11290.9638671875
tensor(11290.9639, grad_fn=<NegBackward0>) tensor(11290.9639, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11290.962890625
tensor(11290.9639, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11290.962890625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9629, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11290.9619140625
tensor(11290.9629, grad_fn=<NegBackward0>) tensor(11290.9619, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11290.9619140625
tensor(11290.9619, grad_fn=<NegBackward0>) tensor(11290.9619, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11290.9609375
tensor(11290.9619, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11290.9609375
tensor(11290.9609, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11290.9609375
tensor(11290.9609, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11290.9599609375
tensor(11290.9609, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11290.9609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11290.9609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11290.9609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11290.9609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11290.9599609375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11290.958984375
tensor(11290.9600, grad_fn=<NegBackward0>) tensor(11290.9590, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11290.958984375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9590, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11290.958984375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9590, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11290.9609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -11290.958984375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9590, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11290.9599609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9600, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11290.9609375
tensor(11290.9590, grad_fn=<NegBackward0>) tensor(11290.9609, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[9.7441e-01, 2.5591e-02],
        [1.0000e+00, 4.2353e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5027, 0.4973], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1724, 0.1079],
         [0.6079, 0.3176]],

        [[0.6184, 0.0813],
         [0.5615, 0.6705]],

        [[0.5684, 0.0622],
         [0.6209, 0.6443]],

        [[0.5037, 0.1802],
         [0.5042, 0.5969]],

        [[0.5779, 0.1921],
         [0.7296, 0.6114]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 1
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 69%|██████▉   | 69/100 [17:51:18<7:51:59, 913.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 70%|███████   | 70/100 [18:07:01<7:41:10, 922.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 71%|███████   | 71/100 [18:17:09<6:40:12, 828.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 72%|███████▏  | 72/100 [18:29:38<6:15:23, 804.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 73%|███████▎  | 73/100 [18:47:13<6:35:50, 879.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 74%|███████▍  | 74/100 [18:59:35<6:03:14, 838.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 75%|███████▌  | 75/100 [19:15:42<6:05:20, 876.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 76%|███████▌  | 76/100 [19:30:43<5:53:39, 884.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 77%|███████▋  | 77/100 [19:43:35<5:26:02, 850.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 78%|███████▊  | 78/100 [20:01:49<5:38:36, 923.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 79%|███████▉  | 79/100 [20:20:03<5:41:09, 974.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 80%|████████  | 80/100 [20:33:45<5:09:40, 929.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 81%|████████  | 81/100 [20:52:03<5:10:15, 979.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 82%|████████▏ | 82/100 [21:07:17<4:47:58, 959.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 83%|████████▎ | 83/100 [21:21:14<4:21:31, 923.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 84%|████████▍ | 84/100 [21:38:21<4:14:28, 954.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 85%|████████▌ | 85/100 [21:50:54<3:43:26, 893.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 86%|████████▌ | 86/100 [22:04:32<3:23:16, 871.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 87%|████████▋ | 87/100 [22:21:57<3:20:02, 923.28s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 88%|████████▊ | 88/100 [22:40:10<3:14:48, 974.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 89%|████████▉ | 89/100 [22:50:44<2:39:52, 872.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 90%|█████████ | 90/100 [23:05:33<2:26:10, 877.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 91%|█████████ | 91/100 [23:21:50<2:16:05, 907.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 92%|█████████▏| 92/100 [23:38:57<2:05:45, 943.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 93%|█████████▎| 93/100 [23:53:19<1:47:10, 918.71s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 94%|█████████▍| 94/100 [24:06:17<1:27:39, 876.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 95%|█████████▌| 95/100 [24:20:59<1:13:10, 878.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 96%|█████████▌| 96/100 [24:36:56<1:00:07, 901.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 97%|█████████▋| 97/100 [24:51:28<44:38, 892.86s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 98%|█████████▊| 98/100 [25:05:03<28:58, 869.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 99%|█████████▉| 99/100 [25:18:38<14:13, 853.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
100%|██████████| 100/100 [25:32:50<00:00, 852.67s/it]100%|██████████| 100/100 [25:32:50<00:00, 919.70s/it]
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05151290705834656
Average Adjusted Rand Index: 0.16748981784716616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23666.443359375
inf tensor(23666.4434, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11348.6669921875
tensor(23666.4434, grad_fn=<NegBackward0>) tensor(11348.6670, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11344.9482421875
tensor(11348.6670, grad_fn=<NegBackward0>) tensor(11344.9482, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11341.962890625
tensor(11344.9482, grad_fn=<NegBackward0>) tensor(11341.9629, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11299.8935546875
tensor(11341.9629, grad_fn=<NegBackward0>) tensor(11299.8936, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11294.80078125
tensor(11299.8936, grad_fn=<NegBackward0>) tensor(11294.8008, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11294.4248046875
tensor(11294.8008, grad_fn=<NegBackward0>) tensor(11294.4248, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11294.2890625
tensor(11294.4248, grad_fn=<NegBackward0>) tensor(11294.2891, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11294.1611328125
tensor(11294.2891, grad_fn=<NegBackward0>) tensor(11294.1611, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11293.9716796875
tensor(11294.1611, grad_fn=<NegBackward0>) tensor(11293.9717, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11293.76171875
tensor(11293.9717, grad_fn=<NegBackward0>) tensor(11293.7617, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11275.3095703125
tensor(11293.7617, grad_fn=<NegBackward0>) tensor(11275.3096, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11237.5703125
tensor(11275.3096, grad_fn=<NegBackward0>) tensor(11237.5703, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11157.1962890625
tensor(11237.5703, grad_fn=<NegBackward0>) tensor(11157.1963, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11148.052734375
tensor(11157.1963, grad_fn=<NegBackward0>) tensor(11148.0527, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11147.7685546875
tensor(11148.0527, grad_fn=<NegBackward0>) tensor(11147.7686, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11147.7001953125
tensor(11147.7686, grad_fn=<NegBackward0>) tensor(11147.7002, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11147.646484375
tensor(11147.7002, grad_fn=<NegBackward0>) tensor(11147.6465, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11147.6044921875
tensor(11147.6465, grad_fn=<NegBackward0>) tensor(11147.6045, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11146.109375
tensor(11147.6045, grad_fn=<NegBackward0>) tensor(11146.1094, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11146.07421875
tensor(11146.1094, grad_fn=<NegBackward0>) tensor(11146.0742, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11146.06640625
tensor(11146.0742, grad_fn=<NegBackward0>) tensor(11146.0664, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11146.056640625
tensor(11146.0664, grad_fn=<NegBackward0>) tensor(11146.0566, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11146.048828125
tensor(11146.0566, grad_fn=<NegBackward0>) tensor(11146.0488, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11145.578125
tensor(11146.0488, grad_fn=<NegBackward0>) tensor(11145.5781, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11145.5263671875
tensor(11145.5781, grad_fn=<NegBackward0>) tensor(11145.5264, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11145.5244140625
tensor(11145.5264, grad_fn=<NegBackward0>) tensor(11145.5244, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11145.51953125
tensor(11145.5244, grad_fn=<NegBackward0>) tensor(11145.5195, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11145.5087890625
tensor(11145.5195, grad_fn=<NegBackward0>) tensor(11145.5088, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11145.46484375
tensor(11145.5088, grad_fn=<NegBackward0>) tensor(11145.4648, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11145.4619140625
tensor(11145.4648, grad_fn=<NegBackward0>) tensor(11145.4619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11145.4619140625
tensor(11145.4619, grad_fn=<NegBackward0>) tensor(11145.4619, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11145.4541015625
tensor(11145.4619, grad_fn=<NegBackward0>) tensor(11145.4541, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11145.443359375
tensor(11145.4541, grad_fn=<NegBackward0>) tensor(11145.4434, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11145.44140625
tensor(11145.4434, grad_fn=<NegBackward0>) tensor(11145.4414, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11145.4404296875
tensor(11145.4414, grad_fn=<NegBackward0>) tensor(11145.4404, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11145.44140625
tensor(11145.4404, grad_fn=<NegBackward0>) tensor(11145.4414, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11145.4541015625
tensor(11145.4404, grad_fn=<NegBackward0>) tensor(11145.4541, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11145.4375
tensor(11145.4404, grad_fn=<NegBackward0>) tensor(11145.4375, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11145.4375
tensor(11145.4375, grad_fn=<NegBackward0>) tensor(11145.4375, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11145.435546875
tensor(11145.4375, grad_fn=<NegBackward0>) tensor(11145.4355, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11145.4365234375
tensor(11145.4355, grad_fn=<NegBackward0>) tensor(11145.4365, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11145.4453125
tensor(11145.4355, grad_fn=<NegBackward0>) tensor(11145.4453, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11145.4345703125
tensor(11145.4355, grad_fn=<NegBackward0>) tensor(11145.4346, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11145.43359375
tensor(11145.4346, grad_fn=<NegBackward0>) tensor(11145.4336, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11145.43359375
tensor(11145.4336, grad_fn=<NegBackward0>) tensor(11145.4336, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11145.431640625
tensor(11145.4336, grad_fn=<NegBackward0>) tensor(11145.4316, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11145.404296875
tensor(11145.4316, grad_fn=<NegBackward0>) tensor(11145.4043, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11145.3671875
tensor(11145.4043, grad_fn=<NegBackward0>) tensor(11145.3672, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11145.3662109375
tensor(11145.3672, grad_fn=<NegBackward0>) tensor(11145.3662, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11145.3642578125
tensor(11145.3662, grad_fn=<NegBackward0>) tensor(11145.3643, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11145.359375
tensor(11145.3643, grad_fn=<NegBackward0>) tensor(11145.3594, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11145.35546875
tensor(11145.3594, grad_fn=<NegBackward0>) tensor(11145.3555, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11145.3525390625
tensor(11145.3555, grad_fn=<NegBackward0>) tensor(11145.3525, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11145.353515625
tensor(11145.3525, grad_fn=<NegBackward0>) tensor(11145.3535, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11145.349609375
tensor(11145.3525, grad_fn=<NegBackward0>) tensor(11145.3496, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11145.3486328125
tensor(11145.3496, grad_fn=<NegBackward0>) tensor(11145.3486, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11145.3486328125
tensor(11145.3486, grad_fn=<NegBackward0>) tensor(11145.3486, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11145.3544921875
tensor(11145.3486, grad_fn=<NegBackward0>) tensor(11145.3545, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11145.3505859375
tensor(11145.3486, grad_fn=<NegBackward0>) tensor(11145.3506, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11145.3544921875
tensor(11145.3486, grad_fn=<NegBackward0>) tensor(11145.3545, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11145.34765625
tensor(11145.3486, grad_fn=<NegBackward0>) tensor(11145.3477, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11145.3251953125
tensor(11145.3477, grad_fn=<NegBackward0>) tensor(11145.3252, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11145.322265625
tensor(11145.3252, grad_fn=<NegBackward0>) tensor(11145.3223, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11145.29296875
tensor(11145.3223, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11145.29296875
tensor(11145.2930, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11145.29296875
tensor(11145.2930, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11145.291015625
tensor(11145.2930, grad_fn=<NegBackward0>) tensor(11145.2910, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11145.29296875
tensor(11145.2910, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11145.2900390625
tensor(11145.2910, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11145.291015625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2910, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11145.2900390625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11145.291015625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2910, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11145.2900390625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11145.2939453125
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2939, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11145.291015625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2910, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11145.29296875
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11145.2919921875
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2920, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11145.2900390625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11145.2900390625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11145.2900390625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11145.2890625
tensor(11145.2900, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11145.2900390625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11145.2900390625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2900, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11145.2939453125
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2939, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11145.3017578125
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.3018, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11145.2890625
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11145.2880859375
tensor(11145.2891, grad_fn=<NegBackward0>) tensor(11145.2881, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11145.2890625
tensor(11145.2881, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11145.29296875
tensor(11145.2881, grad_fn=<NegBackward0>) tensor(11145.2930, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11145.2890625
tensor(11145.2881, grad_fn=<NegBackward0>) tensor(11145.2891, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11145.4404296875
tensor(11145.2881, grad_fn=<NegBackward0>) tensor(11145.4404, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11145.2822265625
tensor(11145.2881, grad_fn=<NegBackward0>) tensor(11145.2822, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11145.2880859375
tensor(11145.2822, grad_fn=<NegBackward0>) tensor(11145.2881, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11145.2822265625
tensor(11145.2822, grad_fn=<NegBackward0>) tensor(11145.2822, grad_fn=<NegBackward0>)
pi: tensor([[0.7822, 0.2178],
        [0.2857, 0.7143]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4693, 0.5307], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2021, 0.1043],
         [0.6402, 0.2859]],

        [[0.7218, 0.0993],
         [0.6631, 0.7175]],

        [[0.6567, 0.0951],
         [0.7155, 0.6623]],

        [[0.5221, 0.1055],
         [0.7229, 0.6684]],

        [[0.5589, 0.1154],
         [0.6381, 0.7253]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6691181331636993
Global Adjusted Rand Index: 0.853482848285536
Average Adjusted Rand Index: 0.8554362785452823
[0.05151290705834656, 0.853482848285536] [0.16748981784716616, 0.8554362785452823] [11290.9609375, 11145.26171875]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11381.546791656672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24792.701171875
inf tensor(24792.7012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11734.759765625
tensor(24792.7012, grad_fn=<NegBackward0>) tensor(11734.7598, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11703.6728515625
tensor(11734.7598, grad_fn=<NegBackward0>) tensor(11703.6729, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11534.2666015625
tensor(11703.6729, grad_fn=<NegBackward0>) tensor(11534.2666, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11474.9931640625
tensor(11534.2666, grad_fn=<NegBackward0>) tensor(11474.9932, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11466.0771484375
tensor(11474.9932, grad_fn=<NegBackward0>) tensor(11466.0771, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11460.4169921875
tensor(11466.0771, grad_fn=<NegBackward0>) tensor(11460.4170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11454.8330078125
tensor(11460.4170, grad_fn=<NegBackward0>) tensor(11454.8330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11449.33203125
tensor(11454.8330, grad_fn=<NegBackward0>) tensor(11449.3320, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11447.712890625
tensor(11449.3320, grad_fn=<NegBackward0>) tensor(11447.7129, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11445.373046875
tensor(11447.7129, grad_fn=<NegBackward0>) tensor(11445.3730, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11445.203125
tensor(11445.3730, grad_fn=<NegBackward0>) tensor(11445.2031, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11445.15625
tensor(11445.2031, grad_fn=<NegBackward0>) tensor(11445.1562, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11445.0
tensor(11445.1562, grad_fn=<NegBackward0>) tensor(11445., grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11441.7392578125
tensor(11445., grad_fn=<NegBackward0>) tensor(11441.7393, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11434.8447265625
tensor(11441.7393, grad_fn=<NegBackward0>) tensor(11434.8447, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11434.822265625
tensor(11434.8447, grad_fn=<NegBackward0>) tensor(11434.8223, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11434.7978515625
tensor(11434.8223, grad_fn=<NegBackward0>) tensor(11434.7979, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11432.5380859375
tensor(11434.7979, grad_fn=<NegBackward0>) tensor(11432.5381, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11432.52734375
tensor(11432.5381, grad_fn=<NegBackward0>) tensor(11432.5273, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11432.51953125
tensor(11432.5273, grad_fn=<NegBackward0>) tensor(11432.5195, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11432.509765625
tensor(11432.5195, grad_fn=<NegBackward0>) tensor(11432.5098, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11432.427734375
tensor(11432.5098, grad_fn=<NegBackward0>) tensor(11432.4277, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11432.41796875
tensor(11432.4277, grad_fn=<NegBackward0>) tensor(11432.4180, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11432.416015625
tensor(11432.4180, grad_fn=<NegBackward0>) tensor(11432.4160, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11432.4140625
tensor(11432.4160, grad_fn=<NegBackward0>) tensor(11432.4141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11432.4111328125
tensor(11432.4141, grad_fn=<NegBackward0>) tensor(11432.4111, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11432.4091796875
tensor(11432.4111, grad_fn=<NegBackward0>) tensor(11432.4092, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11432.4052734375
tensor(11432.4092, grad_fn=<NegBackward0>) tensor(11432.4053, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11432.4052734375
tensor(11432.4053, grad_fn=<NegBackward0>) tensor(11432.4053, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11432.40234375
tensor(11432.4053, grad_fn=<NegBackward0>) tensor(11432.4023, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11432.400390625
tensor(11432.4023, grad_fn=<NegBackward0>) tensor(11432.4004, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11432.3798828125
tensor(11432.4004, grad_fn=<NegBackward0>) tensor(11432.3799, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11432.359375
tensor(11432.3799, grad_fn=<NegBackward0>) tensor(11432.3594, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11432.357421875
tensor(11432.3594, grad_fn=<NegBackward0>) tensor(11432.3574, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11432.35546875
tensor(11432.3574, grad_fn=<NegBackward0>) tensor(11432.3555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11432.3544921875
tensor(11432.3555, grad_fn=<NegBackward0>) tensor(11432.3545, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11432.3505859375
tensor(11432.3545, grad_fn=<NegBackward0>) tensor(11432.3506, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11432.3505859375
tensor(11432.3506, grad_fn=<NegBackward0>) tensor(11432.3506, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11432.34765625
tensor(11432.3506, grad_fn=<NegBackward0>) tensor(11432.3477, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11432.34765625
tensor(11432.3477, grad_fn=<NegBackward0>) tensor(11432.3477, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11432.3466796875
tensor(11432.3477, grad_fn=<NegBackward0>) tensor(11432.3467, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11432.3466796875
tensor(11432.3467, grad_fn=<NegBackward0>) tensor(11432.3467, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11432.34765625
tensor(11432.3467, grad_fn=<NegBackward0>) tensor(11432.3477, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11432.345703125
tensor(11432.3467, grad_fn=<NegBackward0>) tensor(11432.3457, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11432.3466796875
tensor(11432.3457, grad_fn=<NegBackward0>) tensor(11432.3467, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11432.3466796875
tensor(11432.3457, grad_fn=<NegBackward0>) tensor(11432.3467, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11432.3427734375
tensor(11432.3457, grad_fn=<NegBackward0>) tensor(11432.3428, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11432.34375
tensor(11432.3428, grad_fn=<NegBackward0>) tensor(11432.3438, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11432.341796875
tensor(11432.3428, grad_fn=<NegBackward0>) tensor(11432.3418, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11432.3359375
tensor(11432.3418, grad_fn=<NegBackward0>) tensor(11432.3359, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11430.689453125
tensor(11432.3359, grad_fn=<NegBackward0>) tensor(11430.6895, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11430.6884765625
tensor(11430.6895, grad_fn=<NegBackward0>) tensor(11430.6885, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11430.6826171875
tensor(11430.6885, grad_fn=<NegBackward0>) tensor(11430.6826, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11430.529296875
tensor(11430.6826, grad_fn=<NegBackward0>) tensor(11430.5293, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11430.5283203125
tensor(11430.5293, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11430.5283203125
tensor(11430.5283, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11430.5283203125
tensor(11430.5283, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11430.5283203125
tensor(11430.5283, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11430.52734375
tensor(11430.5283, grad_fn=<NegBackward0>) tensor(11430.5273, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11430.52734375
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5273, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11430.5283203125
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11430.52734375
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5273, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11430.5283203125
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11430.525390625
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11430.5244140625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11430.5244140625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11430.5263671875
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5264, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11430.5244140625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11430.5244140625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11430.5234375
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11430.5244140625
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11430.5244140625
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11430.5458984375
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5459, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11430.5244140625
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11430.525390625
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.7883, 0.2117],
        [0.4011, 0.5989]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5927e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.1560],
         [0.5126, 0.1849]],

        [[0.6441, 0.1114],
         [0.6367, 0.5374]],

        [[0.5225, 0.0920],
         [0.7222, 0.6891]],

        [[0.5345, 0.0878],
         [0.6684, 0.5254]],

        [[0.6357, 0.1064],
         [0.5460, 0.6317]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.5889706051595524
Average Adjusted Rand Index: 0.7294244642952966
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21308.341796875
inf tensor(21308.3418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11739.4580078125
tensor(21308.3418, grad_fn=<NegBackward0>) tensor(11739.4580, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11719.06640625
tensor(11739.4580, grad_fn=<NegBackward0>) tensor(11719.0664, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11544.5068359375
tensor(11719.0664, grad_fn=<NegBackward0>) tensor(11544.5068, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11471.4326171875
tensor(11544.5068, grad_fn=<NegBackward0>) tensor(11471.4326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11459.25
tensor(11471.4326, grad_fn=<NegBackward0>) tensor(11459.2500, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11448.6943359375
tensor(11459.2500, grad_fn=<NegBackward0>) tensor(11448.6943, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11434.8837890625
tensor(11448.6943, grad_fn=<NegBackward0>) tensor(11434.8838, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11432.6201171875
tensor(11434.8838, grad_fn=<NegBackward0>) tensor(11432.6201, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11430.9794921875
tensor(11432.6201, grad_fn=<NegBackward0>) tensor(11430.9795, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11430.9130859375
tensor(11430.9795, grad_fn=<NegBackward0>) tensor(11430.9131, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11430.8681640625
tensor(11430.9131, grad_fn=<NegBackward0>) tensor(11430.8682, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11430.8359375
tensor(11430.8682, grad_fn=<NegBackward0>) tensor(11430.8359, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11430.7001953125
tensor(11430.8359, grad_fn=<NegBackward0>) tensor(11430.7002, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11430.6787109375
tensor(11430.7002, grad_fn=<NegBackward0>) tensor(11430.6787, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11430.6650390625
tensor(11430.6787, grad_fn=<NegBackward0>) tensor(11430.6650, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11430.65234375
tensor(11430.6650, grad_fn=<NegBackward0>) tensor(11430.6523, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11430.6396484375
tensor(11430.6523, grad_fn=<NegBackward0>) tensor(11430.6396, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11430.6298828125
tensor(11430.6396, grad_fn=<NegBackward0>) tensor(11430.6299, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11430.6201171875
tensor(11430.6299, grad_fn=<NegBackward0>) tensor(11430.6201, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11430.6142578125
tensor(11430.6201, grad_fn=<NegBackward0>) tensor(11430.6143, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11430.609375
tensor(11430.6143, grad_fn=<NegBackward0>) tensor(11430.6094, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11430.6064453125
tensor(11430.6094, grad_fn=<NegBackward0>) tensor(11430.6064, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11430.6025390625
tensor(11430.6064, grad_fn=<NegBackward0>) tensor(11430.6025, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11430.5986328125
tensor(11430.6025, grad_fn=<NegBackward0>) tensor(11430.5986, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11430.595703125
tensor(11430.5986, grad_fn=<NegBackward0>) tensor(11430.5957, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11430.5947265625
tensor(11430.5957, grad_fn=<NegBackward0>) tensor(11430.5947, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11430.591796875
tensor(11430.5947, grad_fn=<NegBackward0>) tensor(11430.5918, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11430.58984375
tensor(11430.5918, grad_fn=<NegBackward0>) tensor(11430.5898, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11430.587890625
tensor(11430.5898, grad_fn=<NegBackward0>) tensor(11430.5879, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11430.5859375
tensor(11430.5879, grad_fn=<NegBackward0>) tensor(11430.5859, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11430.5859375
tensor(11430.5859, grad_fn=<NegBackward0>) tensor(11430.5859, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11430.583984375
tensor(11430.5859, grad_fn=<NegBackward0>) tensor(11430.5840, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11430.58203125
tensor(11430.5840, grad_fn=<NegBackward0>) tensor(11430.5820, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11430.5810546875
tensor(11430.5820, grad_fn=<NegBackward0>) tensor(11430.5811, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11430.5810546875
tensor(11430.5811, grad_fn=<NegBackward0>) tensor(11430.5811, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11430.5791015625
tensor(11430.5811, grad_fn=<NegBackward0>) tensor(11430.5791, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11430.578125
tensor(11430.5791, grad_fn=<NegBackward0>) tensor(11430.5781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11430.5771484375
tensor(11430.5781, grad_fn=<NegBackward0>) tensor(11430.5771, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11430.576171875
tensor(11430.5771, grad_fn=<NegBackward0>) tensor(11430.5762, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11430.576171875
tensor(11430.5762, grad_fn=<NegBackward0>) tensor(11430.5762, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11430.5751953125
tensor(11430.5762, grad_fn=<NegBackward0>) tensor(11430.5752, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11430.576171875
tensor(11430.5752, grad_fn=<NegBackward0>) tensor(11430.5762, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11430.5341796875
tensor(11430.5752, grad_fn=<NegBackward0>) tensor(11430.5342, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11430.5322265625
tensor(11430.5342, grad_fn=<NegBackward0>) tensor(11430.5322, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11430.5302734375
tensor(11430.5322, grad_fn=<NegBackward0>) tensor(11430.5303, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11430.5302734375
tensor(11430.5303, grad_fn=<NegBackward0>) tensor(11430.5303, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11430.5302734375
tensor(11430.5303, grad_fn=<NegBackward0>) tensor(11430.5303, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11430.5302734375
tensor(11430.5303, grad_fn=<NegBackward0>) tensor(11430.5303, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11430.529296875
tensor(11430.5303, grad_fn=<NegBackward0>) tensor(11430.5293, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11430.529296875
tensor(11430.5293, grad_fn=<NegBackward0>) tensor(11430.5293, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11430.533203125
tensor(11430.5293, grad_fn=<NegBackward0>) tensor(11430.5332, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11430.5283203125
tensor(11430.5293, grad_fn=<NegBackward0>) tensor(11430.5283, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11430.52734375
tensor(11430.5283, grad_fn=<NegBackward0>) tensor(11430.5273, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11430.525390625
tensor(11430.5273, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11430.5263671875
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5264, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11430.5263671875
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5264, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11430.525390625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11430.5400390625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5400, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11430.525390625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11430.525390625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11430.5244140625
tensor(11430.5254, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11430.5244140625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11430.5244140625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11430.525390625
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5254, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11430.5234375
tensor(11430.5244, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11430.5234375
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11430.5244140625
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11430.5234375
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11430.5224609375
tensor(11430.5234, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11430.5322265625
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5322, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11430.5244140625
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11430.5224609375
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11430.5224609375
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11430.5234375
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11430.521484375
tensor(11430.5225, grad_fn=<NegBackward0>) tensor(11430.5215, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11430.5244140625
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11430.5244140625
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5244, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11430.5224609375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11430.53515625
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5352, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11430.521484375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5215, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11430.7373046875
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.7373, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11430.5224609375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11430.521484375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5215, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11430.53125
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5312, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11430.5224609375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11430.521484375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5215, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11430.5234375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11430.5224609375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11430.521484375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5215, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11430.5234375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5234, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11430.5224609375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5225, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11430.5390625
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5391, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11430.52734375
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5273, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11430.546875
tensor(11430.5215, grad_fn=<NegBackward0>) tensor(11430.5469, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.6004, 0.3996],
        [0.2147, 0.7853]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 1.9988e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1846, 0.1412],
         [0.6993, 0.3078]],

        [[0.6428, 0.1115],
         [0.5500, 0.5509]],

        [[0.6838, 0.0921],
         [0.6416, 0.6367]],

        [[0.6204, 0.0878],
         [0.6440, 0.6815]],

        [[0.5226, 0.1066],
         [0.5452, 0.5679]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.5889706051595524
Average Adjusted Rand Index: 0.7294244642952966
[0.5889706051595524, 0.5889706051595524] [0.7294244642952966, 0.7294244642952966] [11430.525390625, 11430.546875]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11267.316240811346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22103.583984375
inf tensor(22103.5840, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11544.380859375
tensor(22103.5840, grad_fn=<NegBackward0>) tensor(11544.3809, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11542.283203125
tensor(11544.3809, grad_fn=<NegBackward0>) tensor(11542.2832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11539.1708984375
tensor(11542.2832, grad_fn=<NegBackward0>) tensor(11539.1709, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11507.61328125
tensor(11539.1709, grad_fn=<NegBackward0>) tensor(11507.6133, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11386.123046875
tensor(11507.6133, grad_fn=<NegBackward0>) tensor(11386.1230, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11267.677734375
tensor(11386.1230, grad_fn=<NegBackward0>) tensor(11267.6777, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11257.2255859375
tensor(11267.6777, grad_fn=<NegBackward0>) tensor(11257.2256, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11256.1220703125
tensor(11257.2256, grad_fn=<NegBackward0>) tensor(11256.1221, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11251.580078125
tensor(11256.1221, grad_fn=<NegBackward0>) tensor(11251.5801, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11251.517578125
tensor(11251.5801, grad_fn=<NegBackward0>) tensor(11251.5176, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11251.4814453125
tensor(11251.5176, grad_fn=<NegBackward0>) tensor(11251.4814, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11251.4580078125
tensor(11251.4814, grad_fn=<NegBackward0>) tensor(11251.4580, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11251.4404296875
tensor(11251.4580, grad_fn=<NegBackward0>) tensor(11251.4404, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11251.4248046875
tensor(11251.4404, grad_fn=<NegBackward0>) tensor(11251.4248, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11251.4111328125
tensor(11251.4248, grad_fn=<NegBackward0>) tensor(11251.4111, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11250.9853515625
tensor(11251.4111, grad_fn=<NegBackward0>) tensor(11250.9854, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11250.9501953125
tensor(11250.9854, grad_fn=<NegBackward0>) tensor(11250.9502, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11250.9443359375
tensor(11250.9502, grad_fn=<NegBackward0>) tensor(11250.9443, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11250.9384765625
tensor(11250.9443, grad_fn=<NegBackward0>) tensor(11250.9385, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11250.5771484375
tensor(11250.9385, grad_fn=<NegBackward0>) tensor(11250.5771, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11250.5751953125
tensor(11250.5771, grad_fn=<NegBackward0>) tensor(11250.5752, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11250.572265625
tensor(11250.5752, grad_fn=<NegBackward0>) tensor(11250.5723, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11250.5703125
tensor(11250.5723, grad_fn=<NegBackward0>) tensor(11250.5703, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11250.56640625
tensor(11250.5703, grad_fn=<NegBackward0>) tensor(11250.5664, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11250.560546875
tensor(11250.5664, grad_fn=<NegBackward0>) tensor(11250.5605, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11250.5576171875
tensor(11250.5605, grad_fn=<NegBackward0>) tensor(11250.5576, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11250.5556640625
tensor(11250.5576, grad_fn=<NegBackward0>) tensor(11250.5557, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11250.5546875
tensor(11250.5557, grad_fn=<NegBackward0>) tensor(11250.5547, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11250.552734375
tensor(11250.5547, grad_fn=<NegBackward0>) tensor(11250.5527, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11250.548828125
tensor(11250.5527, grad_fn=<NegBackward0>) tensor(11250.5488, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11250.541015625
tensor(11250.5488, grad_fn=<NegBackward0>) tensor(11250.5410, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11250.541015625
tensor(11250.5410, grad_fn=<NegBackward0>) tensor(11250.5410, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11250.5380859375
tensor(11250.5410, grad_fn=<NegBackward0>) tensor(11250.5381, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11250.537109375
tensor(11250.5381, grad_fn=<NegBackward0>) tensor(11250.5371, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11250.53515625
tensor(11250.5371, grad_fn=<NegBackward0>) tensor(11250.5352, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11250.3154296875
tensor(11250.5352, grad_fn=<NegBackward0>) tensor(11250.3154, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11250.3134765625
tensor(11250.3154, grad_fn=<NegBackward0>) tensor(11250.3135, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11250.3056640625
tensor(11250.3135, grad_fn=<NegBackward0>) tensor(11250.3057, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11250.015625
tensor(11250.3057, grad_fn=<NegBackward0>) tensor(11250.0156, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11247.6982421875
tensor(11250.0156, grad_fn=<NegBackward0>) tensor(11247.6982, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11247.697265625
tensor(11247.6982, grad_fn=<NegBackward0>) tensor(11247.6973, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11247.697265625
tensor(11247.6973, grad_fn=<NegBackward0>) tensor(11247.6973, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11247.6953125
tensor(11247.6973, grad_fn=<NegBackward0>) tensor(11247.6953, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11247.681640625
tensor(11247.6953, grad_fn=<NegBackward0>) tensor(11247.6816, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11247.677734375
tensor(11247.6816, grad_fn=<NegBackward0>) tensor(11247.6777, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11247.6796875
tensor(11247.6777, grad_fn=<NegBackward0>) tensor(11247.6797, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11247.6767578125
tensor(11247.6777, grad_fn=<NegBackward0>) tensor(11247.6768, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11247.669921875
tensor(11247.6768, grad_fn=<NegBackward0>) tensor(11247.6699, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11247.654296875
tensor(11247.6699, grad_fn=<NegBackward0>) tensor(11247.6543, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11247.654296875
tensor(11247.6543, grad_fn=<NegBackward0>) tensor(11247.6543, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11247.65234375
tensor(11247.6543, grad_fn=<NegBackward0>) tensor(11247.6523, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11247.6474609375
tensor(11247.6523, grad_fn=<NegBackward0>) tensor(11247.6475, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11247.6396484375
tensor(11247.6475, grad_fn=<NegBackward0>) tensor(11247.6396, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11247.638671875
tensor(11247.6396, grad_fn=<NegBackward0>) tensor(11247.6387, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11247.6376953125
tensor(11247.6387, grad_fn=<NegBackward0>) tensor(11247.6377, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11247.640625
tensor(11247.6377, grad_fn=<NegBackward0>) tensor(11247.6406, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11247.638671875
tensor(11247.6377, grad_fn=<NegBackward0>) tensor(11247.6387, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11247.6396484375
tensor(11247.6377, grad_fn=<NegBackward0>) tensor(11247.6396, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11247.642578125
tensor(11247.6377, grad_fn=<NegBackward0>) tensor(11247.6426, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11247.640625
tensor(11247.6377, grad_fn=<NegBackward0>) tensor(11247.6406, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7312, 0.2688],
        [0.2215, 0.7785]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5254, 0.4746], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3070, 0.0969],
         [0.6315, 0.2062]],

        [[0.7078, 0.1143],
         [0.5306, 0.5496]],

        [[0.5679, 0.1000],
         [0.5407, 0.5736]],

        [[0.6199, 0.0975],
         [0.5204, 0.5599]],

        [[0.6788, 0.1027],
         [0.6314, 0.6763]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206525010987
Average Adjusted Rand Index: 0.9603201972601454
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21896.37109375
inf tensor(21896.3711, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11539.486328125
tensor(21896.3711, grad_fn=<NegBackward0>) tensor(11539.4863, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11454.5048828125
tensor(11539.4863, grad_fn=<NegBackward0>) tensor(11454.5049, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11451.1669921875
tensor(11454.5049, grad_fn=<NegBackward0>) tensor(11451.1670, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11436.91796875
tensor(11451.1670, grad_fn=<NegBackward0>) tensor(11436.9180, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11367.431640625
tensor(11436.9180, grad_fn=<NegBackward0>) tensor(11367.4316, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11287.318359375
tensor(11367.4316, grad_fn=<NegBackward0>) tensor(11287.3184, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11272.212890625
tensor(11287.3184, grad_fn=<NegBackward0>) tensor(11272.2129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11256.572265625
tensor(11272.2129, grad_fn=<NegBackward0>) tensor(11256.5723, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11248.373046875
tensor(11256.5723, grad_fn=<NegBackward0>) tensor(11248.3730, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11248.263671875
tensor(11248.3730, grad_fn=<NegBackward0>) tensor(11248.2637, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11248.1904296875
tensor(11248.2637, grad_fn=<NegBackward0>) tensor(11248.1904, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11248.06640625
tensor(11248.1904, grad_fn=<NegBackward0>) tensor(11248.0664, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11248.0
tensor(11248.0664, grad_fn=<NegBackward0>) tensor(11248., grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11247.966796875
tensor(11248., grad_fn=<NegBackward0>) tensor(11247.9668, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11247.919921875
tensor(11247.9668, grad_fn=<NegBackward0>) tensor(11247.9199, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11247.9091796875
tensor(11247.9199, grad_fn=<NegBackward0>) tensor(11247.9092, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11247.8994140625
tensor(11247.9092, grad_fn=<NegBackward0>) tensor(11247.8994, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11247.8818359375
tensor(11247.8994, grad_fn=<NegBackward0>) tensor(11247.8818, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11247.8740234375
tensor(11247.8818, grad_fn=<NegBackward0>) tensor(11247.8740, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11247.85546875
tensor(11247.8740, grad_fn=<NegBackward0>) tensor(11247.8555, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11247.8466796875
tensor(11247.8555, grad_fn=<NegBackward0>) tensor(11247.8467, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11247.841796875
tensor(11247.8467, grad_fn=<NegBackward0>) tensor(11247.8418, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11247.83203125
tensor(11247.8418, grad_fn=<NegBackward0>) tensor(11247.8320, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11247.828125
tensor(11247.8320, grad_fn=<NegBackward0>) tensor(11247.8281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11247.822265625
tensor(11247.8281, grad_fn=<NegBackward0>) tensor(11247.8223, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11247.8193359375
tensor(11247.8223, grad_fn=<NegBackward0>) tensor(11247.8193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11247.818359375
tensor(11247.8193, grad_fn=<NegBackward0>) tensor(11247.8184, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11247.8173828125
tensor(11247.8184, grad_fn=<NegBackward0>) tensor(11247.8174, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11247.818359375
tensor(11247.8174, grad_fn=<NegBackward0>) tensor(11247.8184, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11247.810546875
tensor(11247.8174, grad_fn=<NegBackward0>) tensor(11247.8105, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11247.693359375
tensor(11247.8105, grad_fn=<NegBackward0>) tensor(11247.6934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11247.6923828125
tensor(11247.6934, grad_fn=<NegBackward0>) tensor(11247.6924, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11247.69140625
tensor(11247.6924, grad_fn=<NegBackward0>) tensor(11247.6914, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11247.69140625
tensor(11247.6914, grad_fn=<NegBackward0>) tensor(11247.6914, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11247.689453125
tensor(11247.6914, grad_fn=<NegBackward0>) tensor(11247.6895, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11247.6904296875
tensor(11247.6895, grad_fn=<NegBackward0>) tensor(11247.6904, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11247.6884765625
tensor(11247.6895, grad_fn=<NegBackward0>) tensor(11247.6885, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11247.689453125
tensor(11247.6885, grad_fn=<NegBackward0>) tensor(11247.6895, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11247.6884765625
tensor(11247.6885, grad_fn=<NegBackward0>) tensor(11247.6885, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11247.6875
tensor(11247.6885, grad_fn=<NegBackward0>) tensor(11247.6875, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11247.689453125
tensor(11247.6875, grad_fn=<NegBackward0>) tensor(11247.6895, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11247.697265625
tensor(11247.6875, grad_fn=<NegBackward0>) tensor(11247.6973, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11247.6865234375
tensor(11247.6875, grad_fn=<NegBackward0>) tensor(11247.6865, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11247.6865234375
tensor(11247.6865, grad_fn=<NegBackward0>) tensor(11247.6865, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11247.6865234375
tensor(11247.6865, grad_fn=<NegBackward0>) tensor(11247.6865, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11247.6845703125
tensor(11247.6865, grad_fn=<NegBackward0>) tensor(11247.6846, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11247.6884765625
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6885, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11247.6845703125
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6846, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11247.6943359375
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6943, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11247.693359375
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6934, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11247.685546875
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6855, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11247.685546875
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6855, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11247.685546875
tensor(11247.6846, grad_fn=<NegBackward0>) tensor(11247.6855, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.7784, 0.2216],
        [0.2704, 0.7296]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4779, 0.5221], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2063, 0.0968],
         [0.5842, 0.3069]],

        [[0.7000, 0.1144],
         [0.5589, 0.6554]],

        [[0.7291, 0.0999],
         [0.5129, 0.5385]],

        [[0.6495, 0.0975],
         [0.5052, 0.6416]],

        [[0.5366, 0.1027],
         [0.5881, 0.5676]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206525010987
Average Adjusted Rand Index: 0.9603201972601454
[0.9603206525010987, 0.9603206525010987] [0.9603201972601454, 0.9603201972601454] [11247.640625, 11247.685546875]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11193.544612769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20715.34765625
inf tensor(20715.3477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11409.5693359375
tensor(20715.3477, grad_fn=<NegBackward0>) tensor(11409.5693, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11408.53515625
tensor(11409.5693, grad_fn=<NegBackward0>) tensor(11408.5352, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11407.7841796875
tensor(11408.5352, grad_fn=<NegBackward0>) tensor(11407.7842, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11407.4423828125
tensor(11407.7842, grad_fn=<NegBackward0>) tensor(11407.4424, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11407.109375
tensor(11407.4424, grad_fn=<NegBackward0>) tensor(11407.1094, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11406.3173828125
tensor(11407.1094, grad_fn=<NegBackward0>) tensor(11406.3174, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11404.93359375
tensor(11406.3174, grad_fn=<NegBackward0>) tensor(11404.9336, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11379.5576171875
tensor(11404.9336, grad_fn=<NegBackward0>) tensor(11379.5576, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11355.693359375
tensor(11379.5576, grad_fn=<NegBackward0>) tensor(11355.6934, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11352.736328125
tensor(11355.6934, grad_fn=<NegBackward0>) tensor(11352.7363, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11192.9326171875
tensor(11352.7363, grad_fn=<NegBackward0>) tensor(11192.9326, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11176.01953125
tensor(11192.9326, grad_fn=<NegBackward0>) tensor(11176.0195, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11168.37890625
tensor(11176.0195, grad_fn=<NegBackward0>) tensor(11168.3789, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11166.478515625
tensor(11168.3789, grad_fn=<NegBackward0>) tensor(11166.4785, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11166.427734375
tensor(11166.4785, grad_fn=<NegBackward0>) tensor(11166.4277, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11166.392578125
tensor(11166.4277, grad_fn=<NegBackward0>) tensor(11166.3926, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11166.3681640625
tensor(11166.3926, grad_fn=<NegBackward0>) tensor(11166.3682, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11166.3427734375
tensor(11166.3682, grad_fn=<NegBackward0>) tensor(11166.3428, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11166.302734375
tensor(11166.3428, grad_fn=<NegBackward0>) tensor(11166.3027, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11166.201171875
tensor(11166.3027, grad_fn=<NegBackward0>) tensor(11166.2012, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11166.185546875
tensor(11166.2012, grad_fn=<NegBackward0>) tensor(11166.1855, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11166.1787109375
tensor(11166.1855, grad_fn=<NegBackward0>) tensor(11166.1787, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11166.1728515625
tensor(11166.1787, grad_fn=<NegBackward0>) tensor(11166.1729, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11166.16796875
tensor(11166.1729, grad_fn=<NegBackward0>) tensor(11166.1680, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11166.1572265625
tensor(11166.1680, grad_fn=<NegBackward0>) tensor(11166.1572, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11166.1533203125
tensor(11166.1572, grad_fn=<NegBackward0>) tensor(11166.1533, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11166.150390625
tensor(11166.1533, grad_fn=<NegBackward0>) tensor(11166.1504, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11166.1474609375
tensor(11166.1504, grad_fn=<NegBackward0>) tensor(11166.1475, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11166.1416015625
tensor(11166.1475, grad_fn=<NegBackward0>) tensor(11166.1416, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11166.1005859375
tensor(11166.1416, grad_fn=<NegBackward0>) tensor(11166.1006, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11166.099609375
tensor(11166.1006, grad_fn=<NegBackward0>) tensor(11166.0996, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11166.09765625
tensor(11166.0996, grad_fn=<NegBackward0>) tensor(11166.0977, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11166.0966796875
tensor(11166.0977, grad_fn=<NegBackward0>) tensor(11166.0967, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11166.095703125
tensor(11166.0967, grad_fn=<NegBackward0>) tensor(11166.0957, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11166.091796875
tensor(11166.0957, grad_fn=<NegBackward0>) tensor(11166.0918, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11166.091796875
tensor(11166.0918, grad_fn=<NegBackward0>) tensor(11166.0918, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11166.091796875
tensor(11166.0918, grad_fn=<NegBackward0>) tensor(11166.0918, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11166.08984375
tensor(11166.0918, grad_fn=<NegBackward0>) tensor(11166.0898, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11166.0908203125
tensor(11166.0898, grad_fn=<NegBackward0>) tensor(11166.0908, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11166.083984375
tensor(11166.0898, grad_fn=<NegBackward0>) tensor(11166.0840, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11166.08203125
tensor(11166.0840, grad_fn=<NegBackward0>) tensor(11166.0820, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11166.0810546875
tensor(11166.0820, grad_fn=<NegBackward0>) tensor(11166.0811, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11166.0810546875
tensor(11166.0811, grad_fn=<NegBackward0>) tensor(11166.0811, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11166.080078125
tensor(11166.0811, grad_fn=<NegBackward0>) tensor(11166.0801, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11166.078125
tensor(11166.0801, grad_fn=<NegBackward0>) tensor(11166.0781, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11166.087890625
tensor(11166.0781, grad_fn=<NegBackward0>) tensor(11166.0879, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11166.0771484375
tensor(11166.0781, grad_fn=<NegBackward0>) tensor(11166.0771, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11166.0791015625
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0791, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11166.0908203125
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0908, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11166.0771484375
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0771, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11166.0771484375
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0771, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11166.083984375
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0840, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11166.076171875
tensor(11166.0771, grad_fn=<NegBackward0>) tensor(11166.0762, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11166.076171875
tensor(11166.0762, grad_fn=<NegBackward0>) tensor(11166.0762, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11166.0869140625
tensor(11166.0762, grad_fn=<NegBackward0>) tensor(11166.0869, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11166.0751953125
tensor(11166.0762, grad_fn=<NegBackward0>) tensor(11166.0752, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11166.076171875
tensor(11166.0752, grad_fn=<NegBackward0>) tensor(11166.0762, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11166.0751953125
tensor(11166.0752, grad_fn=<NegBackward0>) tensor(11166.0752, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11166.0751953125
tensor(11166.0752, grad_fn=<NegBackward0>) tensor(11166.0752, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11166.078125
tensor(11166.0752, grad_fn=<NegBackward0>) tensor(11166.0781, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11166.07421875
tensor(11166.0752, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11166.07421875
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11166.07421875
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11166.0751953125
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0752, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11166.08984375
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0898, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11166.07421875
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11166.0732421875
tensor(11166.0742, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11166.07421875
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11166.0732421875
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11166.0791015625
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0791, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11166.0732421875
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11166.0732421875
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11166.07421875
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0742, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11166.072265625
tensor(11166.0732, grad_fn=<NegBackward0>) tensor(11166.0723, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11166.0732421875
tensor(11166.0723, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11166.0732421875
tensor(11166.0723, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11166.0732421875
tensor(11166.0723, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11166.0732421875
tensor(11166.0723, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11166.0732421875
tensor(11166.0723, grad_fn=<NegBackward0>) tensor(11166.0732, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7861, 0.2139],
        [0.2865, 0.7135]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1027],
         [0.5327, 0.2985]],

        [[0.5155, 0.1042],
         [0.5255, 0.5926]],

        [[0.6911, 0.1040],
         [0.5243, 0.5348]],

        [[0.5378, 0.1029],
         [0.6373, 0.5313]],

        [[0.6607, 0.1011],
         [0.6770, 0.6443]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
Global Adjusted Rand Index: 0.9214420861675072
Average Adjusted Rand Index: 0.9212836636278576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22145.6953125
inf tensor(22145.6953, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11409.9716796875
tensor(22145.6953, grad_fn=<NegBackward0>) tensor(11409.9717, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11408.326171875
tensor(11409.9717, grad_fn=<NegBackward0>) tensor(11408.3262, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11406.4052734375
tensor(11408.3262, grad_fn=<NegBackward0>) tensor(11406.4053, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11404.173828125
tensor(11406.4053, grad_fn=<NegBackward0>) tensor(11404.1738, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11402.177734375
tensor(11404.1738, grad_fn=<NegBackward0>) tensor(11402.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11401.4716796875
tensor(11402.1777, grad_fn=<NegBackward0>) tensor(11401.4717, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11401.0771484375
tensor(11401.4717, grad_fn=<NegBackward0>) tensor(11401.0771, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11400.5546875
tensor(11401.0771, grad_fn=<NegBackward0>) tensor(11400.5547, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11370.2265625
tensor(11400.5547, grad_fn=<NegBackward0>) tensor(11370.2266, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11352.046875
tensor(11370.2266, grad_fn=<NegBackward0>) tensor(11352.0469, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11350.962890625
tensor(11352.0469, grad_fn=<NegBackward0>) tensor(11350.9629, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11350.8037109375
tensor(11350.9629, grad_fn=<NegBackward0>) tensor(11350.8037, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11350.7255859375
tensor(11350.8037, grad_fn=<NegBackward0>) tensor(11350.7256, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11350.677734375
tensor(11350.7256, grad_fn=<NegBackward0>) tensor(11350.6777, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11350.6484375
tensor(11350.6777, grad_fn=<NegBackward0>) tensor(11350.6484, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11350.630859375
tensor(11350.6484, grad_fn=<NegBackward0>) tensor(11350.6309, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11350.6181640625
tensor(11350.6309, grad_fn=<NegBackward0>) tensor(11350.6182, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11350.609375
tensor(11350.6182, grad_fn=<NegBackward0>) tensor(11350.6094, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11350.6015625
tensor(11350.6094, grad_fn=<NegBackward0>) tensor(11350.6016, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11350.5966796875
tensor(11350.6016, grad_fn=<NegBackward0>) tensor(11350.5967, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11350.591796875
tensor(11350.5967, grad_fn=<NegBackward0>) tensor(11350.5918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11350.5849609375
tensor(11350.5918, grad_fn=<NegBackward0>) tensor(11350.5850, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11350.58203125
tensor(11350.5850, grad_fn=<NegBackward0>) tensor(11350.5820, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11350.578125
tensor(11350.5820, grad_fn=<NegBackward0>) tensor(11350.5781, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11350.57421875
tensor(11350.5781, grad_fn=<NegBackward0>) tensor(11350.5742, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11350.568359375
tensor(11350.5742, grad_fn=<NegBackward0>) tensor(11350.5684, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11350.5673828125
tensor(11350.5684, grad_fn=<NegBackward0>) tensor(11350.5674, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11350.564453125
tensor(11350.5674, grad_fn=<NegBackward0>) tensor(11350.5645, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11350.5625
tensor(11350.5645, grad_fn=<NegBackward0>) tensor(11350.5625, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11350.5634765625
tensor(11350.5625, grad_fn=<NegBackward0>) tensor(11350.5635, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11350.560546875
tensor(11350.5625, grad_fn=<NegBackward0>) tensor(11350.5605, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11350.5595703125
tensor(11350.5605, grad_fn=<NegBackward0>) tensor(11350.5596, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11350.55859375
tensor(11350.5596, grad_fn=<NegBackward0>) tensor(11350.5586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11350.55859375
tensor(11350.5586, grad_fn=<NegBackward0>) tensor(11350.5586, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11350.55859375
tensor(11350.5586, grad_fn=<NegBackward0>) tensor(11350.5586, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11350.5576171875
tensor(11350.5586, grad_fn=<NegBackward0>) tensor(11350.5576, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11350.556640625
tensor(11350.5576, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11350.556640625
tensor(11350.5566, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11350.556640625
tensor(11350.5566, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11350.556640625
tensor(11350.5566, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11350.560546875
tensor(11350.5566, grad_fn=<NegBackward0>) tensor(11350.5605, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11350.5556640625
tensor(11350.5566, grad_fn=<NegBackward0>) tensor(11350.5557, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11350.556640625
tensor(11350.5557, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11350.55859375
tensor(11350.5557, grad_fn=<NegBackward0>) tensor(11350.5586, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11350.5546875
tensor(11350.5557, grad_fn=<NegBackward0>) tensor(11350.5547, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11350.5546875
tensor(11350.5547, grad_fn=<NegBackward0>) tensor(11350.5547, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11350.5537109375
tensor(11350.5547, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11350.5556640625
tensor(11350.5537, grad_fn=<NegBackward0>) tensor(11350.5557, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11350.556640625
tensor(11350.5537, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11350.5546875
tensor(11350.5537, grad_fn=<NegBackward0>) tensor(11350.5547, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11350.5537109375
tensor(11350.5537, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11350.552734375
tensor(11350.5537, grad_fn=<NegBackward0>) tensor(11350.5527, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11350.5537109375
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11350.552734375
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5527, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11350.5537109375
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11350.5537109375
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11350.556640625
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11350.556640625
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5566, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11350.5537109375
tensor(11350.5527, grad_fn=<NegBackward0>) tensor(11350.5537, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.9454, 0.0546],
        [0.8828, 0.1172]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4962, 0.5038], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1718, 0.1040],
         [0.5654, 0.3075]],

        [[0.5986, 0.2099],
         [0.5467, 0.5277]],

        [[0.6659, 0.2308],
         [0.7198, 0.6667]],

        [[0.5866, 0.0805],
         [0.6234, 0.5578]],

        [[0.5931, 0.2012],
         [0.6938, 0.6228]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.007272876449007818
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.055895692657722804
Average Adjusted Rand Index: 0.17552511187200906
[0.9214420861675072, 0.055895692657722804] [0.9212836636278576, 0.17552511187200906] [11166.0732421875, 11350.5537109375]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10991.37477298985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23217.27734375
inf tensor(23217.2773, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11238.2373046875
tensor(23217.2773, grad_fn=<NegBackward0>) tensor(11238.2373, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11237.521484375
tensor(11238.2373, grad_fn=<NegBackward0>) tensor(11237.5215, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11237.1591796875
tensor(11237.5215, grad_fn=<NegBackward0>) tensor(11237.1592, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11236.6533203125
tensor(11237.1592, grad_fn=<NegBackward0>) tensor(11236.6533, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11236.220703125
tensor(11236.6533, grad_fn=<NegBackward0>) tensor(11236.2207, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11235.947265625
tensor(11236.2207, grad_fn=<NegBackward0>) tensor(11235.9473, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11235.68359375
tensor(11235.9473, grad_fn=<NegBackward0>) tensor(11235.6836, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11235.466796875
tensor(11235.6836, grad_fn=<NegBackward0>) tensor(11235.4668, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.3056640625
tensor(11235.4668, grad_fn=<NegBackward0>) tensor(11235.3057, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11235.1123046875
tensor(11235.3057, grad_fn=<NegBackward0>) tensor(11235.1123, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11183.900390625
tensor(11235.1123, grad_fn=<NegBackward0>) tensor(11183.9004, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11131.3818359375
tensor(11183.9004, grad_fn=<NegBackward0>) tensor(11131.3818, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10976.7978515625
tensor(11131.3818, grad_fn=<NegBackward0>) tensor(10976.7979, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10964.5986328125
tensor(10976.7979, grad_fn=<NegBackward0>) tensor(10964.5986, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10964.271484375
tensor(10964.5986, grad_fn=<NegBackward0>) tensor(10964.2715, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10962.125
tensor(10964.2715, grad_fn=<NegBackward0>) tensor(10962.1250, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10961.9677734375
tensor(10962.1250, grad_fn=<NegBackward0>) tensor(10961.9678, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10961.9267578125
tensor(10961.9678, grad_fn=<NegBackward0>) tensor(10961.9268, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10961.8984375
tensor(10961.9268, grad_fn=<NegBackward0>) tensor(10961.8984, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10961.875
tensor(10961.8984, grad_fn=<NegBackward0>) tensor(10961.8750, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10961.8525390625
tensor(10961.8750, grad_fn=<NegBackward0>) tensor(10961.8525, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10961.83203125
tensor(10961.8525, grad_fn=<NegBackward0>) tensor(10961.8320, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10961.8193359375
tensor(10961.8320, grad_fn=<NegBackward0>) tensor(10961.8193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10961.8095703125
tensor(10961.8193, grad_fn=<NegBackward0>) tensor(10961.8096, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10961.802734375
tensor(10961.8096, grad_fn=<NegBackward0>) tensor(10961.8027, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10961.7978515625
tensor(10961.8027, grad_fn=<NegBackward0>) tensor(10961.7979, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10961.7919921875
tensor(10961.7979, grad_fn=<NegBackward0>) tensor(10961.7920, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10961.7900390625
tensor(10961.7920, grad_fn=<NegBackward0>) tensor(10961.7900, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10961.7861328125
tensor(10961.7900, grad_fn=<NegBackward0>) tensor(10961.7861, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10961.7841796875
tensor(10961.7861, grad_fn=<NegBackward0>) tensor(10961.7842, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10961.78125
tensor(10961.7842, grad_fn=<NegBackward0>) tensor(10961.7812, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10961.779296875
tensor(10961.7812, grad_fn=<NegBackward0>) tensor(10961.7793, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10961.77734375
tensor(10961.7793, grad_fn=<NegBackward0>) tensor(10961.7773, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10961.7744140625
tensor(10961.7773, grad_fn=<NegBackward0>) tensor(10961.7744, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10961.771484375
tensor(10961.7744, grad_fn=<NegBackward0>) tensor(10961.7715, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10961.76953125
tensor(10961.7715, grad_fn=<NegBackward0>) tensor(10961.7695, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10961.767578125
tensor(10961.7695, grad_fn=<NegBackward0>) tensor(10961.7676, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10961.767578125
tensor(10961.7676, grad_fn=<NegBackward0>) tensor(10961.7676, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10961.7607421875
tensor(10961.7676, grad_fn=<NegBackward0>) tensor(10961.7607, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10961.7578125
tensor(10961.7607, grad_fn=<NegBackward0>) tensor(10961.7578, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10961.7568359375
tensor(10961.7578, grad_fn=<NegBackward0>) tensor(10961.7568, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10961.7529296875
tensor(10961.7568, grad_fn=<NegBackward0>) tensor(10961.7529, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10961.6962890625
tensor(10961.7529, grad_fn=<NegBackward0>) tensor(10961.6963, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10961.6953125
tensor(10961.6963, grad_fn=<NegBackward0>) tensor(10961.6953, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10961.697265625
tensor(10961.6953, grad_fn=<NegBackward0>) tensor(10961.6973, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10961.7021484375
tensor(10961.6953, grad_fn=<NegBackward0>) tensor(10961.7021, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10961.693359375
tensor(10961.6953, grad_fn=<NegBackward0>) tensor(10961.6934, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10961.689453125
tensor(10961.6934, grad_fn=<NegBackward0>) tensor(10961.6895, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10961.67578125
tensor(10961.6895, grad_fn=<NegBackward0>) tensor(10961.6758, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10961.6748046875
tensor(10961.6758, grad_fn=<NegBackward0>) tensor(10961.6748, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10961.673828125
tensor(10961.6748, grad_fn=<NegBackward0>) tensor(10961.6738, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10961.6728515625
tensor(10961.6738, grad_fn=<NegBackward0>) tensor(10961.6729, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10961.673828125
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6738, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10961.6728515625
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6729, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10961.6708984375
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6709, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10961.6728515625
tensor(10961.6709, grad_fn=<NegBackward0>) tensor(10961.6729, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10961.669921875
tensor(10961.6709, grad_fn=<NegBackward0>) tensor(10961.6699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10961.669921875
tensor(10961.6699, grad_fn=<NegBackward0>) tensor(10961.6699, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10961.66796875
tensor(10961.6699, grad_fn=<NegBackward0>) tensor(10961.6680, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10961.6640625
tensor(10961.6680, grad_fn=<NegBackward0>) tensor(10961.6641, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10961.658203125
tensor(10961.6641, grad_fn=<NegBackward0>) tensor(10961.6582, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10961.6572265625
tensor(10961.6582, grad_fn=<NegBackward0>) tensor(10961.6572, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10961.6572265625
tensor(10961.6572, grad_fn=<NegBackward0>) tensor(10961.6572, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10961.6572265625
tensor(10961.6572, grad_fn=<NegBackward0>) tensor(10961.6572, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10961.65625
tensor(10961.6572, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10961.65625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10961.65625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10961.662109375
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6621, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10961.6708984375
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6709, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10961.65625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10961.65625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10961.654296875
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10961.654296875
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10961.6552734375
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6553, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10961.654296875
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10961.6552734375
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6553, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10961.654296875
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10961.6376953125
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10961.63671875
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10961.6376953125
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10961.638671875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6387, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10961.6376953125
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10961.6396484375
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6396, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -10961.63671875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10961.6376953125
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10961.640625
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6406, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10961.671875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6719, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10961.6376953125
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -10961.6357421875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6357, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10961.63671875
tensor(10961.6357, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10961.6396484375
tensor(10961.6357, grad_fn=<NegBackward0>) tensor(10961.6396, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10961.625
tensor(10961.6357, grad_fn=<NegBackward0>) tensor(10961.6250, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10961.625
tensor(10961.6250, grad_fn=<NegBackward0>) tensor(10961.6250, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10961.646484375
tensor(10961.6250, grad_fn=<NegBackward0>) tensor(10961.6465, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10961.6240234375
tensor(10961.6250, grad_fn=<NegBackward0>) tensor(10961.6240, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10961.630859375
tensor(10961.6240, grad_fn=<NegBackward0>) tensor(10961.6309, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10961.6240234375
tensor(10961.6240, grad_fn=<NegBackward0>) tensor(10961.6240, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10961.630859375
tensor(10961.6240, grad_fn=<NegBackward0>) tensor(10961.6309, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10961.623046875
tensor(10961.6240, grad_fn=<NegBackward0>) tensor(10961.6230, grad_fn=<NegBackward0>)
pi: tensor([[0.7228, 0.2772],
        [0.2134, 0.7866]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4226, 0.5774], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3133, 0.0984],
         [0.5707, 0.1923]],

        [[0.6010, 0.0981],
         [0.7249, 0.6567]],

        [[0.6430, 0.0943],
         [0.6759, 0.5502]],

        [[0.6541, 0.1030],
         [0.5401, 0.6040]],

        [[0.6063, 0.1040],
         [0.6908, 0.6830]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080863220989386
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.772135496372924
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.898486627468131
Average Adjusted Rand Index: 0.9000431411016825
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20458.291015625
inf tensor(20458.2910, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11238.068359375
tensor(20458.2910, grad_fn=<NegBackward0>) tensor(11238.0684, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11237.642578125
tensor(11238.0684, grad_fn=<NegBackward0>) tensor(11237.6426, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11237.306640625
tensor(11237.6426, grad_fn=<NegBackward0>) tensor(11237.3066, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11236.8193359375
tensor(11237.3066, grad_fn=<NegBackward0>) tensor(11236.8193, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11236.4599609375
tensor(11236.8193, grad_fn=<NegBackward0>) tensor(11236.4600, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11235.9267578125
tensor(11236.4600, grad_fn=<NegBackward0>) tensor(11235.9268, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11235.611328125
tensor(11235.9268, grad_fn=<NegBackward0>) tensor(11235.6113, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11235.408203125
tensor(11235.6113, grad_fn=<NegBackward0>) tensor(11235.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.212890625
tensor(11235.4082, grad_fn=<NegBackward0>) tensor(11235.2129, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11183.1142578125
tensor(11235.2129, grad_fn=<NegBackward0>) tensor(11183.1143, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11120.583984375
tensor(11183.1143, grad_fn=<NegBackward0>) tensor(11120.5840, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10965.7470703125
tensor(11120.5840, grad_fn=<NegBackward0>) tensor(10965.7471, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10962.279296875
tensor(10965.7471, grad_fn=<NegBackward0>) tensor(10962.2793, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10962.0302734375
tensor(10962.2793, grad_fn=<NegBackward0>) tensor(10962.0303, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10961.9462890625
tensor(10962.0303, grad_fn=<NegBackward0>) tensor(10961.9463, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10961.9052734375
tensor(10961.9463, grad_fn=<NegBackward0>) tensor(10961.9053, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10961.884765625
tensor(10961.9053, grad_fn=<NegBackward0>) tensor(10961.8848, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10961.8681640625
tensor(10961.8848, grad_fn=<NegBackward0>) tensor(10961.8682, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10961.8544921875
tensor(10961.8682, grad_fn=<NegBackward0>) tensor(10961.8545, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10961.8369140625
tensor(10961.8545, grad_fn=<NegBackward0>) tensor(10961.8369, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10961.814453125
tensor(10961.8369, grad_fn=<NegBackward0>) tensor(10961.8145, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10961.8056640625
tensor(10961.8145, grad_fn=<NegBackward0>) tensor(10961.8057, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10961.798828125
tensor(10961.8057, grad_fn=<NegBackward0>) tensor(10961.7988, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10961.7578125
tensor(10961.7988, grad_fn=<NegBackward0>) tensor(10961.7578, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10961.732421875
tensor(10961.7578, grad_fn=<NegBackward0>) tensor(10961.7324, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10961.7236328125
tensor(10961.7324, grad_fn=<NegBackward0>) tensor(10961.7236, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10961.7099609375
tensor(10961.7236, grad_fn=<NegBackward0>) tensor(10961.7100, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10961.7080078125
tensor(10961.7100, grad_fn=<NegBackward0>) tensor(10961.7080, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10961.705078125
tensor(10961.7080, grad_fn=<NegBackward0>) tensor(10961.7051, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10961.703125
tensor(10961.7051, grad_fn=<NegBackward0>) tensor(10961.7031, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10961.7021484375
tensor(10961.7031, grad_fn=<NegBackward0>) tensor(10961.7021, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10961.697265625
tensor(10961.7021, grad_fn=<NegBackward0>) tensor(10961.6973, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10961.6943359375
tensor(10961.6973, grad_fn=<NegBackward0>) tensor(10961.6943, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10961.6923828125
tensor(10961.6943, grad_fn=<NegBackward0>) tensor(10961.6924, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10961.6875
tensor(10961.6924, grad_fn=<NegBackward0>) tensor(10961.6875, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10961.6865234375
tensor(10961.6875, grad_fn=<NegBackward0>) tensor(10961.6865, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10961.681640625
tensor(10961.6865, grad_fn=<NegBackward0>) tensor(10961.6816, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10961.6787109375
tensor(10961.6816, grad_fn=<NegBackward0>) tensor(10961.6787, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10961.677734375
tensor(10961.6787, grad_fn=<NegBackward0>) tensor(10961.6777, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10961.6767578125
tensor(10961.6777, grad_fn=<NegBackward0>) tensor(10961.6768, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10961.677734375
tensor(10961.6768, grad_fn=<NegBackward0>) tensor(10961.6777, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10961.6767578125
tensor(10961.6768, grad_fn=<NegBackward0>) tensor(10961.6768, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10961.6767578125
tensor(10961.6768, grad_fn=<NegBackward0>) tensor(10961.6768, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10961.67578125
tensor(10961.6768, grad_fn=<NegBackward0>) tensor(10961.6758, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10961.677734375
tensor(10961.6758, grad_fn=<NegBackward0>) tensor(10961.6777, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10961.67578125
tensor(10961.6758, grad_fn=<NegBackward0>) tensor(10961.6758, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10961.6728515625
tensor(10961.6758, grad_fn=<NegBackward0>) tensor(10961.6729, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10961.6728515625
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6729, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10961.67578125
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6758, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10961.6708984375
tensor(10961.6729, grad_fn=<NegBackward0>) tensor(10961.6709, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10961.6689453125
tensor(10961.6709, grad_fn=<NegBackward0>) tensor(10961.6689, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10961.6689453125
tensor(10961.6689, grad_fn=<NegBackward0>) tensor(10961.6689, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10961.669921875
tensor(10961.6689, grad_fn=<NegBackward0>) tensor(10961.6699, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10961.6669921875
tensor(10961.6689, grad_fn=<NegBackward0>) tensor(10961.6670, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10961.6669921875
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6670, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10961.6669921875
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6670, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10961.66796875
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6680, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10961.671875
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6719, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10961.6669921875
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6670, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10961.6640625
tensor(10961.6670, grad_fn=<NegBackward0>) tensor(10961.6641, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10961.6640625
tensor(10961.6641, grad_fn=<NegBackward0>) tensor(10961.6641, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10961.6630859375
tensor(10961.6641, grad_fn=<NegBackward0>) tensor(10961.6631, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10961.6630859375
tensor(10961.6631, grad_fn=<NegBackward0>) tensor(10961.6631, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10961.65625
tensor(10961.6631, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10961.6572265625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6572, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10961.65625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6562, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10961.66015625
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6602, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10961.654296875
tensor(10961.6562, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10961.6552734375
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6553, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10961.654296875
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10961.658203125
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6582, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10961.654296875
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6543, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10961.6376953125
tensor(10961.6543, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10961.6396484375
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6396, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10961.6376953125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10961.6376953125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10961.6376953125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10961.638671875
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6387, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10961.6376953125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10961.638671875
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6387, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10961.6533203125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6533, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10961.666015625
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6660, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10961.6376953125
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10961.6396484375
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6396, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10961.63671875
tensor(10961.6377, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10961.63671875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10961.63671875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10961.6708984375
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6709, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10961.6357421875
tensor(10961.6367, grad_fn=<NegBackward0>) tensor(10961.6357, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10961.6376953125
tensor(10961.6357, grad_fn=<NegBackward0>) tensor(10961.6377, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10961.634765625
tensor(10961.6357, grad_fn=<NegBackward0>) tensor(10961.6348, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10961.63671875
tensor(10961.6348, grad_fn=<NegBackward0>) tensor(10961.6367, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10961.6357421875
tensor(10961.6348, grad_fn=<NegBackward0>) tensor(10961.6357, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10961.6396484375
tensor(10961.6348, grad_fn=<NegBackward0>) tensor(10961.6396, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -10961.6357421875
tensor(10961.6348, grad_fn=<NegBackward0>) tensor(10961.6357, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -10961.650390625
tensor(10961.6348, grad_fn=<NegBackward0>) tensor(10961.6504, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.7218, 0.2782],
        [0.2111, 0.7889]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4245, 0.5755], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3138, 0.0988],
         [0.5123, 0.1916]],

        [[0.5682, 0.0982],
         [0.5299, 0.5740]],

        [[0.6086, 0.0948],
         [0.6413, 0.5583]],

        [[0.6625, 0.1031],
         [0.5117, 0.7212]],

        [[0.5522, 0.1042],
         [0.6852, 0.6439]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080863220989386
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.772135496372924
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.898486627468131
Average Adjusted Rand Index: 0.9000431411016825
[0.898486627468131, 0.898486627468131] [0.9000431411016825, 0.9000431411016825] [10961.6259765625, 10961.650390625]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11239.83125867782
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21819.9609375
inf tensor(21819.9609, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11633.3681640625
tensor(21819.9609, grad_fn=<NegBackward0>) tensor(11633.3682, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11627.298828125
tensor(11633.3682, grad_fn=<NegBackward0>) tensor(11627.2988, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11612.224609375
tensor(11627.2988, grad_fn=<NegBackward0>) tensor(11612.2246, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11594.8603515625
tensor(11612.2246, grad_fn=<NegBackward0>) tensor(11594.8604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11428.7978515625
tensor(11594.8604, grad_fn=<NegBackward0>) tensor(11428.7979, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11383.1689453125
tensor(11428.7979, grad_fn=<NegBackward0>) tensor(11383.1689, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11378.3984375
tensor(11383.1689, grad_fn=<NegBackward0>) tensor(11378.3984, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11374.798828125
tensor(11378.3984, grad_fn=<NegBackward0>) tensor(11374.7988, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11374.3720703125
tensor(11374.7988, grad_fn=<NegBackward0>) tensor(11374.3721, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11374.263671875
tensor(11374.3721, grad_fn=<NegBackward0>) tensor(11374.2637, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11374.1962890625
tensor(11374.2637, grad_fn=<NegBackward0>) tensor(11374.1963, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11374.150390625
tensor(11374.1963, grad_fn=<NegBackward0>) tensor(11374.1504, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11374.107421875
tensor(11374.1504, grad_fn=<NegBackward0>) tensor(11374.1074, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11374.029296875
tensor(11374.1074, grad_fn=<NegBackward0>) tensor(11374.0293, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11373.9619140625
tensor(11374.0293, grad_fn=<NegBackward0>) tensor(11373.9619, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11373.9423828125
tensor(11373.9619, grad_fn=<NegBackward0>) tensor(11373.9424, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11373.9287109375
tensor(11373.9424, grad_fn=<NegBackward0>) tensor(11373.9287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11373.9150390625
tensor(11373.9287, grad_fn=<NegBackward0>) tensor(11373.9150, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11373.9033203125
tensor(11373.9150, grad_fn=<NegBackward0>) tensor(11373.9033, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11373.892578125
tensor(11373.9033, grad_fn=<NegBackward0>) tensor(11373.8926, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11373.8828125
tensor(11373.8926, grad_fn=<NegBackward0>) tensor(11373.8828, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11373.8701171875
tensor(11373.8828, grad_fn=<NegBackward0>) tensor(11373.8701, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11373.8525390625
tensor(11373.8701, grad_fn=<NegBackward0>) tensor(11373.8525, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11373.818359375
tensor(11373.8525, grad_fn=<NegBackward0>) tensor(11373.8184, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11373.6552734375
tensor(11373.8184, grad_fn=<NegBackward0>) tensor(11373.6553, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11370.2587890625
tensor(11373.6553, grad_fn=<NegBackward0>) tensor(11370.2588, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11369.45703125
tensor(11370.2588, grad_fn=<NegBackward0>) tensor(11369.4570, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11367.2998046875
tensor(11369.4570, grad_fn=<NegBackward0>) tensor(11367.2998, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11361.13671875
tensor(11367.2998, grad_fn=<NegBackward0>) tensor(11361.1367, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11348.052734375
tensor(11361.1367, grad_fn=<NegBackward0>) tensor(11348.0527, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11334.9189453125
tensor(11348.0527, grad_fn=<NegBackward0>) tensor(11334.9189, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11333.267578125
tensor(11334.9189, grad_fn=<NegBackward0>) tensor(11333.2676, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11333.16796875
tensor(11333.2676, grad_fn=<NegBackward0>) tensor(11333.1680, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11333.142578125
tensor(11333.1680, grad_fn=<NegBackward0>) tensor(11333.1426, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11333.1357421875
tensor(11333.1426, grad_fn=<NegBackward0>) tensor(11333.1357, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11333.1318359375
tensor(11333.1357, grad_fn=<NegBackward0>) tensor(11333.1318, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11333.1298828125
tensor(11333.1318, grad_fn=<NegBackward0>) tensor(11333.1299, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11333.12890625
tensor(11333.1299, grad_fn=<NegBackward0>) tensor(11333.1289, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11333.1259765625
tensor(11333.1289, grad_fn=<NegBackward0>) tensor(11333.1260, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11333.125
tensor(11333.1260, grad_fn=<NegBackward0>) tensor(11333.1250, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11333.1259765625
tensor(11333.1250, grad_fn=<NegBackward0>) tensor(11333.1260, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11333.123046875
tensor(11333.1250, grad_fn=<NegBackward0>) tensor(11333.1230, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11333.1240234375
tensor(11333.1230, grad_fn=<NegBackward0>) tensor(11333.1240, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11333.12109375
tensor(11333.1230, grad_fn=<NegBackward0>) tensor(11333.1211, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11333.12109375
tensor(11333.1211, grad_fn=<NegBackward0>) tensor(11333.1211, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11333.119140625
tensor(11333.1211, grad_fn=<NegBackward0>) tensor(11333.1191, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11333.12109375
tensor(11333.1191, grad_fn=<NegBackward0>) tensor(11333.1211, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11333.119140625
tensor(11333.1191, grad_fn=<NegBackward0>) tensor(11333.1191, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11333.1162109375
tensor(11333.1191, grad_fn=<NegBackward0>) tensor(11333.1162, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11333.125
tensor(11333.1162, grad_fn=<NegBackward0>) tensor(11333.1250, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11333.1171875
tensor(11333.1162, grad_fn=<NegBackward0>) tensor(11333.1172, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11333.1142578125
tensor(11333.1162, grad_fn=<NegBackward0>) tensor(11333.1143, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11333.107421875
tensor(11333.1143, grad_fn=<NegBackward0>) tensor(11333.1074, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11333.1083984375
tensor(11333.1074, grad_fn=<NegBackward0>) tensor(11333.1084, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11333.103515625
tensor(11333.1074, grad_fn=<NegBackward0>) tensor(11333.1035, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11331.76171875
tensor(11333.1035, grad_fn=<NegBackward0>) tensor(11331.7617, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11331.7568359375
tensor(11331.7617, grad_fn=<NegBackward0>) tensor(11331.7568, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11330.7626953125
tensor(11331.7568, grad_fn=<NegBackward0>) tensor(11330.7627, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11330.7626953125
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7627, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11330.763671875
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7637, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11330.765625
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7656, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11330.7666015625
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7666, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11330.759765625
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7598, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11330.6123046875
tensor(11330.7598, grad_fn=<NegBackward0>) tensor(11330.6123, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11330.6015625
tensor(11330.6123, grad_fn=<NegBackward0>) tensor(11330.6016, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11330.6015625
tensor(11330.6016, grad_fn=<NegBackward0>) tensor(11330.6016, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11330.3125
tensor(11330.6016, grad_fn=<NegBackward0>) tensor(11330.3125, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11330.306640625
tensor(11330.3125, grad_fn=<NegBackward0>) tensor(11330.3066, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11330.302734375
tensor(11330.3066, grad_fn=<NegBackward0>) tensor(11330.3027, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11330.2978515625
tensor(11330.3027, grad_fn=<NegBackward0>) tensor(11330.2979, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11330.2978515625
tensor(11330.2979, grad_fn=<NegBackward0>) tensor(11330.2979, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11330.2998046875
tensor(11330.2979, grad_fn=<NegBackward0>) tensor(11330.2998, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11330.3017578125
tensor(11330.2979, grad_fn=<NegBackward0>) tensor(11330.3018, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11330.2978515625
tensor(11330.2979, grad_fn=<NegBackward0>) tensor(11330.2979, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11330.296875
tensor(11330.2979, grad_fn=<NegBackward0>) tensor(11330.2969, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11330.2978515625
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.2979, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11330.296875
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.2969, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11330.32421875
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.3242, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11330.2978515625
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.2979, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11330.3056640625
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.3057, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11330.298828125
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.2988, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11330.341796875
tensor(11330.2969, grad_fn=<NegBackward0>) tensor(11330.3418, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.6799, 0.3201],
        [0.3016, 0.6984]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5941, 0.4059], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2325, 0.0987],
         [0.5847, 0.2888]],

        [[0.6196, 0.0871],
         [0.6491, 0.6361]],

        [[0.6161, 0.1015],
         [0.5628, 0.7061]],

        [[0.6611, 0.0956],
         [0.6919, 0.5640]],

        [[0.5461, 0.0951],
         [0.5881, 0.5100]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7019068242816332
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448376182574403
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.04649017345857551
Average Adjusted Rand Index: 0.8544813313097821
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22343.708984375
inf tensor(22343.7090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11634.1162109375
tensor(22343.7090, grad_fn=<NegBackward0>) tensor(11634.1162, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11616.7861328125
tensor(11634.1162, grad_fn=<NegBackward0>) tensor(11616.7861, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11552.4580078125
tensor(11616.7861, grad_fn=<NegBackward0>) tensor(11552.4580, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11407.3359375
tensor(11552.4580, grad_fn=<NegBackward0>) tensor(11407.3359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11334.3271484375
tensor(11407.3359, grad_fn=<NegBackward0>) tensor(11334.3271, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11285.7392578125
tensor(11334.3271, grad_fn=<NegBackward0>) tensor(11285.7393, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11261.76171875
tensor(11285.7393, grad_fn=<NegBackward0>) tensor(11261.7617, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11245.6259765625
tensor(11261.7617, grad_fn=<NegBackward0>) tensor(11245.6260, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11239.4677734375
tensor(11245.6260, grad_fn=<NegBackward0>) tensor(11239.4678, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11236.1689453125
tensor(11239.4678, grad_fn=<NegBackward0>) tensor(11236.1689, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11228.8525390625
tensor(11236.1689, grad_fn=<NegBackward0>) tensor(11228.8525, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11228.7314453125
tensor(11228.8525, grad_fn=<NegBackward0>) tensor(11228.7314, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11228.6689453125
tensor(11228.7314, grad_fn=<NegBackward0>) tensor(11228.6689, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11228.6328125
tensor(11228.6689, grad_fn=<NegBackward0>) tensor(11228.6328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11228.611328125
tensor(11228.6328, grad_fn=<NegBackward0>) tensor(11228.6113, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11228.595703125
tensor(11228.6113, grad_fn=<NegBackward0>) tensor(11228.5957, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11228.58203125
tensor(11228.5957, grad_fn=<NegBackward0>) tensor(11228.5820, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11228.5703125
tensor(11228.5820, grad_fn=<NegBackward0>) tensor(11228.5703, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11228.5625
tensor(11228.5703, grad_fn=<NegBackward0>) tensor(11228.5625, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11228.552734375
tensor(11228.5625, grad_fn=<NegBackward0>) tensor(11228.5527, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11228.5478515625
tensor(11228.5527, grad_fn=<NegBackward0>) tensor(11228.5479, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11228.5400390625
tensor(11228.5479, grad_fn=<NegBackward0>) tensor(11228.5400, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11228.521484375
tensor(11228.5400, grad_fn=<NegBackward0>) tensor(11228.5215, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11224.87109375
tensor(11228.5215, grad_fn=<NegBackward0>) tensor(11224.8711, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11224.861328125
tensor(11224.8711, grad_fn=<NegBackward0>) tensor(11224.8613, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11224.857421875
tensor(11224.8613, grad_fn=<NegBackward0>) tensor(11224.8574, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11224.8525390625
tensor(11224.8574, grad_fn=<NegBackward0>) tensor(11224.8525, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11224.849609375
tensor(11224.8525, grad_fn=<NegBackward0>) tensor(11224.8496, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11224.8466796875
tensor(11224.8496, grad_fn=<NegBackward0>) tensor(11224.8467, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11224.845703125
tensor(11224.8467, grad_fn=<NegBackward0>) tensor(11224.8457, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11224.84375
tensor(11224.8457, grad_fn=<NegBackward0>) tensor(11224.8438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11224.8427734375
tensor(11224.8438, grad_fn=<NegBackward0>) tensor(11224.8428, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11224.841796875
tensor(11224.8428, grad_fn=<NegBackward0>) tensor(11224.8418, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11224.83984375
tensor(11224.8418, grad_fn=<NegBackward0>) tensor(11224.8398, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11224.8388671875
tensor(11224.8398, grad_fn=<NegBackward0>) tensor(11224.8389, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11224.837890625
tensor(11224.8389, grad_fn=<NegBackward0>) tensor(11224.8379, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11224.8369140625
tensor(11224.8379, grad_fn=<NegBackward0>) tensor(11224.8369, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11224.8349609375
tensor(11224.8369, grad_fn=<NegBackward0>) tensor(11224.8350, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11224.833984375
tensor(11224.8350, grad_fn=<NegBackward0>) tensor(11224.8340, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11224.8349609375
tensor(11224.8340, grad_fn=<NegBackward0>) tensor(11224.8350, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11224.8330078125
tensor(11224.8340, grad_fn=<NegBackward0>) tensor(11224.8330, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11224.8330078125
tensor(11224.8330, grad_fn=<NegBackward0>) tensor(11224.8330, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11224.83203125
tensor(11224.8330, grad_fn=<NegBackward0>) tensor(11224.8320, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11224.8310546875
tensor(11224.8320, grad_fn=<NegBackward0>) tensor(11224.8311, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11224.837890625
tensor(11224.8311, grad_fn=<NegBackward0>) tensor(11224.8379, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11224.8310546875
tensor(11224.8311, grad_fn=<NegBackward0>) tensor(11224.8311, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11224.8291015625
tensor(11224.8311, grad_fn=<NegBackward0>) tensor(11224.8291, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11224.826171875
tensor(11224.8291, grad_fn=<NegBackward0>) tensor(11224.8262, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11224.8095703125
tensor(11224.8262, grad_fn=<NegBackward0>) tensor(11224.8096, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11224.8095703125
tensor(11224.8096, grad_fn=<NegBackward0>) tensor(11224.8096, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11224.8076171875
tensor(11224.8096, grad_fn=<NegBackward0>) tensor(11224.8076, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11224.80859375
tensor(11224.8076, grad_fn=<NegBackward0>) tensor(11224.8086, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11224.80859375
tensor(11224.8076, grad_fn=<NegBackward0>) tensor(11224.8086, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11224.80859375
tensor(11224.8076, grad_fn=<NegBackward0>) tensor(11224.8086, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -11224.80859375
tensor(11224.8076, grad_fn=<NegBackward0>) tensor(11224.8086, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -11224.80859375
tensor(11224.8076, grad_fn=<NegBackward0>) tensor(11224.8086, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.8227, 0.1773],
        [0.2169, 0.7831]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5401, 0.4599], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3052, 0.0979],
         [0.6039, 0.1943]],

        [[0.6531, 0.0978],
         [0.5199, 0.6212]],

        [[0.5004, 0.1030],
         [0.6712, 0.5446]],

        [[0.5827, 0.0962],
         [0.5652, 0.6016]],

        [[0.6369, 0.0952],
         [0.5747, 0.6555]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9603178629296129
Average Adjusted Rand Index: 0.9603183244926422
[0.04649017345857551, 0.9603178629296129] [0.8544813313097821, 0.9603183244926422] [11330.341796875, 11224.80859375]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11119.704018219172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24772.42578125
inf tensor(24772.4258, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11357.0869140625
tensor(24772.4258, grad_fn=<NegBackward0>) tensor(11357.0869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11356.41015625
tensor(11357.0869, grad_fn=<NegBackward0>) tensor(11356.4102, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11356.201171875
tensor(11356.4102, grad_fn=<NegBackward0>) tensor(11356.2012, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11356.150390625
tensor(11356.2012, grad_fn=<NegBackward0>) tensor(11356.1504, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11356.111328125
tensor(11356.1504, grad_fn=<NegBackward0>) tensor(11356.1113, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11356.0751953125
tensor(11356.1113, grad_fn=<NegBackward0>) tensor(11356.0752, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11356.02734375
tensor(11356.0752, grad_fn=<NegBackward0>) tensor(11356.0273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11355.9296875
tensor(11356.0273, grad_fn=<NegBackward0>) tensor(11355.9297, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11355.0556640625
tensor(11355.9297, grad_fn=<NegBackward0>) tensor(11355.0557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11351.6806640625
tensor(11355.0557, grad_fn=<NegBackward0>) tensor(11351.6807, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11239.001953125
tensor(11351.6807, grad_fn=<NegBackward0>) tensor(11239.0020, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11121.4609375
tensor(11239.0020, grad_fn=<NegBackward0>) tensor(11121.4609, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11097.61328125
tensor(11121.4609, grad_fn=<NegBackward0>) tensor(11097.6133, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11097.2265625
tensor(11097.6133, grad_fn=<NegBackward0>) tensor(11097.2266, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11097.1923828125
tensor(11097.2266, grad_fn=<NegBackward0>) tensor(11097.1924, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11097.0771484375
tensor(11097.1924, grad_fn=<NegBackward0>) tensor(11097.0771, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11095.76171875
tensor(11097.0771, grad_fn=<NegBackward0>) tensor(11095.7617, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11095.75390625
tensor(11095.7617, grad_fn=<NegBackward0>) tensor(11095.7539, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11095.744140625
tensor(11095.7539, grad_fn=<NegBackward0>) tensor(11095.7441, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11095.7412109375
tensor(11095.7441, grad_fn=<NegBackward0>) tensor(11095.7412, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11095.7353515625
tensor(11095.7412, grad_fn=<NegBackward0>) tensor(11095.7354, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11095.728515625
tensor(11095.7354, grad_fn=<NegBackward0>) tensor(11095.7285, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11095.419921875
tensor(11095.7285, grad_fn=<NegBackward0>) tensor(11095.4199, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11095.34765625
tensor(11095.4199, grad_fn=<NegBackward0>) tensor(11095.3477, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11095.322265625
tensor(11095.3477, grad_fn=<NegBackward0>) tensor(11095.3223, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11095.3212890625
tensor(11095.3223, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11095.3203125
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3203, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11095.318359375
tensor(11095.3203, grad_fn=<NegBackward0>) tensor(11095.3184, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11095.3173828125
tensor(11095.3184, grad_fn=<NegBackward0>) tensor(11095.3174, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11095.314453125
tensor(11095.3174, grad_fn=<NegBackward0>) tensor(11095.3145, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11095.26953125
tensor(11095.3145, grad_fn=<NegBackward0>) tensor(11095.2695, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11095.265625
tensor(11095.2695, grad_fn=<NegBackward0>) tensor(11095.2656, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11095.265625
tensor(11095.2656, grad_fn=<NegBackward0>) tensor(11095.2656, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11095.2646484375
tensor(11095.2656, grad_fn=<NegBackward0>) tensor(11095.2646, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11095.263671875
tensor(11095.2646, grad_fn=<NegBackward0>) tensor(11095.2637, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11095.263671875
tensor(11095.2637, grad_fn=<NegBackward0>) tensor(11095.2637, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11095.2626953125
tensor(11095.2637, grad_fn=<NegBackward0>) tensor(11095.2627, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11095.26171875
tensor(11095.2627, grad_fn=<NegBackward0>) tensor(11095.2617, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11095.26171875
tensor(11095.2617, grad_fn=<NegBackward0>) tensor(11095.2617, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11095.259765625
tensor(11095.2617, grad_fn=<NegBackward0>) tensor(11095.2598, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11095.255859375
tensor(11095.2598, grad_fn=<NegBackward0>) tensor(11095.2559, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11095.25390625
tensor(11095.2559, grad_fn=<NegBackward0>) tensor(11095.2539, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11095.25390625
tensor(11095.2539, grad_fn=<NegBackward0>) tensor(11095.2539, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11095.248046875
tensor(11095.2539, grad_fn=<NegBackward0>) tensor(11095.2480, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11095.228515625
tensor(11095.2480, grad_fn=<NegBackward0>) tensor(11095.2285, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11095.2255859375
tensor(11095.2285, grad_fn=<NegBackward0>) tensor(11095.2256, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11095.2265625
tensor(11095.2256, grad_fn=<NegBackward0>) tensor(11095.2266, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11095.2001953125
tensor(11095.2256, grad_fn=<NegBackward0>) tensor(11095.2002, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11095.2001953125
tensor(11095.2002, grad_fn=<NegBackward0>) tensor(11095.2002, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11095.150390625
tensor(11095.2002, grad_fn=<NegBackward0>) tensor(11095.1504, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11095.087890625
tensor(11095.1504, grad_fn=<NegBackward0>) tensor(11095.0879, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11095.0859375
tensor(11095.0879, grad_fn=<NegBackward0>) tensor(11095.0859, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11095.0849609375
tensor(11095.0859, grad_fn=<NegBackward0>) tensor(11095.0850, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11095.0849609375
tensor(11095.0850, grad_fn=<NegBackward0>) tensor(11095.0850, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11095.0859375
tensor(11095.0850, grad_fn=<NegBackward0>) tensor(11095.0859, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11095.08203125
tensor(11095.0850, grad_fn=<NegBackward0>) tensor(11095.0820, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11095.08203125
tensor(11095.0820, grad_fn=<NegBackward0>) tensor(11095.0820, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11095.080078125
tensor(11095.0820, grad_fn=<NegBackward0>) tensor(11095.0801, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11095.0810546875
tensor(11095.0801, grad_fn=<NegBackward0>) tensor(11095.0811, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11095.0927734375
tensor(11095.0801, grad_fn=<NegBackward0>) tensor(11095.0928, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11095.0791015625
tensor(11095.0801, grad_fn=<NegBackward0>) tensor(11095.0791, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11095.091796875
tensor(11095.0791, grad_fn=<NegBackward0>) tensor(11095.0918, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11095.0791015625
tensor(11095.0791, grad_fn=<NegBackward0>) tensor(11095.0791, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11095.0810546875
tensor(11095.0791, grad_fn=<NegBackward0>) tensor(11095.0811, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11095.0791015625
tensor(11095.0791, grad_fn=<NegBackward0>) tensor(11095.0791, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11095.078125
tensor(11095.0791, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11095.0791015625
tensor(11095.0781, grad_fn=<NegBackward0>) tensor(11095.0791, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11095.0966796875
tensor(11095.0781, grad_fn=<NegBackward0>) tensor(11095.0967, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11095.0771484375
tensor(11095.0781, grad_fn=<NegBackward0>) tensor(11095.0771, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11095.078125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11095.078125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11095.087890625
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0879, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11095.0771484375
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0771, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11095.0869140625
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0869, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11095.078125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11095.08203125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0820, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11095.078125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11095.078125
tensor(11095.0771, grad_fn=<NegBackward0>) tensor(11095.0781, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.7660, 0.2340],
        [0.2140, 0.7860]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4493, 0.5507], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2967, 0.1072],
         [0.5506, 0.2008]],

        [[0.6357, 0.0982],
         [0.5524, 0.6200]],

        [[0.6390, 0.0987],
         [0.5473, 0.7258]],

        [[0.7150, 0.1094],
         [0.5345, 0.6466]],

        [[0.6009, 0.0957],
         [0.6069, 0.6995]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.9524809382781522
Average Adjusted Rand Index: 0.9529677516362213
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23584.794921875
inf tensor(23584.7949, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11357.1982421875
tensor(23584.7949, grad_fn=<NegBackward0>) tensor(11357.1982, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11356.4521484375
tensor(11357.1982, grad_fn=<NegBackward0>) tensor(11356.4521, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11355.9970703125
tensor(11356.4521, grad_fn=<NegBackward0>) tensor(11355.9971, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11355.564453125
tensor(11355.9971, grad_fn=<NegBackward0>) tensor(11355.5645, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11354.7861328125
tensor(11355.5645, grad_fn=<NegBackward0>) tensor(11354.7861, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11353.5166015625
tensor(11354.7861, grad_fn=<NegBackward0>) tensor(11353.5166, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11353.140625
tensor(11353.5166, grad_fn=<NegBackward0>) tensor(11353.1406, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11352.6806640625
tensor(11353.1406, grad_fn=<NegBackward0>) tensor(11352.6807, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11352.03125
tensor(11352.6807, grad_fn=<NegBackward0>) tensor(11352.0312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11351.1953125
tensor(11352.0312, grad_fn=<NegBackward0>) tensor(11351.1953, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11350.857421875
tensor(11351.1953, grad_fn=<NegBackward0>) tensor(11350.8574, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11350.75
tensor(11350.8574, grad_fn=<NegBackward0>) tensor(11350.7500, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11350.6875
tensor(11350.7500, grad_fn=<NegBackward0>) tensor(11350.6875, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11350.6376953125
tensor(11350.6875, grad_fn=<NegBackward0>) tensor(11350.6377, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11350.58984375
tensor(11350.6377, grad_fn=<NegBackward0>) tensor(11350.5898, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11350.5390625
tensor(11350.5898, grad_fn=<NegBackward0>) tensor(11350.5391, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11350.4658203125
tensor(11350.5391, grad_fn=<NegBackward0>) tensor(11350.4658, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11350.3603515625
tensor(11350.4658, grad_fn=<NegBackward0>) tensor(11350.3604, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11350.2705078125
tensor(11350.3604, grad_fn=<NegBackward0>) tensor(11350.2705, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11350.23828125
tensor(11350.2705, grad_fn=<NegBackward0>) tensor(11350.2383, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11350.21875
tensor(11350.2383, grad_fn=<NegBackward0>) tensor(11350.2188, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11350.2080078125
tensor(11350.2188, grad_fn=<NegBackward0>) tensor(11350.2080, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11350.1962890625
tensor(11350.2080, grad_fn=<NegBackward0>) tensor(11350.1963, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11350.1796875
tensor(11350.1963, grad_fn=<NegBackward0>) tensor(11350.1797, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11350.14453125
tensor(11350.1797, grad_fn=<NegBackward0>) tensor(11350.1445, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11349.9970703125
tensor(11350.1445, grad_fn=<NegBackward0>) tensor(11349.9971, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11174.234375
tensor(11349.9971, grad_fn=<NegBackward0>) tensor(11174.2344, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11120.611328125
tensor(11174.2344, grad_fn=<NegBackward0>) tensor(11120.6113, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11117.4873046875
tensor(11120.6113, grad_fn=<NegBackward0>) tensor(11117.4873, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11114.61328125
tensor(11117.4873, grad_fn=<NegBackward0>) tensor(11114.6133, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11105.2060546875
tensor(11114.6133, grad_fn=<NegBackward0>) tensor(11105.2061, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11100.580078125
tensor(11105.2061, grad_fn=<NegBackward0>) tensor(11100.5801, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11100.578125
tensor(11100.5801, grad_fn=<NegBackward0>) tensor(11100.5781, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11100.564453125
tensor(11100.5781, grad_fn=<NegBackward0>) tensor(11100.5645, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11100.5595703125
tensor(11100.5645, grad_fn=<NegBackward0>) tensor(11100.5596, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11100.5546875
tensor(11100.5596, grad_fn=<NegBackward0>) tensor(11100.5547, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11100.5458984375
tensor(11100.5547, grad_fn=<NegBackward0>) tensor(11100.5459, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11100.5380859375
tensor(11100.5459, grad_fn=<NegBackward0>) tensor(11100.5381, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11100.5341796875
tensor(11100.5381, grad_fn=<NegBackward0>) tensor(11100.5342, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11100.5283203125
tensor(11100.5342, grad_fn=<NegBackward0>) tensor(11100.5283, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11100.5224609375
tensor(11100.5283, grad_fn=<NegBackward0>) tensor(11100.5225, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11100.5205078125
tensor(11100.5225, grad_fn=<NegBackward0>) tensor(11100.5205, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11100.4794921875
tensor(11100.5205, grad_fn=<NegBackward0>) tensor(11100.4795, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11099.7294921875
tensor(11100.4795, grad_fn=<NegBackward0>) tensor(11099.7295, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11099.30859375
tensor(11099.7295, grad_fn=<NegBackward0>) tensor(11099.3086, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11099.2734375
tensor(11099.3086, grad_fn=<NegBackward0>) tensor(11099.2734, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11099.2724609375
tensor(11099.2734, grad_fn=<NegBackward0>) tensor(11099.2725, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11099.271484375
tensor(11099.2725, grad_fn=<NegBackward0>) tensor(11099.2715, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11099.2919921875
tensor(11099.2715, grad_fn=<NegBackward0>) tensor(11099.2920, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11099.26953125
tensor(11099.2715, grad_fn=<NegBackward0>) tensor(11099.2695, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11099.26953125
tensor(11099.2695, grad_fn=<NegBackward0>) tensor(11099.2695, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11099.2685546875
tensor(11099.2695, grad_fn=<NegBackward0>) tensor(11099.2686, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11099.2685546875
tensor(11099.2686, grad_fn=<NegBackward0>) tensor(11099.2686, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11099.267578125
tensor(11099.2686, grad_fn=<NegBackward0>) tensor(11099.2676, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11099.265625
tensor(11099.2676, grad_fn=<NegBackward0>) tensor(11099.2656, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11099.265625
tensor(11099.2656, grad_fn=<NegBackward0>) tensor(11099.2656, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11099.2646484375
tensor(11099.2656, grad_fn=<NegBackward0>) tensor(11099.2646, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11099.2646484375
tensor(11099.2646, grad_fn=<NegBackward0>) tensor(11099.2646, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11099.2646484375
tensor(11099.2646, grad_fn=<NegBackward0>) tensor(11099.2646, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11099.263671875
tensor(11099.2646, grad_fn=<NegBackward0>) tensor(11099.2637, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11099.2626953125
tensor(11099.2637, grad_fn=<NegBackward0>) tensor(11099.2627, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11099.2626953125
tensor(11099.2627, grad_fn=<NegBackward0>) tensor(11099.2627, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11099.26171875
tensor(11099.2627, grad_fn=<NegBackward0>) tensor(11099.2617, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11099.26171875
tensor(11099.2617, grad_fn=<NegBackward0>) tensor(11099.2617, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11099.2626953125
tensor(11099.2617, grad_fn=<NegBackward0>) tensor(11099.2627, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11099.2587890625
tensor(11099.2617, grad_fn=<NegBackward0>) tensor(11099.2588, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11099.2607421875
tensor(11099.2588, grad_fn=<NegBackward0>) tensor(11099.2607, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11099.2431640625
tensor(11099.2588, grad_fn=<NegBackward0>) tensor(11099.2432, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11099.248046875
tensor(11099.2432, grad_fn=<NegBackward0>) tensor(11099.2480, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11099.2421875
tensor(11099.2432, grad_fn=<NegBackward0>) tensor(11099.2422, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11099.2421875
tensor(11099.2422, grad_fn=<NegBackward0>) tensor(11099.2422, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11099.2412109375
tensor(11099.2422, grad_fn=<NegBackward0>) tensor(11099.2412, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11099.2421875
tensor(11099.2412, grad_fn=<NegBackward0>) tensor(11099.2422, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11099.240234375
tensor(11099.2412, grad_fn=<NegBackward0>) tensor(11099.2402, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11099.2353515625
tensor(11099.2402, grad_fn=<NegBackward0>) tensor(11099.2354, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11099.2353515625
tensor(11099.2354, grad_fn=<NegBackward0>) tensor(11099.2354, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11099.234375
tensor(11099.2354, grad_fn=<NegBackward0>) tensor(11099.2344, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11099.234375
tensor(11099.2344, grad_fn=<NegBackward0>) tensor(11099.2344, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11099.234375
tensor(11099.2344, grad_fn=<NegBackward0>) tensor(11099.2344, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11099.2353515625
tensor(11099.2344, grad_fn=<NegBackward0>) tensor(11099.2354, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11099.2333984375
tensor(11099.2344, grad_fn=<NegBackward0>) tensor(11099.2334, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11099.06640625
tensor(11099.2334, grad_fn=<NegBackward0>) tensor(11099.0664, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11095.3232421875
tensor(11099.0664, grad_fn=<NegBackward0>) tensor(11095.3232, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11095.3212890625
tensor(11095.3232, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11095.3212890625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11095.322265625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3223, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11095.32421875
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3242, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11095.3212890625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11095.328125
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3281, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11095.322265625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3223, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11095.322265625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3223, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11095.333984375
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3340, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11095.3212890625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11095.3212890625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11095.3212890625
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11095.3203125
tensor(11095.3213, grad_fn=<NegBackward0>) tensor(11095.3203, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11095.3212890625
tensor(11095.3203, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11095.3212890625
tensor(11095.3203, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11095.3212890625
tensor(11095.3203, grad_fn=<NegBackward0>) tensor(11095.3213, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7859, 0.2141],
        [0.2338, 0.7662]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5511, 0.4489], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2008, 0.1072],
         [0.5013, 0.2967]],

        [[0.6693, 0.0982],
         [0.6988, 0.6936]],

        [[0.6481, 0.0987],
         [0.6637, 0.5437]],

        [[0.5779, 0.1094],
         [0.6984, 0.5613]],

        [[0.5806, 0.0957],
         [0.5818, 0.5351]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.9603206094218953
Average Adjusted Rand Index: 0.9609677516362213
[0.9524809382781522, 0.9603206094218953] [0.9529677516362213, 0.9609677516362213] [11095.078125, 11095.3193359375]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11054.769263192185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20278.701171875
inf tensor(20278.7012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11317.287109375
tensor(20278.7012, grad_fn=<NegBackward0>) tensor(11317.2871, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11316.6162109375
tensor(11317.2871, grad_fn=<NegBackward0>) tensor(11316.6162, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11315.640625
tensor(11316.6162, grad_fn=<NegBackward0>) tensor(11315.6406, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11314.5966796875
tensor(11315.6406, grad_fn=<NegBackward0>) tensor(11314.5967, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11313.9521484375
tensor(11314.5967, grad_fn=<NegBackward0>) tensor(11313.9521, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11312.435546875
tensor(11313.9521, grad_fn=<NegBackward0>) tensor(11312.4355, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11160.578125
tensor(11312.4355, grad_fn=<NegBackward0>) tensor(11160.5781, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11062.275390625
tensor(11160.5781, grad_fn=<NegBackward0>) tensor(11062.2754, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11030.814453125
tensor(11062.2754, grad_fn=<NegBackward0>) tensor(11030.8145, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11028.7763671875
tensor(11030.8145, grad_fn=<NegBackward0>) tensor(11028.7764, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11027.81640625
tensor(11028.7764, grad_fn=<NegBackward0>) tensor(11027.8164, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11027.1982421875
tensor(11027.8164, grad_fn=<NegBackward0>) tensor(11027.1982, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11027.1630859375
tensor(11027.1982, grad_fn=<NegBackward0>) tensor(11027.1631, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11027.1416015625
tensor(11027.1631, grad_fn=<NegBackward0>) tensor(11027.1416, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11027.119140625
tensor(11027.1416, grad_fn=<NegBackward0>) tensor(11027.1191, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11025.5859375
tensor(11027.1191, grad_fn=<NegBackward0>) tensor(11025.5859, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11025.427734375
tensor(11025.5859, grad_fn=<NegBackward0>) tensor(11025.4277, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11025.419921875
tensor(11025.4277, grad_fn=<NegBackward0>) tensor(11025.4199, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11025.4150390625
tensor(11025.4199, grad_fn=<NegBackward0>) tensor(11025.4150, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11025.404296875
tensor(11025.4150, grad_fn=<NegBackward0>) tensor(11025.4043, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11025.38671875
tensor(11025.4043, grad_fn=<NegBackward0>) tensor(11025.3867, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11025.3798828125
tensor(11025.3867, grad_fn=<NegBackward0>) tensor(11025.3799, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11025.376953125
tensor(11025.3799, grad_fn=<NegBackward0>) tensor(11025.3770, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11025.375
tensor(11025.3770, grad_fn=<NegBackward0>) tensor(11025.3750, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11025.373046875
tensor(11025.3750, grad_fn=<NegBackward0>) tensor(11025.3730, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11025.3701171875
tensor(11025.3730, grad_fn=<NegBackward0>) tensor(11025.3701, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11025.3681640625
tensor(11025.3701, grad_fn=<NegBackward0>) tensor(11025.3682, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11025.3671875
tensor(11025.3682, grad_fn=<NegBackward0>) tensor(11025.3672, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11025.3564453125
tensor(11025.3672, grad_fn=<NegBackward0>) tensor(11025.3564, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11025.1220703125
tensor(11025.3564, grad_fn=<NegBackward0>) tensor(11025.1221, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11025.1220703125
tensor(11025.1221, grad_fn=<NegBackward0>) tensor(11025.1221, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11025.1201171875
tensor(11025.1221, grad_fn=<NegBackward0>) tensor(11025.1201, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11025.119140625
tensor(11025.1201, grad_fn=<NegBackward0>) tensor(11025.1191, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11025.1181640625
tensor(11025.1191, grad_fn=<NegBackward0>) tensor(11025.1182, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11025.1162109375
tensor(11025.1182, grad_fn=<NegBackward0>) tensor(11025.1162, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11025.1103515625
tensor(11025.1162, grad_fn=<NegBackward0>) tensor(11025.1104, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11025.1083984375
tensor(11025.1104, grad_fn=<NegBackward0>) tensor(11025.1084, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11025.1083984375
tensor(11025.1084, grad_fn=<NegBackward0>) tensor(11025.1084, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11025.107421875
tensor(11025.1084, grad_fn=<NegBackward0>) tensor(11025.1074, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11025.1064453125
tensor(11025.1074, grad_fn=<NegBackward0>) tensor(11025.1064, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11025.1005859375
tensor(11025.1064, grad_fn=<NegBackward0>) tensor(11025.1006, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11024.9013671875
tensor(11025.1006, grad_fn=<NegBackward0>) tensor(11024.9014, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11024.900390625
tensor(11024.9014, grad_fn=<NegBackward0>) tensor(11024.9004, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11024.9013671875
tensor(11024.9004, grad_fn=<NegBackward0>) tensor(11024.9014, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11024.8994140625
tensor(11024.9004, grad_fn=<NegBackward0>) tensor(11024.8994, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11024.900390625
tensor(11024.8994, grad_fn=<NegBackward0>) tensor(11024.9004, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11024.8994140625
tensor(11024.8994, grad_fn=<NegBackward0>) tensor(11024.8994, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11024.8994140625
tensor(11024.8994, grad_fn=<NegBackward0>) tensor(11024.8994, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11024.9052734375
tensor(11024.8994, grad_fn=<NegBackward0>) tensor(11024.9053, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11024.8984375
tensor(11024.8994, grad_fn=<NegBackward0>) tensor(11024.8984, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11024.8994140625
tensor(11024.8984, grad_fn=<NegBackward0>) tensor(11024.8994, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11024.8984375
tensor(11024.8984, grad_fn=<NegBackward0>) tensor(11024.8984, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11024.8974609375
tensor(11024.8984, grad_fn=<NegBackward0>) tensor(11024.8975, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11024.8974609375
tensor(11024.8975, grad_fn=<NegBackward0>) tensor(11024.8975, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11024.8955078125
tensor(11024.8975, grad_fn=<NegBackward0>) tensor(11024.8955, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11024.8984375
tensor(11024.8955, grad_fn=<NegBackward0>) tensor(11024.8984, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11024.8955078125
tensor(11024.8955, grad_fn=<NegBackward0>) tensor(11024.8955, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11024.892578125
tensor(11024.8955, grad_fn=<NegBackward0>) tensor(11024.8926, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11024.892578125
tensor(11024.8926, grad_fn=<NegBackward0>) tensor(11024.8926, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11024.8916015625
tensor(11024.8926, grad_fn=<NegBackward0>) tensor(11024.8916, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11024.890625
tensor(11024.8916, grad_fn=<NegBackward0>) tensor(11024.8906, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11024.900390625
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.9004, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11024.890625
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.8906, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11024.890625
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.8906, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11024.89453125
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.8945, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11024.890625
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.8906, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11024.8896484375
tensor(11024.8906, grad_fn=<NegBackward0>) tensor(11024.8896, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11024.890625
tensor(11024.8896, grad_fn=<NegBackward0>) tensor(11024.8906, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11024.876953125
tensor(11024.8896, grad_fn=<NegBackward0>) tensor(11024.8770, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11024.89453125
tensor(11024.8770, grad_fn=<NegBackward0>) tensor(11024.8945, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11024.8779296875
tensor(11024.8770, grad_fn=<NegBackward0>) tensor(11024.8779, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11024.87109375
tensor(11024.8770, grad_fn=<NegBackward0>) tensor(11024.8711, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11024.8701171875
tensor(11024.8711, grad_fn=<NegBackward0>) tensor(11024.8701, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11024.8740234375
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8740, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11024.8701171875
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8701, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11024.87109375
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8711, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11024.8701171875
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8701, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11024.87109375
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8711, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11024.8701171875
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8701, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11024.873046875
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8730, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11024.869140625
tensor(11024.8701, grad_fn=<NegBackward0>) tensor(11024.8691, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11024.8681640625
tensor(11024.8691, grad_fn=<NegBackward0>) tensor(11024.8682, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11024.8671875
tensor(11024.8682, grad_fn=<NegBackward0>) tensor(11024.8672, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11024.8671875
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8672, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11024.8681640625
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8682, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11024.8671875
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8672, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11024.8681640625
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8682, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11024.8681640625
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8682, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11024.8701171875
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8701, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11024.84375
tensor(11024.8672, grad_fn=<NegBackward0>) tensor(11024.8438, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11024.97265625
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.9727, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11024.84375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8438, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11024.9912109375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.9912, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11024.8447265625
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8447, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11024.84375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8438, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11024.84375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8438, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11024.84375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8438, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11024.849609375
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8496, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11024.8447265625
tensor(11024.8438, grad_fn=<NegBackward0>) tensor(11024.8447, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7351, 0.2649],
        [0.2119, 0.7881]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4675, 0.5325], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3065, 0.1047],
         [0.5976, 0.1939]],

        [[0.7246, 0.0995],
         [0.6418, 0.7268]],

        [[0.6862, 0.0912],
         [0.6767, 0.6621]],

        [[0.5824, 0.1014],
         [0.6503, 0.5227]],

        [[0.6651, 0.1021],
         [0.5336, 0.6363]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080725364594837
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.9061155834419254
Average Adjusted Rand Index: 0.9060921362777365
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21645.283203125
inf tensor(21645.2832, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11316.8916015625
tensor(21645.2832, grad_fn=<NegBackward0>) tensor(11316.8916, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11315.5576171875
tensor(11316.8916, grad_fn=<NegBackward0>) tensor(11315.5576, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11314.416015625
tensor(11315.5576, grad_fn=<NegBackward0>) tensor(11314.4160, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11313.431640625
tensor(11314.4160, grad_fn=<NegBackward0>) tensor(11313.4316, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11312.4794921875
tensor(11313.4316, grad_fn=<NegBackward0>) tensor(11312.4795, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11311.45703125
tensor(11312.4795, grad_fn=<NegBackward0>) tensor(11311.4570, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11310.9638671875
tensor(11311.4570, grad_fn=<NegBackward0>) tensor(11310.9639, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11310.7734375
tensor(11310.9639, grad_fn=<NegBackward0>) tensor(11310.7734, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11310.6650390625
tensor(11310.7734, grad_fn=<NegBackward0>) tensor(11310.6650, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11310.5830078125
tensor(11310.6650, grad_fn=<NegBackward0>) tensor(11310.5830, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11310.5263671875
tensor(11310.5830, grad_fn=<NegBackward0>) tensor(11310.5264, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11310.4892578125
tensor(11310.5264, grad_fn=<NegBackward0>) tensor(11310.4893, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11310.4609375
tensor(11310.4893, grad_fn=<NegBackward0>) tensor(11310.4609, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11310.443359375
tensor(11310.4609, grad_fn=<NegBackward0>) tensor(11310.4434, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11310.431640625
tensor(11310.4434, grad_fn=<NegBackward0>) tensor(11310.4316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11310.4248046875
tensor(11310.4316, grad_fn=<NegBackward0>) tensor(11310.4248, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11310.4189453125
tensor(11310.4248, grad_fn=<NegBackward0>) tensor(11310.4189, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11310.4140625
tensor(11310.4189, grad_fn=<NegBackward0>) tensor(11310.4141, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11310.4111328125
tensor(11310.4141, grad_fn=<NegBackward0>) tensor(11310.4111, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11310.4091796875
tensor(11310.4111, grad_fn=<NegBackward0>) tensor(11310.4092, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11310.408203125
tensor(11310.4092, grad_fn=<NegBackward0>) tensor(11310.4082, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11310.40625
tensor(11310.4082, grad_fn=<NegBackward0>) tensor(11310.4062, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11310.4052734375
tensor(11310.4062, grad_fn=<NegBackward0>) tensor(11310.4053, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11310.4033203125
tensor(11310.4053, grad_fn=<NegBackward0>) tensor(11310.4033, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11310.40234375
tensor(11310.4033, grad_fn=<NegBackward0>) tensor(11310.4023, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11310.400390625
tensor(11310.4023, grad_fn=<NegBackward0>) tensor(11310.4004, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11310.4013671875
tensor(11310.4004, grad_fn=<NegBackward0>) tensor(11310.4014, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11310.4013671875
tensor(11310.4004, grad_fn=<NegBackward0>) tensor(11310.4014, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -11310.3984375
tensor(11310.4004, grad_fn=<NegBackward0>) tensor(11310.3984, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11310.3984375
tensor(11310.3984, grad_fn=<NegBackward0>) tensor(11310.3984, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11310.3974609375
tensor(11310.3984, grad_fn=<NegBackward0>) tensor(11310.3975, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11310.396484375
tensor(11310.3975, grad_fn=<NegBackward0>) tensor(11310.3965, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11310.4033203125
tensor(11310.3965, grad_fn=<NegBackward0>) tensor(11310.4033, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11310.3974609375
tensor(11310.3965, grad_fn=<NegBackward0>) tensor(11310.3975, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11310.396484375
tensor(11310.3965, grad_fn=<NegBackward0>) tensor(11310.3965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11310.3955078125
tensor(11310.3965, grad_fn=<NegBackward0>) tensor(11310.3955, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11310.3955078125
tensor(11310.3955, grad_fn=<NegBackward0>) tensor(11310.3955, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11310.396484375
tensor(11310.3955, grad_fn=<NegBackward0>) tensor(11310.3965, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11310.39453125
tensor(11310.3955, grad_fn=<NegBackward0>) tensor(11310.3945, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11310.39453125
tensor(11310.3945, grad_fn=<NegBackward0>) tensor(11310.3945, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11310.39453125
tensor(11310.3945, grad_fn=<NegBackward0>) tensor(11310.3945, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11310.39453125
tensor(11310.3945, grad_fn=<NegBackward0>) tensor(11310.3945, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11310.3935546875
tensor(11310.3945, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11310.3935546875
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11310.39453125
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3945, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11310.396484375
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3965, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11310.3935546875
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11310.412109375
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.4121, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11310.3935546875
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11310.41796875
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.4180, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11310.3935546875
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11310.392578125
tensor(11310.3936, grad_fn=<NegBackward0>) tensor(11310.3926, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11310.392578125
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3926, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11310.3935546875
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11310.392578125
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3926, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11310.3935546875
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11310.3935546875
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11310.3935546875
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11310.3916015625
tensor(11310.3926, grad_fn=<NegBackward0>) tensor(11310.3916, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11310.3916015625
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3916, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11310.3935546875
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11310.3935546875
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3936, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11310.392578125
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3926, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11310.3955078125
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3955, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11310.392578125
tensor(11310.3916, grad_fn=<NegBackward0>) tensor(11310.3926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[4.6903e-05, 9.9995e-01],
        [1.7715e-02, 9.8229e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4795, 0.5205], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1895, 0.1793],
         [0.6120, 0.1704]],

        [[0.5470, 0.0580],
         [0.6252, 0.5903]],

        [[0.6699, 0.0476],
         [0.6666, 0.5721]],

        [[0.6145, 0.0850],
         [0.6092, 0.5835]],

        [[0.7003, 0.2467],
         [0.6694, 0.5886]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 29
Adjusted Rand Index: 0.1681655446913764
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.012669014218733052
Average Adjusted Rand Index: 0.03281436550179724
[0.9061155834419254, 0.012669014218733052] [0.9060921362777365, 0.03281436550179724] [11024.859375, 11310.392578125]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11022.94215580235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22367.9375
inf tensor(22367.9375, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11231.642578125
tensor(22367.9375, grad_fn=<NegBackward0>) tensor(11231.6426, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11231.236328125
tensor(11231.6426, grad_fn=<NegBackward0>) tensor(11231.2363, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11231.0068359375
tensor(11231.2363, grad_fn=<NegBackward0>) tensor(11231.0068, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11230.630859375
tensor(11231.0068, grad_fn=<NegBackward0>) tensor(11230.6309, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11230.0498046875
tensor(11230.6309, grad_fn=<NegBackward0>) tensor(11230.0498, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11229.0537109375
tensor(11230.0498, grad_fn=<NegBackward0>) tensor(11229.0537, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11227.271484375
tensor(11229.0537, grad_fn=<NegBackward0>) tensor(11227.2715, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11085.359375
tensor(11227.2715, grad_fn=<NegBackward0>) tensor(11085.3594, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11035.9990234375
tensor(11085.3594, grad_fn=<NegBackward0>) tensor(11035.9990, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11030.169921875
tensor(11035.9990, grad_fn=<NegBackward0>) tensor(11030.1699, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11029.9697265625
tensor(11030.1699, grad_fn=<NegBackward0>) tensor(11029.9697, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11029.8505859375
tensor(11029.9697, grad_fn=<NegBackward0>) tensor(11029.8506, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11029.7900390625
tensor(11029.8506, grad_fn=<NegBackward0>) tensor(11029.7900, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11029.7490234375
tensor(11029.7900, grad_fn=<NegBackward0>) tensor(11029.7490, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11029.7109375
tensor(11029.7490, grad_fn=<NegBackward0>) tensor(11029.7109, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11029.6884765625
tensor(11029.7109, grad_fn=<NegBackward0>) tensor(11029.6885, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11029.6728515625
tensor(11029.6885, grad_fn=<NegBackward0>) tensor(11029.6729, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11029.658203125
tensor(11029.6729, grad_fn=<NegBackward0>) tensor(11029.6582, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11029.646484375
tensor(11029.6582, grad_fn=<NegBackward0>) tensor(11029.6465, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11029.6279296875
tensor(11029.6465, grad_fn=<NegBackward0>) tensor(11029.6279, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11029.611328125
tensor(11029.6279, grad_fn=<NegBackward0>) tensor(11029.6113, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11029.5947265625
tensor(11029.6113, grad_fn=<NegBackward0>) tensor(11029.5947, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11029.5810546875
tensor(11029.5947, grad_fn=<NegBackward0>) tensor(11029.5811, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11029.56640625
tensor(11029.5811, grad_fn=<NegBackward0>) tensor(11029.5664, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11029.537109375
tensor(11029.5664, grad_fn=<NegBackward0>) tensor(11029.5371, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11029.4951171875
tensor(11029.5371, grad_fn=<NegBackward0>) tensor(11029.4951, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11029.4296875
tensor(11029.4951, grad_fn=<NegBackward0>) tensor(11029.4297, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11029.3720703125
tensor(11029.4297, grad_fn=<NegBackward0>) tensor(11029.3721, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11029.3251953125
tensor(11029.3721, grad_fn=<NegBackward0>) tensor(11029.3252, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11029.10546875
tensor(11029.3252, grad_fn=<NegBackward0>) tensor(11029.1055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11028.9453125
tensor(11029.1055, grad_fn=<NegBackward0>) tensor(11028.9453, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11028.9375
tensor(11028.9453, grad_fn=<NegBackward0>) tensor(11028.9375, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11028.9306640625
tensor(11028.9375, grad_fn=<NegBackward0>) tensor(11028.9307, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11028.9248046875
tensor(11028.9307, grad_fn=<NegBackward0>) tensor(11028.9248, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11028.9150390625
tensor(11028.9248, grad_fn=<NegBackward0>) tensor(11028.9150, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11028.9072265625
tensor(11028.9150, grad_fn=<NegBackward0>) tensor(11028.9072, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11028.4814453125
tensor(11028.9072, grad_fn=<NegBackward0>) tensor(11028.4814, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11024.2548828125
tensor(11028.4814, grad_fn=<NegBackward0>) tensor(11024.2549, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11024.216796875
tensor(11024.2549, grad_fn=<NegBackward0>) tensor(11024.2168, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11024.2080078125
tensor(11024.2168, grad_fn=<NegBackward0>) tensor(11024.2080, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11024.2041015625
tensor(11024.2080, grad_fn=<NegBackward0>) tensor(11024.2041, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11024.201171875
tensor(11024.2041, grad_fn=<NegBackward0>) tensor(11024.2012, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11024.19921875
tensor(11024.2012, grad_fn=<NegBackward0>) tensor(11024.1992, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11024.1982421875
tensor(11024.1992, grad_fn=<NegBackward0>) tensor(11024.1982, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11024.197265625
tensor(11024.1982, grad_fn=<NegBackward0>) tensor(11024.1973, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11024.1953125
tensor(11024.1973, grad_fn=<NegBackward0>) tensor(11024.1953, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11024.19140625
tensor(11024.1953, grad_fn=<NegBackward0>) tensor(11024.1914, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11024.1875
tensor(11024.1914, grad_fn=<NegBackward0>) tensor(11024.1875, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11024.1875
tensor(11024.1875, grad_fn=<NegBackward0>) tensor(11024.1875, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11024.1875
tensor(11024.1875, grad_fn=<NegBackward0>) tensor(11024.1875, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11024.189453125
tensor(11024.1875, grad_fn=<NegBackward0>) tensor(11024.1895, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11024.1875
tensor(11024.1875, grad_fn=<NegBackward0>) tensor(11024.1875, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11024.185546875
tensor(11024.1875, grad_fn=<NegBackward0>) tensor(11024.1855, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11024.1845703125
tensor(11024.1855, grad_fn=<NegBackward0>) tensor(11024.1846, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11024.1875
tensor(11024.1846, grad_fn=<NegBackward0>) tensor(11024.1875, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11024.1826171875
tensor(11024.1846, grad_fn=<NegBackward0>) tensor(11024.1826, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11024.1826171875
tensor(11024.1826, grad_fn=<NegBackward0>) tensor(11024.1826, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11024.1806640625
tensor(11024.1826, grad_fn=<NegBackward0>) tensor(11024.1807, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11024.1806640625
tensor(11024.1807, grad_fn=<NegBackward0>) tensor(11024.1807, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11024.1796875
tensor(11024.1807, grad_fn=<NegBackward0>) tensor(11024.1797, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11024.1806640625
tensor(11024.1797, grad_fn=<NegBackward0>) tensor(11024.1807, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11024.1787109375
tensor(11024.1797, grad_fn=<NegBackward0>) tensor(11024.1787, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11024.1796875
tensor(11024.1787, grad_fn=<NegBackward0>) tensor(11024.1797, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11024.177734375
tensor(11024.1787, grad_fn=<NegBackward0>) tensor(11024.1777, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11024.1787109375
tensor(11024.1777, grad_fn=<NegBackward0>) tensor(11024.1787, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11024.177734375
tensor(11024.1777, grad_fn=<NegBackward0>) tensor(11024.1777, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11024.1767578125
tensor(11024.1777, grad_fn=<NegBackward0>) tensor(11024.1768, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11024.1767578125
tensor(11024.1768, grad_fn=<NegBackward0>) tensor(11024.1768, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11024.1767578125
tensor(11024.1768, grad_fn=<NegBackward0>) tensor(11024.1768, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11024.1748046875
tensor(11024.1768, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11024.1748046875
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11024.17578125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1758, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11024.1748046875
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11024.1767578125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1768, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11024.1748046875
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11024.17578125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1758, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11024.17578125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1758, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11024.1748046875
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11024.1767578125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1768, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11024.1748046875
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11024.173828125
tensor(11024.1748, grad_fn=<NegBackward0>) tensor(11024.1738, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11024.17578125
tensor(11024.1738, grad_fn=<NegBackward0>) tensor(11024.1758, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11024.1748046875
tensor(11024.1738, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11024.17578125
tensor(11024.1738, grad_fn=<NegBackward0>) tensor(11024.1758, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11024.1748046875
tensor(11024.1738, grad_fn=<NegBackward0>) tensor(11024.1748, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11024.181640625
tensor(11024.1738, grad_fn=<NegBackward0>) tensor(11024.1816, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.6712, 0.3288],
        [0.3092, 0.6908]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9722, 0.0278], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1856, 0.0645],
         [0.6791, 0.3023]],

        [[0.5714, 0.1010],
         [0.6361, 0.5155]],

        [[0.7148, 0.0933],
         [0.6770, 0.6546]],

        [[0.6020, 0.0999],
         [0.6066, 0.5663]],

        [[0.7189, 0.0964],
         [0.5212, 0.6013]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.008844375641612072
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.6137937154856057
Average Adjusted Rand Index: 0.7006714529747413
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25741.16015625
inf tensor(25741.1602, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11232.373046875
tensor(25741.1602, grad_fn=<NegBackward0>) tensor(11232.3730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11231.029296875
tensor(11232.3730, grad_fn=<NegBackward0>) tensor(11231.0293, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11230.759765625
tensor(11231.0293, grad_fn=<NegBackward0>) tensor(11230.7598, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11230.4033203125
tensor(11230.7598, grad_fn=<NegBackward0>) tensor(11230.4033, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11229.494140625
tensor(11230.4033, grad_fn=<NegBackward0>) tensor(11229.4941, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11228.197265625
tensor(11229.4941, grad_fn=<NegBackward0>) tensor(11228.1973, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11227.576171875
tensor(11228.1973, grad_fn=<NegBackward0>) tensor(11227.5762, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11227.3818359375
tensor(11227.5762, grad_fn=<NegBackward0>) tensor(11227.3818, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11226.9794921875
tensor(11227.3818, grad_fn=<NegBackward0>) tensor(11226.9795, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11221.2041015625
tensor(11226.9795, grad_fn=<NegBackward0>) tensor(11221.2041, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11220.10546875
tensor(11221.2041, grad_fn=<NegBackward0>) tensor(11220.1055, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11087.1015625
tensor(11220.1055, grad_fn=<NegBackward0>) tensor(11087.1016, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11073.349609375
tensor(11087.1016, grad_fn=<NegBackward0>) tensor(11073.3496, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11064.5068359375
tensor(11073.3496, grad_fn=<NegBackward0>) tensor(11064.5068, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11051.732421875
tensor(11064.5068, grad_fn=<NegBackward0>) tensor(11051.7324, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11037.2021484375
tensor(11051.7324, grad_fn=<NegBackward0>) tensor(11037.2021, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11034.9423828125
tensor(11037.2021, grad_fn=<NegBackward0>) tensor(11034.9424, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11027.287109375
tensor(11034.9424, grad_fn=<NegBackward0>) tensor(11027.2871, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11023.1123046875
tensor(11027.2871, grad_fn=<NegBackward0>) tensor(11023.1123, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11003.3671875
tensor(11023.1123, grad_fn=<NegBackward0>) tensor(11003.3672, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10988.859375
tensor(11003.3672, grad_fn=<NegBackward0>) tensor(10988.8594, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10988.7841796875
tensor(10988.8594, grad_fn=<NegBackward0>) tensor(10988.7842, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10988.78125
tensor(10988.7842, grad_fn=<NegBackward0>) tensor(10988.7812, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10988.7626953125
tensor(10988.7812, grad_fn=<NegBackward0>) tensor(10988.7627, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10988.75390625
tensor(10988.7627, grad_fn=<NegBackward0>) tensor(10988.7539, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10986.572265625
tensor(10988.7539, grad_fn=<NegBackward0>) tensor(10986.5723, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10986.5693359375
tensor(10986.5723, grad_fn=<NegBackward0>) tensor(10986.5693, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10986.548828125
tensor(10986.5693, grad_fn=<NegBackward0>) tensor(10986.5488, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10986.5458984375
tensor(10986.5488, grad_fn=<NegBackward0>) tensor(10986.5459, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10986.5439453125
tensor(10986.5459, grad_fn=<NegBackward0>) tensor(10986.5439, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10986.5283203125
tensor(10986.5439, grad_fn=<NegBackward0>) tensor(10986.5283, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10986.5
tensor(10986.5283, grad_fn=<NegBackward0>) tensor(10986.5000, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10986.501953125
tensor(10986.5000, grad_fn=<NegBackward0>) tensor(10986.5020, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10986.49609375
tensor(10986.5000, grad_fn=<NegBackward0>) tensor(10986.4961, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10986.49609375
tensor(10986.4961, grad_fn=<NegBackward0>) tensor(10986.4961, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10986.5
tensor(10986.4961, grad_fn=<NegBackward0>) tensor(10986.5000, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10986.4951171875
tensor(10986.4961, grad_fn=<NegBackward0>) tensor(10986.4951, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10986.48046875
tensor(10986.4951, grad_fn=<NegBackward0>) tensor(10986.4805, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10986.4794921875
tensor(10986.4805, grad_fn=<NegBackward0>) tensor(10986.4795, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10986.4794921875
tensor(10986.4795, grad_fn=<NegBackward0>) tensor(10986.4795, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10986.4697265625
tensor(10986.4795, grad_fn=<NegBackward0>) tensor(10986.4697, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10986.47265625
tensor(10986.4697, grad_fn=<NegBackward0>) tensor(10986.4727, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10986.443359375
tensor(10986.4697, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10986.443359375
tensor(10986.4434, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10986.443359375
tensor(10986.4434, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10986.443359375
tensor(10986.4434, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10986.44921875
tensor(10986.4434, grad_fn=<NegBackward0>) tensor(10986.4492, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10986.4423828125
tensor(10986.4434, grad_fn=<NegBackward0>) tensor(10986.4424, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10986.4423828125
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4424, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10986.443359375
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10986.4443359375
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4443, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10986.443359375
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -10986.4453125
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4453, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -10986.443359375
tensor(10986.4424, grad_fn=<NegBackward0>) tensor(10986.4434, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.7563, 0.2437],
        [0.2737, 0.7263]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5734, 0.4266], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.1060],
         [0.5604, 0.2933]],

        [[0.6925, 0.0981],
         [0.7102, 0.6201]],

        [[0.6759, 0.0933],
         [0.6999, 0.6400]],

        [[0.6023, 0.0998],
         [0.6800, 0.6526]],

        [[0.5323, 0.0959],
         [0.5008, 0.6280]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.9137620885145475
Average Adjusted Rand Index: 0.9145788103778767
[0.6137937154856057, 0.9137620885145475] [0.7006714529747413, 0.9145788103778767] [11024.181640625, 10986.443359375]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11241.377517962863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21829.20703125
inf tensor(21829.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11479.4189453125
tensor(21829.2070, grad_fn=<NegBackward0>) tensor(11479.4189, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11478.6748046875
tensor(11479.4189, grad_fn=<NegBackward0>) tensor(11478.6748, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11477.96484375
tensor(11478.6748, grad_fn=<NegBackward0>) tensor(11477.9648, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11476.94921875
tensor(11477.9648, grad_fn=<NegBackward0>) tensor(11476.9492, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11474.9189453125
tensor(11476.9492, grad_fn=<NegBackward0>) tensor(11474.9189, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11473.8076171875
tensor(11474.9189, grad_fn=<NegBackward0>) tensor(11473.8076, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11368.83203125
tensor(11473.8076, grad_fn=<NegBackward0>) tensor(11368.8320, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11225.427734375
tensor(11368.8320, grad_fn=<NegBackward0>) tensor(11225.4277, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11225.1533203125
tensor(11225.4277, grad_fn=<NegBackward0>) tensor(11225.1533, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11224.9443359375
tensor(11225.1533, grad_fn=<NegBackward0>) tensor(11224.9443, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11224.86328125
tensor(11224.9443, grad_fn=<NegBackward0>) tensor(11224.8633, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11224.8369140625
tensor(11224.8633, grad_fn=<NegBackward0>) tensor(11224.8369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11224.822265625
tensor(11224.8369, grad_fn=<NegBackward0>) tensor(11224.8223, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11224.806640625
tensor(11224.8223, grad_fn=<NegBackward0>) tensor(11224.8066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11224.775390625
tensor(11224.8066, grad_fn=<NegBackward0>) tensor(11224.7754, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11224.7685546875
tensor(11224.7754, grad_fn=<NegBackward0>) tensor(11224.7686, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11224.763671875
tensor(11224.7686, grad_fn=<NegBackward0>) tensor(11224.7637, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11224.759765625
tensor(11224.7637, grad_fn=<NegBackward0>) tensor(11224.7598, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11224.755859375
tensor(11224.7598, grad_fn=<NegBackward0>) tensor(11224.7559, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11224.75390625
tensor(11224.7559, grad_fn=<NegBackward0>) tensor(11224.7539, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11224.7470703125
tensor(11224.7539, grad_fn=<NegBackward0>) tensor(11224.7471, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11224.6572265625
tensor(11224.7471, grad_fn=<NegBackward0>) tensor(11224.6572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11223.6396484375
tensor(11224.6572, grad_fn=<NegBackward0>) tensor(11223.6396, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11223.634765625
tensor(11223.6396, grad_fn=<NegBackward0>) tensor(11223.6348, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11223.61328125
tensor(11223.6348, grad_fn=<NegBackward0>) tensor(11223.6133, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11223.548828125
tensor(11223.6133, grad_fn=<NegBackward0>) tensor(11223.5488, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11223.548828125
tensor(11223.5488, grad_fn=<NegBackward0>) tensor(11223.5488, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11223.5439453125
tensor(11223.5488, grad_fn=<NegBackward0>) tensor(11223.5439, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11223.5419921875
tensor(11223.5439, grad_fn=<NegBackward0>) tensor(11223.5420, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11223.5419921875
tensor(11223.5420, grad_fn=<NegBackward0>) tensor(11223.5420, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11223.5419921875
tensor(11223.5420, grad_fn=<NegBackward0>) tensor(11223.5420, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11223.541015625
tensor(11223.5420, grad_fn=<NegBackward0>) tensor(11223.5410, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11223.541015625
tensor(11223.5410, grad_fn=<NegBackward0>) tensor(11223.5410, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11223.541015625
tensor(11223.5410, grad_fn=<NegBackward0>) tensor(11223.5410, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11223.5439453125
tensor(11223.5410, grad_fn=<NegBackward0>) tensor(11223.5439, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11223.5400390625
tensor(11223.5410, grad_fn=<NegBackward0>) tensor(11223.5400, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11223.5400390625
tensor(11223.5400, grad_fn=<NegBackward0>) tensor(11223.5400, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11223.5400390625
tensor(11223.5400, grad_fn=<NegBackward0>) tensor(11223.5400, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11223.5390625
tensor(11223.5400, grad_fn=<NegBackward0>) tensor(11223.5391, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11223.548828125
tensor(11223.5391, grad_fn=<NegBackward0>) tensor(11223.5488, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11223.5390625
tensor(11223.5391, grad_fn=<NegBackward0>) tensor(11223.5391, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11223.5517578125
tensor(11223.5391, grad_fn=<NegBackward0>) tensor(11223.5518, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11223.5380859375
tensor(11223.5391, grad_fn=<NegBackward0>) tensor(11223.5381, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11223.51953125
tensor(11223.5381, grad_fn=<NegBackward0>) tensor(11223.5195, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11223.5205078125
tensor(11223.5195, grad_fn=<NegBackward0>) tensor(11223.5205, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11223.5478515625
tensor(11223.5195, grad_fn=<NegBackward0>) tensor(11223.5479, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11223.5185546875
tensor(11223.5195, grad_fn=<NegBackward0>) tensor(11223.5186, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11223.5185546875
tensor(11223.5186, grad_fn=<NegBackward0>) tensor(11223.5186, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11223.5166015625
tensor(11223.5186, grad_fn=<NegBackward0>) tensor(11223.5166, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11223.5185546875
tensor(11223.5166, grad_fn=<NegBackward0>) tensor(11223.5186, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11223.5419921875
tensor(11223.5166, grad_fn=<NegBackward0>) tensor(11223.5420, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11223.515625
tensor(11223.5166, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11223.5302734375
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5303, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11223.517578125
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5176, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11223.5166015625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5166, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11223.517578125
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5176, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11223.515625
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5156, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11223.54296875
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5430, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11223.51171875
tensor(11223.5156, grad_fn=<NegBackward0>) tensor(11223.5117, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11223.51171875
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5117, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11223.5126953125
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5127, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11223.5126953125
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5127, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11223.5166015625
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5166, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11223.509765625
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5098, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11223.5126953125
tensor(11223.5098, grad_fn=<NegBackward0>) tensor(11223.5127, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11223.5009765625
tensor(11223.5098, grad_fn=<NegBackward0>) tensor(11223.5010, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11223.51953125
tensor(11223.5010, grad_fn=<NegBackward0>) tensor(11223.5195, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11223.4951171875
tensor(11223.5010, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11223.5517578125
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.5518, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11223.4951171875
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11223.4951171875
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11223.49609375
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.4961, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11223.49609375
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.4961, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11223.5654296875
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.5654, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11223.494140625
tensor(11223.4951, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11223.494140625
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11223.49609375
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4961, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11223.4951171875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11223.51171875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.5117, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11223.494140625
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11223.494140625
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11223.4951171875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11223.4951171875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11223.498046875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11223.494140625
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11223.5029296875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.5029, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11223.4951171875
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4951, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11223.4931640625
tensor(11223.4941, grad_fn=<NegBackward0>) tensor(11223.4932, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11223.49609375
tensor(11223.4932, grad_fn=<NegBackward0>) tensor(11223.4961, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11223.494140625
tensor(11223.4932, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11223.494140625
tensor(11223.4932, grad_fn=<NegBackward0>) tensor(11223.4941, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7114, 0.2886],
        [0.2399, 0.7601]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5210, 0.4790], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3011, 0.0998],
         [0.5911, 0.2053]],

        [[0.6269, 0.0990],
         [0.7151, 0.6474]],

        [[0.6946, 0.1017],
         [0.7269, 0.7306]],

        [[0.5056, 0.1043],
         [0.5006, 0.6039]],

        [[0.7115, 0.1056],
         [0.6439, 0.5689]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9603204383279997
Average Adjusted Rand Index: 0.9604838454161339
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20759.47265625
inf tensor(20759.4727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11480.16796875
tensor(20759.4727, grad_fn=<NegBackward0>) tensor(11480.1680, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11479.7255859375
tensor(11480.1680, grad_fn=<NegBackward0>) tensor(11479.7256, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11478.9755859375
tensor(11479.7256, grad_fn=<NegBackward0>) tensor(11478.9756, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11477.166015625
tensor(11478.9756, grad_fn=<NegBackward0>) tensor(11477.1660, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11476.087890625
tensor(11477.1660, grad_fn=<NegBackward0>) tensor(11476.0879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11475.3798828125
tensor(11476.0879, grad_fn=<NegBackward0>) tensor(11475.3799, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11474.248046875
tensor(11475.3799, grad_fn=<NegBackward0>) tensor(11474.2480, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11473.1865234375
tensor(11474.2480, grad_fn=<NegBackward0>) tensor(11473.1865, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11422.3408203125
tensor(11473.1865, grad_fn=<NegBackward0>) tensor(11422.3408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11226.275390625
tensor(11422.3408, grad_fn=<NegBackward0>) tensor(11226.2754, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11225.4013671875
tensor(11226.2754, grad_fn=<NegBackward0>) tensor(11225.4014, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11225.0478515625
tensor(11225.4014, grad_fn=<NegBackward0>) tensor(11225.0479, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11224.8935546875
tensor(11225.0479, grad_fn=<NegBackward0>) tensor(11224.8936, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11224.8349609375
tensor(11224.8936, grad_fn=<NegBackward0>) tensor(11224.8350, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11224.75390625
tensor(11224.8350, grad_fn=<NegBackward0>) tensor(11224.7539, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11223.6826171875
tensor(11224.7539, grad_fn=<NegBackward0>) tensor(11223.6826, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11223.6611328125
tensor(11223.6826, grad_fn=<NegBackward0>) tensor(11223.6611, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11223.640625
tensor(11223.6611, grad_fn=<NegBackward0>) tensor(11223.6406, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11223.625
tensor(11223.6406, grad_fn=<NegBackward0>) tensor(11223.6250, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11223.61328125
tensor(11223.6250, grad_fn=<NegBackward0>) tensor(11223.6133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11223.6015625
tensor(11223.6133, grad_fn=<NegBackward0>) tensor(11223.6016, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11223.5927734375
tensor(11223.6016, grad_fn=<NegBackward0>) tensor(11223.5928, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11223.5859375
tensor(11223.5928, grad_fn=<NegBackward0>) tensor(11223.5859, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11223.5673828125
tensor(11223.5859, grad_fn=<NegBackward0>) tensor(11223.5674, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11223.5458984375
tensor(11223.5674, grad_fn=<NegBackward0>) tensor(11223.5459, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11223.5341796875
tensor(11223.5459, grad_fn=<NegBackward0>) tensor(11223.5342, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11223.5302734375
tensor(11223.5342, grad_fn=<NegBackward0>) tensor(11223.5303, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11223.52734375
tensor(11223.5303, grad_fn=<NegBackward0>) tensor(11223.5273, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11223.5244140625
tensor(11223.5273, grad_fn=<NegBackward0>) tensor(11223.5244, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11223.5224609375
tensor(11223.5244, grad_fn=<NegBackward0>) tensor(11223.5225, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11223.517578125
tensor(11223.5225, grad_fn=<NegBackward0>) tensor(11223.5176, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11223.5146484375
tensor(11223.5176, grad_fn=<NegBackward0>) tensor(11223.5146, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11223.513671875
tensor(11223.5146, grad_fn=<NegBackward0>) tensor(11223.5137, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11223.5126953125
tensor(11223.5137, grad_fn=<NegBackward0>) tensor(11223.5127, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11223.51171875
tensor(11223.5127, grad_fn=<NegBackward0>) tensor(11223.5117, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11223.51171875
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5117, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11223.509765625
tensor(11223.5117, grad_fn=<NegBackward0>) tensor(11223.5098, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11223.509765625
tensor(11223.5098, grad_fn=<NegBackward0>) tensor(11223.5098, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11223.5087890625
tensor(11223.5098, grad_fn=<NegBackward0>) tensor(11223.5088, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11223.5078125
tensor(11223.5088, grad_fn=<NegBackward0>) tensor(11223.5078, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11223.5087890625
tensor(11223.5078, grad_fn=<NegBackward0>) tensor(11223.5088, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11223.505859375
tensor(11223.5078, grad_fn=<NegBackward0>) tensor(11223.5059, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11223.505859375
tensor(11223.5059, grad_fn=<NegBackward0>) tensor(11223.5059, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11223.5048828125
tensor(11223.5059, grad_fn=<NegBackward0>) tensor(11223.5049, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11223.5029296875
tensor(11223.5049, grad_fn=<NegBackward0>) tensor(11223.5029, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11223.501953125
tensor(11223.5029, grad_fn=<NegBackward0>) tensor(11223.5020, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11223.5009765625
tensor(11223.5020, grad_fn=<NegBackward0>) tensor(11223.5010, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11223.501953125
tensor(11223.5010, grad_fn=<NegBackward0>) tensor(11223.5020, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11223.5
tensor(11223.5010, grad_fn=<NegBackward0>) tensor(11223.5000, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11223.5009765625
tensor(11223.5000, grad_fn=<NegBackward0>) tensor(11223.5010, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11223.4990234375
tensor(11223.5000, grad_fn=<NegBackward0>) tensor(11223.4990, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11223.4990234375
tensor(11223.4990, grad_fn=<NegBackward0>) tensor(11223.4990, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11223.498046875
tensor(11223.4990, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11223.498046875
tensor(11223.4980, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11223.498046875
tensor(11223.4980, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11223.498046875
tensor(11223.4980, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11223.498046875
tensor(11223.4980, grad_fn=<NegBackward0>) tensor(11223.4980, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11223.49609375
tensor(11223.4980, grad_fn=<NegBackward0>) tensor(11223.4961, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11223.4970703125
tensor(11223.4961, grad_fn=<NegBackward0>) tensor(11223.4971, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11223.5009765625
tensor(11223.4961, grad_fn=<NegBackward0>) tensor(11223.5010, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11223.4931640625
tensor(11223.4961, grad_fn=<NegBackward0>) tensor(11223.4932, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11223.4921875
tensor(11223.4932, grad_fn=<NegBackward0>) tensor(11223.4922, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11223.4912109375
tensor(11223.4922, grad_fn=<NegBackward0>) tensor(11223.4912, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11223.490234375
tensor(11223.4912, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11223.4921875
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4922, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11223.4931640625
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4932, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11223.490234375
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11223.4892578125
tensor(11223.4902, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11223.490234375
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11223.4912109375
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4912, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11223.490234375
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11223.4921875
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4922, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11223.4892578125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11223.4892578125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11223.490234375
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11223.4892578125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11223.4892578125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11223.4892578125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11223.490234375
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4902, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11223.48828125
tensor(11223.4893, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11223.4892578125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11223.4892578125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11223.5224609375
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.5225, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11223.4892578125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4893, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11223.4931640625
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4932, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11223.48828125
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.4883, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11223.5224609375
tensor(11223.4883, grad_fn=<NegBackward0>) tensor(11223.5225, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7603, 0.2397],
        [0.2887, 0.7113]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4793, 0.5207], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2052, 0.0999],
         [0.6440, 0.3012]],

        [[0.6320, 0.0990],
         [0.6854, 0.7220]],

        [[0.6204, 0.1017],
         [0.7135, 0.6142]],

        [[0.5602, 0.1045],
         [0.5746, 0.7131]],

        [[0.5915, 0.1056],
         [0.6960, 0.7209]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9603204383279997
Average Adjusted Rand Index: 0.9604838454161339
[0.9603204383279997, 0.9603204383279997] [0.9604838454161339, 0.9604838454161339] [11223.494140625, 11223.48828125]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11297.042782787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24172.33984375
inf tensor(24172.3398, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11542.361328125
tensor(24172.3398, grad_fn=<NegBackward0>) tensor(11542.3613, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11538.03125
tensor(11542.3613, grad_fn=<NegBackward0>) tensor(11538.0312, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11532.794921875
tensor(11538.0312, grad_fn=<NegBackward0>) tensor(11532.7949, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11530.6611328125
tensor(11532.7949, grad_fn=<NegBackward0>) tensor(11530.6611, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11522.2724609375
tensor(11530.6611, grad_fn=<NegBackward0>) tensor(11522.2725, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11463.041015625
tensor(11522.2725, grad_fn=<NegBackward0>) tensor(11463.0410, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11436.5439453125
tensor(11463.0410, grad_fn=<NegBackward0>) tensor(11436.5439, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11396.763671875
tensor(11436.5439, grad_fn=<NegBackward0>) tensor(11396.7637, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11366.896484375
tensor(11396.7637, grad_fn=<NegBackward0>) tensor(11366.8965, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11362.087890625
tensor(11366.8965, grad_fn=<NegBackward0>) tensor(11362.0879, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11355.521484375
tensor(11362.0879, grad_fn=<NegBackward0>) tensor(11355.5215, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11355.3662109375
tensor(11355.5215, grad_fn=<NegBackward0>) tensor(11355.3662, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11355.2060546875
tensor(11355.3662, grad_fn=<NegBackward0>) tensor(11355.2061, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11347.3408203125
tensor(11355.2061, grad_fn=<NegBackward0>) tensor(11347.3408, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11344.3310546875
tensor(11347.3408, grad_fn=<NegBackward0>) tensor(11344.3311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11344.212890625
tensor(11344.3311, grad_fn=<NegBackward0>) tensor(11344.2129, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11344.169921875
tensor(11344.2129, grad_fn=<NegBackward0>) tensor(11344.1699, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11344.1328125
tensor(11344.1699, grad_fn=<NegBackward0>) tensor(11344.1328, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11344.1171875
tensor(11344.1328, grad_fn=<NegBackward0>) tensor(11344.1172, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11344.10546875
tensor(11344.1172, grad_fn=<NegBackward0>) tensor(11344.1055, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11344.0927734375
tensor(11344.1055, grad_fn=<NegBackward0>) tensor(11344.0928, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11344.08203125
tensor(11344.0928, grad_fn=<NegBackward0>) tensor(11344.0820, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11344.0693359375
tensor(11344.0820, grad_fn=<NegBackward0>) tensor(11344.0693, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11344.05859375
tensor(11344.0693, grad_fn=<NegBackward0>) tensor(11344.0586, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11343.8916015625
tensor(11344.0586, grad_fn=<NegBackward0>) tensor(11343.8916, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11343.876953125
tensor(11343.8916, grad_fn=<NegBackward0>) tensor(11343.8770, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11343.818359375
tensor(11343.8770, grad_fn=<NegBackward0>) tensor(11343.8184, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11343.8076171875
tensor(11343.8184, grad_fn=<NegBackward0>) tensor(11343.8076, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11343.8056640625
tensor(11343.8076, grad_fn=<NegBackward0>) tensor(11343.8057, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11343.7998046875
tensor(11343.8057, grad_fn=<NegBackward0>) tensor(11343.7998, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11343.796875
tensor(11343.7998, grad_fn=<NegBackward0>) tensor(11343.7969, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11343.794921875
tensor(11343.7969, grad_fn=<NegBackward0>) tensor(11343.7949, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11343.787109375
tensor(11343.7949, grad_fn=<NegBackward0>) tensor(11343.7871, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11343.78125
tensor(11343.7871, grad_fn=<NegBackward0>) tensor(11343.7812, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11343.7314453125
tensor(11343.7812, grad_fn=<NegBackward0>) tensor(11343.7314, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11343.7275390625
tensor(11343.7314, grad_fn=<NegBackward0>) tensor(11343.7275, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11343.7255859375
tensor(11343.7275, grad_fn=<NegBackward0>) tensor(11343.7256, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11343.7216796875
tensor(11343.7256, grad_fn=<NegBackward0>) tensor(11343.7217, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11343.697265625
tensor(11343.7217, grad_fn=<NegBackward0>) tensor(11343.6973, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11341.619140625
tensor(11343.6973, grad_fn=<NegBackward0>) tensor(11341.6191, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11341.4306640625
tensor(11341.6191, grad_fn=<NegBackward0>) tensor(11341.4307, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11341.4052734375
tensor(11341.4307, grad_fn=<NegBackward0>) tensor(11341.4053, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11341.3974609375
tensor(11341.4053, grad_fn=<NegBackward0>) tensor(11341.3975, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11341.3935546875
tensor(11341.3975, grad_fn=<NegBackward0>) tensor(11341.3936, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11341.390625
tensor(11341.3936, grad_fn=<NegBackward0>) tensor(11341.3906, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11341.384765625
tensor(11341.3906, grad_fn=<NegBackward0>) tensor(11341.3848, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11341.380859375
tensor(11341.3848, grad_fn=<NegBackward0>) tensor(11341.3809, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11339.32421875
tensor(11341.3809, grad_fn=<NegBackward0>) tensor(11339.3242, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11339.3154296875
tensor(11339.3242, grad_fn=<NegBackward0>) tensor(11339.3154, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11339.306640625
tensor(11339.3154, grad_fn=<NegBackward0>) tensor(11339.3066, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11339.2978515625
tensor(11339.3066, grad_fn=<NegBackward0>) tensor(11339.2979, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11339.294921875
tensor(11339.2979, grad_fn=<NegBackward0>) tensor(11339.2949, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11339.265625
tensor(11339.2949, grad_fn=<NegBackward0>) tensor(11339.2656, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11338.5048828125
tensor(11339.2656, grad_fn=<NegBackward0>) tensor(11338.5049, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11338.4677734375
tensor(11338.5049, grad_fn=<NegBackward0>) tensor(11338.4678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11337.5751953125
tensor(11338.4678, grad_fn=<NegBackward0>) tensor(11337.5752, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11337.5556640625
tensor(11337.5752, grad_fn=<NegBackward0>) tensor(11337.5557, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11337.55859375
tensor(11337.5557, grad_fn=<NegBackward0>) tensor(11337.5586, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11337.55078125
tensor(11337.5557, grad_fn=<NegBackward0>) tensor(11337.5508, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11337.5380859375
tensor(11337.5508, grad_fn=<NegBackward0>) tensor(11337.5381, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11337.533203125
tensor(11337.5381, grad_fn=<NegBackward0>) tensor(11337.5332, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11337.5322265625
tensor(11337.5332, grad_fn=<NegBackward0>) tensor(11337.5322, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11337.5380859375
tensor(11337.5322, grad_fn=<NegBackward0>) tensor(11337.5381, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11337.5302734375
tensor(11337.5322, grad_fn=<NegBackward0>) tensor(11337.5303, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11337.5302734375
tensor(11337.5303, grad_fn=<NegBackward0>) tensor(11337.5303, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11337.4384765625
tensor(11337.5303, grad_fn=<NegBackward0>) tensor(11337.4385, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11337.4091796875
tensor(11337.4385, grad_fn=<NegBackward0>) tensor(11337.4092, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11337.3994140625
tensor(11337.4092, grad_fn=<NegBackward0>) tensor(11337.3994, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11337.380859375
tensor(11337.3994, grad_fn=<NegBackward0>) tensor(11337.3809, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11337.37890625
tensor(11337.3809, grad_fn=<NegBackward0>) tensor(11337.3789, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11337.3779296875
tensor(11337.3789, grad_fn=<NegBackward0>) tensor(11337.3779, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11337.3583984375
tensor(11337.3779, grad_fn=<NegBackward0>) tensor(11337.3584, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11337.3564453125
tensor(11337.3584, grad_fn=<NegBackward0>) tensor(11337.3564, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11337.3564453125
tensor(11337.3564, grad_fn=<NegBackward0>) tensor(11337.3564, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11337.3564453125
tensor(11337.3564, grad_fn=<NegBackward0>) tensor(11337.3564, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11337.3564453125
tensor(11337.3564, grad_fn=<NegBackward0>) tensor(11337.3564, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11337.3583984375
tensor(11337.3564, grad_fn=<NegBackward0>) tensor(11337.3584, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11337.337890625
tensor(11337.3564, grad_fn=<NegBackward0>) tensor(11337.3379, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11337.32421875
tensor(11337.3379, grad_fn=<NegBackward0>) tensor(11337.3242, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11337.3212890625
tensor(11337.3242, grad_fn=<NegBackward0>) tensor(11337.3213, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11337.3212890625
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3213, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11337.322265625
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3223, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11337.322265625
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3223, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11337.3212890625
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3213, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11337.322265625
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3223, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11337.3193359375
tensor(11337.3213, grad_fn=<NegBackward0>) tensor(11337.3193, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11337.2138671875
tensor(11337.3193, grad_fn=<NegBackward0>) tensor(11337.2139, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11337.220703125
tensor(11337.2139, grad_fn=<NegBackward0>) tensor(11337.2207, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11337.2158203125
tensor(11337.2139, grad_fn=<NegBackward0>) tensor(11337.2158, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11337.212890625
tensor(11337.2139, grad_fn=<NegBackward0>) tensor(11337.2129, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11335.263671875
tensor(11337.2129, grad_fn=<NegBackward0>) tensor(11335.2637, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11335.2421875
tensor(11335.2637, grad_fn=<NegBackward0>) tensor(11335.2422, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11335.2431640625
tensor(11335.2422, grad_fn=<NegBackward0>) tensor(11335.2432, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11335.244140625
tensor(11335.2422, grad_fn=<NegBackward0>) tensor(11335.2441, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11335.240234375
tensor(11335.2422, grad_fn=<NegBackward0>) tensor(11335.2402, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11335.236328125
tensor(11335.2402, grad_fn=<NegBackward0>) tensor(11335.2363, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11332.0205078125
tensor(11335.2363, grad_fn=<NegBackward0>) tensor(11332.0205, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11331.6640625
tensor(11332.0205, grad_fn=<NegBackward0>) tensor(11331.6641, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11331.6611328125
tensor(11331.6641, grad_fn=<NegBackward0>) tensor(11331.6611, grad_fn=<NegBackward0>)
pi: tensor([[0.7628, 0.2372],
        [0.2996, 0.7004]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1287, 0.8713], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3058, 0.1068],
         [0.6191, 0.1978]],

        [[0.7102, 0.0999],
         [0.5156, 0.6859]],

        [[0.6634, 0.1071],
         [0.7066, 0.7038]],

        [[0.6791, 0.1069],
         [0.6522, 0.6496]],

        [[0.6834, 0.1076],
         [0.6720, 0.5747]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.098076222281447
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.7185204305366488
Average Adjusted Rand Index: 0.7797735844362903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23388.275390625
inf tensor(23388.2754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11520.1728515625
tensor(23388.2754, grad_fn=<NegBackward0>) tensor(11520.1729, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11449.373046875
tensor(11520.1729, grad_fn=<NegBackward0>) tensor(11449.3730, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11399.171875
tensor(11449.3730, grad_fn=<NegBackward0>) tensor(11399.1719, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11387.5810546875
tensor(11399.1719, grad_fn=<NegBackward0>) tensor(11387.5811, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11338.544921875
tensor(11387.5811, grad_fn=<NegBackward0>) tensor(11338.5449, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11332.365234375
tensor(11338.5449, grad_fn=<NegBackward0>) tensor(11332.3652, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11323.5810546875
tensor(11332.3652, grad_fn=<NegBackward0>) tensor(11323.5811, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11318.046875
tensor(11323.5811, grad_fn=<NegBackward0>) tensor(11318.0469, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11318.0185546875
tensor(11318.0469, grad_fn=<NegBackward0>) tensor(11318.0186, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11318.0048828125
tensor(11318.0186, grad_fn=<NegBackward0>) tensor(11318.0049, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11314.83984375
tensor(11318.0049, grad_fn=<NegBackward0>) tensor(11314.8398, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11312.642578125
tensor(11314.8398, grad_fn=<NegBackward0>) tensor(11312.6426, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11312.6220703125
tensor(11312.6426, grad_fn=<NegBackward0>) tensor(11312.6221, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11312.283203125
tensor(11312.6221, grad_fn=<NegBackward0>) tensor(11312.2832, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11312.2431640625
tensor(11312.2832, grad_fn=<NegBackward0>) tensor(11312.2432, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11312.2197265625
tensor(11312.2432, grad_fn=<NegBackward0>) tensor(11312.2197, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11312.216796875
tensor(11312.2197, grad_fn=<NegBackward0>) tensor(11312.2168, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11312.2138671875
tensor(11312.2168, grad_fn=<NegBackward0>) tensor(11312.2139, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11312.2119140625
tensor(11312.2139, grad_fn=<NegBackward0>) tensor(11312.2119, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11312.2099609375
tensor(11312.2119, grad_fn=<NegBackward0>) tensor(11312.2100, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11312.1953125
tensor(11312.2100, grad_fn=<NegBackward0>) tensor(11312.1953, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11311.904296875
tensor(11312.1953, grad_fn=<NegBackward0>) tensor(11311.9043, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11311.8974609375
tensor(11311.9043, grad_fn=<NegBackward0>) tensor(11311.8975, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11311.8662109375
tensor(11311.8975, grad_fn=<NegBackward0>) tensor(11311.8662, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11311.865234375
tensor(11311.8662, grad_fn=<NegBackward0>) tensor(11311.8652, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11311.8642578125
tensor(11311.8652, grad_fn=<NegBackward0>) tensor(11311.8643, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11311.86328125
tensor(11311.8643, grad_fn=<NegBackward0>) tensor(11311.8633, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11311.541015625
tensor(11311.8633, grad_fn=<NegBackward0>) tensor(11311.5410, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11306.015625
tensor(11311.5410, grad_fn=<NegBackward0>) tensor(11306.0156, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11306.0068359375
tensor(11306.0156, grad_fn=<NegBackward0>) tensor(11306.0068, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11306.0048828125
tensor(11306.0068, grad_fn=<NegBackward0>) tensor(11306.0049, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11306.00390625
tensor(11306.0049, grad_fn=<NegBackward0>) tensor(11306.0039, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11306.0029296875
tensor(11306.0039, grad_fn=<NegBackward0>) tensor(11306.0029, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11306.0029296875
tensor(11306.0029, grad_fn=<NegBackward0>) tensor(11306.0029, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11306.001953125
tensor(11306.0029, grad_fn=<NegBackward0>) tensor(11306.0020, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11306.0029296875
tensor(11306.0020, grad_fn=<NegBackward0>) tensor(11306.0029, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11306.0009765625
tensor(11306.0020, grad_fn=<NegBackward0>) tensor(11306.0010, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11306.0009765625
tensor(11306.0010, grad_fn=<NegBackward0>) tensor(11306.0010, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11305.9990234375
tensor(11306.0010, grad_fn=<NegBackward0>) tensor(11305.9990, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11305.9990234375
tensor(11305.9990, grad_fn=<NegBackward0>) tensor(11305.9990, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11305.9951171875
tensor(11305.9990, grad_fn=<NegBackward0>) tensor(11305.9951, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11305.9951171875
tensor(11305.9951, grad_fn=<NegBackward0>) tensor(11305.9951, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11305.994140625
tensor(11305.9951, grad_fn=<NegBackward0>) tensor(11305.9941, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11306.001953125
tensor(11305.9941, grad_fn=<NegBackward0>) tensor(11306.0020, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11305.994140625
tensor(11305.9941, grad_fn=<NegBackward0>) tensor(11305.9941, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11305.9970703125
tensor(11305.9941, grad_fn=<NegBackward0>) tensor(11305.9971, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11305.9921875
tensor(11305.9941, grad_fn=<NegBackward0>) tensor(11305.9922, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11305.9912109375
tensor(11305.9922, grad_fn=<NegBackward0>) tensor(11305.9912, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11305.9892578125
tensor(11305.9912, grad_fn=<NegBackward0>) tensor(11305.9893, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11305.990234375
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9902, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11305.9892578125
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9893, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11305.990234375
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9902, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11305.990234375
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9902, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11305.9892578125
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9893, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11305.9921875
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9922, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11305.9892578125
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9893, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11305.98828125
tensor(11305.9893, grad_fn=<NegBackward0>) tensor(11305.9883, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11305.986328125
tensor(11305.9883, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11305.98828125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9883, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11306.0087890625
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11306.0088, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11305.9873046875
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11305.98828125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9883, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11305.986328125
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9863, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11305.994140625
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9941, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11305.9853515625
tensor(11305.9863, grad_fn=<NegBackward0>) tensor(11305.9854, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11305.994140625
tensor(11305.9854, grad_fn=<NegBackward0>) tensor(11305.9941, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11305.9853515625
tensor(11305.9854, grad_fn=<NegBackward0>) tensor(11305.9854, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11305.9873046875
tensor(11305.9854, grad_fn=<NegBackward0>) tensor(11305.9873, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11305.9697265625
tensor(11305.9854, grad_fn=<NegBackward0>) tensor(11305.9697, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11305.958984375
tensor(11305.9697, grad_fn=<NegBackward0>) tensor(11305.9590, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11305.9599609375
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9600, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11305.958984375
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9590, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11305.958984375
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9590, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11305.9609375
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9609, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11305.9609375
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9609, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11305.9580078125
tensor(11305.9590, grad_fn=<NegBackward0>) tensor(11305.9580, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11305.9931640625
tensor(11305.9580, grad_fn=<NegBackward0>) tensor(11305.9932, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11305.9560546875
tensor(11305.9580, grad_fn=<NegBackward0>) tensor(11305.9561, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11305.95703125
tensor(11305.9561, grad_fn=<NegBackward0>) tensor(11305.9570, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11305.8330078125
tensor(11305.9561, grad_fn=<NegBackward0>) tensor(11305.8330, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11305.84765625
tensor(11305.8330, grad_fn=<NegBackward0>) tensor(11305.8477, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11305.83203125
tensor(11305.8330, grad_fn=<NegBackward0>) tensor(11305.8320, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11305.83203125
tensor(11305.8320, grad_fn=<NegBackward0>) tensor(11305.8320, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11305.8447265625
tensor(11305.8320, grad_fn=<NegBackward0>) tensor(11305.8447, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11305.8310546875
tensor(11305.8320, grad_fn=<NegBackward0>) tensor(11305.8311, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11305.8310546875
tensor(11305.8311, grad_fn=<NegBackward0>) tensor(11305.8311, grad_fn=<NegBackward0>)
pi: tensor([[0.2829, 0.7171],
        [0.8020, 0.1980]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5664, 0.4336], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2342, 0.0927],
         [0.5183, 0.2726]],

        [[0.5271, 0.1001],
         [0.6712, 0.7216]],

        [[0.5946, 0.1076],
         [0.6387, 0.5414]],

        [[0.6511, 0.1031],
         [0.5217, 0.5486]],

        [[0.6922, 0.1061],
         [0.7016, 0.6483]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 10
Adjusted Rand Index: 0.6365003576249006
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
Global Adjusted Rand Index: 0.05008096990729021
Average Adjusted Rand Index: 0.8266907391508853
[0.7185204305366488, 0.05008096990729021] [0.7797735844362903, 0.8266907391508853] [11331.8125, 11305.8330078125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -10972.475399975014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22776.2890625
inf tensor(22776.2891, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11160.5849609375
tensor(22776.2891, grad_fn=<NegBackward0>) tensor(11160.5850, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11150.9453125
tensor(11160.5850, grad_fn=<NegBackward0>) tensor(11150.9453, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11150.7138671875
tensor(11150.9453, grad_fn=<NegBackward0>) tensor(11150.7139, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11150.6201171875
tensor(11150.7139, grad_fn=<NegBackward0>) tensor(11150.6201, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11150.544921875
tensor(11150.6201, grad_fn=<NegBackward0>) tensor(11150.5449, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11150.427734375
tensor(11150.5449, grad_fn=<NegBackward0>) tensor(11150.4277, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11150.1298828125
tensor(11150.4277, grad_fn=<NegBackward0>) tensor(11150.1299, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11149.7978515625
tensor(11150.1299, grad_fn=<NegBackward0>) tensor(11149.7979, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11149.6708984375
tensor(11149.7979, grad_fn=<NegBackward0>) tensor(11149.6709, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11149.5546875
tensor(11149.6709, grad_fn=<NegBackward0>) tensor(11149.5547, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11149.4326171875
tensor(11149.5547, grad_fn=<NegBackward0>) tensor(11149.4326, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11149.30859375
tensor(11149.4326, grad_fn=<NegBackward0>) tensor(11149.3086, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11149.1552734375
tensor(11149.3086, grad_fn=<NegBackward0>) tensor(11149.1553, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11149.009765625
tensor(11149.1553, grad_fn=<NegBackward0>) tensor(11149.0098, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11148.9267578125
tensor(11149.0098, grad_fn=<NegBackward0>) tensor(11148.9268, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11148.888671875
tensor(11148.9268, grad_fn=<NegBackward0>) tensor(11148.8887, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11148.87109375
tensor(11148.8887, grad_fn=<NegBackward0>) tensor(11148.8711, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11148.8642578125
tensor(11148.8711, grad_fn=<NegBackward0>) tensor(11148.8643, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11148.8583984375
tensor(11148.8643, grad_fn=<NegBackward0>) tensor(11148.8584, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11148.8564453125
tensor(11148.8584, grad_fn=<NegBackward0>) tensor(11148.8564, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11148.8544921875
tensor(11148.8564, grad_fn=<NegBackward0>) tensor(11148.8545, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11148.8505859375
tensor(11148.8545, grad_fn=<NegBackward0>) tensor(11148.8506, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11148.8486328125
tensor(11148.8506, grad_fn=<NegBackward0>) tensor(11148.8486, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11148.84765625
tensor(11148.8486, grad_fn=<NegBackward0>) tensor(11148.8477, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11148.8466796875
tensor(11148.8477, grad_fn=<NegBackward0>) tensor(11148.8467, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11148.845703125
tensor(11148.8467, grad_fn=<NegBackward0>) tensor(11148.8457, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11148.8486328125
tensor(11148.8457, grad_fn=<NegBackward0>) tensor(11148.8486, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11148.84375
tensor(11148.8457, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11148.8447265625
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8447, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11148.84375
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11148.84375
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11148.84375
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11148.8427734375
tensor(11148.8438, grad_fn=<NegBackward0>) tensor(11148.8428, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11148.8427734375
tensor(11148.8428, grad_fn=<NegBackward0>) tensor(11148.8428, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11148.8427734375
tensor(11148.8428, grad_fn=<NegBackward0>) tensor(11148.8428, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11148.8408203125
tensor(11148.8428, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11148.8408203125
tensor(11148.8408, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11148.8408203125
tensor(11148.8408, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11148.8408203125
tensor(11148.8408, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11148.83984375
tensor(11148.8408, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11148.83984375
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11148.8466796875
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8467, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11148.83984375
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11148.83984375
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11148.83984375
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11148.8408203125
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11148.8388671875
tensor(11148.8398, grad_fn=<NegBackward0>) tensor(11148.8389, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11148.8408203125
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11148.84375
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8438, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11148.8408203125
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8408, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11148.8388671875
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8389, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11148.8388671875
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8389, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11148.8388671875
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8389, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11148.837890625
tensor(11148.8389, grad_fn=<NegBackward0>) tensor(11148.8379, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11148.8369140625
tensor(11148.8379, grad_fn=<NegBackward0>) tensor(11148.8369, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11148.837890625
tensor(11148.8369, grad_fn=<NegBackward0>) tensor(11148.8379, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11148.83984375
tensor(11148.8369, grad_fn=<NegBackward0>) tensor(11148.8398, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11148.837890625
tensor(11148.8369, grad_fn=<NegBackward0>) tensor(11148.8379, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11148.837890625
tensor(11148.8369, grad_fn=<NegBackward0>) tensor(11148.8379, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11148.841796875
tensor(11148.8369, grad_fn=<NegBackward0>) tensor(11148.8418, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[9.6403e-01, 3.5967e-02],
        [9.9996e-01, 3.8460e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5614, 0.4386], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1678, 0.1074],
         [0.7260, 0.2845]],

        [[0.6864, 0.0923],
         [0.7038, 0.7240]],

        [[0.7088, 0.1555],
         [0.5387, 0.7207]],

        [[0.6866, 0.2205],
         [0.6821, 0.6246]],

        [[0.6539, 0.2237],
         [0.6771, 0.5915]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0617284522334269
Average Adjusted Rand Index: 0.15321242289070292
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23540.447265625
inf tensor(23540.4473, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11169.9970703125
tensor(23540.4473, grad_fn=<NegBackward0>) tensor(11169.9971, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11169.15625
tensor(11169.9971, grad_fn=<NegBackward0>) tensor(11169.1562, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11168.8642578125
tensor(11169.1562, grad_fn=<NegBackward0>) tensor(11168.8643, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11168.4228515625
tensor(11168.8643, grad_fn=<NegBackward0>) tensor(11168.4229, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11167.9384765625
tensor(11168.4229, grad_fn=<NegBackward0>) tensor(11167.9385, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11167.47265625
tensor(11167.9385, grad_fn=<NegBackward0>) tensor(11167.4727, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11167.18359375
tensor(11167.4727, grad_fn=<NegBackward0>) tensor(11167.1836, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11166.9765625
tensor(11167.1836, grad_fn=<NegBackward0>) tensor(11166.9766, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11166.8359375
tensor(11166.9766, grad_fn=<NegBackward0>) tensor(11166.8359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11166.7314453125
tensor(11166.8359, grad_fn=<NegBackward0>) tensor(11166.7314, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11166.640625
tensor(11166.7314, grad_fn=<NegBackward0>) tensor(11166.6406, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11166.5576171875
tensor(11166.6406, grad_fn=<NegBackward0>) tensor(11166.5576, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11166.4755859375
tensor(11166.5576, grad_fn=<NegBackward0>) tensor(11166.4756, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11166.3974609375
tensor(11166.4756, grad_fn=<NegBackward0>) tensor(11166.3975, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11166.3212890625
tensor(11166.3975, grad_fn=<NegBackward0>) tensor(11166.3213, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11166.2451171875
tensor(11166.3213, grad_fn=<NegBackward0>) tensor(11166.2451, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11166.1572265625
tensor(11166.2451, grad_fn=<NegBackward0>) tensor(11166.1572, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11166.0263671875
tensor(11166.1572, grad_fn=<NegBackward0>) tensor(11166.0264, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11165.7607421875
tensor(11166.0264, grad_fn=<NegBackward0>) tensor(11165.7607, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11164.720703125
tensor(11165.7607, grad_fn=<NegBackward0>) tensor(11164.7207, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10994.7822265625
tensor(11164.7207, grad_fn=<NegBackward0>) tensor(10994.7822, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10991.166015625
tensor(10994.7822, grad_fn=<NegBackward0>) tensor(10991.1660, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10986.0478515625
tensor(10991.1660, grad_fn=<NegBackward0>) tensor(10986.0479, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10985.8662109375
tensor(10986.0479, grad_fn=<NegBackward0>) tensor(10985.8662, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10985.8251953125
tensor(10985.8662, grad_fn=<NegBackward0>) tensor(10985.8252, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10985.767578125
tensor(10985.8252, grad_fn=<NegBackward0>) tensor(10985.7676, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10984.9873046875
tensor(10985.7676, grad_fn=<NegBackward0>) tensor(10984.9873, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10984.9658203125
tensor(10984.9873, grad_fn=<NegBackward0>) tensor(10984.9658, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10984.9521484375
tensor(10984.9658, grad_fn=<NegBackward0>) tensor(10984.9521, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10984.9384765625
tensor(10984.9521, grad_fn=<NegBackward0>) tensor(10984.9385, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10984.9267578125
tensor(10984.9385, grad_fn=<NegBackward0>) tensor(10984.9268, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10984.91796875
tensor(10984.9268, grad_fn=<NegBackward0>) tensor(10984.9180, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10984.91015625
tensor(10984.9180, grad_fn=<NegBackward0>) tensor(10984.9102, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10984.90234375
tensor(10984.9102, grad_fn=<NegBackward0>) tensor(10984.9023, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10984.896484375
tensor(10984.9023, grad_fn=<NegBackward0>) tensor(10984.8965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10984.8134765625
tensor(10984.8965, grad_fn=<NegBackward0>) tensor(10984.8135, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10984.810546875
tensor(10984.8135, grad_fn=<NegBackward0>) tensor(10984.8105, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10984.80859375
tensor(10984.8105, grad_fn=<NegBackward0>) tensor(10984.8086, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10984.806640625
tensor(10984.8086, grad_fn=<NegBackward0>) tensor(10984.8066, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10984.8046875
tensor(10984.8066, grad_fn=<NegBackward0>) tensor(10984.8047, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10984.8037109375
tensor(10984.8047, grad_fn=<NegBackward0>) tensor(10984.8037, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10984.8046875
tensor(10984.8037, grad_fn=<NegBackward0>) tensor(10984.8047, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10984.80078125
tensor(10984.8037, grad_fn=<NegBackward0>) tensor(10984.8008, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10984.7998046875
tensor(10984.8008, grad_fn=<NegBackward0>) tensor(10984.7998, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10984.798828125
tensor(10984.7998, grad_fn=<NegBackward0>) tensor(10984.7988, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10984.798828125
tensor(10984.7988, grad_fn=<NegBackward0>) tensor(10984.7988, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10984.7958984375
tensor(10984.7988, grad_fn=<NegBackward0>) tensor(10984.7959, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10984.7978515625
tensor(10984.7959, grad_fn=<NegBackward0>) tensor(10984.7979, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10984.7958984375
tensor(10984.7959, grad_fn=<NegBackward0>) tensor(10984.7959, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10984.7939453125
tensor(10984.7959, grad_fn=<NegBackward0>) tensor(10984.7939, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10984.794921875
tensor(10984.7939, grad_fn=<NegBackward0>) tensor(10984.7949, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10984.791015625
tensor(10984.7939, grad_fn=<NegBackward0>) tensor(10984.7910, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10984.7861328125
tensor(10984.7910, grad_fn=<NegBackward0>) tensor(10984.7861, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10984.7861328125
tensor(10984.7861, grad_fn=<NegBackward0>) tensor(10984.7861, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10984.78515625
tensor(10984.7861, grad_fn=<NegBackward0>) tensor(10984.7852, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10984.7841796875
tensor(10984.7852, grad_fn=<NegBackward0>) tensor(10984.7842, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10984.7841796875
tensor(10984.7842, grad_fn=<NegBackward0>) tensor(10984.7842, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10984.7841796875
tensor(10984.7842, grad_fn=<NegBackward0>) tensor(10984.7842, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10984.7822265625
tensor(10984.7842, grad_fn=<NegBackward0>) tensor(10984.7822, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10984.783203125
tensor(10984.7822, grad_fn=<NegBackward0>) tensor(10984.7832, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10984.783203125
tensor(10984.7822, grad_fn=<NegBackward0>) tensor(10984.7832, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10984.7822265625
tensor(10984.7822, grad_fn=<NegBackward0>) tensor(10984.7822, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10984.78125
tensor(10984.7822, grad_fn=<NegBackward0>) tensor(10984.7812, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10984.7802734375
tensor(10984.7812, grad_fn=<NegBackward0>) tensor(10984.7803, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10984.7783203125
tensor(10984.7803, grad_fn=<NegBackward0>) tensor(10984.7783, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10984.78125
tensor(10984.7783, grad_fn=<NegBackward0>) tensor(10984.7812, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10984.77734375
tensor(10984.7783, grad_fn=<NegBackward0>) tensor(10984.7773, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10984.779296875
tensor(10984.7773, grad_fn=<NegBackward0>) tensor(10984.7793, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10984.77734375
tensor(10984.7773, grad_fn=<NegBackward0>) tensor(10984.7773, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10984.7763671875
tensor(10984.7773, grad_fn=<NegBackward0>) tensor(10984.7764, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10984.77734375
tensor(10984.7764, grad_fn=<NegBackward0>) tensor(10984.7773, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10984.7763671875
tensor(10984.7764, grad_fn=<NegBackward0>) tensor(10984.7764, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10984.775390625
tensor(10984.7764, grad_fn=<NegBackward0>) tensor(10984.7754, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10984.775390625
tensor(10984.7754, grad_fn=<NegBackward0>) tensor(10984.7754, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10984.7763671875
tensor(10984.7754, grad_fn=<NegBackward0>) tensor(10984.7764, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10984.775390625
tensor(10984.7754, grad_fn=<NegBackward0>) tensor(10984.7754, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10984.775390625
tensor(10984.7754, grad_fn=<NegBackward0>) tensor(10984.7754, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10984.7744140625
tensor(10984.7754, grad_fn=<NegBackward0>) tensor(10984.7744, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10984.7763671875
tensor(10984.7744, grad_fn=<NegBackward0>) tensor(10984.7764, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10984.7763671875
tensor(10984.7744, grad_fn=<NegBackward0>) tensor(10984.7764, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10984.787109375
tensor(10984.7744, grad_fn=<NegBackward0>) tensor(10984.7871, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10984.7548828125
tensor(10984.7744, grad_fn=<NegBackward0>) tensor(10984.7549, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10984.755859375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7559, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10984.7548828125
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7549, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10984.755859375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7559, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10984.755859375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7559, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10984.755859375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7559, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10984.7724609375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7725, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -10984.755859375
tensor(10984.7549, grad_fn=<NegBackward0>) tensor(10984.7559, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7614, 0.2386],
        [0.2957, 0.7043]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.1086e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2949, 0.1789],
         [0.5162, 0.1829]],

        [[0.5924, 0.0947],
         [0.6537, 0.6675]],

        [[0.6454, 0.1030],
         [0.7053, 0.6494]],

        [[0.7244, 0.0978],
         [0.5979, 0.6210]],

        [[0.6367, 0.0960],
         [0.7112, 0.6514]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844331361923687
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
Global Adjusted Rand Index: 0.594856156339583
Average Adjusted Rand Index: 0.7061550887883059
[0.0617284522334269, 0.594856156339583] [0.15321242289070292, 0.7061550887883059] [11148.841796875, 10984.755859375]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11266.906146132842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22664.091796875
inf tensor(22664.0918, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11505.701171875
tensor(22664.0918, grad_fn=<NegBackward0>) tensor(11505.7012, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11504.5302734375
tensor(11505.7012, grad_fn=<NegBackward0>) tensor(11504.5303, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11503.119140625
tensor(11504.5303, grad_fn=<NegBackward0>) tensor(11503.1191, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11493.66015625
tensor(11503.1191, grad_fn=<NegBackward0>) tensor(11493.6602, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11443.3369140625
tensor(11493.6602, grad_fn=<NegBackward0>) tensor(11443.3369, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11252.5615234375
tensor(11443.3369, grad_fn=<NegBackward0>) tensor(11252.5615, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11235.673828125
tensor(11252.5615, grad_fn=<NegBackward0>) tensor(11235.6738, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11235.021484375
tensor(11235.6738, grad_fn=<NegBackward0>) tensor(11235.0215, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11234.6796875
tensor(11235.0215, grad_fn=<NegBackward0>) tensor(11234.6797, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11234.515625
tensor(11234.6797, grad_fn=<NegBackward0>) tensor(11234.5156, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11234.0693359375
tensor(11234.5156, grad_fn=<NegBackward0>) tensor(11234.0693, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11233.32421875
tensor(11234.0693, grad_fn=<NegBackward0>) tensor(11233.3242, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11231.2939453125
tensor(11233.3242, grad_fn=<NegBackward0>) tensor(11231.2939, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11231.2021484375
tensor(11231.2939, grad_fn=<NegBackward0>) tensor(11231.2021, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11231.1748046875
tensor(11231.2021, grad_fn=<NegBackward0>) tensor(11231.1748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11231.0830078125
tensor(11231.1748, grad_fn=<NegBackward0>) tensor(11231.0830, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11230.927734375
tensor(11231.0830, grad_fn=<NegBackward0>) tensor(11230.9277, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11230.912109375
tensor(11230.9277, grad_fn=<NegBackward0>) tensor(11230.9121, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11230.90234375
tensor(11230.9121, grad_fn=<NegBackward0>) tensor(11230.9023, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11230.890625
tensor(11230.9023, grad_fn=<NegBackward0>) tensor(11230.8906, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11230.8779296875
tensor(11230.8906, grad_fn=<NegBackward0>) tensor(11230.8779, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11230.8681640625
tensor(11230.8779, grad_fn=<NegBackward0>) tensor(11230.8682, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11230.86328125
tensor(11230.8682, grad_fn=<NegBackward0>) tensor(11230.8633, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11230.8583984375
tensor(11230.8633, grad_fn=<NegBackward0>) tensor(11230.8584, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11230.8544921875
tensor(11230.8584, grad_fn=<NegBackward0>) tensor(11230.8545, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11230.849609375
tensor(11230.8545, grad_fn=<NegBackward0>) tensor(11230.8496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11230.8447265625
tensor(11230.8496, grad_fn=<NegBackward0>) tensor(11230.8447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11230.8359375
tensor(11230.8447, grad_fn=<NegBackward0>) tensor(11230.8359, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11230.830078125
tensor(11230.8359, grad_fn=<NegBackward0>) tensor(11230.8301, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11230.8271484375
tensor(11230.8301, grad_fn=<NegBackward0>) tensor(11230.8271, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11230.8232421875
tensor(11230.8271, grad_fn=<NegBackward0>) tensor(11230.8232, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11230.82421875
tensor(11230.8232, grad_fn=<NegBackward0>) tensor(11230.8242, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11230.8125
tensor(11230.8232, grad_fn=<NegBackward0>) tensor(11230.8125, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11230.8095703125
tensor(11230.8125, grad_fn=<NegBackward0>) tensor(11230.8096, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11230.8095703125
tensor(11230.8096, grad_fn=<NegBackward0>) tensor(11230.8096, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11230.8076171875
tensor(11230.8096, grad_fn=<NegBackward0>) tensor(11230.8076, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11230.8056640625
tensor(11230.8076, grad_fn=<NegBackward0>) tensor(11230.8057, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11230.798828125
tensor(11230.8057, grad_fn=<NegBackward0>) tensor(11230.7988, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11230.7958984375
tensor(11230.7988, grad_fn=<NegBackward0>) tensor(11230.7959, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11230.794921875
tensor(11230.7959, grad_fn=<NegBackward0>) tensor(11230.7949, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11230.79296875
tensor(11230.7949, grad_fn=<NegBackward0>) tensor(11230.7930, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11230.7919921875
tensor(11230.7930, grad_fn=<NegBackward0>) tensor(11230.7920, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11230.7919921875
tensor(11230.7920, grad_fn=<NegBackward0>) tensor(11230.7920, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11230.791015625
tensor(11230.7920, grad_fn=<NegBackward0>) tensor(11230.7910, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11230.7890625
tensor(11230.7910, grad_fn=<NegBackward0>) tensor(11230.7891, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11230.7890625
tensor(11230.7891, grad_fn=<NegBackward0>) tensor(11230.7891, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11230.7880859375
tensor(11230.7891, grad_fn=<NegBackward0>) tensor(11230.7881, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11230.787109375
tensor(11230.7881, grad_fn=<NegBackward0>) tensor(11230.7871, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11228.115234375
tensor(11230.7871, grad_fn=<NegBackward0>) tensor(11228.1152, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11228.1142578125
tensor(11228.1152, grad_fn=<NegBackward0>) tensor(11228.1143, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11228.115234375
tensor(11228.1143, grad_fn=<NegBackward0>) tensor(11228.1152, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11228.11328125
tensor(11228.1143, grad_fn=<NegBackward0>) tensor(11228.1133, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11228.111328125
tensor(11228.1133, grad_fn=<NegBackward0>) tensor(11228.1113, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11228.111328125
tensor(11228.1113, grad_fn=<NegBackward0>) tensor(11228.1113, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11228.109375
tensor(11228.1113, grad_fn=<NegBackward0>) tensor(11228.1094, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11228.111328125
tensor(11228.1094, grad_fn=<NegBackward0>) tensor(11228.1113, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11228.1083984375
tensor(11228.1094, grad_fn=<NegBackward0>) tensor(11228.1084, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11228.109375
tensor(11228.1084, grad_fn=<NegBackward0>) tensor(11228.1094, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11228.107421875
tensor(11228.1084, grad_fn=<NegBackward0>) tensor(11228.1074, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11228.1083984375
tensor(11228.1074, grad_fn=<NegBackward0>) tensor(11228.1084, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11228.109375
tensor(11228.1074, grad_fn=<NegBackward0>) tensor(11228.1094, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11228.1083984375
tensor(11228.1074, grad_fn=<NegBackward0>) tensor(11228.1084, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11228.1083984375
tensor(11228.1074, grad_fn=<NegBackward0>) tensor(11228.1084, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11228.1064453125
tensor(11228.1074, grad_fn=<NegBackward0>) tensor(11228.1064, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11228.1123046875
tensor(11228.1064, grad_fn=<NegBackward0>) tensor(11228.1123, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11228.10546875
tensor(11228.1064, grad_fn=<NegBackward0>) tensor(11228.1055, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11228.1044921875
tensor(11228.1055, grad_fn=<NegBackward0>) tensor(11228.1045, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11228.1005859375
tensor(11228.1045, grad_fn=<NegBackward0>) tensor(11228.1006, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11228.0634765625
tensor(11228.1006, grad_fn=<NegBackward0>) tensor(11228.0635, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11228.0458984375
tensor(11228.0635, grad_fn=<NegBackward0>) tensor(11228.0459, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11228.0439453125
tensor(11228.0459, grad_fn=<NegBackward0>) tensor(11228.0439, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11228.044921875
tensor(11228.0439, grad_fn=<NegBackward0>) tensor(11228.0449, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11228.0439453125
tensor(11228.0439, grad_fn=<NegBackward0>) tensor(11228.0439, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11228.0439453125
tensor(11228.0439, grad_fn=<NegBackward0>) tensor(11228.0439, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11228.0576171875
tensor(11228.0439, grad_fn=<NegBackward0>) tensor(11228.0576, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11228.04296875
tensor(11228.0439, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11228.04296875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11228.0439453125
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0439, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11228.04296875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11228.044921875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0449, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11228.04296875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11228.0458984375
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0459, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11228.04296875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11228.0419921875
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0420, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11228.041015625
tensor(11228.0420, grad_fn=<NegBackward0>) tensor(11228.0410, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11228.0390625
tensor(11228.0410, grad_fn=<NegBackward0>) tensor(11228.0391, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11228.0302734375
tensor(11228.0391, grad_fn=<NegBackward0>) tensor(11228.0303, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11228.1015625
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.1016, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11228.0302734375
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0303, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11228.14453125
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.1445, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11228.03125
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0312, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11228.0341796875
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0342, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11228.0302734375
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0303, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11228.04296875
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11228.056640625
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0566, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11228.0283203125
tensor(11228.0303, grad_fn=<NegBackward0>) tensor(11228.0283, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11228.0302734375
tensor(11228.0283, grad_fn=<NegBackward0>) tensor(11228.0303, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11228.03515625
tensor(11228.0283, grad_fn=<NegBackward0>) tensor(11228.0352, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11228.0224609375
tensor(11228.0283, grad_fn=<NegBackward0>) tensor(11228.0225, grad_fn=<NegBackward0>)
pi: tensor([[0.7857, 0.2143],
        [0.2438, 0.7562]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4629, 0.5371], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1956, 0.1078],
         [0.5154, 0.2991]],

        [[0.6517, 0.1082],
         [0.5286, 0.7166]],

        [[0.6542, 0.1038],
         [0.6655, 0.5415]],

        [[0.6496, 0.0953],
         [0.7073, 0.5504]],

        [[0.5832, 0.1038],
         [0.7251, 0.5662]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7717887095983128
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.8909174658891703
Average Adjusted Rand Index: 0.8911653477076648
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22418.431640625
inf tensor(22418.4316, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11505.001953125
tensor(22418.4316, grad_fn=<NegBackward0>) tensor(11505.0020, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11503.990234375
tensor(11505.0020, grad_fn=<NegBackward0>) tensor(11503.9902, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11499.48828125
tensor(11503.9902, grad_fn=<NegBackward0>) tensor(11499.4883, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11492.9794921875
tensor(11499.4883, grad_fn=<NegBackward0>) tensor(11492.9795, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11479.361328125
tensor(11492.9795, grad_fn=<NegBackward0>) tensor(11479.3613, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11265.75
tensor(11479.3613, grad_fn=<NegBackward0>) tensor(11265.7500, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11243.1572265625
tensor(11265.7500, grad_fn=<NegBackward0>) tensor(11243.1572, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11234.48046875
tensor(11243.1572, grad_fn=<NegBackward0>) tensor(11234.4805, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11234.111328125
tensor(11234.4805, grad_fn=<NegBackward0>) tensor(11234.1113, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11233.91015625
tensor(11234.1113, grad_fn=<NegBackward0>) tensor(11233.9102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11233.7958984375
tensor(11233.9102, grad_fn=<NegBackward0>) tensor(11233.7959, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11231.2890625
tensor(11233.7959, grad_fn=<NegBackward0>) tensor(11231.2891, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11231.1123046875
tensor(11231.2891, grad_fn=<NegBackward0>) tensor(11231.1123, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11228.294921875
tensor(11231.1123, grad_fn=<NegBackward0>) tensor(11228.2949, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11228.2470703125
tensor(11228.2949, grad_fn=<NegBackward0>) tensor(11228.2471, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11228.2001953125
tensor(11228.2471, grad_fn=<NegBackward0>) tensor(11228.2002, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11228.150390625
tensor(11228.2002, grad_fn=<NegBackward0>) tensor(11228.1504, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11228.138671875
tensor(11228.1504, grad_fn=<NegBackward0>) tensor(11228.1387, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11228.126953125
tensor(11228.1387, grad_fn=<NegBackward0>) tensor(11228.1270, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11228.1181640625
tensor(11228.1270, grad_fn=<NegBackward0>) tensor(11228.1182, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11228.10546875
tensor(11228.1182, grad_fn=<NegBackward0>) tensor(11228.1055, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11228.0927734375
tensor(11228.1055, grad_fn=<NegBackward0>) tensor(11228.0928, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11228.0849609375
tensor(11228.0928, grad_fn=<NegBackward0>) tensor(11228.0850, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11228.078125
tensor(11228.0850, grad_fn=<NegBackward0>) tensor(11228.0781, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11228.072265625
tensor(11228.0781, grad_fn=<NegBackward0>) tensor(11228.0723, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11228.0615234375
tensor(11228.0723, grad_fn=<NegBackward0>) tensor(11228.0615, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11228.052734375
tensor(11228.0615, grad_fn=<NegBackward0>) tensor(11228.0527, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11228.0498046875
tensor(11228.0527, grad_fn=<NegBackward0>) tensor(11228.0498, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11228.0458984375
tensor(11228.0498, grad_fn=<NegBackward0>) tensor(11228.0459, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11228.0458984375
tensor(11228.0459, grad_fn=<NegBackward0>) tensor(11228.0459, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11228.04296875
tensor(11228.0459, grad_fn=<NegBackward0>) tensor(11228.0430, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11228.041015625
tensor(11228.0430, grad_fn=<NegBackward0>) tensor(11228.0410, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11228.0380859375
tensor(11228.0410, grad_fn=<NegBackward0>) tensor(11228.0381, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11228.0341796875
tensor(11228.0381, grad_fn=<NegBackward0>) tensor(11228.0342, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11228.025390625
tensor(11228.0342, grad_fn=<NegBackward0>) tensor(11228.0254, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11228.0244140625
tensor(11228.0254, grad_fn=<NegBackward0>) tensor(11228.0244, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11228.021484375
tensor(11228.0244, grad_fn=<NegBackward0>) tensor(11228.0215, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11228.0224609375
tensor(11228.0215, grad_fn=<NegBackward0>) tensor(11228.0225, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11228.021484375
tensor(11228.0215, grad_fn=<NegBackward0>) tensor(11228.0215, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11228.0205078125
tensor(11228.0215, grad_fn=<NegBackward0>) tensor(11228.0205, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11228.01953125
tensor(11228.0205, grad_fn=<NegBackward0>) tensor(11228.0195, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11228.0185546875
tensor(11228.0195, grad_fn=<NegBackward0>) tensor(11228.0186, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11228.017578125
tensor(11228.0186, grad_fn=<NegBackward0>) tensor(11228.0176, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11228.01953125
tensor(11228.0176, grad_fn=<NegBackward0>) tensor(11228.0195, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11228.0166015625
tensor(11228.0176, grad_fn=<NegBackward0>) tensor(11228.0166, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11228.021484375
tensor(11228.0166, grad_fn=<NegBackward0>) tensor(11228.0215, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11228.01953125
tensor(11228.0166, grad_fn=<NegBackward0>) tensor(11228.0195, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11228.015625
tensor(11228.0166, grad_fn=<NegBackward0>) tensor(11228.0156, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11228.013671875
tensor(11228.0156, grad_fn=<NegBackward0>) tensor(11228.0137, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11228.013671875
tensor(11228.0137, grad_fn=<NegBackward0>) tensor(11228.0137, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11228.013671875
tensor(11228.0137, grad_fn=<NegBackward0>) tensor(11228.0137, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11228.0126953125
tensor(11228.0137, grad_fn=<NegBackward0>) tensor(11228.0127, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11228.0087890625
tensor(11228.0127, grad_fn=<NegBackward0>) tensor(11228.0088, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11228.0107421875
tensor(11228.0088, grad_fn=<NegBackward0>) tensor(11228.0107, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11228.0068359375
tensor(11228.0088, grad_fn=<NegBackward0>) tensor(11228.0068, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11228.005859375
tensor(11228.0068, grad_fn=<NegBackward0>) tensor(11228.0059, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11228.0087890625
tensor(11228.0059, grad_fn=<NegBackward0>) tensor(11228.0088, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11228.0029296875
tensor(11228.0059, grad_fn=<NegBackward0>) tensor(11228.0029, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11228.001953125
tensor(11228.0029, grad_fn=<NegBackward0>) tensor(11228.0020, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11228.0029296875
tensor(11228.0020, grad_fn=<NegBackward0>) tensor(11228.0029, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11228.001953125
tensor(11228.0020, grad_fn=<NegBackward0>) tensor(11228.0020, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11228.001953125
tensor(11228.0020, grad_fn=<NegBackward0>) tensor(11228.0020, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11228.001953125
tensor(11228.0020, grad_fn=<NegBackward0>) tensor(11228.0020, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11228.0
tensor(11228.0020, grad_fn=<NegBackward0>) tensor(11228., grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11228.0068359375
tensor(11228., grad_fn=<NegBackward0>) tensor(11228.0068, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11228.0029296875
tensor(11228., grad_fn=<NegBackward0>) tensor(11228.0029, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11228.00390625
tensor(11228., grad_fn=<NegBackward0>) tensor(11228.0039, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11227.99609375
tensor(11228., grad_fn=<NegBackward0>) tensor(11227.9961, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11227.9951171875
tensor(11227.9961, grad_fn=<NegBackward0>) tensor(11227.9951, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11227.99609375
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9961, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11227.9951171875
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9951, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11227.99609375
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9961, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11227.9951171875
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9951, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11228.0048828125
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11228.0049, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11227.9951171875
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9951, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11227.99609375
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9961, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11228.0029296875
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11228.0029, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11227.994140625
tensor(11227.9951, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11228.0146484375
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11228.0146, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11227.9951171875
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9951, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11227.994140625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11228.072265625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11228.0723, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11227.9931640625
tensor(11227.9941, grad_fn=<NegBackward0>) tensor(11227.9932, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11227.9970703125
tensor(11227.9932, grad_fn=<NegBackward0>) tensor(11227.9971, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11227.9921875
tensor(11227.9932, grad_fn=<NegBackward0>) tensor(11227.9922, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11227.994140625
tensor(11227.9922, grad_fn=<NegBackward0>) tensor(11227.9941, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11228.0615234375
tensor(11227.9922, grad_fn=<NegBackward0>) tensor(11228.0615, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11227.9892578125
tensor(11227.9922, grad_fn=<NegBackward0>) tensor(11227.9893, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11227.990234375
tensor(11227.9893, grad_fn=<NegBackward0>) tensor(11227.9902, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11227.9970703125
tensor(11227.9893, grad_fn=<NegBackward0>) tensor(11227.9971, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11227.9912109375
tensor(11227.9893, grad_fn=<NegBackward0>) tensor(11227.9912, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11227.9892578125
tensor(11227.9893, grad_fn=<NegBackward0>) tensor(11227.9893, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11227.98828125
tensor(11227.9893, grad_fn=<NegBackward0>) tensor(11227.9883, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11227.9892578125
tensor(11227.9883, grad_fn=<NegBackward0>) tensor(11227.9893, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7540, 0.2460],
        [0.2104, 0.7896]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5317, 0.4683], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3014, 0.1088],
         [0.6444, 0.1938]],

        [[0.6062, 0.1092],
         [0.5381, 0.7298]],

        [[0.5389, 0.1043],
         [0.5758, 0.5462]],

        [[0.6735, 0.0955],
         [0.5669, 0.5469]],

        [[0.7308, 0.1049],
         [0.6512, 0.6383]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7717887095983128
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.8909174658891703
Average Adjusted Rand Index: 0.8911653477076648
[0.8909174658891703, 0.8909174658891703] [0.8911653477076648, 0.8911653477076648] [11228.01953125, 11228.1396484375]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11161.012324635527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22428.833984375
inf tensor(22428.8340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11384.1953125
tensor(22428.8340, grad_fn=<NegBackward0>) tensor(11384.1953, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11383.6064453125
tensor(11384.1953, grad_fn=<NegBackward0>) tensor(11383.6064, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11383.4140625
tensor(11383.6064, grad_fn=<NegBackward0>) tensor(11383.4141, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11383.2734375
tensor(11383.4141, grad_fn=<NegBackward0>) tensor(11383.2734, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11383.1337890625
tensor(11383.2734, grad_fn=<NegBackward0>) tensor(11383.1338, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11382.9443359375
tensor(11383.1338, grad_fn=<NegBackward0>) tensor(11382.9443, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11382.15625
tensor(11382.9443, grad_fn=<NegBackward0>) tensor(11382.1562, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11381.05078125
tensor(11382.1562, grad_fn=<NegBackward0>) tensor(11381.0508, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11380.7587890625
tensor(11381.0508, grad_fn=<NegBackward0>) tensor(11380.7588, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11380.6357421875
tensor(11380.7588, grad_fn=<NegBackward0>) tensor(11380.6357, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11380.5361328125
tensor(11380.6357, grad_fn=<NegBackward0>) tensor(11380.5361, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11380.4248046875
tensor(11380.5361, grad_fn=<NegBackward0>) tensor(11380.4248, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11380.234375
tensor(11380.4248, grad_fn=<NegBackward0>) tensor(11380.2344, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11379.6083984375
tensor(11380.2344, grad_fn=<NegBackward0>) tensor(11379.6084, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11378.396484375
tensor(11379.6084, grad_fn=<NegBackward0>) tensor(11378.3965, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11363.7138671875
tensor(11378.3965, grad_fn=<NegBackward0>) tensor(11363.7139, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11338.6787109375
tensor(11363.7139, grad_fn=<NegBackward0>) tensor(11338.6787, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11338.3740234375
tensor(11338.6787, grad_fn=<NegBackward0>) tensor(11338.3740, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11338.296875
tensor(11338.3740, grad_fn=<NegBackward0>) tensor(11338.2969, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11338.263671875
tensor(11338.2969, grad_fn=<NegBackward0>) tensor(11338.2637, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11338.244140625
tensor(11338.2637, grad_fn=<NegBackward0>) tensor(11338.2441, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11338.2294921875
tensor(11338.2441, grad_fn=<NegBackward0>) tensor(11338.2295, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11338.2197265625
tensor(11338.2295, grad_fn=<NegBackward0>) tensor(11338.2197, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11338.21484375
tensor(11338.2197, grad_fn=<NegBackward0>) tensor(11338.2148, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11338.208984375
tensor(11338.2148, grad_fn=<NegBackward0>) tensor(11338.2090, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11338.2060546875
tensor(11338.2090, grad_fn=<NegBackward0>) tensor(11338.2061, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11338.203125
tensor(11338.2061, grad_fn=<NegBackward0>) tensor(11338.2031, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11338.2001953125
tensor(11338.2031, grad_fn=<NegBackward0>) tensor(11338.2002, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11338.19921875
tensor(11338.2002, grad_fn=<NegBackward0>) tensor(11338.1992, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11338.1982421875
tensor(11338.1992, grad_fn=<NegBackward0>) tensor(11338.1982, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11338.1962890625
tensor(11338.1982, grad_fn=<NegBackward0>) tensor(11338.1963, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11338.1953125
tensor(11338.1963, grad_fn=<NegBackward0>) tensor(11338.1953, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11338.1943359375
tensor(11338.1953, grad_fn=<NegBackward0>) tensor(11338.1943, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11338.1923828125
tensor(11338.1943, grad_fn=<NegBackward0>) tensor(11338.1924, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11338.1923828125
tensor(11338.1924, grad_fn=<NegBackward0>) tensor(11338.1924, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11338.1904296875
tensor(11338.1924, grad_fn=<NegBackward0>) tensor(11338.1904, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11338.1904296875
tensor(11338.1904, grad_fn=<NegBackward0>) tensor(11338.1904, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11338.189453125
tensor(11338.1904, grad_fn=<NegBackward0>) tensor(11338.1895, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11338.189453125
tensor(11338.1895, grad_fn=<NegBackward0>) tensor(11338.1895, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11338.189453125
tensor(11338.1895, grad_fn=<NegBackward0>) tensor(11338.1895, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11338.193359375
tensor(11338.1895, grad_fn=<NegBackward0>) tensor(11338.1934, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11338.1884765625
tensor(11338.1895, grad_fn=<NegBackward0>) tensor(11338.1885, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11338.1884765625
tensor(11338.1885, grad_fn=<NegBackward0>) tensor(11338.1885, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11338.189453125
tensor(11338.1885, grad_fn=<NegBackward0>) tensor(11338.1895, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11338.1875
tensor(11338.1885, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11338.189453125
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1895, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11338.1875
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11338.1875
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11338.1875
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11338.1884765625
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1885, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11338.19140625
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1914, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11338.1884765625
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1885, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11338.1875
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11338.1875
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11338.1884765625
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1885, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11338.1865234375
tensor(11338.1875, grad_fn=<NegBackward0>) tensor(11338.1865, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11338.1875
tensor(11338.1865, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11338.185546875
tensor(11338.1865, grad_fn=<NegBackward0>) tensor(11338.1855, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11338.1875
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11338.185546875
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1855, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11338.1865234375
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1865, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11338.1875
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11338.1875
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11338.1875
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1875, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11338.1865234375
tensor(11338.1855, grad_fn=<NegBackward0>) tensor(11338.1865, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.2159, 0.7841],
        [0.0154, 0.9846]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5662, 0.4338], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2733, 0.0985],
         [0.6566, 0.1696]],

        [[0.5260, 0.1963],
         [0.6809, 0.7100]],

        [[0.6561, 0.2598],
         [0.5513, 0.5242]],

        [[0.5592, 0.2263],
         [0.5770, 0.7061]],

        [[0.6373, 0.1529],
         [0.6427, 0.5044]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.018778022358394132
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.08626577519960732
Average Adjusted Rand Index: 0.17435242632522913
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22235.798828125
inf tensor(22235.7988, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11383.1767578125
tensor(22235.7988, grad_fn=<NegBackward0>) tensor(11383.1768, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11382.8095703125
tensor(11383.1768, grad_fn=<NegBackward0>) tensor(11382.8096, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11382.634765625
tensor(11382.8096, grad_fn=<NegBackward0>) tensor(11382.6348, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11382.4853515625
tensor(11382.6348, grad_fn=<NegBackward0>) tensor(11382.4854, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11382.3681640625
tensor(11382.4854, grad_fn=<NegBackward0>) tensor(11382.3682, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11382.2724609375
tensor(11382.3682, grad_fn=<NegBackward0>) tensor(11382.2725, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11382.197265625
tensor(11382.2725, grad_fn=<NegBackward0>) tensor(11382.1973, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11382.13671875
tensor(11382.1973, grad_fn=<NegBackward0>) tensor(11382.1367, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11382.0869140625
tensor(11382.1367, grad_fn=<NegBackward0>) tensor(11382.0869, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11382.044921875
tensor(11382.0869, grad_fn=<NegBackward0>) tensor(11382.0449, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11382.0078125
tensor(11382.0449, grad_fn=<NegBackward0>) tensor(11382.0078, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11381.9755859375
tensor(11382.0078, grad_fn=<NegBackward0>) tensor(11381.9756, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11381.9423828125
tensor(11381.9756, grad_fn=<NegBackward0>) tensor(11381.9424, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11381.9111328125
tensor(11381.9424, grad_fn=<NegBackward0>) tensor(11381.9111, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11381.8837890625
tensor(11381.9111, grad_fn=<NegBackward0>) tensor(11381.8838, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11381.8515625
tensor(11381.8838, grad_fn=<NegBackward0>) tensor(11381.8516, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11381.8154296875
tensor(11381.8516, grad_fn=<NegBackward0>) tensor(11381.8154, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11381.76953125
tensor(11381.8154, grad_fn=<NegBackward0>) tensor(11381.7695, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11381.708984375
tensor(11381.7695, grad_fn=<NegBackward0>) tensor(11381.7090, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11381.6201171875
tensor(11381.7090, grad_fn=<NegBackward0>) tensor(11381.6201, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11380.54296875
tensor(11381.6201, grad_fn=<NegBackward0>) tensor(11380.5430, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11379.0791015625
tensor(11380.5430, grad_fn=<NegBackward0>) tensor(11379.0791, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11318.12109375
tensor(11379.0791, grad_fn=<NegBackward0>) tensor(11318.1211, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11206.7451171875
tensor(11318.1211, grad_fn=<NegBackward0>) tensor(11206.7451, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11200.6533203125
tensor(11206.7451, grad_fn=<NegBackward0>) tensor(11200.6533, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11190.095703125
tensor(11200.6533, grad_fn=<NegBackward0>) tensor(11190.0957, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11189.37109375
tensor(11190.0957, grad_fn=<NegBackward0>) tensor(11189.3711, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11189.2646484375
tensor(11189.3711, grad_fn=<NegBackward0>) tensor(11189.2646, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11189.20703125
tensor(11189.2646, grad_fn=<NegBackward0>) tensor(11189.2070, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11184.5634765625
tensor(11189.2070, grad_fn=<NegBackward0>) tensor(11184.5635, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11181.693359375
tensor(11184.5635, grad_fn=<NegBackward0>) tensor(11181.6934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11181.5693359375
tensor(11181.6934, grad_fn=<NegBackward0>) tensor(11181.5693, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11181.55859375
tensor(11181.5693, grad_fn=<NegBackward0>) tensor(11181.5586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11181.5068359375
tensor(11181.5586, grad_fn=<NegBackward0>) tensor(11181.5068, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11181.2509765625
tensor(11181.5068, grad_fn=<NegBackward0>) tensor(11181.2510, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11181.2021484375
tensor(11181.2510, grad_fn=<NegBackward0>) tensor(11181.2021, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11181.1728515625
tensor(11181.2021, grad_fn=<NegBackward0>) tensor(11181.1729, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11181.1708984375
tensor(11181.1729, grad_fn=<NegBackward0>) tensor(11181.1709, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11181.169921875
tensor(11181.1709, grad_fn=<NegBackward0>) tensor(11181.1699, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11181.1669921875
tensor(11181.1699, grad_fn=<NegBackward0>) tensor(11181.1670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11181.166015625
tensor(11181.1670, grad_fn=<NegBackward0>) tensor(11181.1660, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11181.1220703125
tensor(11181.1660, grad_fn=<NegBackward0>) tensor(11181.1221, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11181.1201171875
tensor(11181.1221, grad_fn=<NegBackward0>) tensor(11181.1201, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11181.1181640625
tensor(11181.1201, grad_fn=<NegBackward0>) tensor(11181.1182, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11181.1044921875
tensor(11181.1182, grad_fn=<NegBackward0>) tensor(11181.1045, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11181.119140625
tensor(11181.1045, grad_fn=<NegBackward0>) tensor(11181.1191, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11181.103515625
tensor(11181.1045, grad_fn=<NegBackward0>) tensor(11181.1035, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11181.11328125
tensor(11181.1035, grad_fn=<NegBackward0>) tensor(11181.1133, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11181.095703125
tensor(11181.1035, grad_fn=<NegBackward0>) tensor(11181.0957, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11180.87109375
tensor(11181.0957, grad_fn=<NegBackward0>) tensor(11180.8711, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11180.861328125
tensor(11180.8711, grad_fn=<NegBackward0>) tensor(11180.8613, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11180.8525390625
tensor(11180.8613, grad_fn=<NegBackward0>) tensor(11180.8525, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11180.8515625
tensor(11180.8525, grad_fn=<NegBackward0>) tensor(11180.8516, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11180.8564453125
tensor(11180.8516, grad_fn=<NegBackward0>) tensor(11180.8564, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11180.8515625
tensor(11180.8516, grad_fn=<NegBackward0>) tensor(11180.8516, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11180.8505859375
tensor(11180.8516, grad_fn=<NegBackward0>) tensor(11180.8506, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11180.8525390625
tensor(11180.8506, grad_fn=<NegBackward0>) tensor(11180.8525, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11180.8505859375
tensor(11180.8506, grad_fn=<NegBackward0>) tensor(11180.8506, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11180.8544921875
tensor(11180.8506, grad_fn=<NegBackward0>) tensor(11180.8545, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11180.8623046875
tensor(11180.8506, grad_fn=<NegBackward0>) tensor(11180.8623, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11180.837890625
tensor(11180.8506, grad_fn=<NegBackward0>) tensor(11180.8379, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11180.8388671875
tensor(11180.8379, grad_fn=<NegBackward0>) tensor(11180.8389, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11180.8369140625
tensor(11180.8379, grad_fn=<NegBackward0>) tensor(11180.8369, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11180.755859375
tensor(11180.8369, grad_fn=<NegBackward0>) tensor(11180.7559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11180.1103515625
tensor(11180.7559, grad_fn=<NegBackward0>) tensor(11180.1104, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11180.1064453125
tensor(11180.1104, grad_fn=<NegBackward0>) tensor(11180.1064, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11180.1123046875
tensor(11180.1064, grad_fn=<NegBackward0>) tensor(11180.1123, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11180.0234375
tensor(11180.1064, grad_fn=<NegBackward0>) tensor(11180.0234, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11180.0087890625
tensor(11180.0234, grad_fn=<NegBackward0>) tensor(11180.0088, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11180.013671875
tensor(11180.0088, grad_fn=<NegBackward0>) tensor(11180.0137, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11180.015625
tensor(11180.0088, grad_fn=<NegBackward0>) tensor(11180.0156, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11180.0078125
tensor(11180.0088, grad_fn=<NegBackward0>) tensor(11180.0078, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11180.005859375
tensor(11180.0078, grad_fn=<NegBackward0>) tensor(11180.0059, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11180.0029296875
tensor(11180.0059, grad_fn=<NegBackward0>) tensor(11180.0029, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11180.0078125
tensor(11180.0029, grad_fn=<NegBackward0>) tensor(11180.0078, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11180.0029296875
tensor(11180.0029, grad_fn=<NegBackward0>) tensor(11180.0029, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11180.0029296875
tensor(11180.0029, grad_fn=<NegBackward0>) tensor(11180.0029, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11180.001953125
tensor(11180.0029, grad_fn=<NegBackward0>) tensor(11180.0020, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11179.9990234375
tensor(11180.0020, grad_fn=<NegBackward0>) tensor(11179.9990, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11179.9609375
tensor(11179.9990, grad_fn=<NegBackward0>) tensor(11179.9609, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11179.9697265625
tensor(11179.9609, grad_fn=<NegBackward0>) tensor(11179.9697, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11179.9609375
tensor(11179.9609, grad_fn=<NegBackward0>) tensor(11179.9609, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11179.95703125
tensor(11179.9609, grad_fn=<NegBackward0>) tensor(11179.9570, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11179.9521484375
tensor(11179.9570, grad_fn=<NegBackward0>) tensor(11179.9521, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11179.958984375
tensor(11179.9521, grad_fn=<NegBackward0>) tensor(11179.9590, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11179.9521484375
tensor(11179.9521, grad_fn=<NegBackward0>) tensor(11179.9521, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11179.9482421875
tensor(11179.9521, grad_fn=<NegBackward0>) tensor(11179.9482, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11179.9501953125
tensor(11179.9482, grad_fn=<NegBackward0>) tensor(11179.9502, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11179.94921875
tensor(11179.9482, grad_fn=<NegBackward0>) tensor(11179.9492, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11179.96875
tensor(11179.9482, grad_fn=<NegBackward0>) tensor(11179.9688, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11179.947265625
tensor(11179.9482, grad_fn=<NegBackward0>) tensor(11179.9473, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11179.947265625
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9473, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11179.947265625
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9473, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11179.947265625
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9473, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11179.94921875
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9492, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11179.947265625
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9473, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11179.953125
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9531, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11179.9462890625
tensor(11179.9473, grad_fn=<NegBackward0>) tensor(11179.9463, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11179.9443359375
tensor(11179.9463, grad_fn=<NegBackward0>) tensor(11179.9443, grad_fn=<NegBackward0>)
pi: tensor([[0.3027, 0.6973],
        [0.7344, 0.2656]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4076, 0.5924], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2317, 0.0960],
         [0.6011, 0.2573]],

        [[0.5674, 0.1086],
         [0.5134, 0.5801]],

        [[0.7038, 0.0995],
         [0.6786, 0.5213]],

        [[0.6663, 0.1103],
         [0.5414, 0.6151]],

        [[0.5606, 0.0857],
         [0.7293, 0.6906]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721545392564556
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.5733734495489129
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
time is 4
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
Global Adjusted Rand Index: 0.04649929029419325
Average Adjusted Rand Index: 0.7987227797987274
[0.08626577519960732, 0.04649929029419325] [0.17435242632522913, 0.7987227797987274] [11338.1865234375, 11179.9443359375]
-------------------------------------
This iteration is 82
True Objective function: Loss = -10990.855426454336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21386.13671875
inf tensor(21386.1367, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11201.767578125
tensor(21386.1367, grad_fn=<NegBackward0>) tensor(11201.7676, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11201.3095703125
tensor(11201.7676, grad_fn=<NegBackward0>) tensor(11201.3096, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11201.17578125
tensor(11201.3096, grad_fn=<NegBackward0>) tensor(11201.1758, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11201.1240234375
tensor(11201.1758, grad_fn=<NegBackward0>) tensor(11201.1240, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11201.083984375
tensor(11201.1240, grad_fn=<NegBackward0>) tensor(11201.0840, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11201.0390625
tensor(11201.0840, grad_fn=<NegBackward0>) tensor(11201.0391, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11200.9775390625
tensor(11201.0391, grad_fn=<NegBackward0>) tensor(11200.9775, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11200.87890625
tensor(11200.9775, grad_fn=<NegBackward0>) tensor(11200.8789, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11200.72265625
tensor(11200.8789, grad_fn=<NegBackward0>) tensor(11200.7227, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11200.5771484375
tensor(11200.7227, grad_fn=<NegBackward0>) tensor(11200.5771, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11200.501953125
tensor(11200.5771, grad_fn=<NegBackward0>) tensor(11200.5020, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11200.4580078125
tensor(11200.5020, grad_fn=<NegBackward0>) tensor(11200.4580, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11200.4248046875
tensor(11200.4580, grad_fn=<NegBackward0>) tensor(11200.4248, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11200.392578125
tensor(11200.4248, grad_fn=<NegBackward0>) tensor(11200.3926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11200.359375
tensor(11200.3926, grad_fn=<NegBackward0>) tensor(11200.3594, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11200.330078125
tensor(11200.3594, grad_fn=<NegBackward0>) tensor(11200.3301, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11200.296875
tensor(11200.3301, grad_fn=<NegBackward0>) tensor(11200.2969, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11200.24609375
tensor(11200.2969, grad_fn=<NegBackward0>) tensor(11200.2461, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11200.146484375
tensor(11200.2461, grad_fn=<NegBackward0>) tensor(11200.1465, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11199.767578125
tensor(11200.1465, grad_fn=<NegBackward0>) tensor(11199.7676, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11196.064453125
tensor(11199.7676, grad_fn=<NegBackward0>) tensor(11196.0645, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10954.849609375
tensor(11196.0645, grad_fn=<NegBackward0>) tensor(10954.8496, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10953.2607421875
tensor(10954.8496, grad_fn=<NegBackward0>) tensor(10953.2607, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10953.0908203125
tensor(10953.2607, grad_fn=<NegBackward0>) tensor(10953.0908, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10953.0546875
tensor(10953.0908, grad_fn=<NegBackward0>) tensor(10953.0547, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10953.0166015625
tensor(10953.0547, grad_fn=<NegBackward0>) tensor(10953.0166, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10952.87890625
tensor(10953.0166, grad_fn=<NegBackward0>) tensor(10952.8789, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10952.865234375
tensor(10952.8789, grad_fn=<NegBackward0>) tensor(10952.8652, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10952.8564453125
tensor(10952.8652, grad_fn=<NegBackward0>) tensor(10952.8564, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10952.8515625
tensor(10952.8564, grad_fn=<NegBackward0>) tensor(10952.8516, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10952.84765625
tensor(10952.8516, grad_fn=<NegBackward0>) tensor(10952.8477, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10952.84375
tensor(10952.8477, grad_fn=<NegBackward0>) tensor(10952.8438, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10952.8369140625
tensor(10952.8438, grad_fn=<NegBackward0>) tensor(10952.8369, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10952.8232421875
tensor(10952.8369, grad_fn=<NegBackward0>) tensor(10952.8232, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10952.791015625
tensor(10952.8232, grad_fn=<NegBackward0>) tensor(10952.7910, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10952.771484375
tensor(10952.7910, grad_fn=<NegBackward0>) tensor(10952.7715, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10952.7373046875
tensor(10952.7715, grad_fn=<NegBackward0>) tensor(10952.7373, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10952.73046875
tensor(10952.7373, grad_fn=<NegBackward0>) tensor(10952.7305, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10952.728515625
tensor(10952.7305, grad_fn=<NegBackward0>) tensor(10952.7285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10952.7275390625
tensor(10952.7285, grad_fn=<NegBackward0>) tensor(10952.7275, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10952.7265625
tensor(10952.7275, grad_fn=<NegBackward0>) tensor(10952.7266, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10952.7255859375
tensor(10952.7266, grad_fn=<NegBackward0>) tensor(10952.7256, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10952.7236328125
tensor(10952.7256, grad_fn=<NegBackward0>) tensor(10952.7236, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10952.7236328125
tensor(10952.7236, grad_fn=<NegBackward0>) tensor(10952.7236, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10952.7158203125
tensor(10952.7236, grad_fn=<NegBackward0>) tensor(10952.7158, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10952.71484375
tensor(10952.7158, grad_fn=<NegBackward0>) tensor(10952.7148, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10952.71484375
tensor(10952.7148, grad_fn=<NegBackward0>) tensor(10952.7148, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10952.7138671875
tensor(10952.7148, grad_fn=<NegBackward0>) tensor(10952.7139, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10952.712890625
tensor(10952.7139, grad_fn=<NegBackward0>) tensor(10952.7129, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10952.7119140625
tensor(10952.7129, grad_fn=<NegBackward0>) tensor(10952.7119, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10952.7099609375
tensor(10952.7119, grad_fn=<NegBackward0>) tensor(10952.7100, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10952.673828125
tensor(10952.7100, grad_fn=<NegBackward0>) tensor(10952.6738, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10952.671875
tensor(10952.6738, grad_fn=<NegBackward0>) tensor(10952.6719, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10952.669921875
tensor(10952.6719, grad_fn=<NegBackward0>) tensor(10952.6699, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10952.6689453125
tensor(10952.6699, grad_fn=<NegBackward0>) tensor(10952.6689, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10952.67578125
tensor(10952.6689, grad_fn=<NegBackward0>) tensor(10952.6758, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10952.66796875
tensor(10952.6689, grad_fn=<NegBackward0>) tensor(10952.6680, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10952.66796875
tensor(10952.6680, grad_fn=<NegBackward0>) tensor(10952.6680, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10952.666015625
tensor(10952.6680, grad_fn=<NegBackward0>) tensor(10952.6660, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10952.666015625
tensor(10952.6660, grad_fn=<NegBackward0>) tensor(10952.6660, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10952.65234375
tensor(10952.6660, grad_fn=<NegBackward0>) tensor(10952.6523, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10952.640625
tensor(10952.6523, grad_fn=<NegBackward0>) tensor(10952.6406, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10952.6337890625
tensor(10952.6406, grad_fn=<NegBackward0>) tensor(10952.6338, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10952.640625
tensor(10952.6338, grad_fn=<NegBackward0>) tensor(10952.6406, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10952.634765625
tensor(10952.6338, grad_fn=<NegBackward0>) tensor(10952.6348, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10952.6328125
tensor(10952.6338, grad_fn=<NegBackward0>) tensor(10952.6328, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10952.64453125
tensor(10952.6328, grad_fn=<NegBackward0>) tensor(10952.6445, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10952.6328125
tensor(10952.6328, grad_fn=<NegBackward0>) tensor(10952.6328, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10952.638671875
tensor(10952.6328, grad_fn=<NegBackward0>) tensor(10952.6387, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10952.6318359375
tensor(10952.6328, grad_fn=<NegBackward0>) tensor(10952.6318, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10952.630859375
tensor(10952.6318, grad_fn=<NegBackward0>) tensor(10952.6309, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10952.6318359375
tensor(10952.6309, grad_fn=<NegBackward0>) tensor(10952.6318, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10952.640625
tensor(10952.6309, grad_fn=<NegBackward0>) tensor(10952.6406, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10952.6318359375
tensor(10952.6309, grad_fn=<NegBackward0>) tensor(10952.6318, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10952.6318359375
tensor(10952.6309, grad_fn=<NegBackward0>) tensor(10952.6318, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10952.6162109375
tensor(10952.6309, grad_fn=<NegBackward0>) tensor(10952.6162, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10952.6162109375
tensor(10952.6162, grad_fn=<NegBackward0>) tensor(10952.6162, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10952.61328125
tensor(10952.6162, grad_fn=<NegBackward0>) tensor(10952.6133, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10952.6142578125
tensor(10952.6133, grad_fn=<NegBackward0>) tensor(10952.6143, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10952.615234375
tensor(10952.6133, grad_fn=<NegBackward0>) tensor(10952.6152, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -10952.6142578125
tensor(10952.6133, grad_fn=<NegBackward0>) tensor(10952.6143, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -10952.626953125
tensor(10952.6133, grad_fn=<NegBackward0>) tensor(10952.6270, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -10952.6142578125
tensor(10952.6133, grad_fn=<NegBackward0>) tensor(10952.6143, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7999, 0.2001],
        [0.2333, 0.7667]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4786, 0.5214], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0910],
         [0.7170, 0.2800]],

        [[0.6589, 0.0981],
         [0.6796, 0.6505]],

        [[0.5940, 0.1035],
         [0.6850, 0.6776]],

        [[0.5919, 0.0972],
         [0.6217, 0.6965]],

        [[0.5507, 0.0996],
         [0.5349, 0.6775]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733803437637726
Global Adjusted Rand Index: 0.8460924850125714
Average Adjusted Rand Index: 0.8514803617646989
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23405.400390625
inf tensor(23405.4004, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11201.419921875
tensor(23405.4004, grad_fn=<NegBackward0>) tensor(11201.4199, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11201.0791015625
tensor(11201.4199, grad_fn=<NegBackward0>) tensor(11201.0791, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11200.958984375
tensor(11201.0791, grad_fn=<NegBackward0>) tensor(11200.9590, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11200.802734375
tensor(11200.9590, grad_fn=<NegBackward0>) tensor(11200.8027, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11200.568359375
tensor(11200.8027, grad_fn=<NegBackward0>) tensor(11200.5684, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11200.3935546875
tensor(11200.5684, grad_fn=<NegBackward0>) tensor(11200.3936, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11200.28125
tensor(11200.3936, grad_fn=<NegBackward0>) tensor(11200.2812, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11200.09765625
tensor(11200.2812, grad_fn=<NegBackward0>) tensor(11200.0977, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11199.5546875
tensor(11200.0977, grad_fn=<NegBackward0>) tensor(11199.5547, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11196.6201171875
tensor(11199.5547, grad_fn=<NegBackward0>) tensor(11196.6201, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11109.951171875
tensor(11196.6201, grad_fn=<NegBackward0>) tensor(11109.9512, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10953.6728515625
tensor(11109.9512, grad_fn=<NegBackward0>) tensor(10953.6729, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10953.265625
tensor(10953.6729, grad_fn=<NegBackward0>) tensor(10953.2656, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10953.1572265625
tensor(10953.2656, grad_fn=<NegBackward0>) tensor(10953.1572, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10953.10546875
tensor(10953.1572, grad_fn=<NegBackward0>) tensor(10953.1055, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10953.0078125
tensor(10953.1055, grad_fn=<NegBackward0>) tensor(10953.0078, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10952.8642578125
tensor(10953.0078, grad_fn=<NegBackward0>) tensor(10952.8643, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10952.853515625
tensor(10952.8643, grad_fn=<NegBackward0>) tensor(10952.8535, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10952.8515625
tensor(10952.8535, grad_fn=<NegBackward0>) tensor(10952.8516, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10952.822265625
tensor(10952.8516, grad_fn=<NegBackward0>) tensor(10952.8223, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10952.7822265625
tensor(10952.8223, grad_fn=<NegBackward0>) tensor(10952.7822, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10952.771484375
tensor(10952.7822, grad_fn=<NegBackward0>) tensor(10952.7715, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10952.765625
tensor(10952.7715, grad_fn=<NegBackward0>) tensor(10952.7656, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10952.75390625
tensor(10952.7656, grad_fn=<NegBackward0>) tensor(10952.7539, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10952.748046875
tensor(10952.7539, grad_fn=<NegBackward0>) tensor(10952.7480, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10952.74609375
tensor(10952.7480, grad_fn=<NegBackward0>) tensor(10952.7461, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10952.740234375
tensor(10952.7461, grad_fn=<NegBackward0>) tensor(10952.7402, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10952.732421875
tensor(10952.7402, grad_fn=<NegBackward0>) tensor(10952.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10952.728515625
tensor(10952.7324, grad_fn=<NegBackward0>) tensor(10952.7285, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10952.724609375
tensor(10952.7285, grad_fn=<NegBackward0>) tensor(10952.7246, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10952.712890625
tensor(10952.7246, grad_fn=<NegBackward0>) tensor(10952.7129, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10952.7060546875
tensor(10952.7129, grad_fn=<NegBackward0>) tensor(10952.7061, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10952.7060546875
tensor(10952.7061, grad_fn=<NegBackward0>) tensor(10952.7061, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10952.705078125
tensor(10952.7061, grad_fn=<NegBackward0>) tensor(10952.7051, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10952.705078125
tensor(10952.7051, grad_fn=<NegBackward0>) tensor(10952.7051, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10952.7060546875
tensor(10952.7051, grad_fn=<NegBackward0>) tensor(10952.7061, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10952.703125
tensor(10952.7051, grad_fn=<NegBackward0>) tensor(10952.7031, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10952.7021484375
tensor(10952.7031, grad_fn=<NegBackward0>) tensor(10952.7021, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10952.6845703125
tensor(10952.7021, grad_fn=<NegBackward0>) tensor(10952.6846, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10952.6796875
tensor(10952.6846, grad_fn=<NegBackward0>) tensor(10952.6797, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10952.6787109375
tensor(10952.6797, grad_fn=<NegBackward0>) tensor(10952.6787, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10952.6708984375
tensor(10952.6787, grad_fn=<NegBackward0>) tensor(10952.6709, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10952.666015625
tensor(10952.6709, grad_fn=<NegBackward0>) tensor(10952.6660, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10952.6650390625
tensor(10952.6660, grad_fn=<NegBackward0>) tensor(10952.6650, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10952.6728515625
tensor(10952.6650, grad_fn=<NegBackward0>) tensor(10952.6729, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10952.6611328125
tensor(10952.6650, grad_fn=<NegBackward0>) tensor(10952.6611, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10952.662109375
tensor(10952.6611, grad_fn=<NegBackward0>) tensor(10952.6621, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10952.6826171875
tensor(10952.6611, grad_fn=<NegBackward0>) tensor(10952.6826, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10952.662109375
tensor(10952.6611, grad_fn=<NegBackward0>) tensor(10952.6621, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10952.6806640625
tensor(10952.6611, grad_fn=<NegBackward0>) tensor(10952.6807, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10952.66015625
tensor(10952.6611, grad_fn=<NegBackward0>) tensor(10952.6602, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10952.6611328125
tensor(10952.6602, grad_fn=<NegBackward0>) tensor(10952.6611, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10952.66015625
tensor(10952.6602, grad_fn=<NegBackward0>) tensor(10952.6602, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10952.6591796875
tensor(10952.6602, grad_fn=<NegBackward0>) tensor(10952.6592, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10952.6611328125
tensor(10952.6592, grad_fn=<NegBackward0>) tensor(10952.6611, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10952.658203125
tensor(10952.6592, grad_fn=<NegBackward0>) tensor(10952.6582, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10952.666015625
tensor(10952.6582, grad_fn=<NegBackward0>) tensor(10952.6660, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10952.654296875
tensor(10952.6582, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10952.6572265625
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6572, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10952.6552734375
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6553, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10952.654296875
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10952.66015625
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6602, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10952.6669921875
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6670, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10952.6533203125
tensor(10952.6543, grad_fn=<NegBackward0>) tensor(10952.6533, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10952.654296875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10952.6533203125
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6533, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10952.654296875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10952.6591796875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6592, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10952.654296875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10952.654296875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10952.654296875
tensor(10952.6533, grad_fn=<NegBackward0>) tensor(10952.6543, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.8000, 0.2000],
        [0.2332, 0.7668]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4782, 0.5218], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0910],
         [0.6247, 0.2799]],

        [[0.5373, 0.0981],
         [0.6812, 0.6969]],

        [[0.5913, 0.1035],
         [0.5464, 0.5054]],

        [[0.5437, 0.0972],
         [0.5585, 0.6763]],

        [[0.5560, 0.0996],
         [0.7259, 0.6146]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733803437637726
Global Adjusted Rand Index: 0.8460924850125714
Average Adjusted Rand Index: 0.8514803617646989
[0.8460924850125714, 0.8460924850125714] [0.8514803617646989, 0.8514803617646989] [10952.6142578125, 10952.654296875]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11180.198097272678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21818.333984375
inf tensor(21818.3340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11451.1875
tensor(21818.3340, grad_fn=<NegBackward0>) tensor(11451.1875, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11447.3173828125
tensor(11451.1875, grad_fn=<NegBackward0>) tensor(11447.3174, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11443.1923828125
tensor(11447.3174, grad_fn=<NegBackward0>) tensor(11443.1924, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11438.927734375
tensor(11443.1924, grad_fn=<NegBackward0>) tensor(11438.9277, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11416.3984375
tensor(11438.9277, grad_fn=<NegBackward0>) tensor(11416.3984, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11249.85546875
tensor(11416.3984, grad_fn=<NegBackward0>) tensor(11249.8555, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11186.7119140625
tensor(11249.8555, grad_fn=<NegBackward0>) tensor(11186.7119, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11180.8525390625
tensor(11186.7119, grad_fn=<NegBackward0>) tensor(11180.8525, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11179.4130859375
tensor(11180.8525, grad_fn=<NegBackward0>) tensor(11179.4131, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11175.2373046875
tensor(11179.4131, grad_fn=<NegBackward0>) tensor(11175.2373, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11168.3916015625
tensor(11175.2373, grad_fn=<NegBackward0>) tensor(11168.3916, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11167.4765625
tensor(11168.3916, grad_fn=<NegBackward0>) tensor(11167.4766, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11167.4375
tensor(11167.4766, grad_fn=<NegBackward0>) tensor(11167.4375, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11164.65625
tensor(11167.4375, grad_fn=<NegBackward0>) tensor(11164.6562, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11164.5654296875
tensor(11164.6562, grad_fn=<NegBackward0>) tensor(11164.5654, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11164.546875
tensor(11164.5654, grad_fn=<NegBackward0>) tensor(11164.5469, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11164.53125
tensor(11164.5469, grad_fn=<NegBackward0>) tensor(11164.5312, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11164.5068359375
tensor(11164.5312, grad_fn=<NegBackward0>) tensor(11164.5068, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11164.4716796875
tensor(11164.5068, grad_fn=<NegBackward0>) tensor(11164.4717, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11164.46484375
tensor(11164.4717, grad_fn=<NegBackward0>) tensor(11164.4648, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11164.4599609375
tensor(11164.4648, grad_fn=<NegBackward0>) tensor(11164.4600, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11164.4248046875
tensor(11164.4600, grad_fn=<NegBackward0>) tensor(11164.4248, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11164.4208984375
tensor(11164.4248, grad_fn=<NegBackward0>) tensor(11164.4209, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11164.4169921875
tensor(11164.4209, grad_fn=<NegBackward0>) tensor(11164.4170, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11159.2060546875
tensor(11164.4170, grad_fn=<NegBackward0>) tensor(11159.2061, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11159.1884765625
tensor(11159.2061, grad_fn=<NegBackward0>) tensor(11159.1885, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11159.185546875
tensor(11159.1885, grad_fn=<NegBackward0>) tensor(11159.1855, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11159.18359375
tensor(11159.1855, grad_fn=<NegBackward0>) tensor(11159.1836, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11159.1826171875
tensor(11159.1836, grad_fn=<NegBackward0>) tensor(11159.1826, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11159.1806640625
tensor(11159.1826, grad_fn=<NegBackward0>) tensor(11159.1807, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11159.1806640625
tensor(11159.1807, grad_fn=<NegBackward0>) tensor(11159.1807, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11159.1767578125
tensor(11159.1807, grad_fn=<NegBackward0>) tensor(11159.1768, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11159.173828125
tensor(11159.1768, grad_fn=<NegBackward0>) tensor(11159.1738, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11159.171875
tensor(11159.1738, grad_fn=<NegBackward0>) tensor(11159.1719, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11159.1689453125
tensor(11159.1719, grad_fn=<NegBackward0>) tensor(11159.1689, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11159.16796875
tensor(11159.1689, grad_fn=<NegBackward0>) tensor(11159.1680, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11159.166015625
tensor(11159.1680, grad_fn=<NegBackward0>) tensor(11159.1660, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11159.15625
tensor(11159.1660, grad_fn=<NegBackward0>) tensor(11159.1562, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11159.1533203125
tensor(11159.1562, grad_fn=<NegBackward0>) tensor(11159.1533, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11154.884765625
tensor(11159.1533, grad_fn=<NegBackward0>) tensor(11154.8848, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11154.8798828125
tensor(11154.8848, grad_fn=<NegBackward0>) tensor(11154.8799, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11154.876953125
tensor(11154.8799, grad_fn=<NegBackward0>) tensor(11154.8770, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11154.876953125
tensor(11154.8770, grad_fn=<NegBackward0>) tensor(11154.8770, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11154.8759765625
tensor(11154.8770, grad_fn=<NegBackward0>) tensor(11154.8760, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11154.8740234375
tensor(11154.8760, grad_fn=<NegBackward0>) tensor(11154.8740, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11154.8720703125
tensor(11154.8740, grad_fn=<NegBackward0>) tensor(11154.8721, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11154.8603515625
tensor(11154.8721, grad_fn=<NegBackward0>) tensor(11154.8604, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11154.8359375
tensor(11154.8604, grad_fn=<NegBackward0>) tensor(11154.8359, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11154.8515625
tensor(11154.8359, grad_fn=<NegBackward0>) tensor(11154.8516, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11154.8359375
tensor(11154.8359, grad_fn=<NegBackward0>) tensor(11154.8359, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11154.8349609375
tensor(11154.8359, grad_fn=<NegBackward0>) tensor(11154.8350, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11154.8349609375
tensor(11154.8350, grad_fn=<NegBackward0>) tensor(11154.8350, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11154.833984375
tensor(11154.8350, grad_fn=<NegBackward0>) tensor(11154.8340, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11154.833984375
tensor(11154.8340, grad_fn=<NegBackward0>) tensor(11154.8340, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11154.8408203125
tensor(11154.8340, grad_fn=<NegBackward0>) tensor(11154.8408, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11154.8330078125
tensor(11154.8340, grad_fn=<NegBackward0>) tensor(11154.8330, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11154.8330078125
tensor(11154.8330, grad_fn=<NegBackward0>) tensor(11154.8330, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11154.83203125
tensor(11154.8330, grad_fn=<NegBackward0>) tensor(11154.8320, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11154.8330078125
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8330, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11154.8349609375
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8350, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11154.83203125
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8320, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11154.8359375
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8359, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11154.83203125
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8320, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11154.83203125
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8320, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11154.8291015625
tensor(11154.8320, grad_fn=<NegBackward0>) tensor(11154.8291, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11154.828125
tensor(11154.8291, grad_fn=<NegBackward0>) tensor(11154.8281, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11154.8359375
tensor(11154.8281, grad_fn=<NegBackward0>) tensor(11154.8359, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11154.826171875
tensor(11154.8281, grad_fn=<NegBackward0>) tensor(11154.8262, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11154.826171875
tensor(11154.8262, grad_fn=<NegBackward0>) tensor(11154.8262, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11154.8251953125
tensor(11154.8262, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11154.828125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8281, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11154.8251953125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11154.826171875
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8262, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11154.8251953125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11154.826171875
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8262, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11154.8251953125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11154.830078125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8301, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11154.8251953125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11154.8251953125
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8252, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11154.8212890625
tensor(11154.8252, grad_fn=<NegBackward0>) tensor(11154.8213, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11154.82421875
tensor(11154.8213, grad_fn=<NegBackward0>) tensor(11154.8242, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11154.8212890625
tensor(11154.8213, grad_fn=<NegBackward0>) tensor(11154.8213, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11154.9384765625
tensor(11154.8213, grad_fn=<NegBackward0>) tensor(11154.9385, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11154.802734375
tensor(11154.8213, grad_fn=<NegBackward0>) tensor(11154.8027, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11154.8095703125
tensor(11154.8027, grad_fn=<NegBackward0>) tensor(11154.8096, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11154.7919921875
tensor(11154.8027, grad_fn=<NegBackward0>) tensor(11154.7920, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11154.7919921875
tensor(11154.7920, grad_fn=<NegBackward0>) tensor(11154.7920, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11154.7802734375
tensor(11154.7920, grad_fn=<NegBackward0>) tensor(11154.7803, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11154.783203125
tensor(11154.7803, grad_fn=<NegBackward0>) tensor(11154.7832, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11154.779296875
tensor(11154.7803, grad_fn=<NegBackward0>) tensor(11154.7793, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11154.830078125
tensor(11154.7793, grad_fn=<NegBackward0>) tensor(11154.8301, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11154.775390625
tensor(11154.7793, grad_fn=<NegBackward0>) tensor(11154.7754, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11154.7724609375
tensor(11154.7754, grad_fn=<NegBackward0>) tensor(11154.7725, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11154.767578125
tensor(11154.7725, grad_fn=<NegBackward0>) tensor(11154.7676, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11154.767578125
tensor(11154.7676, grad_fn=<NegBackward0>) tensor(11154.7676, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11154.7705078125
tensor(11154.7676, grad_fn=<NegBackward0>) tensor(11154.7705, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11154.7666015625
tensor(11154.7676, grad_fn=<NegBackward0>) tensor(11154.7666, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11154.767578125
tensor(11154.7666, grad_fn=<NegBackward0>) tensor(11154.7676, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11154.76953125
tensor(11154.7666, grad_fn=<NegBackward0>) tensor(11154.7695, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7657, 0.2343],
        [0.2491, 0.7509]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4563, 0.5437], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.1024],
         [0.5619, 0.2970]],

        [[0.5918, 0.0994],
         [0.5337, 0.5816]],

        [[0.5259, 0.0938],
         [0.6908, 0.6359]],

        [[0.5684, 0.1009],
         [0.5392, 0.6213]],

        [[0.7196, 0.1041],
         [0.6898, 0.7041]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.9137632159435181
Average Adjusted Rand Index: 0.9149062545305784
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23172.318359375
inf tensor(23172.3184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11450.3349609375
tensor(23172.3184, grad_fn=<NegBackward0>) tensor(11450.3350, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11446.0703125
tensor(11450.3350, grad_fn=<NegBackward0>) tensor(11446.0703, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11439.3720703125
tensor(11446.0703, grad_fn=<NegBackward0>) tensor(11439.3721, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11433.8564453125
tensor(11439.3721, grad_fn=<NegBackward0>) tensor(11433.8564, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11403.822265625
tensor(11433.8564, grad_fn=<NegBackward0>) tensor(11403.8223, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11392.3232421875
tensor(11403.8223, grad_fn=<NegBackward0>) tensor(11392.3232, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11357.865234375
tensor(11392.3232, grad_fn=<NegBackward0>) tensor(11357.8652, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11257.2265625
tensor(11357.8652, grad_fn=<NegBackward0>) tensor(11257.2266, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11252.9970703125
tensor(11257.2266, grad_fn=<NegBackward0>) tensor(11252.9971, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11252.705078125
tensor(11252.9971, grad_fn=<NegBackward0>) tensor(11252.7051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11252.5908203125
tensor(11252.7051, grad_fn=<NegBackward0>) tensor(11252.5908, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11252.541015625
tensor(11252.5908, grad_fn=<NegBackward0>) tensor(11252.5410, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11252.4892578125
tensor(11252.5410, grad_fn=<NegBackward0>) tensor(11252.4893, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11251.7236328125
tensor(11252.4893, grad_fn=<NegBackward0>) tensor(11251.7236, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11251.0693359375
tensor(11251.7236, grad_fn=<NegBackward0>) tensor(11251.0693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11248.0341796875
tensor(11251.0693, grad_fn=<NegBackward0>) tensor(11248.0342, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11247.947265625
tensor(11248.0342, grad_fn=<NegBackward0>) tensor(11247.9473, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11241.486328125
tensor(11247.9473, grad_fn=<NegBackward0>) tensor(11241.4863, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11241.443359375
tensor(11241.4863, grad_fn=<NegBackward0>) tensor(11241.4434, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11241.431640625
tensor(11241.4434, grad_fn=<NegBackward0>) tensor(11241.4316, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11241.421875
tensor(11241.4316, grad_fn=<NegBackward0>) tensor(11241.4219, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11241.400390625
tensor(11241.4219, grad_fn=<NegBackward0>) tensor(11241.4004, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11241.1845703125
tensor(11241.4004, grad_fn=<NegBackward0>) tensor(11241.1846, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11241.1806640625
tensor(11241.1846, grad_fn=<NegBackward0>) tensor(11241.1807, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11241.16796875
tensor(11241.1807, grad_fn=<NegBackward0>) tensor(11241.1680, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11241.1513671875
tensor(11241.1680, grad_fn=<NegBackward0>) tensor(11241.1514, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11240.9482421875
tensor(11241.1514, grad_fn=<NegBackward0>) tensor(11240.9482, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11240.943359375
tensor(11240.9482, grad_fn=<NegBackward0>) tensor(11240.9434, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11240.90234375
tensor(11240.9434, grad_fn=<NegBackward0>) tensor(11240.9023, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11240.8935546875
tensor(11240.9023, grad_fn=<NegBackward0>) tensor(11240.8936, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11240.8896484375
tensor(11240.8936, grad_fn=<NegBackward0>) tensor(11240.8896, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11240.8896484375
tensor(11240.8896, grad_fn=<NegBackward0>) tensor(11240.8896, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11240.8896484375
tensor(11240.8896, grad_fn=<NegBackward0>) tensor(11240.8896, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11240.8935546875
tensor(11240.8896, grad_fn=<NegBackward0>) tensor(11240.8936, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11240.884765625
tensor(11240.8896, grad_fn=<NegBackward0>) tensor(11240.8848, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11240.8798828125
tensor(11240.8848, grad_fn=<NegBackward0>) tensor(11240.8799, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11240.8798828125
tensor(11240.8799, grad_fn=<NegBackward0>) tensor(11240.8799, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11240.8779296875
tensor(11240.8799, grad_fn=<NegBackward0>) tensor(11240.8779, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11240.8701171875
tensor(11240.8779, grad_fn=<NegBackward0>) tensor(11240.8701, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11237.904296875
tensor(11240.8701, grad_fn=<NegBackward0>) tensor(11237.9043, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11234.146484375
tensor(11237.9043, grad_fn=<NegBackward0>) tensor(11234.1465, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11234.1083984375
tensor(11234.1465, grad_fn=<NegBackward0>) tensor(11234.1084, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11234.0791015625
tensor(11234.1084, grad_fn=<NegBackward0>) tensor(11234.0791, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11234.1064453125
tensor(11234.0791, grad_fn=<NegBackward0>) tensor(11234.1064, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11234.078125
tensor(11234.0791, grad_fn=<NegBackward0>) tensor(11234.0781, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11234.078125
tensor(11234.0781, grad_fn=<NegBackward0>) tensor(11234.0781, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11234.078125
tensor(11234.0781, grad_fn=<NegBackward0>) tensor(11234.0781, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11234.0830078125
tensor(11234.0781, grad_fn=<NegBackward0>) tensor(11234.0830, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11234.076171875
tensor(11234.0781, grad_fn=<NegBackward0>) tensor(11234.0762, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11234.076171875
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0762, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11234.078125
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0781, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11234.076171875
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0762, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11234.076171875
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0762, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11234.0771484375
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0771, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11234.0751953125
tensor(11234.0762, grad_fn=<NegBackward0>) tensor(11234.0752, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11234.076171875
tensor(11234.0752, grad_fn=<NegBackward0>) tensor(11234.0762, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11234.07421875
tensor(11234.0752, grad_fn=<NegBackward0>) tensor(11234.0742, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11234.0634765625
tensor(11234.0742, grad_fn=<NegBackward0>) tensor(11234.0635, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11234.06640625
tensor(11234.0635, grad_fn=<NegBackward0>) tensor(11234.0664, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11234.0146484375
tensor(11234.0635, grad_fn=<NegBackward0>) tensor(11234.0146, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11233.9970703125
tensor(11234.0146, grad_fn=<NegBackward0>) tensor(11233.9971, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11233.99609375
tensor(11233.9971, grad_fn=<NegBackward0>) tensor(11233.9961, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11233.9970703125
tensor(11233.9961, grad_fn=<NegBackward0>) tensor(11233.9971, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11233.9951171875
tensor(11233.9961, grad_fn=<NegBackward0>) tensor(11233.9951, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11233.994140625
tensor(11233.9951, grad_fn=<NegBackward0>) tensor(11233.9941, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11233.9951171875
tensor(11233.9941, grad_fn=<NegBackward0>) tensor(11233.9951, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11233.994140625
tensor(11233.9941, grad_fn=<NegBackward0>) tensor(11233.9941, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11233.9931640625
tensor(11233.9941, grad_fn=<NegBackward0>) tensor(11233.9932, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11234.0009765625
tensor(11233.9932, grad_fn=<NegBackward0>) tensor(11234.0010, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11233.9921875
tensor(11233.9932, grad_fn=<NegBackward0>) tensor(11233.9922, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11233.9921875
tensor(11233.9922, grad_fn=<NegBackward0>) tensor(11233.9922, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11233.986328125
tensor(11233.9922, grad_fn=<NegBackward0>) tensor(11233.9863, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11233.923828125
tensor(11233.9863, grad_fn=<NegBackward0>) tensor(11233.9238, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11233.9228515625
tensor(11233.9238, grad_fn=<NegBackward0>) tensor(11233.9229, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11233.939453125
tensor(11233.9229, grad_fn=<NegBackward0>) tensor(11233.9395, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11233.9228515625
tensor(11233.9229, grad_fn=<NegBackward0>) tensor(11233.9229, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11233.92578125
tensor(11233.9229, grad_fn=<NegBackward0>) tensor(11233.9258, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11233.921875
tensor(11233.9229, grad_fn=<NegBackward0>) tensor(11233.9219, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11233.921875
tensor(11233.9219, grad_fn=<NegBackward0>) tensor(11233.9219, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11234.0390625
tensor(11233.9219, grad_fn=<NegBackward0>) tensor(11234.0391, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11233.916015625
tensor(11233.9219, grad_fn=<NegBackward0>) tensor(11233.9160, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11233.919921875
tensor(11233.9160, grad_fn=<NegBackward0>) tensor(11233.9199, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11233.9150390625
tensor(11233.9160, grad_fn=<NegBackward0>) tensor(11233.9150, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11233.904296875
tensor(11233.9150, grad_fn=<NegBackward0>) tensor(11233.9043, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11233.90234375
tensor(11233.9043, grad_fn=<NegBackward0>) tensor(11233.9023, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11233.904296875
tensor(11233.9023, grad_fn=<NegBackward0>) tensor(11233.9043, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11233.9033203125
tensor(11233.9023, grad_fn=<NegBackward0>) tensor(11233.9033, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11233.904296875
tensor(11233.9023, grad_fn=<NegBackward0>) tensor(11233.9043, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11233.9033203125
tensor(11233.9023, grad_fn=<NegBackward0>) tensor(11233.9033, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11233.9130859375
tensor(11233.9023, grad_fn=<NegBackward0>) tensor(11233.9131, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.6173, 0.3827],
        [0.4461, 0.5539]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4520, 0.5480], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2031, 0.1022],
         [0.5785, 0.2944]],

        [[0.5583, 0.1002],
         [0.5428, 0.6976]],

        [[0.6510, 0.0977],
         [0.5498, 0.6362]],

        [[0.5950, 0.1015],
         [0.6110, 0.5224]],

        [[0.6190, 0.1046],
         [0.6297, 0.6710]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 28
Adjusted Rand Index: 0.18545454545454546
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.461322963597674
Average Adjusted Rand Index: 0.7743809202921553
[0.9137632159435181, 0.461322963597674] [0.9149062545305784, 0.7743809202921553] [11154.767578125, 11233.9130859375]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11085.753699763682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23751.8359375
inf tensor(23751.8359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11367.73828125
tensor(23751.8359, grad_fn=<NegBackward0>) tensor(11367.7383, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11366.3798828125
tensor(11367.7383, grad_fn=<NegBackward0>) tensor(11366.3799, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11365.71875
tensor(11366.3799, grad_fn=<NegBackward0>) tensor(11365.7188, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11364.2734375
tensor(11365.7188, grad_fn=<NegBackward0>) tensor(11364.2734, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11362.9365234375
tensor(11364.2734, grad_fn=<NegBackward0>) tensor(11362.9365, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11362.646484375
tensor(11362.9365, grad_fn=<NegBackward0>) tensor(11362.6465, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11362.46875
tensor(11362.6465, grad_fn=<NegBackward0>) tensor(11362.4688, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11362.345703125
tensor(11362.4688, grad_fn=<NegBackward0>) tensor(11362.3457, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11362.2333984375
tensor(11362.3457, grad_fn=<NegBackward0>) tensor(11362.2334, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11362.0712890625
tensor(11362.2334, grad_fn=<NegBackward0>) tensor(11362.0713, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11360.923828125
tensor(11362.0713, grad_fn=<NegBackward0>) tensor(11360.9238, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11138.4130859375
tensor(11360.9238, grad_fn=<NegBackward0>) tensor(11138.4131, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11126.9716796875
tensor(11138.4131, grad_fn=<NegBackward0>) tensor(11126.9717, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11126.6611328125
tensor(11126.9717, grad_fn=<NegBackward0>) tensor(11126.6611, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11126.560546875
tensor(11126.6611, grad_fn=<NegBackward0>) tensor(11126.5605, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11126.4970703125
tensor(11126.5605, grad_fn=<NegBackward0>) tensor(11126.4971, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11126.447265625
tensor(11126.4971, grad_fn=<NegBackward0>) tensor(11126.4473, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11126.408203125
tensor(11126.4473, grad_fn=<NegBackward0>) tensor(11126.4082, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11126.375
tensor(11126.4082, grad_fn=<NegBackward0>) tensor(11126.3750, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11125.58984375
tensor(11126.3750, grad_fn=<NegBackward0>) tensor(11125.5898, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11125.5654296875
tensor(11125.5898, grad_fn=<NegBackward0>) tensor(11125.5654, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11125.53515625
tensor(11125.5654, grad_fn=<NegBackward0>) tensor(11125.5352, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11125.49609375
tensor(11125.5352, grad_fn=<NegBackward0>) tensor(11125.4961, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11125.486328125
tensor(11125.4961, grad_fn=<NegBackward0>) tensor(11125.4863, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11125.4755859375
tensor(11125.4863, grad_fn=<NegBackward0>) tensor(11125.4756, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11125.4638671875
tensor(11125.4756, grad_fn=<NegBackward0>) tensor(11125.4639, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11125.44921875
tensor(11125.4639, grad_fn=<NegBackward0>) tensor(11125.4492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11125.419921875
tensor(11125.4492, grad_fn=<NegBackward0>) tensor(11125.4199, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11125.359375
tensor(11125.4199, grad_fn=<NegBackward0>) tensor(11125.3594, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11124.6708984375
tensor(11125.3594, grad_fn=<NegBackward0>) tensor(11124.6709, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11118.353515625
tensor(11124.6709, grad_fn=<NegBackward0>) tensor(11118.3535, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11118.00390625
tensor(11118.3535, grad_fn=<NegBackward0>) tensor(11118.0039, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11117.990234375
tensor(11118.0039, grad_fn=<NegBackward0>) tensor(11117.9902, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11117.9853515625
tensor(11117.9902, grad_fn=<NegBackward0>) tensor(11117.9854, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11117.982421875
tensor(11117.9854, grad_fn=<NegBackward0>) tensor(11117.9824, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11117.9814453125
tensor(11117.9824, grad_fn=<NegBackward0>) tensor(11117.9814, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11117.978515625
tensor(11117.9814, grad_fn=<NegBackward0>) tensor(11117.9785, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11117.9775390625
tensor(11117.9785, grad_fn=<NegBackward0>) tensor(11117.9775, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11117.9755859375
tensor(11117.9775, grad_fn=<NegBackward0>) tensor(11117.9756, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11117.9755859375
tensor(11117.9756, grad_fn=<NegBackward0>) tensor(11117.9756, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11117.9755859375
tensor(11117.9756, grad_fn=<NegBackward0>) tensor(11117.9756, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11117.9755859375
tensor(11117.9756, grad_fn=<NegBackward0>) tensor(11117.9756, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11117.974609375
tensor(11117.9756, grad_fn=<NegBackward0>) tensor(11117.9746, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11117.9736328125
tensor(11117.9746, grad_fn=<NegBackward0>) tensor(11117.9736, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11117.9736328125
tensor(11117.9736, grad_fn=<NegBackward0>) tensor(11117.9736, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11117.9736328125
tensor(11117.9736, grad_fn=<NegBackward0>) tensor(11117.9736, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11117.98046875
tensor(11117.9736, grad_fn=<NegBackward0>) tensor(11117.9805, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11117.970703125
tensor(11117.9736, grad_fn=<NegBackward0>) tensor(11117.9707, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11117.97265625
tensor(11117.9707, grad_fn=<NegBackward0>) tensor(11117.9727, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11117.9716796875
tensor(11117.9707, grad_fn=<NegBackward0>) tensor(11117.9717, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11117.970703125
tensor(11117.9707, grad_fn=<NegBackward0>) tensor(11117.9707, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11117.970703125
tensor(11117.9707, grad_fn=<NegBackward0>) tensor(11117.9707, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11117.9697265625
tensor(11117.9707, grad_fn=<NegBackward0>) tensor(11117.9697, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11117.96875
tensor(11117.9697, grad_fn=<NegBackward0>) tensor(11117.9688, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11117.9658203125
tensor(11117.9688, grad_fn=<NegBackward0>) tensor(11117.9658, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11117.9736328125
tensor(11117.9658, grad_fn=<NegBackward0>) tensor(11117.9736, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11117.9658203125
tensor(11117.9658, grad_fn=<NegBackward0>) tensor(11117.9658, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11117.96875
tensor(11117.9658, grad_fn=<NegBackward0>) tensor(11117.9688, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11117.96484375
tensor(11117.9658, grad_fn=<NegBackward0>) tensor(11117.9648, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11117.978515625
tensor(11117.9648, grad_fn=<NegBackward0>) tensor(11117.9785, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11117.9658203125
tensor(11117.9648, grad_fn=<NegBackward0>) tensor(11117.9658, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11117.96484375
tensor(11117.9648, grad_fn=<NegBackward0>) tensor(11117.9648, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11117.96875
tensor(11117.9648, grad_fn=<NegBackward0>) tensor(11117.9688, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11117.962890625
tensor(11117.9648, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11117.9638671875
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9639, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11117.9638671875
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9639, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11117.962890625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11117.962890625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11117.966796875
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9668, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11117.962890625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11117.962890625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11117.962890625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9629, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11117.970703125
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9707, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11117.9619140625
tensor(11117.9629, grad_fn=<NegBackward0>) tensor(11117.9619, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11117.9638671875
tensor(11117.9619, grad_fn=<NegBackward0>) tensor(11117.9639, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11117.9638671875
tensor(11117.9619, grad_fn=<NegBackward0>) tensor(11117.9639, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11117.9609375
tensor(11117.9619, grad_fn=<NegBackward0>) tensor(11117.9609, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11117.8271484375
tensor(11117.9609, grad_fn=<NegBackward0>) tensor(11117.8271, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11117.828125
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8281, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11117.8271484375
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8271, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11117.8349609375
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8350, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11117.8359375
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8359, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11117.8271484375
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8271, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11117.8251953125
tensor(11117.8271, grad_fn=<NegBackward0>) tensor(11117.8252, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11117.826171875
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8262, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11117.8291015625
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8291, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11117.826171875
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8262, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11117.826171875
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8262, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11117.8330078125
tensor(11117.8252, grad_fn=<NegBackward0>) tensor(11117.8330, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7172, 0.2828],
        [0.3166, 0.6834]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1201, 0.8799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3116, 0.1011],
         [0.5112, 0.1938]],

        [[0.6969, 0.0935],
         [0.7196, 0.6178]],

        [[0.6274, 0.1081],
         [0.5715, 0.6792]],

        [[0.6035, 0.0968],
         [0.6415, 0.5949]],

        [[0.7097, 0.0950],
         [0.5398, 0.5324]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.022043795620437956
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5467012045989653
Average Adjusted Rand Index: 0.7492149936860891
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23166.0078125
inf tensor(23166.0078, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11336.8193359375
tensor(23166.0078, grad_fn=<NegBackward0>) tensor(11336.8193, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11246.681640625
tensor(11336.8193, grad_fn=<NegBackward0>) tensor(11246.6816, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11194.04296875
tensor(11246.6816, grad_fn=<NegBackward0>) tensor(11194.0430, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11191.6640625
tensor(11194.0430, grad_fn=<NegBackward0>) tensor(11191.6641, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11182.12109375
tensor(11191.6641, grad_fn=<NegBackward0>) tensor(11182.1211, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11165.310546875
tensor(11182.1211, grad_fn=<NegBackward0>) tensor(11165.3105, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11165.1748046875
tensor(11165.3105, grad_fn=<NegBackward0>) tensor(11165.1748, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11165.130859375
tensor(11165.1748, grad_fn=<NegBackward0>) tensor(11165.1309, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11165.107421875
tensor(11165.1309, grad_fn=<NegBackward0>) tensor(11165.1074, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11165.095703125
tensor(11165.1074, grad_fn=<NegBackward0>) tensor(11165.0957, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11163.5986328125
tensor(11165.0957, grad_fn=<NegBackward0>) tensor(11163.5986, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11162.9365234375
tensor(11163.5986, grad_fn=<NegBackward0>) tensor(11162.9365, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11162.552734375
tensor(11162.9365, grad_fn=<NegBackward0>) tensor(11162.5527, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11162.5458984375
tensor(11162.5527, grad_fn=<NegBackward0>) tensor(11162.5459, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11162.5537109375
tensor(11162.5459, grad_fn=<NegBackward0>) tensor(11162.5537, grad_fn=<NegBackward0>)
1
Iteration 1600: Loss = -11162.5185546875
tensor(11162.5459, grad_fn=<NegBackward0>) tensor(11162.5186, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11162.521484375
tensor(11162.5186, grad_fn=<NegBackward0>) tensor(11162.5215, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11162.513671875
tensor(11162.5186, grad_fn=<NegBackward0>) tensor(11162.5137, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11162.513671875
tensor(11162.5137, grad_fn=<NegBackward0>) tensor(11162.5137, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11162.5126953125
tensor(11162.5137, grad_fn=<NegBackward0>) tensor(11162.5127, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11162.5146484375
tensor(11162.5127, grad_fn=<NegBackward0>) tensor(11162.5146, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11162.5107421875
tensor(11162.5127, grad_fn=<NegBackward0>) tensor(11162.5107, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11162.5107421875
tensor(11162.5107, grad_fn=<NegBackward0>) tensor(11162.5107, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11162.509765625
tensor(11162.5107, grad_fn=<NegBackward0>) tensor(11162.5098, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11162.5087890625
tensor(11162.5098, grad_fn=<NegBackward0>) tensor(11162.5088, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11162.5107421875
tensor(11162.5088, grad_fn=<NegBackward0>) tensor(11162.5107, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11162.5078125
tensor(11162.5088, grad_fn=<NegBackward0>) tensor(11162.5078, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11162.5078125
tensor(11162.5078, grad_fn=<NegBackward0>) tensor(11162.5078, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11162.505859375
tensor(11162.5078, grad_fn=<NegBackward0>) tensor(11162.5059, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11161.6162109375
tensor(11162.5059, grad_fn=<NegBackward0>) tensor(11161.6162, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11161.5712890625
tensor(11161.6162, grad_fn=<NegBackward0>) tensor(11161.5713, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11161.5693359375
tensor(11161.5713, grad_fn=<NegBackward0>) tensor(11161.5693, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11161.564453125
tensor(11161.5693, grad_fn=<NegBackward0>) tensor(11161.5645, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11161.564453125
tensor(11161.5645, grad_fn=<NegBackward0>) tensor(11161.5645, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11161.5703125
tensor(11161.5645, grad_fn=<NegBackward0>) tensor(11161.5703, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11161.5654296875
tensor(11161.5645, grad_fn=<NegBackward0>) tensor(11161.5654, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11161.564453125
tensor(11161.5645, grad_fn=<NegBackward0>) tensor(11161.5645, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11161.5634765625
tensor(11161.5645, grad_fn=<NegBackward0>) tensor(11161.5635, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11161.5634765625
tensor(11161.5635, grad_fn=<NegBackward0>) tensor(11161.5635, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11161.56640625
tensor(11161.5635, grad_fn=<NegBackward0>) tensor(11161.5664, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11161.5625
tensor(11161.5635, grad_fn=<NegBackward0>) tensor(11161.5625, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11161.5625
tensor(11161.5625, grad_fn=<NegBackward0>) tensor(11161.5625, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11161.5634765625
tensor(11161.5625, grad_fn=<NegBackward0>) tensor(11161.5635, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11161.5615234375
tensor(11161.5625, grad_fn=<NegBackward0>) tensor(11161.5615, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11161.5625
tensor(11161.5615, grad_fn=<NegBackward0>) tensor(11161.5625, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11161.5625
tensor(11161.5615, grad_fn=<NegBackward0>) tensor(11161.5625, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11161.5859375
tensor(11161.5615, grad_fn=<NegBackward0>) tensor(11161.5859, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11161.572265625
tensor(11161.5615, grad_fn=<NegBackward0>) tensor(11161.5723, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -11161.5634765625
tensor(11161.5615, grad_fn=<NegBackward0>) tensor(11161.5635, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.3702, 0.6298],
        [0.5133, 0.4867]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3418, 0.6582], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2872, 0.0994],
         [0.5944, 0.2165]],

        [[0.6773, 0.0922],
         [0.6716, 0.6417]],

        [[0.6432, 0.1084],
         [0.5379, 0.6746]],

        [[0.5874, 0.0957],
         [0.5957, 0.6127]],

        [[0.7156, 0.0949],
         [0.5666, 0.6883]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4852686308492201
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 84
Adjusted Rand Index: 0.45728784888638285
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.09060321902695127
Average Adjusted Rand Index: 0.7491557130476842
[0.5467012045989653, 0.09060321902695127] [0.7492149936860891, 0.7491557130476842] [11117.8330078125, 11161.5634765625]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11167.879553183191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23480.759765625
inf tensor(23480.7598, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11408.9345703125
tensor(23480.7598, grad_fn=<NegBackward0>) tensor(11408.9346, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11407.765625
tensor(11408.9346, grad_fn=<NegBackward0>) tensor(11407.7656, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11406.5419921875
tensor(11407.7656, grad_fn=<NegBackward0>) tensor(11406.5420, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11405.810546875
tensor(11406.5420, grad_fn=<NegBackward0>) tensor(11405.8105, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11405.4267578125
tensor(11405.8105, grad_fn=<NegBackward0>) tensor(11405.4268, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11405.076171875
tensor(11405.4268, grad_fn=<NegBackward0>) tensor(11405.0762, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11404.908203125
tensor(11405.0762, grad_fn=<NegBackward0>) tensor(11404.9082, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11404.80078125
tensor(11404.9082, grad_fn=<NegBackward0>) tensor(11404.8008, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11404.703125
tensor(11404.8008, grad_fn=<NegBackward0>) tensor(11404.7031, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11404.607421875
tensor(11404.7031, grad_fn=<NegBackward0>) tensor(11404.6074, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11404.5029296875
tensor(11404.6074, grad_fn=<NegBackward0>) tensor(11404.5029, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11404.375
tensor(11404.5029, grad_fn=<NegBackward0>) tensor(11404.3750, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11404.20703125
tensor(11404.3750, grad_fn=<NegBackward0>) tensor(11404.2070, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11404.04296875
tensor(11404.2070, grad_fn=<NegBackward0>) tensor(11404.0430, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11403.919921875
tensor(11404.0430, grad_fn=<NegBackward0>) tensor(11403.9199, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11403.7646484375
tensor(11403.9199, grad_fn=<NegBackward0>) tensor(11403.7646, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11403.58984375
tensor(11403.7646, grad_fn=<NegBackward0>) tensor(11403.5898, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11403.47265625
tensor(11403.5898, grad_fn=<NegBackward0>) tensor(11403.4727, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11403.412109375
tensor(11403.4727, grad_fn=<NegBackward0>) tensor(11403.4121, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11403.37890625
tensor(11403.4121, grad_fn=<NegBackward0>) tensor(11403.3789, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11403.3525390625
tensor(11403.3789, grad_fn=<NegBackward0>) tensor(11403.3525, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11403.3310546875
tensor(11403.3525, grad_fn=<NegBackward0>) tensor(11403.3311, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11403.314453125
tensor(11403.3311, grad_fn=<NegBackward0>) tensor(11403.3145, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11403.3037109375
tensor(11403.3145, grad_fn=<NegBackward0>) tensor(11403.3037, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11403.29296875
tensor(11403.3037, grad_fn=<NegBackward0>) tensor(11403.2930, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11403.283203125
tensor(11403.2930, grad_fn=<NegBackward0>) tensor(11403.2832, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11403.2763671875
tensor(11403.2832, grad_fn=<NegBackward0>) tensor(11403.2764, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11403.26953125
tensor(11403.2764, grad_fn=<NegBackward0>) tensor(11403.2695, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11403.265625
tensor(11403.2695, grad_fn=<NegBackward0>) tensor(11403.2656, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11403.2607421875
tensor(11403.2656, grad_fn=<NegBackward0>) tensor(11403.2607, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11403.255859375
tensor(11403.2607, grad_fn=<NegBackward0>) tensor(11403.2559, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11403.251953125
tensor(11403.2559, grad_fn=<NegBackward0>) tensor(11403.2520, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11403.2509765625
tensor(11403.2520, grad_fn=<NegBackward0>) tensor(11403.2510, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11403.24609375
tensor(11403.2510, grad_fn=<NegBackward0>) tensor(11403.2461, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11403.2451171875
tensor(11403.2461, grad_fn=<NegBackward0>) tensor(11403.2451, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11403.2431640625
tensor(11403.2451, grad_fn=<NegBackward0>) tensor(11403.2432, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11403.2412109375
tensor(11403.2432, grad_fn=<NegBackward0>) tensor(11403.2412, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11403.240234375
tensor(11403.2412, grad_fn=<NegBackward0>) tensor(11403.2402, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11403.2373046875
tensor(11403.2402, grad_fn=<NegBackward0>) tensor(11403.2373, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11403.236328125
tensor(11403.2373, grad_fn=<NegBackward0>) tensor(11403.2363, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11403.2353515625
tensor(11403.2363, grad_fn=<NegBackward0>) tensor(11403.2354, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11403.2333984375
tensor(11403.2354, grad_fn=<NegBackward0>) tensor(11403.2334, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11403.232421875
tensor(11403.2334, grad_fn=<NegBackward0>) tensor(11403.2324, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11403.232421875
tensor(11403.2324, grad_fn=<NegBackward0>) tensor(11403.2324, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11403.2294921875
tensor(11403.2324, grad_fn=<NegBackward0>) tensor(11403.2295, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11403.2294921875
tensor(11403.2295, grad_fn=<NegBackward0>) tensor(11403.2295, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11403.23046875
tensor(11403.2295, grad_fn=<NegBackward0>) tensor(11403.2305, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11403.2294921875
tensor(11403.2295, grad_fn=<NegBackward0>) tensor(11403.2295, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11403.2294921875
tensor(11403.2295, grad_fn=<NegBackward0>) tensor(11403.2295, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11403.228515625
tensor(11403.2295, grad_fn=<NegBackward0>) tensor(11403.2285, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11403.2275390625
tensor(11403.2285, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11403.2275390625
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11403.2255859375
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2256, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11403.2255859375
tensor(11403.2256, grad_fn=<NegBackward0>) tensor(11403.2256, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11403.2255859375
tensor(11403.2256, grad_fn=<NegBackward0>) tensor(11403.2256, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11403.2294921875
tensor(11403.2256, grad_fn=<NegBackward0>) tensor(11403.2295, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11403.2275390625
tensor(11403.2256, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11403.224609375
tensor(11403.2256, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11403.224609375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11403.2255859375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2256, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11403.2236328125
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11403.224609375
tensor(11403.2236, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11403.2236328125
tensor(11403.2236, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11403.224609375
tensor(11403.2236, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11403.22265625
tensor(11403.2236, grad_fn=<NegBackward0>) tensor(11403.2227, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11403.2236328125
tensor(11403.2227, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11403.22265625
tensor(11403.2227, grad_fn=<NegBackward0>) tensor(11403.2227, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11403.220703125
tensor(11403.2227, grad_fn=<NegBackward0>) tensor(11403.2207, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11403.224609375
tensor(11403.2207, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11403.22265625
tensor(11403.2207, grad_fn=<NegBackward0>) tensor(11403.2227, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11403.2216796875
tensor(11403.2207, grad_fn=<NegBackward0>) tensor(11403.2217, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11403.2197265625
tensor(11403.2207, grad_fn=<NegBackward0>) tensor(11403.2197, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11403.2265625
tensor(11403.2197, grad_fn=<NegBackward0>) tensor(11403.2266, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11403.2216796875
tensor(11403.2197, grad_fn=<NegBackward0>) tensor(11403.2217, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11403.220703125
tensor(11403.2197, grad_fn=<NegBackward0>) tensor(11403.2207, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11403.228515625
tensor(11403.2197, grad_fn=<NegBackward0>) tensor(11403.2285, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11403.2216796875
tensor(11403.2197, grad_fn=<NegBackward0>) tensor(11403.2217, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[6.2554e-01, 3.7446e-01],
        [1.1512e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2624, 0.7376], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2317, 0.1876],
         [0.6482, 0.1661]],

        [[0.6035, 0.2115],
         [0.5730, 0.5685]],

        [[0.5858, 0.1865],
         [0.5299, 0.6024]],

        [[0.5216, 0.2080],
         [0.6620, 0.6553]],

        [[0.7112, 0.1448],
         [0.7254, 0.6109]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05818181818181818
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.011643993543924371
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.011164126353192123
Average Adjusted Rand Index: 0.01113077005362408
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25098.94140625
inf tensor(25098.9414, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11408.31640625
tensor(25098.9414, grad_fn=<NegBackward0>) tensor(11408.3164, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11407.4755859375
tensor(11408.3164, grad_fn=<NegBackward0>) tensor(11407.4756, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11406.869140625
tensor(11407.4756, grad_fn=<NegBackward0>) tensor(11406.8691, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11406.3193359375
tensor(11406.8691, grad_fn=<NegBackward0>) tensor(11406.3193, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11405.896484375
tensor(11406.3193, grad_fn=<NegBackward0>) tensor(11405.8965, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11405.603515625
tensor(11405.8965, grad_fn=<NegBackward0>) tensor(11405.6035, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11405.39453125
tensor(11405.6035, grad_fn=<NegBackward0>) tensor(11405.3945, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11405.2353515625
tensor(11405.3945, grad_fn=<NegBackward0>) tensor(11405.2354, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11405.109375
tensor(11405.2354, grad_fn=<NegBackward0>) tensor(11405.1094, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11405.01953125
tensor(11405.1094, grad_fn=<NegBackward0>) tensor(11405.0195, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11404.9580078125
tensor(11405.0195, grad_fn=<NegBackward0>) tensor(11404.9580, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11404.9150390625
tensor(11404.9580, grad_fn=<NegBackward0>) tensor(11404.9150, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11404.8818359375
tensor(11404.9150, grad_fn=<NegBackward0>) tensor(11404.8818, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11404.85546875
tensor(11404.8818, grad_fn=<NegBackward0>) tensor(11404.8555, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11404.8251953125
tensor(11404.8555, grad_fn=<NegBackward0>) tensor(11404.8252, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11404.7890625
tensor(11404.8252, grad_fn=<NegBackward0>) tensor(11404.7891, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11404.7451171875
tensor(11404.7891, grad_fn=<NegBackward0>) tensor(11404.7451, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11404.693359375
tensor(11404.7451, grad_fn=<NegBackward0>) tensor(11404.6934, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11404.63671875
tensor(11404.6934, grad_fn=<NegBackward0>) tensor(11404.6367, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11404.564453125
tensor(11404.6367, grad_fn=<NegBackward0>) tensor(11404.5645, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11404.462890625
tensor(11404.5645, grad_fn=<NegBackward0>) tensor(11404.4629, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11404.294921875
tensor(11404.4629, grad_fn=<NegBackward0>) tensor(11404.2949, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11404.05859375
tensor(11404.2949, grad_fn=<NegBackward0>) tensor(11404.0586, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11403.888671875
tensor(11404.0586, grad_fn=<NegBackward0>) tensor(11403.8887, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11403.744140625
tensor(11403.8887, grad_fn=<NegBackward0>) tensor(11403.7441, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11403.587890625
tensor(11403.7441, grad_fn=<NegBackward0>) tensor(11403.5879, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11403.4453125
tensor(11403.5879, grad_fn=<NegBackward0>) tensor(11403.4453, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11403.3681640625
tensor(11403.4453, grad_fn=<NegBackward0>) tensor(11403.3682, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11403.3310546875
tensor(11403.3682, grad_fn=<NegBackward0>) tensor(11403.3311, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11403.3095703125
tensor(11403.3311, grad_fn=<NegBackward0>) tensor(11403.3096, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11403.294921875
tensor(11403.3096, grad_fn=<NegBackward0>) tensor(11403.2949, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11403.2841796875
tensor(11403.2949, grad_fn=<NegBackward0>) tensor(11403.2842, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11403.2763671875
tensor(11403.2842, grad_fn=<NegBackward0>) tensor(11403.2764, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11403.271484375
tensor(11403.2764, grad_fn=<NegBackward0>) tensor(11403.2715, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11403.263671875
tensor(11403.2715, grad_fn=<NegBackward0>) tensor(11403.2637, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11403.2587890625
tensor(11403.2637, grad_fn=<NegBackward0>) tensor(11403.2588, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11403.2568359375
tensor(11403.2588, grad_fn=<NegBackward0>) tensor(11403.2568, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11403.2509765625
tensor(11403.2568, grad_fn=<NegBackward0>) tensor(11403.2510, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11403.2490234375
tensor(11403.2510, grad_fn=<NegBackward0>) tensor(11403.2490, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11403.24609375
tensor(11403.2490, grad_fn=<NegBackward0>) tensor(11403.2461, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11403.2431640625
tensor(11403.2461, grad_fn=<NegBackward0>) tensor(11403.2432, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11403.244140625
tensor(11403.2432, grad_fn=<NegBackward0>) tensor(11403.2441, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11403.2392578125
tensor(11403.2432, grad_fn=<NegBackward0>) tensor(11403.2393, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11403.2392578125
tensor(11403.2393, grad_fn=<NegBackward0>) tensor(11403.2393, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11403.23828125
tensor(11403.2393, grad_fn=<NegBackward0>) tensor(11403.2383, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11403.236328125
tensor(11403.2383, grad_fn=<NegBackward0>) tensor(11403.2363, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11403.2353515625
tensor(11403.2363, grad_fn=<NegBackward0>) tensor(11403.2354, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11403.234375
tensor(11403.2354, grad_fn=<NegBackward0>) tensor(11403.2344, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11403.232421875
tensor(11403.2344, grad_fn=<NegBackward0>) tensor(11403.2324, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11403.23046875
tensor(11403.2324, grad_fn=<NegBackward0>) tensor(11403.2305, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11403.2314453125
tensor(11403.2305, grad_fn=<NegBackward0>) tensor(11403.2314, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11403.228515625
tensor(11403.2305, grad_fn=<NegBackward0>) tensor(11403.2285, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11403.2275390625
tensor(11403.2285, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11403.23046875
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2305, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11403.2275390625
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11403.24609375
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2461, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11403.2275390625
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2275, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11403.2265625
tensor(11403.2275, grad_fn=<NegBackward0>) tensor(11403.2266, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11403.2265625
tensor(11403.2266, grad_fn=<NegBackward0>) tensor(11403.2266, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11403.224609375
tensor(11403.2266, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11403.224609375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11403.224609375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11403.2255859375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2256, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11403.224609375
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2246, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11403.2216796875
tensor(11403.2246, grad_fn=<NegBackward0>) tensor(11403.2217, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11403.2236328125
tensor(11403.2217, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11403.2236328125
tensor(11403.2217, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11403.2236328125
tensor(11403.2217, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11403.22265625
tensor(11403.2217, grad_fn=<NegBackward0>) tensor(11403.2227, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11403.2236328125
tensor(11403.2217, grad_fn=<NegBackward0>) tensor(11403.2236, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[9.9978e-01, 2.1979e-04],
        [3.7508e-01, 6.2492e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7375, 0.2625], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1661, 0.1876],
         [0.6387, 0.2316]],

        [[0.5135, 0.2116],
         [0.7047, 0.6853]],

        [[0.6078, 0.1865],
         [0.5001, 0.6957]],

        [[0.6134, 0.2080],
         [0.6583, 0.7251]],

        [[0.7076, 0.1449],
         [0.5171, 0.6156]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.05818181818181818
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.011643993543924371
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.011164126353192123
Average Adjusted Rand Index: 0.01113077005362408
[0.011164126353192123, 0.011164126353192123] [0.01113077005362408, 0.01113077005362408] [11403.2216796875, 11403.2236328125]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11122.854995453867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23209.029296875
inf tensor(23209.0293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11402.03515625
tensor(23209.0293, grad_fn=<NegBackward0>) tensor(11402.0352, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11397.98828125
tensor(11402.0352, grad_fn=<NegBackward0>) tensor(11397.9883, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11390.3818359375
tensor(11397.9883, grad_fn=<NegBackward0>) tensor(11390.3818, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11389.7861328125
tensor(11390.3818, grad_fn=<NegBackward0>) tensor(11389.7861, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11389.490234375
tensor(11389.7861, grad_fn=<NegBackward0>) tensor(11389.4902, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11389.31640625
tensor(11389.4902, grad_fn=<NegBackward0>) tensor(11389.3164, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11389.1845703125
tensor(11389.3164, grad_fn=<NegBackward0>) tensor(11389.1846, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11388.96484375
tensor(11389.1846, grad_fn=<NegBackward0>) tensor(11388.9648, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11198.46484375
tensor(11388.9648, grad_fn=<NegBackward0>) tensor(11198.4648, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11166.2197265625
tensor(11198.4648, grad_fn=<NegBackward0>) tensor(11166.2197, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11164.51171875
tensor(11166.2197, grad_fn=<NegBackward0>) tensor(11164.5117, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11163.2939453125
tensor(11164.5117, grad_fn=<NegBackward0>) tensor(11163.2939, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11162.8369140625
tensor(11163.2939, grad_fn=<NegBackward0>) tensor(11162.8369, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11162.728515625
tensor(11162.8369, grad_fn=<NegBackward0>) tensor(11162.7285, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11162.6884765625
tensor(11162.7285, grad_fn=<NegBackward0>) tensor(11162.6885, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11162.6689453125
tensor(11162.6885, grad_fn=<NegBackward0>) tensor(11162.6689, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11162.6181640625
tensor(11162.6689, grad_fn=<NegBackward0>) tensor(11162.6182, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11161.263671875
tensor(11162.6182, grad_fn=<NegBackward0>) tensor(11161.2637, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11160.1328125
tensor(11161.2637, grad_fn=<NegBackward0>) tensor(11160.1328, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11160.044921875
tensor(11160.1328, grad_fn=<NegBackward0>) tensor(11160.0449, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11160.01171875
tensor(11160.0449, grad_fn=<NegBackward0>) tensor(11160.0117, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11160.00390625
tensor(11160.0117, grad_fn=<NegBackward0>) tensor(11160.0039, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11160.0009765625
tensor(11160.0039, grad_fn=<NegBackward0>) tensor(11160.0010, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11159.9951171875
tensor(11160.0010, grad_fn=<NegBackward0>) tensor(11159.9951, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11159.98828125
tensor(11159.9951, grad_fn=<NegBackward0>) tensor(11159.9883, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11159.984375
tensor(11159.9883, grad_fn=<NegBackward0>) tensor(11159.9844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11159.982421875
tensor(11159.9844, grad_fn=<NegBackward0>) tensor(11159.9824, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11159.9794921875
tensor(11159.9824, grad_fn=<NegBackward0>) tensor(11159.9795, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11159.9765625
tensor(11159.9795, grad_fn=<NegBackward0>) tensor(11159.9766, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11159.9736328125
tensor(11159.9766, grad_fn=<NegBackward0>) tensor(11159.9736, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11159.9697265625
tensor(11159.9736, grad_fn=<NegBackward0>) tensor(11159.9697, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11159.9423828125
tensor(11159.9697, grad_fn=<NegBackward0>) tensor(11159.9424, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11159.9345703125
tensor(11159.9424, grad_fn=<NegBackward0>) tensor(11159.9346, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11159.931640625
tensor(11159.9346, grad_fn=<NegBackward0>) tensor(11159.9316, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11159.9306640625
tensor(11159.9316, grad_fn=<NegBackward0>) tensor(11159.9307, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11159.9287109375
tensor(11159.9307, grad_fn=<NegBackward0>) tensor(11159.9287, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11159.9267578125
tensor(11159.9287, grad_fn=<NegBackward0>) tensor(11159.9268, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11159.9033203125
tensor(11159.9268, grad_fn=<NegBackward0>) tensor(11159.9033, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11159.806640625
tensor(11159.9033, grad_fn=<NegBackward0>) tensor(11159.8066, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11159.8056640625
tensor(11159.8066, grad_fn=<NegBackward0>) tensor(11159.8057, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11159.806640625
tensor(11159.8057, grad_fn=<NegBackward0>) tensor(11159.8066, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11159.8056640625
tensor(11159.8057, grad_fn=<NegBackward0>) tensor(11159.8057, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11159.8046875
tensor(11159.8057, grad_fn=<NegBackward0>) tensor(11159.8047, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11159.8046875
tensor(11159.8047, grad_fn=<NegBackward0>) tensor(11159.8047, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11159.802734375
tensor(11159.8047, grad_fn=<NegBackward0>) tensor(11159.8027, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11159.802734375
tensor(11159.8027, grad_fn=<NegBackward0>) tensor(11159.8027, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11159.802734375
tensor(11159.8027, grad_fn=<NegBackward0>) tensor(11159.8027, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11159.802734375
tensor(11159.8027, grad_fn=<NegBackward0>) tensor(11159.8027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11159.7998046875
tensor(11159.8027, grad_fn=<NegBackward0>) tensor(11159.7998, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11159.75
tensor(11159.7998, grad_fn=<NegBackward0>) tensor(11159.7500, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11159.7392578125
tensor(11159.7500, grad_fn=<NegBackward0>) tensor(11159.7393, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11159.7490234375
tensor(11159.7393, grad_fn=<NegBackward0>) tensor(11159.7490, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11159.73828125
tensor(11159.7393, grad_fn=<NegBackward0>) tensor(11159.7383, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11159.73828125
tensor(11159.7383, grad_fn=<NegBackward0>) tensor(11159.7383, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11159.732421875
tensor(11159.7383, grad_fn=<NegBackward0>) tensor(11159.7324, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11159.732421875
tensor(11159.7324, grad_fn=<NegBackward0>) tensor(11159.7324, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11159.7294921875
tensor(11159.7324, grad_fn=<NegBackward0>) tensor(11159.7295, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11159.728515625
tensor(11159.7295, grad_fn=<NegBackward0>) tensor(11159.7285, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11159.7275390625
tensor(11159.7285, grad_fn=<NegBackward0>) tensor(11159.7275, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11159.7333984375
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7334, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11159.7275390625
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7275, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11159.7314453125
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7314, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11159.7275390625
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7275, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11159.7275390625
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7275, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11159.728515625
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7285, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11159.7275390625
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7275, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11159.701171875
tensor(11159.7275, grad_fn=<NegBackward0>) tensor(11159.7012, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11159.701171875
tensor(11159.7012, grad_fn=<NegBackward0>) tensor(11159.7012, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11159.701171875
tensor(11159.7012, grad_fn=<NegBackward0>) tensor(11159.7012, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11159.7001953125
tensor(11159.7012, grad_fn=<NegBackward0>) tensor(11159.7002, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11159.73046875
tensor(11159.7002, grad_fn=<NegBackward0>) tensor(11159.7305, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11159.697265625
tensor(11159.7002, grad_fn=<NegBackward0>) tensor(11159.6973, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11159.697265625
tensor(11159.6973, grad_fn=<NegBackward0>) tensor(11159.6973, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11159.6962890625
tensor(11159.6973, grad_fn=<NegBackward0>) tensor(11159.6963, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11159.697265625
tensor(11159.6963, grad_fn=<NegBackward0>) tensor(11159.6973, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11159.6923828125
tensor(11159.6963, grad_fn=<NegBackward0>) tensor(11159.6924, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11159.693359375
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6934, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11159.693359375
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6934, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11159.6923828125
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6924, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11159.693359375
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6934, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11159.6923828125
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6924, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11159.6923828125
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6924, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11159.693359375
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6934, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11159.6943359375
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6943, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11159.6923828125
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6924, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11159.681640625
tensor(11159.6924, grad_fn=<NegBackward0>) tensor(11159.6816, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11159.6806640625
tensor(11159.6816, grad_fn=<NegBackward0>) tensor(11159.6807, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11159.6787109375
tensor(11159.6807, grad_fn=<NegBackward0>) tensor(11159.6787, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11159.6806640625
tensor(11159.6787, grad_fn=<NegBackward0>) tensor(11159.6807, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11159.6806640625
tensor(11159.6787, grad_fn=<NegBackward0>) tensor(11159.6807, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11159.6875
tensor(11159.6787, grad_fn=<NegBackward0>) tensor(11159.6875, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11159.68359375
tensor(11159.6787, grad_fn=<NegBackward0>) tensor(11159.6836, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11159.70703125
tensor(11159.6787, grad_fn=<NegBackward0>) tensor(11159.7070, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.6603, 0.3397],
        [0.3381, 0.6619]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8684, 0.1316], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1920, 0.0844],
         [0.6059, 0.3199]],

        [[0.6613, 0.1023],
         [0.6375, 0.7272]],

        [[0.6488, 0.1025],
         [0.5286, 0.7198]],

        [[0.6449, 0.1063],
         [0.5653, 0.5307]],

        [[0.5955, 0.0966],
         [0.6964, 0.5362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.04410228386827175
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.5407851651362628
Average Adjusted Rand Index: 0.7689791768984493
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23046.998046875
inf tensor(23046.9980, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11404.4052734375
tensor(23046.9980, grad_fn=<NegBackward0>) tensor(11404.4053, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11403.2109375
tensor(11404.4053, grad_fn=<NegBackward0>) tensor(11403.2109, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11402.5751953125
tensor(11403.2109, grad_fn=<NegBackward0>) tensor(11402.5752, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11400.470703125
tensor(11402.5752, grad_fn=<NegBackward0>) tensor(11400.4707, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11398.572265625
tensor(11400.4707, grad_fn=<NegBackward0>) tensor(11398.5723, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11387.4228515625
tensor(11398.5723, grad_fn=<NegBackward0>) tensor(11387.4229, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11203.6982421875
tensor(11387.4229, grad_fn=<NegBackward0>) tensor(11203.6982, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11179.4462890625
tensor(11203.6982, grad_fn=<NegBackward0>) tensor(11179.4463, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11178.7333984375
tensor(11179.4463, grad_fn=<NegBackward0>) tensor(11178.7334, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11178.1162109375
tensor(11178.7334, grad_fn=<NegBackward0>) tensor(11178.1162, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11176.755859375
tensor(11178.1162, grad_fn=<NegBackward0>) tensor(11176.7559, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11176.6669921875
tensor(11176.7559, grad_fn=<NegBackward0>) tensor(11176.6670, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11176.60546875
tensor(11176.6670, grad_fn=<NegBackward0>) tensor(11176.6055, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11176.4638671875
tensor(11176.6055, grad_fn=<NegBackward0>) tensor(11176.4639, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11176.4169921875
tensor(11176.4639, grad_fn=<NegBackward0>) tensor(11176.4170, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11176.3935546875
tensor(11176.4170, grad_fn=<NegBackward0>) tensor(11176.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11176.373046875
tensor(11176.3936, grad_fn=<NegBackward0>) tensor(11176.3730, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11176.3564453125
tensor(11176.3730, grad_fn=<NegBackward0>) tensor(11176.3564, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11176.3408203125
tensor(11176.3564, grad_fn=<NegBackward0>) tensor(11176.3408, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11175.9404296875
tensor(11176.3408, grad_fn=<NegBackward0>) tensor(11175.9404, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11175.091796875
tensor(11175.9404, grad_fn=<NegBackward0>) tensor(11175.0918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11173.73828125
tensor(11175.0918, grad_fn=<NegBackward0>) tensor(11173.7383, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11173.7197265625
tensor(11173.7383, grad_fn=<NegBackward0>) tensor(11173.7197, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11173.7099609375
tensor(11173.7197, grad_fn=<NegBackward0>) tensor(11173.7100, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11173.705078125
tensor(11173.7100, grad_fn=<NegBackward0>) tensor(11173.7051, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11173.69921875
tensor(11173.7051, grad_fn=<NegBackward0>) tensor(11173.6992, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11173.693359375
tensor(11173.6992, grad_fn=<NegBackward0>) tensor(11173.6934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11173.6904296875
tensor(11173.6934, grad_fn=<NegBackward0>) tensor(11173.6904, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11173.685546875
tensor(11173.6904, grad_fn=<NegBackward0>) tensor(11173.6855, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11173.68359375
tensor(11173.6855, grad_fn=<NegBackward0>) tensor(11173.6836, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11173.6806640625
tensor(11173.6836, grad_fn=<NegBackward0>) tensor(11173.6807, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11173.677734375
tensor(11173.6807, grad_fn=<NegBackward0>) tensor(11173.6777, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11173.673828125
tensor(11173.6777, grad_fn=<NegBackward0>) tensor(11173.6738, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11173.6484375
tensor(11173.6738, grad_fn=<NegBackward0>) tensor(11173.6484, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11173.6328125
tensor(11173.6484, grad_fn=<NegBackward0>) tensor(11173.6328, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11173.6318359375
tensor(11173.6328, grad_fn=<NegBackward0>) tensor(11173.6318, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11173.6298828125
tensor(11173.6318, grad_fn=<NegBackward0>) tensor(11173.6299, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11173.62890625
tensor(11173.6299, grad_fn=<NegBackward0>) tensor(11173.6289, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11173.62890625
tensor(11173.6289, grad_fn=<NegBackward0>) tensor(11173.6289, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11173.626953125
tensor(11173.6289, grad_fn=<NegBackward0>) tensor(11173.6270, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11173.6259765625
tensor(11173.6270, grad_fn=<NegBackward0>) tensor(11173.6260, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11173.626953125
tensor(11173.6260, grad_fn=<NegBackward0>) tensor(11173.6270, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11173.623046875
tensor(11173.6260, grad_fn=<NegBackward0>) tensor(11173.6230, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11173.62109375
tensor(11173.6230, grad_fn=<NegBackward0>) tensor(11173.6211, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11173.6162109375
tensor(11173.6211, grad_fn=<NegBackward0>) tensor(11173.6162, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11173.595703125
tensor(11173.6162, grad_fn=<NegBackward0>) tensor(11173.5957, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11173.595703125
tensor(11173.5957, grad_fn=<NegBackward0>) tensor(11173.5957, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11173.59375
tensor(11173.5957, grad_fn=<NegBackward0>) tensor(11173.5938, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11173.591796875
tensor(11173.5938, grad_fn=<NegBackward0>) tensor(11173.5918, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11173.591796875
tensor(11173.5918, grad_fn=<NegBackward0>) tensor(11173.5918, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11173.5859375
tensor(11173.5918, grad_fn=<NegBackward0>) tensor(11173.5859, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11173.537109375
tensor(11173.5859, grad_fn=<NegBackward0>) tensor(11173.5371, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11173.333984375
tensor(11173.5371, grad_fn=<NegBackward0>) tensor(11173.3340, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11173.15625
tensor(11173.3340, grad_fn=<NegBackward0>) tensor(11173.1562, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11099.15234375
tensor(11173.1562, grad_fn=<NegBackward0>) tensor(11099.1523, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11099.125
tensor(11099.1523, grad_fn=<NegBackward0>) tensor(11099.1250, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11099.115234375
tensor(11099.1250, grad_fn=<NegBackward0>) tensor(11099.1152, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11098.4228515625
tensor(11099.1152, grad_fn=<NegBackward0>) tensor(11098.4229, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11098.4150390625
tensor(11098.4229, grad_fn=<NegBackward0>) tensor(11098.4150, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11098.4091796875
tensor(11098.4150, grad_fn=<NegBackward0>) tensor(11098.4092, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11098.37109375
tensor(11098.4092, grad_fn=<NegBackward0>) tensor(11098.3711, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11098.21484375
tensor(11098.3711, grad_fn=<NegBackward0>) tensor(11098.2148, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11098.2138671875
tensor(11098.2148, grad_fn=<NegBackward0>) tensor(11098.2139, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11098.212890625
tensor(11098.2139, grad_fn=<NegBackward0>) tensor(11098.2129, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11098.2119140625
tensor(11098.2129, grad_fn=<NegBackward0>) tensor(11098.2119, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11098.208984375
tensor(11098.2119, grad_fn=<NegBackward0>) tensor(11098.2090, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11098.1943359375
tensor(11098.2090, grad_fn=<NegBackward0>) tensor(11098.1943, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11098.193359375
tensor(11098.1943, grad_fn=<NegBackward0>) tensor(11098.1934, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11098.1923828125
tensor(11098.1934, grad_fn=<NegBackward0>) tensor(11098.1924, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11098.19140625
tensor(11098.1924, grad_fn=<NegBackward0>) tensor(11098.1914, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11098.1904296875
tensor(11098.1914, grad_fn=<NegBackward0>) tensor(11098.1904, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11098.1884765625
tensor(11098.1904, grad_fn=<NegBackward0>) tensor(11098.1885, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11098.1884765625
tensor(11098.1885, grad_fn=<NegBackward0>) tensor(11098.1885, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11098.1875
tensor(11098.1885, grad_fn=<NegBackward0>) tensor(11098.1875, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11098.1875
tensor(11098.1875, grad_fn=<NegBackward0>) tensor(11098.1875, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11098.1875
tensor(11098.1875, grad_fn=<NegBackward0>) tensor(11098.1875, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11098.1884765625
tensor(11098.1875, grad_fn=<NegBackward0>) tensor(11098.1885, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11098.203125
tensor(11098.1875, grad_fn=<NegBackward0>) tensor(11098.2031, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11098.1865234375
tensor(11098.1875, grad_fn=<NegBackward0>) tensor(11098.1865, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11098.1728515625
tensor(11098.1865, grad_fn=<NegBackward0>) tensor(11098.1729, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11098.0341796875
tensor(11098.1729, grad_fn=<NegBackward0>) tensor(11098.0342, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11098.0146484375
tensor(11098.0342, grad_fn=<NegBackward0>) tensor(11098.0146, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11097.998046875
tensor(11098.0146, grad_fn=<NegBackward0>) tensor(11097.9980, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11098.0810546875
tensor(11097.9980, grad_fn=<NegBackward0>) tensor(11098.0811, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11097.998046875
tensor(11097.9980, grad_fn=<NegBackward0>) tensor(11097.9980, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11097.9970703125
tensor(11097.9980, grad_fn=<NegBackward0>) tensor(11097.9971, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11098.009765625
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11098.0098, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11097.9970703125
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11097.9971, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11098.005859375
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11098.0059, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11097.9970703125
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11097.9971, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11097.9990234375
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11097.9990, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11097.99609375
tensor(11097.9971, grad_fn=<NegBackward0>) tensor(11097.9961, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11097.994140625
tensor(11097.9961, grad_fn=<NegBackward0>) tensor(11097.9941, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11097.994140625
tensor(11097.9941, grad_fn=<NegBackward0>) tensor(11097.9941, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11097.994140625
tensor(11097.9941, grad_fn=<NegBackward0>) tensor(11097.9941, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11097.9931640625
tensor(11097.9941, grad_fn=<NegBackward0>) tensor(11097.9932, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11097.9931640625
tensor(11097.9932, grad_fn=<NegBackward0>) tensor(11097.9932, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11097.9931640625
tensor(11097.9932, grad_fn=<NegBackward0>) tensor(11097.9932, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11097.9931640625
tensor(11097.9932, grad_fn=<NegBackward0>) tensor(11097.9932, grad_fn=<NegBackward0>)
pi: tensor([[0.7019, 0.2981],
        [0.2608, 0.7392]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4856, 0.5144], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3141, 0.0934],
         [0.5771, 0.1940]],

        [[0.6447, 0.1019],
         [0.5143, 0.5527]],

        [[0.6353, 0.1023],
         [0.6360, 0.6826]],

        [[0.7086, 0.1060],
         [0.5079, 0.5789]],

        [[0.6282, 0.0965],
         [0.6435, 0.6495]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.936897848708282
Average Adjusted Rand Index: 0.9368026825616884
[0.5407851651362628, 0.936897848708282] [0.7689791768984493, 0.9368026825616884] [11159.70703125, 11098.0078125]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11117.069088403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21804.208984375
inf tensor(21804.2090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11492.5947265625
tensor(21804.2090, grad_fn=<NegBackward0>) tensor(11492.5947, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11486.990234375
tensor(11492.5947, grad_fn=<NegBackward0>) tensor(11486.9902, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11251.7783203125
tensor(11486.9902, grad_fn=<NegBackward0>) tensor(11251.7783, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11236.0966796875
tensor(11251.7783, grad_fn=<NegBackward0>) tensor(11236.0967, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11235.7373046875
tensor(11236.0967, grad_fn=<NegBackward0>) tensor(11235.7373, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11235.634765625
tensor(11235.7373, grad_fn=<NegBackward0>) tensor(11235.6348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11235.5625
tensor(11235.6348, grad_fn=<NegBackward0>) tensor(11235.5625, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11235.5283203125
tensor(11235.5625, grad_fn=<NegBackward0>) tensor(11235.5283, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11235.505859375
tensor(11235.5283, grad_fn=<NegBackward0>) tensor(11235.5059, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11235.486328125
tensor(11235.5059, grad_fn=<NegBackward0>) tensor(11235.4863, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11235.4677734375
tensor(11235.4863, grad_fn=<NegBackward0>) tensor(11235.4678, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11235.451171875
tensor(11235.4678, grad_fn=<NegBackward0>) tensor(11235.4512, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11235.4296875
tensor(11235.4512, grad_fn=<NegBackward0>) tensor(11235.4297, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11235.404296875
tensor(11235.4297, grad_fn=<NegBackward0>) tensor(11235.4043, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11234.1171875
tensor(11235.4043, grad_fn=<NegBackward0>) tensor(11234.1172, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11234.064453125
tensor(11234.1172, grad_fn=<NegBackward0>) tensor(11234.0645, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11233.927734375
tensor(11234.0645, grad_fn=<NegBackward0>) tensor(11233.9277, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11233.7890625
tensor(11233.9277, grad_fn=<NegBackward0>) tensor(11233.7891, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11233.6650390625
tensor(11233.7891, grad_fn=<NegBackward0>) tensor(11233.6650, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11233.59765625
tensor(11233.6650, grad_fn=<NegBackward0>) tensor(11233.5977, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11233.55078125
tensor(11233.5977, grad_fn=<NegBackward0>) tensor(11233.5508, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11233.509765625
tensor(11233.5508, grad_fn=<NegBackward0>) tensor(11233.5098, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11232.9892578125
tensor(11233.5098, grad_fn=<NegBackward0>) tensor(11232.9893, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11232.9267578125
tensor(11232.9893, grad_fn=<NegBackward0>) tensor(11232.9268, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11232.91796875
tensor(11232.9268, grad_fn=<NegBackward0>) tensor(11232.9180, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11232.9150390625
tensor(11232.9180, grad_fn=<NegBackward0>) tensor(11232.9150, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11232.9130859375
tensor(11232.9150, grad_fn=<NegBackward0>) tensor(11232.9131, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11232.9111328125
tensor(11232.9131, grad_fn=<NegBackward0>) tensor(11232.9111, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11232.91015625
tensor(11232.9111, grad_fn=<NegBackward0>) tensor(11232.9102, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11232.908203125
tensor(11232.9102, grad_fn=<NegBackward0>) tensor(11232.9082, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11232.9091796875
tensor(11232.9082, grad_fn=<NegBackward0>) tensor(11232.9092, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11232.908203125
tensor(11232.9082, grad_fn=<NegBackward0>) tensor(11232.9082, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11232.7666015625
tensor(11232.9082, grad_fn=<NegBackward0>) tensor(11232.7666, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11232.7353515625
tensor(11232.7666, grad_fn=<NegBackward0>) tensor(11232.7354, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11228.4814453125
tensor(11232.7354, grad_fn=<NegBackward0>) tensor(11228.4814, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11228.4453125
tensor(11228.4814, grad_fn=<NegBackward0>) tensor(11228.4453, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11228.443359375
tensor(11228.4453, grad_fn=<NegBackward0>) tensor(11228.4434, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11228.439453125
tensor(11228.4434, grad_fn=<NegBackward0>) tensor(11228.4395, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11228.439453125
tensor(11228.4395, grad_fn=<NegBackward0>) tensor(11228.4395, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11228.4404296875
tensor(11228.4395, grad_fn=<NegBackward0>) tensor(11228.4404, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11228.4384765625
tensor(11228.4395, grad_fn=<NegBackward0>) tensor(11228.4385, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11228.4384765625
tensor(11228.4385, grad_fn=<NegBackward0>) tensor(11228.4385, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11228.4384765625
tensor(11228.4385, grad_fn=<NegBackward0>) tensor(11228.4385, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11228.435546875
tensor(11228.4385, grad_fn=<NegBackward0>) tensor(11228.4355, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11225.8828125
tensor(11228.4355, grad_fn=<NegBackward0>) tensor(11225.8828, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11225.8818359375
tensor(11225.8828, grad_fn=<NegBackward0>) tensor(11225.8818, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11225.8818359375
tensor(11225.8818, grad_fn=<NegBackward0>) tensor(11225.8818, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11225.8828125
tensor(11225.8818, grad_fn=<NegBackward0>) tensor(11225.8828, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11225.8818359375
tensor(11225.8818, grad_fn=<NegBackward0>) tensor(11225.8818, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11223.341796875
tensor(11225.8818, grad_fn=<NegBackward0>) tensor(11223.3418, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11210.1923828125
tensor(11223.3418, grad_fn=<NegBackward0>) tensor(11210.1924, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11203.7978515625
tensor(11210.1924, grad_fn=<NegBackward0>) tensor(11203.7979, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11196.53515625
tensor(11203.7979, grad_fn=<NegBackward0>) tensor(11196.5352, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11193.4140625
tensor(11196.5352, grad_fn=<NegBackward0>) tensor(11193.4141, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11193.3291015625
tensor(11193.4141, grad_fn=<NegBackward0>) tensor(11193.3291, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11193.3251953125
tensor(11193.3291, grad_fn=<NegBackward0>) tensor(11193.3252, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11193.32421875
tensor(11193.3252, grad_fn=<NegBackward0>) tensor(11193.3242, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11193.3203125
tensor(11193.3242, grad_fn=<NegBackward0>) tensor(11193.3203, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11182.7138671875
tensor(11193.3203, grad_fn=<NegBackward0>) tensor(11182.7139, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11182.705078125
tensor(11182.7139, grad_fn=<NegBackward0>) tensor(11182.7051, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11182.705078125
tensor(11182.7051, grad_fn=<NegBackward0>) tensor(11182.7051, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11182.7060546875
tensor(11182.7051, grad_fn=<NegBackward0>) tensor(11182.7061, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11182.7041015625
tensor(11182.7051, grad_fn=<NegBackward0>) tensor(11182.7041, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11182.7041015625
tensor(11182.7041, grad_fn=<NegBackward0>) tensor(11182.7041, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11182.703125
tensor(11182.7041, grad_fn=<NegBackward0>) tensor(11182.7031, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11182.705078125
tensor(11182.7031, grad_fn=<NegBackward0>) tensor(11182.7051, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11182.7021484375
tensor(11182.7031, grad_fn=<NegBackward0>) tensor(11182.7021, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11182.705078125
tensor(11182.7021, grad_fn=<NegBackward0>) tensor(11182.7051, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11182.7021484375
tensor(11182.7021, grad_fn=<NegBackward0>) tensor(11182.7021, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11182.701171875
tensor(11182.7021, grad_fn=<NegBackward0>) tensor(11182.7012, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11182.7021484375
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7021, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11182.712890625
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7129, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11182.701171875
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7012, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11182.7021484375
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7021, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11182.7021484375
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7021, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11182.701171875
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7012, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11182.701171875
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7012, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11182.7041015625
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11182.7041, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11181.404296875
tensor(11182.7012, grad_fn=<NegBackward0>) tensor(11181.4043, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11181.1796875
tensor(11181.4043, grad_fn=<NegBackward0>) tensor(11181.1797, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11181.1767578125
tensor(11181.1797, grad_fn=<NegBackward0>) tensor(11181.1768, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11176.5537109375
tensor(11181.1768, grad_fn=<NegBackward0>) tensor(11176.5537, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11176.5546875
tensor(11176.5537, grad_fn=<NegBackward0>) tensor(11176.5547, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11176.5546875
tensor(11176.5537, grad_fn=<NegBackward0>) tensor(11176.5547, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11176.55078125
tensor(11176.5537, grad_fn=<NegBackward0>) tensor(11176.5508, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11176.5498046875
tensor(11176.5508, grad_fn=<NegBackward0>) tensor(11176.5498, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11176.5849609375
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5850, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11176.55078125
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5508, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11176.55078125
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5508, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11176.5498046875
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5498, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11176.5498046875
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5498, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11176.5673828125
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5674, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11176.55078125
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5508, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11176.51171875
tensor(11176.5498, grad_fn=<NegBackward0>) tensor(11176.5117, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11176.51171875
tensor(11176.5117, grad_fn=<NegBackward0>) tensor(11176.5117, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11176.51171875
tensor(11176.5117, grad_fn=<NegBackward0>) tensor(11176.5117, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11176.51171875
tensor(11176.5117, grad_fn=<NegBackward0>) tensor(11176.5117, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11176.525390625
tensor(11176.5117, grad_fn=<NegBackward0>) tensor(11176.5254, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11176.51171875
tensor(11176.5117, grad_fn=<NegBackward0>) tensor(11176.5117, grad_fn=<NegBackward0>)
pi: tensor([[0.6535, 0.3465],
        [0.3714, 0.6286]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3700, 0.6300], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.0868],
         [0.5461, 0.2096]],

        [[0.6584, 0.0991],
         [0.6177, 0.6443]],

        [[0.6514, 0.0919],
         [0.5962, 0.6828]],

        [[0.7091, 0.0967],
         [0.7236, 0.6725]],

        [[0.5790, 0.1051],
         [0.6879, 0.6077]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 86
Adjusted Rand Index: 0.5138096559663675
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
Global Adjusted Rand Index: 0.40841718323673126
Average Adjusted Rand Index: 0.8712442880956142
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22336.828125
inf tensor(22336.8281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11498.33984375
tensor(22336.8281, grad_fn=<NegBackward0>) tensor(11498.3398, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11496.0185546875
tensor(11498.3398, grad_fn=<NegBackward0>) tensor(11496.0186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11493.845703125
tensor(11496.0186, grad_fn=<NegBackward0>) tensor(11493.8457, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11493.3583984375
tensor(11493.8457, grad_fn=<NegBackward0>) tensor(11493.3584, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11491.2529296875
tensor(11493.3584, grad_fn=<NegBackward0>) tensor(11491.2529, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11486.748046875
tensor(11491.2529, grad_fn=<NegBackward0>) tensor(11486.7480, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11364.40625
tensor(11486.7480, grad_fn=<NegBackward0>) tensor(11364.4062, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11323.544921875
tensor(11364.4062, grad_fn=<NegBackward0>) tensor(11323.5449, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11279.1259765625
tensor(11323.5449, grad_fn=<NegBackward0>) tensor(11279.1260, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11260.2099609375
tensor(11279.1260, grad_fn=<NegBackward0>) tensor(11260.2100, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11231.689453125
tensor(11260.2100, grad_fn=<NegBackward0>) tensor(11231.6895, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11229.2373046875
tensor(11231.6895, grad_fn=<NegBackward0>) tensor(11229.2373, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11229.09375
tensor(11229.2373, grad_fn=<NegBackward0>) tensor(11229.0938, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11224.8466796875
tensor(11229.0938, grad_fn=<NegBackward0>) tensor(11224.8467, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11224.828125
tensor(11224.8467, grad_fn=<NegBackward0>) tensor(11224.8281, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11224.8115234375
tensor(11224.8281, grad_fn=<NegBackward0>) tensor(11224.8115, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11221.7958984375
tensor(11224.8115, grad_fn=<NegBackward0>) tensor(11221.7959, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11221.5361328125
tensor(11221.7959, grad_fn=<NegBackward0>) tensor(11221.5361, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11221.498046875
tensor(11221.5361, grad_fn=<NegBackward0>) tensor(11221.4980, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11216.314453125
tensor(11221.4980, grad_fn=<NegBackward0>) tensor(11216.3145, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11216.255859375
tensor(11216.3145, grad_fn=<NegBackward0>) tensor(11216.2559, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11212.96484375
tensor(11216.2559, grad_fn=<NegBackward0>) tensor(11212.9648, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11212.5419921875
tensor(11212.9648, grad_fn=<NegBackward0>) tensor(11212.5420, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11212.5341796875
tensor(11212.5420, grad_fn=<NegBackward0>) tensor(11212.5342, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11210.935546875
tensor(11212.5342, grad_fn=<NegBackward0>) tensor(11210.9355, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11210.9208984375
tensor(11210.9355, grad_fn=<NegBackward0>) tensor(11210.9209, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11210.9169921875
tensor(11210.9209, grad_fn=<NegBackward0>) tensor(11210.9170, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11210.9150390625
tensor(11210.9170, grad_fn=<NegBackward0>) tensor(11210.9150, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11210.9130859375
tensor(11210.9150, grad_fn=<NegBackward0>) tensor(11210.9131, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11210.8427734375
tensor(11210.9131, grad_fn=<NegBackward0>) tensor(11210.8428, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11210.8232421875
tensor(11210.8428, grad_fn=<NegBackward0>) tensor(11210.8232, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11210.822265625
tensor(11210.8232, grad_fn=<NegBackward0>) tensor(11210.8223, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11210.8212890625
tensor(11210.8223, grad_fn=<NegBackward0>) tensor(11210.8213, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11210.8212890625
tensor(11210.8213, grad_fn=<NegBackward0>) tensor(11210.8213, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11210.8193359375
tensor(11210.8213, grad_fn=<NegBackward0>) tensor(11210.8193, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11210.8173828125
tensor(11210.8193, grad_fn=<NegBackward0>) tensor(11210.8174, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11210.814453125
tensor(11210.8174, grad_fn=<NegBackward0>) tensor(11210.8145, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11210.7919921875
tensor(11210.8145, grad_fn=<NegBackward0>) tensor(11210.7920, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11210.78125
tensor(11210.7920, grad_fn=<NegBackward0>) tensor(11210.7812, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11210.1904296875
tensor(11210.7812, grad_fn=<NegBackward0>) tensor(11210.1904, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11210.1923828125
tensor(11210.1904, grad_fn=<NegBackward0>) tensor(11210.1924, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11210.1904296875
tensor(11210.1904, grad_fn=<NegBackward0>) tensor(11210.1904, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11210.185546875
tensor(11210.1904, grad_fn=<NegBackward0>) tensor(11210.1855, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11210.1845703125
tensor(11210.1855, grad_fn=<NegBackward0>) tensor(11210.1846, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11210.1845703125
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1846, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11210.1845703125
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1846, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11210.185546875
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1855, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11210.185546875
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1855, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11210.1943359375
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1943, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11210.18359375
tensor(11210.1846, grad_fn=<NegBackward0>) tensor(11210.1836, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11210.2021484375
tensor(11210.1836, grad_fn=<NegBackward0>) tensor(11210.2021, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11210.1826171875
tensor(11210.1836, grad_fn=<NegBackward0>) tensor(11210.1826, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11210.181640625
tensor(11210.1826, grad_fn=<NegBackward0>) tensor(11210.1816, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11210.1826171875
tensor(11210.1816, grad_fn=<NegBackward0>) tensor(11210.1826, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11210.181640625
tensor(11210.1816, grad_fn=<NegBackward0>) tensor(11210.1816, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11210.1845703125
tensor(11210.1816, grad_fn=<NegBackward0>) tensor(11210.1846, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11210.197265625
tensor(11210.1816, grad_fn=<NegBackward0>) tensor(11210.1973, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11210.1806640625
tensor(11210.1816, grad_fn=<NegBackward0>) tensor(11210.1807, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11210.1787109375
tensor(11210.1807, grad_fn=<NegBackward0>) tensor(11210.1787, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11210.1787109375
tensor(11210.1787, grad_fn=<NegBackward0>) tensor(11210.1787, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11210.1796875
tensor(11210.1787, grad_fn=<NegBackward0>) tensor(11210.1797, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11210.17578125
tensor(11210.1787, grad_fn=<NegBackward0>) tensor(11210.1758, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11210.1748046875
tensor(11210.1758, grad_fn=<NegBackward0>) tensor(11210.1748, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11210.1767578125
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1768, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11210.17578125
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1758, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11210.1748046875
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1748, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11210.1748046875
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1748, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11210.1748046875
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1748, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11210.173828125
tensor(11210.1748, grad_fn=<NegBackward0>) tensor(11210.1738, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11210.173828125
tensor(11210.1738, grad_fn=<NegBackward0>) tensor(11210.1738, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11210.177734375
tensor(11210.1738, grad_fn=<NegBackward0>) tensor(11210.1777, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11210.1728515625
tensor(11210.1738, grad_fn=<NegBackward0>) tensor(11210.1729, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11210.17578125
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1758, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11210.1728515625
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1729, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11210.1728515625
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1729, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11210.1826171875
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1826, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11210.17578125
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1758, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11210.1728515625
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1729, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11210.173828125
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1738, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11210.1728515625
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1729, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11210.17578125
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1758, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11210.1689453125
tensor(11210.1729, grad_fn=<NegBackward0>) tensor(11210.1689, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11210.05859375
tensor(11210.1689, grad_fn=<NegBackward0>) tensor(11210.0586, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11210.0576171875
tensor(11210.0586, grad_fn=<NegBackward0>) tensor(11210.0576, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11210.060546875
tensor(11210.0576, grad_fn=<NegBackward0>) tensor(11210.0605, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11210.0576171875
tensor(11210.0576, grad_fn=<NegBackward0>) tensor(11210.0576, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11210.056640625
tensor(11210.0576, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11210.056640625
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11210.05859375
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0586, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11210.0595703125
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0596, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11210.056640625
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11210.0634765625
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0635, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11210.0556640625
tensor(11210.0566, grad_fn=<NegBackward0>) tensor(11210.0557, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11210.056640625
tensor(11210.0557, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11210.056640625
tensor(11210.0557, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11210.056640625
tensor(11210.0557, grad_fn=<NegBackward0>) tensor(11210.0566, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11210.09375
tensor(11210.0557, grad_fn=<NegBackward0>) tensor(11210.0938, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11210.0283203125
tensor(11210.0557, grad_fn=<NegBackward0>) tensor(11210.0283, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11210.310546875
tensor(11210.0283, grad_fn=<NegBackward0>) tensor(11210.3105, grad_fn=<NegBackward0>)
1
pi: tensor([[0.4132, 0.5868],
        [0.6074, 0.3926]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4735, 0.5265], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2699, 0.0829],
         [0.5012, 0.2460]],

        [[0.6642, 0.0930],
         [0.6500, 0.5046]],

        [[0.5551, 0.0912],
         [0.7195, 0.5858]],

        [[0.6589, 0.0959],
         [0.6716, 0.5073]],

        [[0.6832, 0.1040],
         [0.6546, 0.7019]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.03648526222461742
Average Adjusted Rand Index: 0.9131261588743558
[0.40841718323673126, 0.03648526222461742] [0.8712442880956142, 0.9131261588743558] [11176.51171875, 11210.0234375]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11200.134435995691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24530.931640625
inf tensor(24530.9316, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11463.1064453125
tensor(24530.9316, grad_fn=<NegBackward0>) tensor(11463.1064, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11462.4296875
tensor(11463.1064, grad_fn=<NegBackward0>) tensor(11462.4297, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11460.953125
tensor(11462.4297, grad_fn=<NegBackward0>) tensor(11460.9531, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11458.2109375
tensor(11460.9531, grad_fn=<NegBackward0>) tensor(11458.2109, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11457.509765625
tensor(11458.2109, grad_fn=<NegBackward0>) tensor(11457.5098, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11457.001953125
tensor(11457.5098, grad_fn=<NegBackward0>) tensor(11457.0020, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11456.5087890625
tensor(11457.0020, grad_fn=<NegBackward0>) tensor(11456.5088, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11456.009765625
tensor(11456.5088, grad_fn=<NegBackward0>) tensor(11456.0098, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11454.787109375
tensor(11456.0098, grad_fn=<NegBackward0>) tensor(11454.7871, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11383.3369140625
tensor(11454.7871, grad_fn=<NegBackward0>) tensor(11383.3369, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11281.1435546875
tensor(11383.3369, grad_fn=<NegBackward0>) tensor(11281.1436, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11276.4482421875
tensor(11281.1436, grad_fn=<NegBackward0>) tensor(11276.4482, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11273.2021484375
tensor(11276.4482, grad_fn=<NegBackward0>) tensor(11273.2021, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11264.7763671875
tensor(11273.2021, grad_fn=<NegBackward0>) tensor(11264.7764, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11264.5537109375
tensor(11264.7764, grad_fn=<NegBackward0>) tensor(11264.5537, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11256.6767578125
tensor(11264.5537, grad_fn=<NegBackward0>) tensor(11256.6768, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11256.640625
tensor(11256.6768, grad_fn=<NegBackward0>) tensor(11256.6406, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11256.6220703125
tensor(11256.6406, grad_fn=<NegBackward0>) tensor(11256.6221, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11256.390625
tensor(11256.6221, grad_fn=<NegBackward0>) tensor(11256.3906, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11256.384765625
tensor(11256.3906, grad_fn=<NegBackward0>) tensor(11256.3848, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11255.96875
tensor(11256.3848, grad_fn=<NegBackward0>) tensor(11255.9688, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11254.6884765625
tensor(11255.9688, grad_fn=<NegBackward0>) tensor(11254.6885, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11251.900390625
tensor(11254.6885, grad_fn=<NegBackward0>) tensor(11251.9004, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11251.8359375
tensor(11251.9004, grad_fn=<NegBackward0>) tensor(11251.8359, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11251.83203125
tensor(11251.8359, grad_fn=<NegBackward0>) tensor(11251.8320, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11251.826171875
tensor(11251.8320, grad_fn=<NegBackward0>) tensor(11251.8262, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11251.81640625
tensor(11251.8262, grad_fn=<NegBackward0>) tensor(11251.8164, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11251.8125
tensor(11251.8164, grad_fn=<NegBackward0>) tensor(11251.8125, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11248.8935546875
tensor(11251.8125, grad_fn=<NegBackward0>) tensor(11248.8936, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11248.830078125
tensor(11248.8936, grad_fn=<NegBackward0>) tensor(11248.8301, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11248.8271484375
tensor(11248.8301, grad_fn=<NegBackward0>) tensor(11248.8271, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11248.830078125
tensor(11248.8271, grad_fn=<NegBackward0>) tensor(11248.8301, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11248.81640625
tensor(11248.8271, grad_fn=<NegBackward0>) tensor(11248.8164, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11248.8232421875
tensor(11248.8164, grad_fn=<NegBackward0>) tensor(11248.8232, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11248.814453125
tensor(11248.8164, grad_fn=<NegBackward0>) tensor(11248.8145, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11248.80859375
tensor(11248.8145, grad_fn=<NegBackward0>) tensor(11248.8086, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11248.8046875
tensor(11248.8086, grad_fn=<NegBackward0>) tensor(11248.8047, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11248.716796875
tensor(11248.8047, grad_fn=<NegBackward0>) tensor(11248.7168, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11248.7138671875
tensor(11248.7168, grad_fn=<NegBackward0>) tensor(11248.7139, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11248.7109375
tensor(11248.7139, grad_fn=<NegBackward0>) tensor(11248.7109, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11248.7236328125
tensor(11248.7109, grad_fn=<NegBackward0>) tensor(11248.7236, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11248.708984375
tensor(11248.7109, grad_fn=<NegBackward0>) tensor(11248.7090, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11248.662109375
tensor(11248.7090, grad_fn=<NegBackward0>) tensor(11248.6621, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11248.6611328125
tensor(11248.6621, grad_fn=<NegBackward0>) tensor(11248.6611, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11248.662109375
tensor(11248.6611, grad_fn=<NegBackward0>) tensor(11248.6621, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11248.662109375
tensor(11248.6611, grad_fn=<NegBackward0>) tensor(11248.6621, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11248.6591796875
tensor(11248.6611, grad_fn=<NegBackward0>) tensor(11248.6592, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11248.6552734375
tensor(11248.6592, grad_fn=<NegBackward0>) tensor(11248.6553, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11248.654296875
tensor(11248.6553, grad_fn=<NegBackward0>) tensor(11248.6543, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11248.646484375
tensor(11248.6543, grad_fn=<NegBackward0>) tensor(11248.6465, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11248.6357421875
tensor(11248.6465, grad_fn=<NegBackward0>) tensor(11248.6357, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11248.634765625
tensor(11248.6357, grad_fn=<NegBackward0>) tensor(11248.6348, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11248.6337890625
tensor(11248.6348, grad_fn=<NegBackward0>) tensor(11248.6338, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11248.6337890625
tensor(11248.6338, grad_fn=<NegBackward0>) tensor(11248.6338, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11248.6328125
tensor(11248.6338, grad_fn=<NegBackward0>) tensor(11248.6328, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11248.6337890625
tensor(11248.6328, grad_fn=<NegBackward0>) tensor(11248.6338, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11248.62890625
tensor(11248.6328, grad_fn=<NegBackward0>) tensor(11248.6289, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11248.6005859375
tensor(11248.6289, grad_fn=<NegBackward0>) tensor(11248.6006, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11248.615234375
tensor(11248.6006, grad_fn=<NegBackward0>) tensor(11248.6152, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11248.6015625
tensor(11248.6006, grad_fn=<NegBackward0>) tensor(11248.6016, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11248.6025390625
tensor(11248.6006, grad_fn=<NegBackward0>) tensor(11248.6025, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11248.6025390625
tensor(11248.6006, grad_fn=<NegBackward0>) tensor(11248.6025, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11248.6015625
tensor(11248.6006, grad_fn=<NegBackward0>) tensor(11248.6016, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.6968, 0.3032],
        [0.3886, 0.6114]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2050, 0.7950], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2820, 0.0999],
         [0.6854, 0.2109]],

        [[0.5846, 0.0990],
         [0.5476, 0.5996]],

        [[0.7066, 0.0998],
         [0.6491, 0.6043]],

        [[0.6743, 0.0968],
         [0.5214, 0.6880]],

        [[0.5494, 0.0972],
         [0.7070, 0.6573]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 70
Adjusted Rand Index: 0.154052734375
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.4778215714232557
Average Adjusted Rand Index: 0.7756166581400553
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23672.818359375
inf tensor(23672.8184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11461.4638671875
tensor(23672.8184, grad_fn=<NegBackward0>) tensor(11461.4639, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11457.5546875
tensor(11461.4639, grad_fn=<NegBackward0>) tensor(11457.5547, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11454.1689453125
tensor(11457.5547, grad_fn=<NegBackward0>) tensor(11454.1689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11450.1220703125
tensor(11454.1689, grad_fn=<NegBackward0>) tensor(11450.1221, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11384.17578125
tensor(11450.1221, grad_fn=<NegBackward0>) tensor(11384.1758, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11222.1025390625
tensor(11384.1758, grad_fn=<NegBackward0>) tensor(11222.1025, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11201.02734375
tensor(11222.1025, grad_fn=<NegBackward0>) tensor(11201.0273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11200.5869140625
tensor(11201.0273, grad_fn=<NegBackward0>) tensor(11200.5869, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11196.05859375
tensor(11200.5869, grad_fn=<NegBackward0>) tensor(11196.0586, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11196.017578125
tensor(11196.0586, grad_fn=<NegBackward0>) tensor(11196.0176, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11192.955078125
tensor(11196.0176, grad_fn=<NegBackward0>) tensor(11192.9551, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11189.130859375
tensor(11192.9551, grad_fn=<NegBackward0>) tensor(11189.1309, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11189.1103515625
tensor(11189.1309, grad_fn=<NegBackward0>) tensor(11189.1104, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11189.0986328125
tensor(11189.1104, grad_fn=<NegBackward0>) tensor(11189.0986, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11189.08984375
tensor(11189.0986, grad_fn=<NegBackward0>) tensor(11189.0898, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11189.0302734375
tensor(11189.0898, grad_fn=<NegBackward0>) tensor(11189.0303, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11186.935546875
tensor(11189.0303, grad_fn=<NegBackward0>) tensor(11186.9355, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11184.2705078125
tensor(11186.9355, grad_fn=<NegBackward0>) tensor(11184.2705, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11184.2216796875
tensor(11184.2705, grad_fn=<NegBackward0>) tensor(11184.2217, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11184.21484375
tensor(11184.2217, grad_fn=<NegBackward0>) tensor(11184.2148, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11184.2001953125
tensor(11184.2148, grad_fn=<NegBackward0>) tensor(11184.2002, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11173.3974609375
tensor(11184.2002, grad_fn=<NegBackward0>) tensor(11173.3975, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11173.38671875
tensor(11173.3975, grad_fn=<NegBackward0>) tensor(11173.3867, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11173.3818359375
tensor(11173.3867, grad_fn=<NegBackward0>) tensor(11173.3818, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11173.384765625
tensor(11173.3818, grad_fn=<NegBackward0>) tensor(11173.3848, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11173.375
tensor(11173.3818, grad_fn=<NegBackward0>) tensor(11173.3750, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11173.37109375
tensor(11173.3750, grad_fn=<NegBackward0>) tensor(11173.3711, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11173.3662109375
tensor(11173.3711, grad_fn=<NegBackward0>) tensor(11173.3662, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11173.361328125
tensor(11173.3662, grad_fn=<NegBackward0>) tensor(11173.3613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11173.3681640625
tensor(11173.3613, grad_fn=<NegBackward0>) tensor(11173.3682, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11173.35546875
tensor(11173.3613, grad_fn=<NegBackward0>) tensor(11173.3555, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11173.353515625
tensor(11173.3555, grad_fn=<NegBackward0>) tensor(11173.3535, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11173.3515625
tensor(11173.3535, grad_fn=<NegBackward0>) tensor(11173.3516, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11173.3505859375
tensor(11173.3516, grad_fn=<NegBackward0>) tensor(11173.3506, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11173.3486328125
tensor(11173.3506, grad_fn=<NegBackward0>) tensor(11173.3486, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11173.3466796875
tensor(11173.3486, grad_fn=<NegBackward0>) tensor(11173.3467, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11173.3466796875
tensor(11173.3467, grad_fn=<NegBackward0>) tensor(11173.3467, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11173.3466796875
tensor(11173.3467, grad_fn=<NegBackward0>) tensor(11173.3467, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11173.345703125
tensor(11173.3467, grad_fn=<NegBackward0>) tensor(11173.3457, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11173.3486328125
tensor(11173.3457, grad_fn=<NegBackward0>) tensor(11173.3486, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11173.349609375
tensor(11173.3457, grad_fn=<NegBackward0>) tensor(11173.3496, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11173.34375
tensor(11173.3457, grad_fn=<NegBackward0>) tensor(11173.3438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11173.3447265625
tensor(11173.3438, grad_fn=<NegBackward0>) tensor(11173.3447, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11173.345703125
tensor(11173.3438, grad_fn=<NegBackward0>) tensor(11173.3457, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11173.3349609375
tensor(11173.3438, grad_fn=<NegBackward0>) tensor(11173.3350, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11173.32421875
tensor(11173.3350, grad_fn=<NegBackward0>) tensor(11173.3242, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11173.31640625
tensor(11173.3242, grad_fn=<NegBackward0>) tensor(11173.3164, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11173.310546875
tensor(11173.3164, grad_fn=<NegBackward0>) tensor(11173.3105, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11173.3115234375
tensor(11173.3105, grad_fn=<NegBackward0>) tensor(11173.3115, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11173.3115234375
tensor(11173.3105, grad_fn=<NegBackward0>) tensor(11173.3115, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11173.3115234375
tensor(11173.3105, grad_fn=<NegBackward0>) tensor(11173.3115, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11173.3115234375
tensor(11173.3105, grad_fn=<NegBackward0>) tensor(11173.3115, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11173.3125
tensor(11173.3105, grad_fn=<NegBackward0>) tensor(11173.3125, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.7277, 0.2723],
        [0.2486, 0.7514]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4757, 0.5243], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2069, 0.0980],
         [0.6198, 0.2889]],

        [[0.5829, 0.0986],
         [0.6757, 0.5425]],

        [[0.6000, 0.1001],
         [0.5620, 0.7204]],

        [[0.5110, 0.0976],
         [0.6690, 0.6598]],

        [[0.6031, 0.0975],
         [0.6786, 0.7203]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9368977708572352
Average Adjusted Rand Index: 0.9368048174890229
[0.4778215714232557, 0.9368977708572352] [0.7756166581400553, 0.9368048174890229] [11248.6015625, 11173.3125]
-------------------------------------
This iteration is 89
True Objective function: Loss = -10998.2373034395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21643.1640625
inf tensor(21643.1641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11354.7119140625
tensor(21643.1641, grad_fn=<NegBackward0>) tensor(11354.7119, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11353.8212890625
tensor(11354.7119, grad_fn=<NegBackward0>) tensor(11353.8213, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11349.615234375
tensor(11353.8213, grad_fn=<NegBackward0>) tensor(11349.6152, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11341.798828125
tensor(11349.6152, grad_fn=<NegBackward0>) tensor(11341.7988, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11123.197265625
tensor(11341.7988, grad_fn=<NegBackward0>) tensor(11123.1973, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11044.7255859375
tensor(11123.1973, grad_fn=<NegBackward0>) tensor(11044.7256, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11028.4482421875
tensor(11044.7256, grad_fn=<NegBackward0>) tensor(11028.4482, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11020.40234375
tensor(11028.4482, grad_fn=<NegBackward0>) tensor(11020.4023, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11017.1943359375
tensor(11020.4023, grad_fn=<NegBackward0>) tensor(11017.1943, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11011.7890625
tensor(11017.1943, grad_fn=<NegBackward0>) tensor(11011.7891, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11003.654296875
tensor(11011.7891, grad_fn=<NegBackward0>) tensor(11003.6543, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10992.470703125
tensor(11003.6543, grad_fn=<NegBackward0>) tensor(10992.4707, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10992.4375
tensor(10992.4707, grad_fn=<NegBackward0>) tensor(10992.4375, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10992.4248046875
tensor(10992.4375, grad_fn=<NegBackward0>) tensor(10992.4248, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10992.3876953125
tensor(10992.4248, grad_fn=<NegBackward0>) tensor(10992.3877, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10992.3427734375
tensor(10992.3877, grad_fn=<NegBackward0>) tensor(10992.3428, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10991.873046875
tensor(10992.3428, grad_fn=<NegBackward0>) tensor(10991.8730, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10987.18359375
tensor(10991.8730, grad_fn=<NegBackward0>) tensor(10987.1836, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10987.103515625
tensor(10987.1836, grad_fn=<NegBackward0>) tensor(10987.1035, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10987.0458984375
tensor(10987.1035, grad_fn=<NegBackward0>) tensor(10987.0459, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10986.9609375
tensor(10987.0459, grad_fn=<NegBackward0>) tensor(10986.9609, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10982.5732421875
tensor(10986.9609, grad_fn=<NegBackward0>) tensor(10982.5732, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10982.568359375
tensor(10982.5732, grad_fn=<NegBackward0>) tensor(10982.5684, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10982.564453125
tensor(10982.5684, grad_fn=<NegBackward0>) tensor(10982.5645, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10982.556640625
tensor(10982.5645, grad_fn=<NegBackward0>) tensor(10982.5566, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10979.453125
tensor(10982.5566, grad_fn=<NegBackward0>) tensor(10979.4531, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10978.49609375
tensor(10979.4531, grad_fn=<NegBackward0>) tensor(10978.4961, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10978.48828125
tensor(10978.4961, grad_fn=<NegBackward0>) tensor(10978.4883, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10978.3701171875
tensor(10978.4883, grad_fn=<NegBackward0>) tensor(10978.3701, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10977.974609375
tensor(10978.3701, grad_fn=<NegBackward0>) tensor(10977.9746, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10977.9736328125
tensor(10977.9746, grad_fn=<NegBackward0>) tensor(10977.9736, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10977.970703125
tensor(10977.9736, grad_fn=<NegBackward0>) tensor(10977.9707, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10977.9697265625
tensor(10977.9707, grad_fn=<NegBackward0>) tensor(10977.9697, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10977.9677734375
tensor(10977.9697, grad_fn=<NegBackward0>) tensor(10977.9678, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10977.966796875
tensor(10977.9678, grad_fn=<NegBackward0>) tensor(10977.9668, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10977.970703125
tensor(10977.9668, grad_fn=<NegBackward0>) tensor(10977.9707, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10977.962890625
tensor(10977.9668, grad_fn=<NegBackward0>) tensor(10977.9629, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10977.9462890625
tensor(10977.9629, grad_fn=<NegBackward0>) tensor(10977.9463, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10977.9375
tensor(10977.9463, grad_fn=<NegBackward0>) tensor(10977.9375, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10977.9189453125
tensor(10977.9375, grad_fn=<NegBackward0>) tensor(10977.9189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10977.9150390625
tensor(10977.9189, grad_fn=<NegBackward0>) tensor(10977.9150, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10977.9150390625
tensor(10977.9150, grad_fn=<NegBackward0>) tensor(10977.9150, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10977.9150390625
tensor(10977.9150, grad_fn=<NegBackward0>) tensor(10977.9150, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10977.916015625
tensor(10977.9150, grad_fn=<NegBackward0>) tensor(10977.9160, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10977.9140625
tensor(10977.9150, grad_fn=<NegBackward0>) tensor(10977.9141, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10977.9150390625
tensor(10977.9141, grad_fn=<NegBackward0>) tensor(10977.9150, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10977.9130859375
tensor(10977.9141, grad_fn=<NegBackward0>) tensor(10977.9131, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10977.9130859375
tensor(10977.9131, grad_fn=<NegBackward0>) tensor(10977.9131, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10977.9130859375
tensor(10977.9131, grad_fn=<NegBackward0>) tensor(10977.9131, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10977.912109375
tensor(10977.9131, grad_fn=<NegBackward0>) tensor(10977.9121, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10977.912109375
tensor(10977.9121, grad_fn=<NegBackward0>) tensor(10977.9121, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10977.9140625
tensor(10977.9121, grad_fn=<NegBackward0>) tensor(10977.9141, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10977.91015625
tensor(10977.9121, grad_fn=<NegBackward0>) tensor(10977.9102, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10977.9228515625
tensor(10977.9102, grad_fn=<NegBackward0>) tensor(10977.9229, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10977.908203125
tensor(10977.9102, grad_fn=<NegBackward0>) tensor(10977.9082, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10977.908203125
tensor(10977.9082, grad_fn=<NegBackward0>) tensor(10977.9082, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10977.90625
tensor(10977.9082, grad_fn=<NegBackward0>) tensor(10977.9062, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10977.9072265625
tensor(10977.9062, grad_fn=<NegBackward0>) tensor(10977.9072, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10977.9072265625
tensor(10977.9062, grad_fn=<NegBackward0>) tensor(10977.9072, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10977.90625
tensor(10977.9062, grad_fn=<NegBackward0>) tensor(10977.9062, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10977.873046875
tensor(10977.9062, grad_fn=<NegBackward0>) tensor(10977.8730, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10977.8720703125
tensor(10977.8730, grad_fn=<NegBackward0>) tensor(10977.8721, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10977.875
tensor(10977.8721, grad_fn=<NegBackward0>) tensor(10977.8750, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10977.87109375
tensor(10977.8721, grad_fn=<NegBackward0>) tensor(10977.8711, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10977.8720703125
tensor(10977.8711, grad_fn=<NegBackward0>) tensor(10977.8721, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10977.8701171875
tensor(10977.8711, grad_fn=<NegBackward0>) tensor(10977.8701, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10977.8681640625
tensor(10977.8701, grad_fn=<NegBackward0>) tensor(10977.8682, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10977.861328125
tensor(10977.8682, grad_fn=<NegBackward0>) tensor(10977.8613, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10977.865234375
tensor(10977.8613, grad_fn=<NegBackward0>) tensor(10977.8652, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10977.9384765625
tensor(10977.8613, grad_fn=<NegBackward0>) tensor(10977.9385, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10977.861328125
tensor(10977.8613, grad_fn=<NegBackward0>) tensor(10977.8613, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10977.7890625
tensor(10977.8613, grad_fn=<NegBackward0>) tensor(10977.7891, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10977.79296875
tensor(10977.7891, grad_fn=<NegBackward0>) tensor(10977.7930, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10977.79296875
tensor(10977.7891, grad_fn=<NegBackward0>) tensor(10977.7930, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10977.779296875
tensor(10977.7891, grad_fn=<NegBackward0>) tensor(10977.7793, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10977.77734375
tensor(10977.7793, grad_fn=<NegBackward0>) tensor(10977.7773, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10977.7783203125
tensor(10977.7773, grad_fn=<NegBackward0>) tensor(10977.7783, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10977.779296875
tensor(10977.7773, grad_fn=<NegBackward0>) tensor(10977.7793, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10977.779296875
tensor(10977.7773, grad_fn=<NegBackward0>) tensor(10977.7793, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10977.7783203125
tensor(10977.7773, grad_fn=<NegBackward0>) tensor(10977.7783, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10977.779296875
tensor(10977.7773, grad_fn=<NegBackward0>) tensor(10977.7793, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.7796, 0.2204],
        [0.1943, 0.8057]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4876, 0.5124], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.1005],
         [0.5951, 0.2938]],

        [[0.5525, 0.0857],
         [0.5925, 0.6031]],

        [[0.5162, 0.0930],
         [0.5658, 0.5233]],

        [[0.6444, 0.0858],
         [0.5865, 0.5417]],

        [[0.6944, 0.1016],
         [0.6022, 0.6282]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9681923862084126
Average Adjusted Rand Index: 0.9683223479635957
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22268.28515625
inf tensor(22268.2852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11355.7919921875
tensor(22268.2852, grad_fn=<NegBackward0>) tensor(11355.7920, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11355.0576171875
tensor(11355.7920, grad_fn=<NegBackward0>) tensor(11355.0576, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11354.0107421875
tensor(11355.0576, grad_fn=<NegBackward0>) tensor(11354.0107, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11351.7021484375
tensor(11354.0107, grad_fn=<NegBackward0>) tensor(11351.7021, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11335.04296875
tensor(11351.7021, grad_fn=<NegBackward0>) tensor(11335.0430, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11054.630859375
tensor(11335.0430, grad_fn=<NegBackward0>) tensor(11054.6309, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11018.2451171875
tensor(11054.6309, grad_fn=<NegBackward0>) tensor(11018.2451, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10998.9404296875
tensor(11018.2451, grad_fn=<NegBackward0>) tensor(10998.9404, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10989.0771484375
tensor(10998.9404, grad_fn=<NegBackward0>) tensor(10989.0771, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10979.248046875
tensor(10989.0771, grad_fn=<NegBackward0>) tensor(10979.2480, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10978.630859375
tensor(10979.2480, grad_fn=<NegBackward0>) tensor(10978.6309, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10978.2998046875
tensor(10978.6309, grad_fn=<NegBackward0>) tensor(10978.2998, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10978.1474609375
tensor(10978.2998, grad_fn=<NegBackward0>) tensor(10978.1475, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10978.1162109375
tensor(10978.1475, grad_fn=<NegBackward0>) tensor(10978.1162, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10978.095703125
tensor(10978.1162, grad_fn=<NegBackward0>) tensor(10978.0957, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10978.0751953125
tensor(10978.0957, grad_fn=<NegBackward0>) tensor(10978.0752, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10978.0615234375
tensor(10978.0752, grad_fn=<NegBackward0>) tensor(10978.0615, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10978.046875
tensor(10978.0615, grad_fn=<NegBackward0>) tensor(10978.0469, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10978.03515625
tensor(10978.0469, grad_fn=<NegBackward0>) tensor(10978.0352, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10978.01171875
tensor(10978.0352, grad_fn=<NegBackward0>) tensor(10978.0117, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10977.9189453125
tensor(10978.0117, grad_fn=<NegBackward0>) tensor(10977.9189, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10977.9072265625
tensor(10977.9189, grad_fn=<NegBackward0>) tensor(10977.9072, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10977.900390625
tensor(10977.9072, grad_fn=<NegBackward0>) tensor(10977.9004, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10977.89453125
tensor(10977.9004, grad_fn=<NegBackward0>) tensor(10977.8945, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10977.8876953125
tensor(10977.8945, grad_fn=<NegBackward0>) tensor(10977.8877, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10977.8759765625
tensor(10977.8877, grad_fn=<NegBackward0>) tensor(10977.8760, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10977.8740234375
tensor(10977.8760, grad_fn=<NegBackward0>) tensor(10977.8740, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10977.869140625
tensor(10977.8740, grad_fn=<NegBackward0>) tensor(10977.8691, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10977.865234375
tensor(10977.8691, grad_fn=<NegBackward0>) tensor(10977.8652, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10977.8583984375
tensor(10977.8652, grad_fn=<NegBackward0>) tensor(10977.8584, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10977.85546875
tensor(10977.8584, grad_fn=<NegBackward0>) tensor(10977.8555, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10977.853515625
tensor(10977.8555, grad_fn=<NegBackward0>) tensor(10977.8535, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10977.8515625
tensor(10977.8535, grad_fn=<NegBackward0>) tensor(10977.8516, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10977.86328125
tensor(10977.8516, grad_fn=<NegBackward0>) tensor(10977.8633, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10977.849609375
tensor(10977.8516, grad_fn=<NegBackward0>) tensor(10977.8496, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10977.849609375
tensor(10977.8496, grad_fn=<NegBackward0>) tensor(10977.8496, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10977.8486328125
tensor(10977.8496, grad_fn=<NegBackward0>) tensor(10977.8486, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10977.84765625
tensor(10977.8486, grad_fn=<NegBackward0>) tensor(10977.8477, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10977.8466796875
tensor(10977.8477, grad_fn=<NegBackward0>) tensor(10977.8467, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10977.845703125
tensor(10977.8467, grad_fn=<NegBackward0>) tensor(10977.8457, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10977.84375
tensor(10977.8457, grad_fn=<NegBackward0>) tensor(10977.8438, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10977.8486328125
tensor(10977.8438, grad_fn=<NegBackward0>) tensor(10977.8486, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10977.841796875
tensor(10977.8438, grad_fn=<NegBackward0>) tensor(10977.8418, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10977.771484375
tensor(10977.8418, grad_fn=<NegBackward0>) tensor(10977.7715, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10977.76953125
tensor(10977.7715, grad_fn=<NegBackward0>) tensor(10977.7695, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10977.7685546875
tensor(10977.7695, grad_fn=<NegBackward0>) tensor(10977.7686, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10977.7685546875
tensor(10977.7686, grad_fn=<NegBackward0>) tensor(10977.7686, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10977.767578125
tensor(10977.7686, grad_fn=<NegBackward0>) tensor(10977.7676, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10977.767578125
tensor(10977.7676, grad_fn=<NegBackward0>) tensor(10977.7676, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10977.7666015625
tensor(10977.7676, grad_fn=<NegBackward0>) tensor(10977.7666, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10977.767578125
tensor(10977.7666, grad_fn=<NegBackward0>) tensor(10977.7676, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10977.7646484375
tensor(10977.7666, grad_fn=<NegBackward0>) tensor(10977.7646, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10977.7646484375
tensor(10977.7646, grad_fn=<NegBackward0>) tensor(10977.7646, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10977.763671875
tensor(10977.7646, grad_fn=<NegBackward0>) tensor(10977.7637, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10977.7646484375
tensor(10977.7637, grad_fn=<NegBackward0>) tensor(10977.7646, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10977.763671875
tensor(10977.7637, grad_fn=<NegBackward0>) tensor(10977.7637, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10977.7646484375
tensor(10977.7637, grad_fn=<NegBackward0>) tensor(10977.7646, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10977.7626953125
tensor(10977.7637, grad_fn=<NegBackward0>) tensor(10977.7627, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10977.76171875
tensor(10977.7627, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10977.7626953125
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7627, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10977.7626953125
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7627, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -10977.76171875
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10977.76171875
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10977.7666015625
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7666, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10977.7607421875
tensor(10977.7617, grad_fn=<NegBackward0>) tensor(10977.7607, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10977.76171875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10977.7607421875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7607, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10977.7626953125
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7627, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10977.76171875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10977.76171875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10977.7626953125
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7627, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -10977.7607421875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7607, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10977.76171875
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7617, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10977.759765625
tensor(10977.7607, grad_fn=<NegBackward0>) tensor(10977.7598, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10977.7734375
tensor(10977.7598, grad_fn=<NegBackward0>) tensor(10977.7734, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10977.7958984375
tensor(10977.7598, grad_fn=<NegBackward0>) tensor(10977.7959, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10977.7587890625
tensor(10977.7598, grad_fn=<NegBackward0>) tensor(10977.7588, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10977.759765625
tensor(10977.7588, grad_fn=<NegBackward0>) tensor(10977.7598, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10977.759765625
tensor(10977.7588, grad_fn=<NegBackward0>) tensor(10977.7598, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10977.7607421875
tensor(10977.7588, grad_fn=<NegBackward0>) tensor(10977.7607, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10977.759765625
tensor(10977.7588, grad_fn=<NegBackward0>) tensor(10977.7598, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10977.970703125
tensor(10977.7588, grad_fn=<NegBackward0>) tensor(10977.9707, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7842, 0.2158],
        [0.1995, 0.8005]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4915, 0.5085], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1986, 0.1005],
         [0.7245, 0.2958]],

        [[0.5394, 0.0857],
         [0.6116, 0.6882]],

        [[0.5778, 0.0930],
         [0.6715, 0.6663]],

        [[0.5875, 0.0858],
         [0.6897, 0.5578]],

        [[0.7223, 0.1017],
         [0.6413, 0.7038]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9681923862084126
Average Adjusted Rand Index: 0.9683223479635957
[0.9681923862084126, 0.9681923862084126] [0.9683223479635957, 0.9683223479635957] [10977.779296875, 10977.970703125]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11127.368646805035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23621.05078125
inf tensor(23621.0508, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11332.5849609375
tensor(23621.0508, grad_fn=<NegBackward0>) tensor(11332.5850, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11331.783203125
tensor(11332.5850, grad_fn=<NegBackward0>) tensor(11331.7832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11331.576171875
tensor(11331.7832, grad_fn=<NegBackward0>) tensor(11331.5762, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11331.4892578125
tensor(11331.5762, grad_fn=<NegBackward0>) tensor(11331.4893, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11331.435546875
tensor(11331.4893, grad_fn=<NegBackward0>) tensor(11331.4355, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11331.3857421875
tensor(11331.4355, grad_fn=<NegBackward0>) tensor(11331.3857, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11331.31640625
tensor(11331.3857, grad_fn=<NegBackward0>) tensor(11331.3164, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11331.21875
tensor(11331.3164, grad_fn=<NegBackward0>) tensor(11331.2188, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11331.1015625
tensor(11331.2188, grad_fn=<NegBackward0>) tensor(11331.1016, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11331.0107421875
tensor(11331.1016, grad_fn=<NegBackward0>) tensor(11331.0107, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11330.953125
tensor(11331.0107, grad_fn=<NegBackward0>) tensor(11330.9531, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11330.9052734375
tensor(11330.9531, grad_fn=<NegBackward0>) tensor(11330.9053, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11330.8603515625
tensor(11330.9053, grad_fn=<NegBackward0>) tensor(11330.8604, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11330.810546875
tensor(11330.8604, grad_fn=<NegBackward0>) tensor(11330.8105, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11330.7626953125
tensor(11330.8105, grad_fn=<NegBackward0>) tensor(11330.7627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11330.71484375
tensor(11330.7627, grad_fn=<NegBackward0>) tensor(11330.7148, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11330.66796875
tensor(11330.7148, grad_fn=<NegBackward0>) tensor(11330.6680, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11330.6240234375
tensor(11330.6680, grad_fn=<NegBackward0>) tensor(11330.6240, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11330.556640625
tensor(11330.6240, grad_fn=<NegBackward0>) tensor(11330.5566, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11330.3701171875
tensor(11330.5566, grad_fn=<NegBackward0>) tensor(11330.3701, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11329.92578125
tensor(11330.3701, grad_fn=<NegBackward0>) tensor(11329.9258, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11329.12109375
tensor(11329.9258, grad_fn=<NegBackward0>) tensor(11329.1211, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11328.919921875
tensor(11329.1211, grad_fn=<NegBackward0>) tensor(11328.9199, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11328.8525390625
tensor(11328.9199, grad_fn=<NegBackward0>) tensor(11328.8525, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11328.8203125
tensor(11328.8525, grad_fn=<NegBackward0>) tensor(11328.8203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11328.7939453125
tensor(11328.8203, grad_fn=<NegBackward0>) tensor(11328.7939, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11328.7822265625
tensor(11328.7939, grad_fn=<NegBackward0>) tensor(11328.7822, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11328.76953125
tensor(11328.7822, grad_fn=<NegBackward0>) tensor(11328.7695, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11328.7548828125
tensor(11328.7695, grad_fn=<NegBackward0>) tensor(11328.7549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11328.7490234375
tensor(11328.7549, grad_fn=<NegBackward0>) tensor(11328.7490, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11328.7431640625
tensor(11328.7490, grad_fn=<NegBackward0>) tensor(11328.7432, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11328.7392578125
tensor(11328.7432, grad_fn=<NegBackward0>) tensor(11328.7393, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11328.734375
tensor(11328.7393, grad_fn=<NegBackward0>) tensor(11328.7344, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11328.7294921875
tensor(11328.7344, grad_fn=<NegBackward0>) tensor(11328.7295, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11328.7275390625
tensor(11328.7295, grad_fn=<NegBackward0>) tensor(11328.7275, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11328.7255859375
tensor(11328.7275, grad_fn=<NegBackward0>) tensor(11328.7256, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11328.72265625
tensor(11328.7256, grad_fn=<NegBackward0>) tensor(11328.7227, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11328.7197265625
tensor(11328.7227, grad_fn=<NegBackward0>) tensor(11328.7197, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11328.7177734375
tensor(11328.7197, grad_fn=<NegBackward0>) tensor(11328.7178, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11328.71484375
tensor(11328.7178, grad_fn=<NegBackward0>) tensor(11328.7148, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11328.71484375
tensor(11328.7148, grad_fn=<NegBackward0>) tensor(11328.7148, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11328.7119140625
tensor(11328.7148, grad_fn=<NegBackward0>) tensor(11328.7119, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11328.7119140625
tensor(11328.7119, grad_fn=<NegBackward0>) tensor(11328.7119, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11328.7109375
tensor(11328.7119, grad_fn=<NegBackward0>) tensor(11328.7109, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11328.7099609375
tensor(11328.7109, grad_fn=<NegBackward0>) tensor(11328.7100, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11328.70703125
tensor(11328.7100, grad_fn=<NegBackward0>) tensor(11328.7070, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11328.7060546875
tensor(11328.7070, grad_fn=<NegBackward0>) tensor(11328.7061, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11328.705078125
tensor(11328.7061, grad_fn=<NegBackward0>) tensor(11328.7051, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11328.701171875
tensor(11328.7051, grad_fn=<NegBackward0>) tensor(11328.7012, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11328.7001953125
tensor(11328.7012, grad_fn=<NegBackward0>) tensor(11328.7002, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11328.6982421875
tensor(11328.7002, grad_fn=<NegBackward0>) tensor(11328.6982, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11328.697265625
tensor(11328.6982, grad_fn=<NegBackward0>) tensor(11328.6973, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11328.6962890625
tensor(11328.6973, grad_fn=<NegBackward0>) tensor(11328.6963, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11328.6962890625
tensor(11328.6963, grad_fn=<NegBackward0>) tensor(11328.6963, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11328.6943359375
tensor(11328.6963, grad_fn=<NegBackward0>) tensor(11328.6943, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11328.6953125
tensor(11328.6943, grad_fn=<NegBackward0>) tensor(11328.6953, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11328.6953125
tensor(11328.6943, grad_fn=<NegBackward0>) tensor(11328.6953, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11328.6904296875
tensor(11328.6943, grad_fn=<NegBackward0>) tensor(11328.6904, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11328.6865234375
tensor(11328.6904, grad_fn=<NegBackward0>) tensor(11328.6865, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11328.6845703125
tensor(11328.6865, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11328.685546875
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6855, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11328.6865234375
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6865, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11328.685546875
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6855, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11328.685546875
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6855, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11328.6845703125
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11328.6865234375
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6865, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11328.6845703125
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11328.68359375
tensor(11328.6846, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11328.6845703125
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11328.6845703125
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11328.68359375
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11328.6845703125
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6846, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11328.68359375
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11328.6826171875
tensor(11328.6836, grad_fn=<NegBackward0>) tensor(11328.6826, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11328.6826171875
tensor(11328.6826, grad_fn=<NegBackward0>) tensor(11328.6826, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11328.68359375
tensor(11328.6826, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11328.681640625
tensor(11328.6826, grad_fn=<NegBackward0>) tensor(11328.6816, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11328.681640625
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.6816, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11328.68359375
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11328.685546875
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.6855, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11328.8583984375
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.8584, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11328.68359375
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.6836, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11328.685546875
tensor(11328.6816, grad_fn=<NegBackward0>) tensor(11328.6855, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[9.9928e-01, 7.2097e-04],
        [1.8619e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0299, 0.9701], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1473, 0.1316],
         [0.5783, 0.1734]],

        [[0.5805, 0.1969],
         [0.5159, 0.7290]],

        [[0.6810, 0.1313],
         [0.6998, 0.6059]],

        [[0.5132, 0.0846],
         [0.5982, 0.5119]],

        [[0.5601, 0.1723],
         [0.6790, 0.6243]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.015837733814333767
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -0.00673803335865207
Average Adjusted Rand Index: -0.007145679162338471
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25044.0234375
inf tensor(25044.0234, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11332.177734375
tensor(25044.0234, grad_fn=<NegBackward0>) tensor(11332.1777, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11331.7041015625
tensor(11332.1777, grad_fn=<NegBackward0>) tensor(11331.7041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11331.59765625
tensor(11331.7041, grad_fn=<NegBackward0>) tensor(11331.5977, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11331.5419921875
tensor(11331.5977, grad_fn=<NegBackward0>) tensor(11331.5420, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11331.5087890625
tensor(11331.5420, grad_fn=<NegBackward0>) tensor(11331.5088, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11331.4853515625
tensor(11331.5088, grad_fn=<NegBackward0>) tensor(11331.4854, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11331.470703125
tensor(11331.4854, grad_fn=<NegBackward0>) tensor(11331.4707, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11331.458984375
tensor(11331.4707, grad_fn=<NegBackward0>) tensor(11331.4590, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11331.4482421875
tensor(11331.4590, grad_fn=<NegBackward0>) tensor(11331.4482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11331.4404296875
tensor(11331.4482, grad_fn=<NegBackward0>) tensor(11331.4404, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11331.435546875
tensor(11331.4404, grad_fn=<NegBackward0>) tensor(11331.4355, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11331.4306640625
tensor(11331.4355, grad_fn=<NegBackward0>) tensor(11331.4307, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11331.423828125
tensor(11331.4307, grad_fn=<NegBackward0>) tensor(11331.4238, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11331.419921875
tensor(11331.4238, grad_fn=<NegBackward0>) tensor(11331.4199, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11331.4150390625
tensor(11331.4199, grad_fn=<NegBackward0>) tensor(11331.4150, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11331.412109375
tensor(11331.4150, grad_fn=<NegBackward0>) tensor(11331.4121, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11331.4072265625
tensor(11331.4121, grad_fn=<NegBackward0>) tensor(11331.4072, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11331.400390625
tensor(11331.4072, grad_fn=<NegBackward0>) tensor(11331.4004, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11331.3935546875
tensor(11331.4004, grad_fn=<NegBackward0>) tensor(11331.3936, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11331.37890625
tensor(11331.3936, grad_fn=<NegBackward0>) tensor(11331.3789, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11331.345703125
tensor(11331.3789, grad_fn=<NegBackward0>) tensor(11331.3457, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11331.2255859375
tensor(11331.3457, grad_fn=<NegBackward0>) tensor(11331.2256, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11330.9013671875
tensor(11331.2256, grad_fn=<NegBackward0>) tensor(11330.9014, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11330.8134765625
tensor(11330.9014, grad_fn=<NegBackward0>) tensor(11330.8135, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11330.7109375
tensor(11330.8135, grad_fn=<NegBackward0>) tensor(11330.7109, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11329.7021484375
tensor(11330.7109, grad_fn=<NegBackward0>) tensor(11329.7021, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11328.7998046875
tensor(11329.7021, grad_fn=<NegBackward0>) tensor(11328.7998, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11328.6640625
tensor(11328.7998, grad_fn=<NegBackward0>) tensor(11328.6641, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11328.6220703125
tensor(11328.6641, grad_fn=<NegBackward0>) tensor(11328.6221, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11328.5927734375
tensor(11328.6221, grad_fn=<NegBackward0>) tensor(11328.5928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11328.560546875
tensor(11328.5928, grad_fn=<NegBackward0>) tensor(11328.5605, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11328.296875
tensor(11328.5605, grad_fn=<NegBackward0>) tensor(11328.2969, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11163.755859375
tensor(11328.2969, grad_fn=<NegBackward0>) tensor(11163.7559, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11162.2685546875
tensor(11163.7559, grad_fn=<NegBackward0>) tensor(11162.2686, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11162.099609375
tensor(11162.2686, grad_fn=<NegBackward0>) tensor(11162.0996, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11161.9716796875
tensor(11162.0996, grad_fn=<NegBackward0>) tensor(11161.9717, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11161.837890625
tensor(11161.9717, grad_fn=<NegBackward0>) tensor(11161.8379, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11161.8154296875
tensor(11161.8379, grad_fn=<NegBackward0>) tensor(11161.8154, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11161.7685546875
tensor(11161.8154, grad_fn=<NegBackward0>) tensor(11161.7686, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11161.71875
tensor(11161.7686, grad_fn=<NegBackward0>) tensor(11161.7188, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11161.693359375
tensor(11161.7188, grad_fn=<NegBackward0>) tensor(11161.6934, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11161.673828125
tensor(11161.6934, grad_fn=<NegBackward0>) tensor(11161.6738, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11161.6611328125
tensor(11161.6738, grad_fn=<NegBackward0>) tensor(11161.6611, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11161.6220703125
tensor(11161.6611, grad_fn=<NegBackward0>) tensor(11161.6221, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11161.583984375
tensor(11161.6221, grad_fn=<NegBackward0>) tensor(11161.5840, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11161.57421875
tensor(11161.5840, grad_fn=<NegBackward0>) tensor(11161.5742, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11161.572265625
tensor(11161.5742, grad_fn=<NegBackward0>) tensor(11161.5723, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11161.5703125
tensor(11161.5723, grad_fn=<NegBackward0>) tensor(11161.5703, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11161.5537109375
tensor(11161.5703, grad_fn=<NegBackward0>) tensor(11161.5537, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11161.5537109375
tensor(11161.5537, grad_fn=<NegBackward0>) tensor(11161.5537, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11161.4873046875
tensor(11161.5537, grad_fn=<NegBackward0>) tensor(11161.4873, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11161.2646484375
tensor(11161.4873, grad_fn=<NegBackward0>) tensor(11161.2646, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11161.2626953125
tensor(11161.2646, grad_fn=<NegBackward0>) tensor(11161.2627, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11161.259765625
tensor(11161.2627, grad_fn=<NegBackward0>) tensor(11161.2598, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11161.2099609375
tensor(11161.2598, grad_fn=<NegBackward0>) tensor(11161.2100, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11161.2041015625
tensor(11161.2100, grad_fn=<NegBackward0>) tensor(11161.2041, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11161.2099609375
tensor(11161.2041, grad_fn=<NegBackward0>) tensor(11161.2100, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11161.171875
tensor(11161.2041, grad_fn=<NegBackward0>) tensor(11161.1719, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11161.1552734375
tensor(11161.1719, grad_fn=<NegBackward0>) tensor(11161.1553, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11161.1123046875
tensor(11161.1553, grad_fn=<NegBackward0>) tensor(11161.1123, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11161.1005859375
tensor(11161.1123, grad_fn=<NegBackward0>) tensor(11161.1006, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11161.08984375
tensor(11161.1006, grad_fn=<NegBackward0>) tensor(11161.0898, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11161.0693359375
tensor(11161.0898, grad_fn=<NegBackward0>) tensor(11161.0693, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11161.0556640625
tensor(11161.0693, grad_fn=<NegBackward0>) tensor(11161.0557, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11160.994140625
tensor(11161.0557, grad_fn=<NegBackward0>) tensor(11160.9941, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11160.9736328125
tensor(11160.9941, grad_fn=<NegBackward0>) tensor(11160.9736, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11160.943359375
tensor(11160.9736, grad_fn=<NegBackward0>) tensor(11160.9434, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11160.943359375
tensor(11160.9434, grad_fn=<NegBackward0>) tensor(11160.9434, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11160.9375
tensor(11160.9434, grad_fn=<NegBackward0>) tensor(11160.9375, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11160.9365234375
tensor(11160.9375, grad_fn=<NegBackward0>) tensor(11160.9365, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11160.9365234375
tensor(11160.9365, grad_fn=<NegBackward0>) tensor(11160.9365, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11160.935546875
tensor(11160.9365, grad_fn=<NegBackward0>) tensor(11160.9355, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11160.93359375
tensor(11160.9355, grad_fn=<NegBackward0>) tensor(11160.9336, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11160.9326171875
tensor(11160.9336, grad_fn=<NegBackward0>) tensor(11160.9326, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11160.919921875
tensor(11160.9326, grad_fn=<NegBackward0>) tensor(11160.9199, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11160.912109375
tensor(11160.9199, grad_fn=<NegBackward0>) tensor(11160.9121, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11160.912109375
tensor(11160.9121, grad_fn=<NegBackward0>) tensor(11160.9121, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11160.91015625
tensor(11160.9121, grad_fn=<NegBackward0>) tensor(11160.9102, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11160.900390625
tensor(11160.9102, grad_fn=<NegBackward0>) tensor(11160.9004, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11160.9404296875
tensor(11160.9004, grad_fn=<NegBackward0>) tensor(11160.9404, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11160.921875
tensor(11160.9004, grad_fn=<NegBackward0>) tensor(11160.9219, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11160.9033203125
tensor(11160.9004, grad_fn=<NegBackward0>) tensor(11160.9033, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11160.8994140625
tensor(11160.9004, grad_fn=<NegBackward0>) tensor(11160.8994, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11160.8994140625
tensor(11160.8994, grad_fn=<NegBackward0>) tensor(11160.8994, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11160.89453125
tensor(11160.8994, grad_fn=<NegBackward0>) tensor(11160.8945, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11160.8876953125
tensor(11160.8945, grad_fn=<NegBackward0>) tensor(11160.8877, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11160.896484375
tensor(11160.8877, grad_fn=<NegBackward0>) tensor(11160.8965, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11160.892578125
tensor(11160.8877, grad_fn=<NegBackward0>) tensor(11160.8926, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11160.9052734375
tensor(11160.8877, grad_fn=<NegBackward0>) tensor(11160.9053, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11160.88671875
tensor(11160.8877, grad_fn=<NegBackward0>) tensor(11160.8867, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11160.9130859375
tensor(11160.8867, grad_fn=<NegBackward0>) tensor(11160.9131, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11160.8994140625
tensor(11160.8867, grad_fn=<NegBackward0>) tensor(11160.8994, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11160.9375
tensor(11160.8867, grad_fn=<NegBackward0>) tensor(11160.9375, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11160.8876953125
tensor(11160.8867, grad_fn=<NegBackward0>) tensor(11160.8877, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11160.8876953125
tensor(11160.8867, grad_fn=<NegBackward0>) tensor(11160.8877, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7307, 0.2693],
        [0.3024, 0.6976]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0375, 0.9625], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2975, 0.1421],
         [0.7083, 0.1872]],

        [[0.6515, 0.1034],
         [0.6467, 0.6810]],

        [[0.5548, 0.0924],
         [0.5409, 0.7242]],

        [[0.6417, 0.1094],
         [0.7176, 0.6856]],

        [[0.6525, 0.1071],
         [0.5897, 0.6305]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7719210443888802
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.5765496591515208
Average Adjusted Rand Index: 0.6782629967565639
[-0.00673803335865207, 0.5765496591515208] [-0.007145679162338471, 0.6782629967565639] [11328.685546875, 11160.8876953125]
-------------------------------------
This iteration is 91
True Objective function: Loss = -10942.269242668353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20399.037109375
inf tensor(20399.0371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11103.5302734375
tensor(20399.0371, grad_fn=<NegBackward0>) tensor(11103.5303, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11103.037109375
tensor(11103.5303, grad_fn=<NegBackward0>) tensor(11103.0371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11102.9189453125
tensor(11103.0371, grad_fn=<NegBackward0>) tensor(11102.9189, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11102.8564453125
tensor(11102.9189, grad_fn=<NegBackward0>) tensor(11102.8564, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11102.796875
tensor(11102.8564, grad_fn=<NegBackward0>) tensor(11102.7969, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11102.73046875
tensor(11102.7969, grad_fn=<NegBackward0>) tensor(11102.7305, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11102.646484375
tensor(11102.7305, grad_fn=<NegBackward0>) tensor(11102.6465, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11102.517578125
tensor(11102.6465, grad_fn=<NegBackward0>) tensor(11102.5176, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11102.333984375
tensor(11102.5176, grad_fn=<NegBackward0>) tensor(11102.3340, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11102.1865234375
tensor(11102.3340, grad_fn=<NegBackward0>) tensor(11102.1865, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.0859375
tensor(11102.1865, grad_fn=<NegBackward0>) tensor(11102.0859, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.0048828125
tensor(11102.0859, grad_fn=<NegBackward0>) tensor(11102.0049, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11101.919921875
tensor(11102.0049, grad_fn=<NegBackward0>) tensor(11101.9199, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11101.8212890625
tensor(11101.9199, grad_fn=<NegBackward0>) tensor(11101.8213, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11101.7060546875
tensor(11101.8213, grad_fn=<NegBackward0>) tensor(11101.7061, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11101.576171875
tensor(11101.7061, grad_fn=<NegBackward0>) tensor(11101.5762, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11101.4326171875
tensor(11101.5762, grad_fn=<NegBackward0>) tensor(11101.4326, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11101.2802734375
tensor(11101.4326, grad_fn=<NegBackward0>) tensor(11101.2803, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11101.1357421875
tensor(11101.2803, grad_fn=<NegBackward0>) tensor(11101.1357, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11101.0107421875
tensor(11101.1357, grad_fn=<NegBackward0>) tensor(11101.0107, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11100.912109375
tensor(11101.0107, grad_fn=<NegBackward0>) tensor(11100.9121, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11100.837890625
tensor(11100.9121, grad_fn=<NegBackward0>) tensor(11100.8379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11100.7763671875
tensor(11100.8379, grad_fn=<NegBackward0>) tensor(11100.7764, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11100.7177734375
tensor(11100.7764, grad_fn=<NegBackward0>) tensor(11100.7178, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11100.65625
tensor(11100.7178, grad_fn=<NegBackward0>) tensor(11100.6562, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11100.5927734375
tensor(11100.6562, grad_fn=<NegBackward0>) tensor(11100.5928, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11100.541015625
tensor(11100.5928, grad_fn=<NegBackward0>) tensor(11100.5410, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11100.5009765625
tensor(11100.5410, grad_fn=<NegBackward0>) tensor(11100.5010, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11100.47265625
tensor(11100.5010, grad_fn=<NegBackward0>) tensor(11100.4727, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11100.4501953125
tensor(11100.4727, grad_fn=<NegBackward0>) tensor(11100.4502, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11100.43359375
tensor(11100.4502, grad_fn=<NegBackward0>) tensor(11100.4336, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11100.421875
tensor(11100.4336, grad_fn=<NegBackward0>) tensor(11100.4219, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11100.41015625
tensor(11100.4219, grad_fn=<NegBackward0>) tensor(11100.4102, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11100.4033203125
tensor(11100.4102, grad_fn=<NegBackward0>) tensor(11100.4033, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11100.396484375
tensor(11100.4033, grad_fn=<NegBackward0>) tensor(11100.3965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11100.3896484375
tensor(11100.3965, grad_fn=<NegBackward0>) tensor(11100.3896, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11100.3857421875
tensor(11100.3896, grad_fn=<NegBackward0>) tensor(11100.3857, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11100.3798828125
tensor(11100.3857, grad_fn=<NegBackward0>) tensor(11100.3799, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11100.376953125
tensor(11100.3799, grad_fn=<NegBackward0>) tensor(11100.3770, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11100.373046875
tensor(11100.3770, grad_fn=<NegBackward0>) tensor(11100.3730, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11100.37109375
tensor(11100.3730, grad_fn=<NegBackward0>) tensor(11100.3711, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11100.3671875
tensor(11100.3711, grad_fn=<NegBackward0>) tensor(11100.3672, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11100.3662109375
tensor(11100.3672, grad_fn=<NegBackward0>) tensor(11100.3662, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11100.365234375
tensor(11100.3662, grad_fn=<NegBackward0>) tensor(11100.3652, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11100.3642578125
tensor(11100.3652, grad_fn=<NegBackward0>) tensor(11100.3643, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11100.3623046875
tensor(11100.3643, grad_fn=<NegBackward0>) tensor(11100.3623, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11100.361328125
tensor(11100.3623, grad_fn=<NegBackward0>) tensor(11100.3613, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11100.359375
tensor(11100.3613, grad_fn=<NegBackward0>) tensor(11100.3594, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11100.359375
tensor(11100.3594, grad_fn=<NegBackward0>) tensor(11100.3594, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11100.3583984375
tensor(11100.3594, grad_fn=<NegBackward0>) tensor(11100.3584, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11100.357421875
tensor(11100.3584, grad_fn=<NegBackward0>) tensor(11100.3574, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11100.35546875
tensor(11100.3574, grad_fn=<NegBackward0>) tensor(11100.3555, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11100.35546875
tensor(11100.3555, grad_fn=<NegBackward0>) tensor(11100.3555, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11100.35546875
tensor(11100.3555, grad_fn=<NegBackward0>) tensor(11100.3555, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11100.35546875
tensor(11100.3555, grad_fn=<NegBackward0>) tensor(11100.3555, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11100.3544921875
tensor(11100.3555, grad_fn=<NegBackward0>) tensor(11100.3545, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11100.353515625
tensor(11100.3545, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11100.3525390625
tensor(11100.3535, grad_fn=<NegBackward0>) tensor(11100.3525, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11100.353515625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11100.359375
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3594, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11100.3525390625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3525, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11100.3564453125
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11100.3525390625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3525, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11100.3515625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11100.3515625
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11100.3505859375
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3506, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11100.349609375
tensor(11100.3506, grad_fn=<NegBackward0>) tensor(11100.3496, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11100.3544921875
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3545, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11100.3515625
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11100.349609375
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3496, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11100.3486328125
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11100.3486328125
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11100.349609375
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3496, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11100.34765625
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3477, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11100.3486328125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11100.34765625
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3477, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11100.3486328125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11100.4228515625
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.4229, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11100.3515625
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11100.3564453125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11100.3466796875
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3467, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11100.3486328125
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11100.3544921875
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3545, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11100.4033203125
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.4033, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11100.3515625
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11100.369140625
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3691, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[3.6689e-01, 6.3311e-01],
        [1.0747e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9507, 0.0493], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1722, 0.1756],
         [0.6438, 0.1634]],

        [[0.5327, 0.1651],
         [0.6193, 0.5480]],

        [[0.5115, 0.1643],
         [0.6618, 0.5413]],

        [[0.6351, 0.1198],
         [0.6997, 0.5798]],

        [[0.6321, 0.2582],
         [0.6813, 0.6955]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.023297732239618434
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: 0.008816304566880349
Average Adjusted Rand Index: 0.014153051274144382
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22695.8046875
inf tensor(22695.8047, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11104.267578125
tensor(22695.8047, grad_fn=<NegBackward0>) tensor(11104.2676, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11103.3681640625
tensor(11104.2676, grad_fn=<NegBackward0>) tensor(11103.3682, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11103.0791015625
tensor(11103.3682, grad_fn=<NegBackward0>) tensor(11103.0791, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11102.869140625
tensor(11103.0791, grad_fn=<NegBackward0>) tensor(11102.8691, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11102.740234375
tensor(11102.8691, grad_fn=<NegBackward0>) tensor(11102.7402, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11102.67578125
tensor(11102.7402, grad_fn=<NegBackward0>) tensor(11102.6758, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11102.6171875
tensor(11102.6758, grad_fn=<NegBackward0>) tensor(11102.6172, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11102.5537109375
tensor(11102.6172, grad_fn=<NegBackward0>) tensor(11102.5537, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11102.4794921875
tensor(11102.5537, grad_fn=<NegBackward0>) tensor(11102.4795, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11102.380859375
tensor(11102.4795, grad_fn=<NegBackward0>) tensor(11102.3809, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11102.2421875
tensor(11102.3809, grad_fn=<NegBackward0>) tensor(11102.2422, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11102.0634765625
tensor(11102.2422, grad_fn=<NegBackward0>) tensor(11102.0635, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11101.8818359375
tensor(11102.0635, grad_fn=<NegBackward0>) tensor(11101.8818, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11101.689453125
tensor(11101.8818, grad_fn=<NegBackward0>) tensor(11101.6895, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11101.48828125
tensor(11101.6895, grad_fn=<NegBackward0>) tensor(11101.4883, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11101.294921875
tensor(11101.4883, grad_fn=<NegBackward0>) tensor(11101.2949, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11101.134765625
tensor(11101.2949, grad_fn=<NegBackward0>) tensor(11101.1348, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11101.0234375
tensor(11101.1348, grad_fn=<NegBackward0>) tensor(11101.0234, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11100.951171875
tensor(11101.0234, grad_fn=<NegBackward0>) tensor(11100.9512, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11100.90234375
tensor(11100.9512, grad_fn=<NegBackward0>) tensor(11100.9023, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11100.8662109375
tensor(11100.9023, grad_fn=<NegBackward0>) tensor(11100.8662, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11100.8349609375
tensor(11100.8662, grad_fn=<NegBackward0>) tensor(11100.8350, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11100.8134765625
tensor(11100.8350, grad_fn=<NegBackward0>) tensor(11100.8135, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11100.7958984375
tensor(11100.8135, grad_fn=<NegBackward0>) tensor(11100.7959, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11100.7822265625
tensor(11100.7959, grad_fn=<NegBackward0>) tensor(11100.7822, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11100.76953125
tensor(11100.7822, grad_fn=<NegBackward0>) tensor(11100.7695, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11100.7607421875
tensor(11100.7695, grad_fn=<NegBackward0>) tensor(11100.7607, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11100.75
tensor(11100.7607, grad_fn=<NegBackward0>) tensor(11100.7500, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11100.740234375
tensor(11100.7500, grad_fn=<NegBackward0>) tensor(11100.7402, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11100.7314453125
tensor(11100.7402, grad_fn=<NegBackward0>) tensor(11100.7314, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11100.7216796875
tensor(11100.7314, grad_fn=<NegBackward0>) tensor(11100.7217, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11100.7099609375
tensor(11100.7217, grad_fn=<NegBackward0>) tensor(11100.7100, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11100.6923828125
tensor(11100.7100, grad_fn=<NegBackward0>) tensor(11100.6924, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11100.66796875
tensor(11100.6924, grad_fn=<NegBackward0>) tensor(11100.6680, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11100.6337890625
tensor(11100.6680, grad_fn=<NegBackward0>) tensor(11100.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11100.5869140625
tensor(11100.6338, grad_fn=<NegBackward0>) tensor(11100.5869, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11100.53515625
tensor(11100.5869, grad_fn=<NegBackward0>) tensor(11100.5352, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11100.49609375
tensor(11100.5352, grad_fn=<NegBackward0>) tensor(11100.4961, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11100.46875
tensor(11100.4961, grad_fn=<NegBackward0>) tensor(11100.4688, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11100.44921875
tensor(11100.4688, grad_fn=<NegBackward0>) tensor(11100.4492, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11100.4384765625
tensor(11100.4492, grad_fn=<NegBackward0>) tensor(11100.4385, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11100.4228515625
tensor(11100.4385, grad_fn=<NegBackward0>) tensor(11100.4229, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11100.412109375
tensor(11100.4229, grad_fn=<NegBackward0>) tensor(11100.4121, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11100.40625
tensor(11100.4121, grad_fn=<NegBackward0>) tensor(11100.4062, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11100.3984375
tensor(11100.4062, grad_fn=<NegBackward0>) tensor(11100.3984, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11100.39453125
tensor(11100.3984, grad_fn=<NegBackward0>) tensor(11100.3945, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11100.388671875
tensor(11100.3945, grad_fn=<NegBackward0>) tensor(11100.3887, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11100.404296875
tensor(11100.3887, grad_fn=<NegBackward0>) tensor(11100.4043, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11100.3828125
tensor(11100.3887, grad_fn=<NegBackward0>) tensor(11100.3828, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11100.376953125
tensor(11100.3828, grad_fn=<NegBackward0>) tensor(11100.3770, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11100.3759765625
tensor(11100.3770, grad_fn=<NegBackward0>) tensor(11100.3760, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11100.373046875
tensor(11100.3760, grad_fn=<NegBackward0>) tensor(11100.3730, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11100.3720703125
tensor(11100.3730, grad_fn=<NegBackward0>) tensor(11100.3721, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11100.369140625
tensor(11100.3721, grad_fn=<NegBackward0>) tensor(11100.3691, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11100.369140625
tensor(11100.3691, grad_fn=<NegBackward0>) tensor(11100.3691, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11100.365234375
tensor(11100.3691, grad_fn=<NegBackward0>) tensor(11100.3652, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11100.3759765625
tensor(11100.3652, grad_fn=<NegBackward0>) tensor(11100.3760, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11100.3642578125
tensor(11100.3652, grad_fn=<NegBackward0>) tensor(11100.3643, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11100.361328125
tensor(11100.3643, grad_fn=<NegBackward0>) tensor(11100.3613, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11100.3603515625
tensor(11100.3613, grad_fn=<NegBackward0>) tensor(11100.3604, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11100.3603515625
tensor(11100.3604, grad_fn=<NegBackward0>) tensor(11100.3604, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11100.3583984375
tensor(11100.3604, grad_fn=<NegBackward0>) tensor(11100.3584, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11100.3603515625
tensor(11100.3584, grad_fn=<NegBackward0>) tensor(11100.3604, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11100.3564453125
tensor(11100.3584, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11100.3564453125
tensor(11100.3564, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11100.3564453125
tensor(11100.3564, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11100.3544921875
tensor(11100.3564, grad_fn=<NegBackward0>) tensor(11100.3545, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11100.3544921875
tensor(11100.3545, grad_fn=<NegBackward0>) tensor(11100.3545, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11100.353515625
tensor(11100.3545, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11100.353515625
tensor(11100.3535, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11100.353515625
tensor(11100.3535, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11100.3525390625
tensor(11100.3535, grad_fn=<NegBackward0>) tensor(11100.3525, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11100.3525390625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3525, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11100.359375
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3594, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11100.3515625
tensor(11100.3525, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11100.3515625
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11100.3515625
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3516, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11100.3623046875
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3623, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11100.3505859375
tensor(11100.3516, grad_fn=<NegBackward0>) tensor(11100.3506, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11100.3603515625
tensor(11100.3506, grad_fn=<NegBackward0>) tensor(11100.3604, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11100.349609375
tensor(11100.3506, grad_fn=<NegBackward0>) tensor(11100.3496, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11100.353515625
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3535, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11100.357421875
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3574, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11100.349609375
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3496, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11100.3564453125
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3564, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11100.375
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3750, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11100.3505859375
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3506, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11100.3486328125
tensor(11100.3496, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11100.3486328125
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11100.3701171875
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3701, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11100.34765625
tensor(11100.3486, grad_fn=<NegBackward0>) tensor(11100.3477, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11100.3486328125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11100.3486328125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11100.3486328125
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3486, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11100.544921875
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.5449, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11100.3466796875
tensor(11100.3477, grad_fn=<NegBackward0>) tensor(11100.3467, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11100.3583984375
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3584, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11100.34765625
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3477, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11100.345703125
tensor(11100.3467, grad_fn=<NegBackward0>) tensor(11100.3457, grad_fn=<NegBackward0>)
pi: tensor([[9.9989e-01, 1.0612e-04],
        [6.3543e-01, 3.6457e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0503, 0.9497], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1629, 0.1755],
         [0.6082, 0.1728]],

        [[0.6166, 0.1653],
         [0.5765, 0.6824]],

        [[0.6369, 0.1645],
         [0.6426, 0.5513]],

        [[0.6796, 0.1199],
         [0.5263, 0.5883]],

        [[0.6948, 0.2582],
         [0.6800, 0.6008]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.023297732239618434
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: 0.008816304566880349
Average Adjusted Rand Index: 0.014153051274144382
[0.008816304566880349, 0.008816304566880349] [0.014153051274144382, 0.014153051274144382] [11100.369140625, 11100.34765625]
-------------------------------------
This iteration is 92
True Objective function: Loss = -10939.119787558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21882.57421875
inf tensor(21882.5742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11253.30859375
tensor(21882.5742, grad_fn=<NegBackward0>) tensor(11253.3086, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11243.634765625
tensor(11253.3086, grad_fn=<NegBackward0>) tensor(11243.6348, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11195.6640625
tensor(11243.6348, grad_fn=<NegBackward0>) tensor(11195.6641, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11120.6181640625
tensor(11195.6641, grad_fn=<NegBackward0>) tensor(11120.6182, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11026.1474609375
tensor(11120.6182, grad_fn=<NegBackward0>) tensor(11026.1475, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10922.1376953125
tensor(11026.1475, grad_fn=<NegBackward0>) tensor(10922.1377, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10921.10546875
tensor(10922.1377, grad_fn=<NegBackward0>) tensor(10921.1055, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10920.81640625
tensor(10921.1055, grad_fn=<NegBackward0>) tensor(10920.8164, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10920.6669921875
tensor(10920.8164, grad_fn=<NegBackward0>) tensor(10920.6670, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10920.5703125
tensor(10920.6670, grad_fn=<NegBackward0>) tensor(10920.5703, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10920.5
tensor(10920.5703, grad_fn=<NegBackward0>) tensor(10920.5000, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10920.4462890625
tensor(10920.5000, grad_fn=<NegBackward0>) tensor(10920.4463, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10920.3916015625
tensor(10920.4463, grad_fn=<NegBackward0>) tensor(10920.3916, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10920.32421875
tensor(10920.3916, grad_fn=<NegBackward0>) tensor(10920.3242, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10920.2685546875
tensor(10920.3242, grad_fn=<NegBackward0>) tensor(10920.2686, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10920.2451171875
tensor(10920.2686, grad_fn=<NegBackward0>) tensor(10920.2451, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10920.2314453125
tensor(10920.2451, grad_fn=<NegBackward0>) tensor(10920.2314, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10920.220703125
tensor(10920.2314, grad_fn=<NegBackward0>) tensor(10920.2207, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10920.212890625
tensor(10920.2207, grad_fn=<NegBackward0>) tensor(10920.2129, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10920.203125
tensor(10920.2129, grad_fn=<NegBackward0>) tensor(10920.2031, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10920.197265625
tensor(10920.2031, grad_fn=<NegBackward0>) tensor(10920.1973, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10920.19140625
tensor(10920.1973, grad_fn=<NegBackward0>) tensor(10920.1914, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10920.18359375
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.1836, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10920.1796875
tensor(10920.1836, grad_fn=<NegBackward0>) tensor(10920.1797, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10920.1748046875
tensor(10920.1797, grad_fn=<NegBackward0>) tensor(10920.1748, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10920.1708984375
tensor(10920.1748, grad_fn=<NegBackward0>) tensor(10920.1709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10920.17578125
tensor(10920.1709, grad_fn=<NegBackward0>) tensor(10920.1758, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10920.162109375
tensor(10920.1709, grad_fn=<NegBackward0>) tensor(10920.1621, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10920.158203125
tensor(10920.1621, grad_fn=<NegBackward0>) tensor(10920.1582, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10920.15625
tensor(10920.1582, grad_fn=<NegBackward0>) tensor(10920.1562, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10920.154296875
tensor(10920.1562, grad_fn=<NegBackward0>) tensor(10920.1543, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10920.1513671875
tensor(10920.1543, grad_fn=<NegBackward0>) tensor(10920.1514, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10920.1484375
tensor(10920.1514, grad_fn=<NegBackward0>) tensor(10920.1484, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10920.1455078125
tensor(10920.1484, grad_fn=<NegBackward0>) tensor(10920.1455, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10920.1015625
tensor(10920.1455, grad_fn=<NegBackward0>) tensor(10920.1016, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10920.099609375
tensor(10920.1016, grad_fn=<NegBackward0>) tensor(10920.0996, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10920.09765625
tensor(10920.0996, grad_fn=<NegBackward0>) tensor(10920.0977, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10920.09765625
tensor(10920.0977, grad_fn=<NegBackward0>) tensor(10920.0977, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10920.0966796875
tensor(10920.0977, grad_fn=<NegBackward0>) tensor(10920.0967, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10920.083984375
tensor(10920.0967, grad_fn=<NegBackward0>) tensor(10920.0840, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10920.0859375
tensor(10920.0840, grad_fn=<NegBackward0>) tensor(10920.0859, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10920.0810546875
tensor(10920.0840, grad_fn=<NegBackward0>) tensor(10920.0811, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10920.080078125
tensor(10920.0811, grad_fn=<NegBackward0>) tensor(10920.0801, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10920.0771484375
tensor(10920.0801, grad_fn=<NegBackward0>) tensor(10920.0771, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10920.0830078125
tensor(10920.0771, grad_fn=<NegBackward0>) tensor(10920.0830, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10920.076171875
tensor(10920.0771, grad_fn=<NegBackward0>) tensor(10920.0762, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10920.072265625
tensor(10920.0762, grad_fn=<NegBackward0>) tensor(10920.0723, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10920.044921875
tensor(10920.0723, grad_fn=<NegBackward0>) tensor(10920.0449, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10920.0380859375
tensor(10920.0449, grad_fn=<NegBackward0>) tensor(10920.0381, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10920.044921875
tensor(10920.0381, grad_fn=<NegBackward0>) tensor(10920.0449, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10920.0380859375
tensor(10920.0381, grad_fn=<NegBackward0>) tensor(10920.0381, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10920.037109375
tensor(10920.0381, grad_fn=<NegBackward0>) tensor(10920.0371, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10920.03515625
tensor(10920.0371, grad_fn=<NegBackward0>) tensor(10920.0352, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10920.03515625
tensor(10920.0352, grad_fn=<NegBackward0>) tensor(10920.0352, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10920.033203125
tensor(10920.0352, grad_fn=<NegBackward0>) tensor(10920.0332, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10920.033203125
tensor(10920.0332, grad_fn=<NegBackward0>) tensor(10920.0332, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10920.0400390625
tensor(10920.0332, grad_fn=<NegBackward0>) tensor(10920.0400, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10920.0302734375
tensor(10920.0332, grad_fn=<NegBackward0>) tensor(10920.0303, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10920.0302734375
tensor(10920.0303, grad_fn=<NegBackward0>) tensor(10920.0303, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10920.0283203125
tensor(10920.0303, grad_fn=<NegBackward0>) tensor(10920.0283, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10920.029296875
tensor(10920.0283, grad_fn=<NegBackward0>) tensor(10920.0293, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10920.0263671875
tensor(10920.0283, grad_fn=<NegBackward0>) tensor(10920.0264, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10920.0263671875
tensor(10920.0264, grad_fn=<NegBackward0>) tensor(10920.0264, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10920.0263671875
tensor(10920.0264, grad_fn=<NegBackward0>) tensor(10920.0264, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10920.0234375
tensor(10920.0264, grad_fn=<NegBackward0>) tensor(10920.0234, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10920.015625
tensor(10920.0234, grad_fn=<NegBackward0>) tensor(10920.0156, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10920.0283203125
tensor(10920.0156, grad_fn=<NegBackward0>) tensor(10920.0283, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10920.013671875
tensor(10920.0156, grad_fn=<NegBackward0>) tensor(10920.0137, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10920.0126953125
tensor(10920.0137, grad_fn=<NegBackward0>) tensor(10920.0127, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10920.013671875
tensor(10920.0127, grad_fn=<NegBackward0>) tensor(10920.0137, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10920.0126953125
tensor(10920.0127, grad_fn=<NegBackward0>) tensor(10920.0127, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10920.01171875
tensor(10920.0127, grad_fn=<NegBackward0>) tensor(10920.0117, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10920.01171875
tensor(10920.0117, grad_fn=<NegBackward0>) tensor(10920.0117, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10920.01171875
tensor(10920.0117, grad_fn=<NegBackward0>) tensor(10920.0117, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10920.01171875
tensor(10920.0117, grad_fn=<NegBackward0>) tensor(10920.0117, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10920.0107421875
tensor(10920.0117, grad_fn=<NegBackward0>) tensor(10920.0107, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10920.0107421875
tensor(10920.0107, grad_fn=<NegBackward0>) tensor(10920.0107, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10920.009765625
tensor(10920.0107, grad_fn=<NegBackward0>) tensor(10920.0098, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10920.009765625
tensor(10920.0098, grad_fn=<NegBackward0>) tensor(10920.0098, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10919.1494140625
tensor(10920.0098, grad_fn=<NegBackward0>) tensor(10919.1494, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10918.982421875
tensor(10919.1494, grad_fn=<NegBackward0>) tensor(10918.9824, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10918.982421875
tensor(10918.9824, grad_fn=<NegBackward0>) tensor(10918.9824, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10918.990234375
tensor(10918.9824, grad_fn=<NegBackward0>) tensor(10918.9902, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10918.9765625
tensor(10918.9824, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10918.9765625
tensor(10918.9766, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10918.9765625
tensor(10918.9766, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10918.9765625
tensor(10918.9766, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10918.9755859375
tensor(10918.9766, grad_fn=<NegBackward0>) tensor(10918.9756, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10918.9775390625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9775, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10918.9755859375
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9756, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10918.9775390625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9775, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10918.978515625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9785, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10918.9765625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10918.9765625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10918.9755859375
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9756, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10918.9765625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10918.9765625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10918.9765625
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9766, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -10918.9794921875
tensor(10918.9756, grad_fn=<NegBackward0>) tensor(10918.9795, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7661, 0.2339],
        [0.2883, 0.7117]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4475, 0.5525], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1948, 0.0857],
         [0.5079, 0.3037]],

        [[0.6432, 0.0989],
         [0.7011, 0.5755]],

        [[0.7098, 0.0948],
         [0.6840, 0.7007]],

        [[0.5858, 0.0892],
         [0.5252, 0.6889]],

        [[0.6138, 0.0995],
         [0.6865, 0.5054]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.944673345271342
Average Adjusted Rand Index: 0.9446426303716107
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20012.638671875
inf tensor(20012.6387, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11152.1689453125
tensor(20012.6387, grad_fn=<NegBackward0>) tensor(11152.1689, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11150.6103515625
tensor(11152.1689, grad_fn=<NegBackward0>) tensor(11150.6104, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11120.208984375
tensor(11150.6104, grad_fn=<NegBackward0>) tensor(11120.2090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10960.0
tensor(11120.2090, grad_fn=<NegBackward0>) tensor(10960., grad_fn=<NegBackward0>)
Iteration 500: Loss = -10921.0087890625
tensor(10960., grad_fn=<NegBackward0>) tensor(10921.0088, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10920.619140625
tensor(10921.0088, grad_fn=<NegBackward0>) tensor(10920.6191, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10920.490234375
tensor(10920.6191, grad_fn=<NegBackward0>) tensor(10920.4902, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10920.427734375
tensor(10920.4902, grad_fn=<NegBackward0>) tensor(10920.4277, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10920.390625
tensor(10920.4277, grad_fn=<NegBackward0>) tensor(10920.3906, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10920.3642578125
tensor(10920.3906, grad_fn=<NegBackward0>) tensor(10920.3643, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10920.3505859375
tensor(10920.3643, grad_fn=<NegBackward0>) tensor(10920.3506, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10920.337890625
tensor(10920.3506, grad_fn=<NegBackward0>) tensor(10920.3379, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10920.333984375
tensor(10920.3379, grad_fn=<NegBackward0>) tensor(10920.3340, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10920.3232421875
tensor(10920.3340, grad_fn=<NegBackward0>) tensor(10920.3232, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10920.3173828125
tensor(10920.3232, grad_fn=<NegBackward0>) tensor(10920.3174, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10920.3125
tensor(10920.3174, grad_fn=<NegBackward0>) tensor(10920.3125, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10920.310546875
tensor(10920.3125, grad_fn=<NegBackward0>) tensor(10920.3105, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10920.3037109375
tensor(10920.3105, grad_fn=<NegBackward0>) tensor(10920.3037, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10920.26953125
tensor(10920.3037, grad_fn=<NegBackward0>) tensor(10920.2695, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10920.25390625
tensor(10920.2695, grad_fn=<NegBackward0>) tensor(10920.2539, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10920.251953125
tensor(10920.2539, grad_fn=<NegBackward0>) tensor(10920.2520, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10920.251953125
tensor(10920.2520, grad_fn=<NegBackward0>) tensor(10920.2520, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10920.248046875
tensor(10920.2520, grad_fn=<NegBackward0>) tensor(10920.2480, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10920.2470703125
tensor(10920.2480, grad_fn=<NegBackward0>) tensor(10920.2471, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10920.2451171875
tensor(10920.2471, grad_fn=<NegBackward0>) tensor(10920.2451, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10920.244140625
tensor(10920.2451, grad_fn=<NegBackward0>) tensor(10920.2441, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10920.2421875
tensor(10920.2441, grad_fn=<NegBackward0>) tensor(10920.2422, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10920.240234375
tensor(10920.2422, grad_fn=<NegBackward0>) tensor(10920.2402, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10920.23828125
tensor(10920.2402, grad_fn=<NegBackward0>) tensor(10920.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10920.2421875
tensor(10920.2383, grad_fn=<NegBackward0>) tensor(10920.2422, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10920.2333984375
tensor(10920.2383, grad_fn=<NegBackward0>) tensor(10920.2334, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10920.232421875
tensor(10920.2334, grad_fn=<NegBackward0>) tensor(10920.2324, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10920.2333984375
tensor(10920.2324, grad_fn=<NegBackward0>) tensor(10920.2334, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10920.2294921875
tensor(10920.2324, grad_fn=<NegBackward0>) tensor(10920.2295, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10920.2294921875
tensor(10920.2295, grad_fn=<NegBackward0>) tensor(10920.2295, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10920.21875
tensor(10920.2295, grad_fn=<NegBackward0>) tensor(10920.2188, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10920.20703125
tensor(10920.2188, grad_fn=<NegBackward0>) tensor(10920.2070, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10920.19921875
tensor(10920.2070, grad_fn=<NegBackward0>) tensor(10920.1992, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10920.19921875
tensor(10920.1992, grad_fn=<NegBackward0>) tensor(10920.1992, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10920.2001953125
tensor(10920.1992, grad_fn=<NegBackward0>) tensor(10920.2002, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10920.19921875
tensor(10920.1992, grad_fn=<NegBackward0>) tensor(10920.1992, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10920.1982421875
tensor(10920.1992, grad_fn=<NegBackward0>) tensor(10920.1982, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10920.1982421875
tensor(10920.1982, grad_fn=<NegBackward0>) tensor(10920.1982, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10920.1982421875
tensor(10920.1982, grad_fn=<NegBackward0>) tensor(10920.1982, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10920.1962890625
tensor(10920.1982, grad_fn=<NegBackward0>) tensor(10920.1963, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10920.1953125
tensor(10920.1963, grad_fn=<NegBackward0>) tensor(10920.1953, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10920.1953125
tensor(10920.1953, grad_fn=<NegBackward0>) tensor(10920.1953, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10920.197265625
tensor(10920.1953, grad_fn=<NegBackward0>) tensor(10920.1973, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10920.19921875
tensor(10920.1953, grad_fn=<NegBackward0>) tensor(10920.1992, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10920.1943359375
tensor(10920.1953, grad_fn=<NegBackward0>) tensor(10920.1943, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10920.1943359375
tensor(10920.1943, grad_fn=<NegBackward0>) tensor(10920.1943, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10920.193359375
tensor(10920.1943, grad_fn=<NegBackward0>) tensor(10920.1934, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10920.1953125
tensor(10920.1934, grad_fn=<NegBackward0>) tensor(10920.1953, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10920.1923828125
tensor(10920.1934, grad_fn=<NegBackward0>) tensor(10920.1924, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10920.19140625
tensor(10920.1924, grad_fn=<NegBackward0>) tensor(10920.1914, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10920.1943359375
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.1943, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10920.1923828125
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.1924, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10920.1923828125
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.1924, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10920.1953125
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.1953, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10920.2001953125
tensor(10920.1914, grad_fn=<NegBackward0>) tensor(10920.2002, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7716, 0.2284],
        [0.2789, 0.7211]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4487, 0.5513], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.0856],
         [0.7309, 0.3034]],

        [[0.6369, 0.0988],
         [0.6144, 0.6398]],

        [[0.5394, 0.0955],
         [0.5168, 0.7226]],

        [[0.5601, 0.0894],
         [0.5083, 0.6960]],

        [[0.6890, 0.0997],
         [0.7240, 0.6396]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.944673345271342
Average Adjusted Rand Index: 0.9446426303716107
[0.944673345271342, 0.944673345271342] [0.9446426303716107, 0.9446426303716107] [10918.98046875, 10920.2001953125]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11163.717300141367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23121.49609375
inf tensor(23121.4961, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11468.083984375
tensor(23121.4961, grad_fn=<NegBackward0>) tensor(11468.0840, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11461.734375
tensor(11468.0840, grad_fn=<NegBackward0>) tensor(11461.7344, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11444.5517578125
tensor(11461.7344, grad_fn=<NegBackward0>) tensor(11444.5518, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11239.5673828125
tensor(11444.5518, grad_fn=<NegBackward0>) tensor(11239.5674, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11176.349609375
tensor(11239.5674, grad_fn=<NegBackward0>) tensor(11176.3496, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11168.2734375
tensor(11176.3496, grad_fn=<NegBackward0>) tensor(11168.2734, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11167.8193359375
tensor(11168.2734, grad_fn=<NegBackward0>) tensor(11167.8193, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11167.712890625
tensor(11167.8193, grad_fn=<NegBackward0>) tensor(11167.7129, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11167.646484375
tensor(11167.7129, grad_fn=<NegBackward0>) tensor(11167.6465, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11167.591796875
tensor(11167.6465, grad_fn=<NegBackward0>) tensor(11167.5918, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11166.03125
tensor(11167.5918, grad_fn=<NegBackward0>) tensor(11166.0312, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11164.1025390625
tensor(11166.0312, grad_fn=<NegBackward0>) tensor(11164.1025, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11164.0712890625
tensor(11164.1025, grad_fn=<NegBackward0>) tensor(11164.0713, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11158.4970703125
tensor(11164.0713, grad_fn=<NegBackward0>) tensor(11158.4971, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11158.4873046875
tensor(11158.4971, grad_fn=<NegBackward0>) tensor(11158.4873, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11158.46875
tensor(11158.4873, grad_fn=<NegBackward0>) tensor(11158.4688, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11158.4560546875
tensor(11158.4688, grad_fn=<NegBackward0>) tensor(11158.4561, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11157.748046875
tensor(11158.4561, grad_fn=<NegBackward0>) tensor(11157.7480, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11157.05078125
tensor(11157.7480, grad_fn=<NegBackward0>) tensor(11157.0508, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11157.046875
tensor(11157.0508, grad_fn=<NegBackward0>) tensor(11157.0469, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11157.044921875
tensor(11157.0469, grad_fn=<NegBackward0>) tensor(11157.0449, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11157.0400390625
tensor(11157.0449, grad_fn=<NegBackward0>) tensor(11157.0400, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11157.0380859375
tensor(11157.0400, grad_fn=<NegBackward0>) tensor(11157.0381, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11157.03515625
tensor(11157.0381, grad_fn=<NegBackward0>) tensor(11157.0352, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11157.0341796875
tensor(11157.0352, grad_fn=<NegBackward0>) tensor(11157.0342, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11157.0322265625
tensor(11157.0342, grad_fn=<NegBackward0>) tensor(11157.0322, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11157.03125
tensor(11157.0322, grad_fn=<NegBackward0>) tensor(11157.0312, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11157.02734375
tensor(11157.0312, grad_fn=<NegBackward0>) tensor(11157.0273, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11153.224609375
tensor(11157.0273, grad_fn=<NegBackward0>) tensor(11153.2246, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11153.10546875
tensor(11153.2246, grad_fn=<NegBackward0>) tensor(11153.1055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11153.1025390625
tensor(11153.1055, grad_fn=<NegBackward0>) tensor(11153.1025, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11153.1064453125
tensor(11153.1025, grad_fn=<NegBackward0>) tensor(11153.1064, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11152.748046875
tensor(11153.1025, grad_fn=<NegBackward0>) tensor(11152.7480, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11152.6953125
tensor(11152.7480, grad_fn=<NegBackward0>) tensor(11152.6953, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11152.6923828125
tensor(11152.6953, grad_fn=<NegBackward0>) tensor(11152.6924, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11152.6923828125
tensor(11152.6924, grad_fn=<NegBackward0>) tensor(11152.6924, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11152.693359375
tensor(11152.6924, grad_fn=<NegBackward0>) tensor(11152.6934, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11152.6904296875
tensor(11152.6924, grad_fn=<NegBackward0>) tensor(11152.6904, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11152.689453125
tensor(11152.6904, grad_fn=<NegBackward0>) tensor(11152.6895, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11151.60546875
tensor(11152.6895, grad_fn=<NegBackward0>) tensor(11151.6055, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11151.44140625
tensor(11151.6055, grad_fn=<NegBackward0>) tensor(11151.4414, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11151.4404296875
tensor(11151.4414, grad_fn=<NegBackward0>) tensor(11151.4404, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11151.4345703125
tensor(11151.4404, grad_fn=<NegBackward0>) tensor(11151.4346, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11151.4345703125
tensor(11151.4346, grad_fn=<NegBackward0>) tensor(11151.4346, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11151.4326171875
tensor(11151.4346, grad_fn=<NegBackward0>) tensor(11151.4326, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11151.44140625
tensor(11151.4326, grad_fn=<NegBackward0>) tensor(11151.4414, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11151.431640625
tensor(11151.4326, grad_fn=<NegBackward0>) tensor(11151.4316, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11151.431640625
tensor(11151.4316, grad_fn=<NegBackward0>) tensor(11151.4316, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11151.4306640625
tensor(11151.4316, grad_fn=<NegBackward0>) tensor(11151.4307, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11151.4326171875
tensor(11151.4307, grad_fn=<NegBackward0>) tensor(11151.4326, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11151.4326171875
tensor(11151.4307, grad_fn=<NegBackward0>) tensor(11151.4326, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11151.4296875
tensor(11151.4307, grad_fn=<NegBackward0>) tensor(11151.4297, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11151.4453125
tensor(11151.4297, grad_fn=<NegBackward0>) tensor(11151.4453, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11151.4296875
tensor(11151.4297, grad_fn=<NegBackward0>) tensor(11151.4297, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11151.4267578125
tensor(11151.4297, grad_fn=<NegBackward0>) tensor(11151.4268, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11151.4267578125
tensor(11151.4268, grad_fn=<NegBackward0>) tensor(11151.4268, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11151.4267578125
tensor(11151.4268, grad_fn=<NegBackward0>) tensor(11151.4268, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11151.42578125
tensor(11151.4268, grad_fn=<NegBackward0>) tensor(11151.4258, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11151.423828125
tensor(11151.4258, grad_fn=<NegBackward0>) tensor(11151.4238, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11151.3935546875
tensor(11151.4238, grad_fn=<NegBackward0>) tensor(11151.3936, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11151.39453125
tensor(11151.3936, grad_fn=<NegBackward0>) tensor(11151.3945, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11151.3935546875
tensor(11151.3936, grad_fn=<NegBackward0>) tensor(11151.3936, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11151.392578125
tensor(11151.3936, grad_fn=<NegBackward0>) tensor(11151.3926, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11151.392578125
tensor(11151.3926, grad_fn=<NegBackward0>) tensor(11151.3926, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11151.396484375
tensor(11151.3926, grad_fn=<NegBackward0>) tensor(11151.3965, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11151.3857421875
tensor(11151.3926, grad_fn=<NegBackward0>) tensor(11151.3857, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11151.3759765625
tensor(11151.3857, grad_fn=<NegBackward0>) tensor(11151.3760, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11151.3798828125
tensor(11151.3760, grad_fn=<NegBackward0>) tensor(11151.3799, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11151.3740234375
tensor(11151.3760, grad_fn=<NegBackward0>) tensor(11151.3740, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11151.3759765625
tensor(11151.3740, grad_fn=<NegBackward0>) tensor(11151.3760, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11151.3525390625
tensor(11151.3740, grad_fn=<NegBackward0>) tensor(11151.3525, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11147.0185546875
tensor(11151.3525, grad_fn=<NegBackward0>) tensor(11147.0186, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11147.0087890625
tensor(11147.0186, grad_fn=<NegBackward0>) tensor(11147.0088, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11147.0166015625
tensor(11147.0088, grad_fn=<NegBackward0>) tensor(11147.0166, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11147.01171875
tensor(11147.0088, grad_fn=<NegBackward0>) tensor(11147.0117, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11147.009765625
tensor(11147.0088, grad_fn=<NegBackward0>) tensor(11147.0098, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11147.013671875
tensor(11147.0088, grad_fn=<NegBackward0>) tensor(11147.0137, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11147.041015625
tensor(11147.0088, grad_fn=<NegBackward0>) tensor(11147.0410, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.6858, 0.3142],
        [0.2616, 0.7384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5231, 0.4769], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.1038],
         [0.5009, 0.3004]],

        [[0.5023, 0.0948],
         [0.5428, 0.6406]],

        [[0.5726, 0.0983],
         [0.5707, 0.6926]],

        [[0.5359, 0.1021],
         [0.7095, 0.6796]],

        [[0.7215, 0.0893],
         [0.6919, 0.6458]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.968192372302021
Average Adjusted Rand Index: 0.9681599547324892
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20375.052734375
inf tensor(20375.0527, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11467.8994140625
tensor(20375.0527, grad_fn=<NegBackward0>) tensor(11467.8994, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11463.96875
tensor(11467.8994, grad_fn=<NegBackward0>) tensor(11463.9688, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11452.283203125
tensor(11463.9688, grad_fn=<NegBackward0>) tensor(11452.2832, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11339.9560546875
tensor(11452.2832, grad_fn=<NegBackward0>) tensor(11339.9561, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11192.8310546875
tensor(11339.9561, grad_fn=<NegBackward0>) tensor(11192.8311, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11161.404296875
tensor(11192.8311, grad_fn=<NegBackward0>) tensor(11161.4043, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11160.623046875
tensor(11161.4043, grad_fn=<NegBackward0>) tensor(11160.6230, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11159.3056640625
tensor(11160.6230, grad_fn=<NegBackward0>) tensor(11159.3057, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11155.3681640625
tensor(11159.3057, grad_fn=<NegBackward0>) tensor(11155.3682, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11150.0419921875
tensor(11155.3682, grad_fn=<NegBackward0>) tensor(11150.0420, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11147.7880859375
tensor(11150.0420, grad_fn=<NegBackward0>) tensor(11147.7881, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11147.6953125
tensor(11147.7881, grad_fn=<NegBackward0>) tensor(11147.6953, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11147.6689453125
tensor(11147.6953, grad_fn=<NegBackward0>) tensor(11147.6689, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11147.6474609375
tensor(11147.6689, grad_fn=<NegBackward0>) tensor(11147.6475, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11147.6279296875
tensor(11147.6475, grad_fn=<NegBackward0>) tensor(11147.6279, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11147.611328125
tensor(11147.6279, grad_fn=<NegBackward0>) tensor(11147.6113, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11147.5908203125
tensor(11147.6113, grad_fn=<NegBackward0>) tensor(11147.5908, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11147.5810546875
tensor(11147.5908, grad_fn=<NegBackward0>) tensor(11147.5811, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11147.556640625
tensor(11147.5811, grad_fn=<NegBackward0>) tensor(11147.5566, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11147.533203125
tensor(11147.5566, grad_fn=<NegBackward0>) tensor(11147.5332, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11147.5244140625
tensor(11147.5332, grad_fn=<NegBackward0>) tensor(11147.5244, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11146.9111328125
tensor(11147.5244, grad_fn=<NegBackward0>) tensor(11146.9111, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11146.9072265625
tensor(11146.9111, grad_fn=<NegBackward0>) tensor(11146.9072, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11146.8955078125
tensor(11146.9072, grad_fn=<NegBackward0>) tensor(11146.8955, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11146.875
tensor(11146.8955, grad_fn=<NegBackward0>) tensor(11146.8750, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11146.4296875
tensor(11146.8750, grad_fn=<NegBackward0>) tensor(11146.4297, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11146.3798828125
tensor(11146.4297, grad_fn=<NegBackward0>) tensor(11146.3799, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11146.37890625
tensor(11146.3799, grad_fn=<NegBackward0>) tensor(11146.3789, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11146.376953125
tensor(11146.3789, grad_fn=<NegBackward0>) tensor(11146.3770, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11146.3759765625
tensor(11146.3770, grad_fn=<NegBackward0>) tensor(11146.3760, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11146.375
tensor(11146.3760, grad_fn=<NegBackward0>) tensor(11146.3750, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11146.373046875
tensor(11146.3750, grad_fn=<NegBackward0>) tensor(11146.3730, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11146.3720703125
tensor(11146.3730, grad_fn=<NegBackward0>) tensor(11146.3721, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11146.37109375
tensor(11146.3721, grad_fn=<NegBackward0>) tensor(11146.3711, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11146.369140625
tensor(11146.3711, grad_fn=<NegBackward0>) tensor(11146.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11146.3671875
tensor(11146.3691, grad_fn=<NegBackward0>) tensor(11146.3672, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11146.36328125
tensor(11146.3672, grad_fn=<NegBackward0>) tensor(11146.3633, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11146.3505859375
tensor(11146.3633, grad_fn=<NegBackward0>) tensor(11146.3506, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11146.341796875
tensor(11146.3506, grad_fn=<NegBackward0>) tensor(11146.3418, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11146.333984375
tensor(11146.3418, grad_fn=<NegBackward0>) tensor(11146.3340, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11146.3330078125
tensor(11146.3340, grad_fn=<NegBackward0>) tensor(11146.3330, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11146.330078125
tensor(11146.3330, grad_fn=<NegBackward0>) tensor(11146.3301, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11146.330078125
tensor(11146.3301, grad_fn=<NegBackward0>) tensor(11146.3301, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11146.330078125
tensor(11146.3301, grad_fn=<NegBackward0>) tensor(11146.3301, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11146.326171875
tensor(11146.3301, grad_fn=<NegBackward0>) tensor(11146.3262, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11146.3212890625
tensor(11146.3262, grad_fn=<NegBackward0>) tensor(11146.3213, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11146.3212890625
tensor(11146.3213, grad_fn=<NegBackward0>) tensor(11146.3213, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11146.3125
tensor(11146.3213, grad_fn=<NegBackward0>) tensor(11146.3125, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11146.310546875
tensor(11146.3125, grad_fn=<NegBackward0>) tensor(11146.3105, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11146.2724609375
tensor(11146.3105, grad_fn=<NegBackward0>) tensor(11146.2725, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11146.2724609375
tensor(11146.2725, grad_fn=<NegBackward0>) tensor(11146.2725, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11146.2724609375
tensor(11146.2725, grad_fn=<NegBackward0>) tensor(11146.2725, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11146.2705078125
tensor(11146.2725, grad_fn=<NegBackward0>) tensor(11146.2705, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11146.26953125
tensor(11146.2705, grad_fn=<NegBackward0>) tensor(11146.2695, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11146.2705078125
tensor(11146.2695, grad_fn=<NegBackward0>) tensor(11146.2705, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11146.2724609375
tensor(11146.2695, grad_fn=<NegBackward0>) tensor(11146.2725, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11146.2685546875
tensor(11146.2695, grad_fn=<NegBackward0>) tensor(11146.2686, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11146.2705078125
tensor(11146.2686, grad_fn=<NegBackward0>) tensor(11146.2705, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11146.2666015625
tensor(11146.2686, grad_fn=<NegBackward0>) tensor(11146.2666, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11146.2685546875
tensor(11146.2666, grad_fn=<NegBackward0>) tensor(11146.2686, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11146.267578125
tensor(11146.2666, grad_fn=<NegBackward0>) tensor(11146.2676, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11146.267578125
tensor(11146.2666, grad_fn=<NegBackward0>) tensor(11146.2676, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11146.267578125
tensor(11146.2666, grad_fn=<NegBackward0>) tensor(11146.2676, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11146.2734375
tensor(11146.2666, grad_fn=<NegBackward0>) tensor(11146.2734, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[0.6838, 0.3162],
        [0.2623, 0.7377]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5285, 0.4715], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1038],
         [0.6324, 0.3003]],

        [[0.5735, 0.0948],
         [0.5357, 0.5714]],

        [[0.6895, 0.0984],
         [0.5696, 0.5452]],

        [[0.5586, 0.1017],
         [0.6483, 0.7289]],

        [[0.6344, 0.0893],
         [0.7136, 0.5778]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.968192372302021
Average Adjusted Rand Index: 0.9681599547324892
[0.968192372302021, 0.968192372302021] [0.9681599547324892, 0.9681599547324892] [11147.041015625, 11146.2734375]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11162.138629579336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22698.6484375
inf tensor(22698.6484, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11411.1767578125
tensor(22698.6484, grad_fn=<NegBackward0>) tensor(11411.1768, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11410.3564453125
tensor(11411.1768, grad_fn=<NegBackward0>) tensor(11410.3564, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11410.1171875
tensor(11410.3564, grad_fn=<NegBackward0>) tensor(11410.1172, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11409.84375
tensor(11410.1172, grad_fn=<NegBackward0>) tensor(11409.8438, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11407.8466796875
tensor(11409.8438, grad_fn=<NegBackward0>) tensor(11407.8467, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11407.38671875
tensor(11407.8467, grad_fn=<NegBackward0>) tensor(11407.3867, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11406.9873046875
tensor(11407.3867, grad_fn=<NegBackward0>) tensor(11406.9873, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11406.5693359375
tensor(11406.9873, grad_fn=<NegBackward0>) tensor(11406.5693, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11406.130859375
tensor(11406.5693, grad_fn=<NegBackward0>) tensor(11406.1309, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11405.7568359375
tensor(11406.1309, grad_fn=<NegBackward0>) tensor(11405.7568, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11405.46484375
tensor(11405.7568, grad_fn=<NegBackward0>) tensor(11405.4648, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11405.201171875
tensor(11405.4648, grad_fn=<NegBackward0>) tensor(11405.2012, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11404.921875
tensor(11405.2012, grad_fn=<NegBackward0>) tensor(11404.9219, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11404.474609375
tensor(11404.9219, grad_fn=<NegBackward0>) tensor(11404.4746, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11371.2294921875
tensor(11404.4746, grad_fn=<NegBackward0>) tensor(11371.2295, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11203.9267578125
tensor(11371.2295, grad_fn=<NegBackward0>) tensor(11203.9268, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11154.7275390625
tensor(11203.9268, grad_fn=<NegBackward0>) tensor(11154.7275, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11154.322265625
tensor(11154.7275, grad_fn=<NegBackward0>) tensor(11154.3223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11148.56640625
tensor(11154.3223, grad_fn=<NegBackward0>) tensor(11148.5664, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11148.474609375
tensor(11148.5664, grad_fn=<NegBackward0>) tensor(11148.4746, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11148.4189453125
tensor(11148.4746, grad_fn=<NegBackward0>) tensor(11148.4189, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11148.146484375
tensor(11148.4189, grad_fn=<NegBackward0>) tensor(11148.1465, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11148.01953125
tensor(11148.1465, grad_fn=<NegBackward0>) tensor(11148.0195, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11148.0
tensor(11148.0195, grad_fn=<NegBackward0>) tensor(11148., grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11147.98828125
tensor(11148., grad_fn=<NegBackward0>) tensor(11147.9883, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11147.98046875
tensor(11147.9883, grad_fn=<NegBackward0>) tensor(11147.9805, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11147.9697265625
tensor(11147.9805, grad_fn=<NegBackward0>) tensor(11147.9697, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11147.9541015625
tensor(11147.9697, grad_fn=<NegBackward0>) tensor(11147.9541, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11147.9267578125
tensor(11147.9541, grad_fn=<NegBackward0>) tensor(11147.9268, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11147.916015625
tensor(11147.9268, grad_fn=<NegBackward0>) tensor(11147.9160, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11147.90234375
tensor(11147.9160, grad_fn=<NegBackward0>) tensor(11147.9023, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11147.8837890625
tensor(11147.9023, grad_fn=<NegBackward0>) tensor(11147.8838, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11147.8798828125
tensor(11147.8838, grad_fn=<NegBackward0>) tensor(11147.8799, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11147.8798828125
tensor(11147.8799, grad_fn=<NegBackward0>) tensor(11147.8799, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11147.875
tensor(11147.8799, grad_fn=<NegBackward0>) tensor(11147.8750, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11147.8759765625
tensor(11147.8750, grad_fn=<NegBackward0>) tensor(11147.8760, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11147.8720703125
tensor(11147.8750, grad_fn=<NegBackward0>) tensor(11147.8721, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11147.8701171875
tensor(11147.8721, grad_fn=<NegBackward0>) tensor(11147.8701, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11147.8681640625
tensor(11147.8701, grad_fn=<NegBackward0>) tensor(11147.8682, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11147.8671875
tensor(11147.8682, grad_fn=<NegBackward0>) tensor(11147.8672, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11147.8671875
tensor(11147.8672, grad_fn=<NegBackward0>) tensor(11147.8672, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11147.865234375
tensor(11147.8672, grad_fn=<NegBackward0>) tensor(11147.8652, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11147.86328125
tensor(11147.8652, grad_fn=<NegBackward0>) tensor(11147.8633, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11147.8681640625
tensor(11147.8633, grad_fn=<NegBackward0>) tensor(11147.8682, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11147.8623046875
tensor(11147.8633, grad_fn=<NegBackward0>) tensor(11147.8623, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11147.861328125
tensor(11147.8623, grad_fn=<NegBackward0>) tensor(11147.8613, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11147.861328125
tensor(11147.8613, grad_fn=<NegBackward0>) tensor(11147.8613, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11147.86328125
tensor(11147.8613, grad_fn=<NegBackward0>) tensor(11147.8633, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11147.861328125
tensor(11147.8613, grad_fn=<NegBackward0>) tensor(11147.8613, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11147.8583984375
tensor(11147.8613, grad_fn=<NegBackward0>) tensor(11147.8584, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11147.8505859375
tensor(11147.8584, grad_fn=<NegBackward0>) tensor(11147.8506, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11147.8466796875
tensor(11147.8506, grad_fn=<NegBackward0>) tensor(11147.8467, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11147.845703125
tensor(11147.8467, grad_fn=<NegBackward0>) tensor(11147.8457, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11147.8447265625
tensor(11147.8457, grad_fn=<NegBackward0>) tensor(11147.8447, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11147.8447265625
tensor(11147.8447, grad_fn=<NegBackward0>) tensor(11147.8447, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11147.8466796875
tensor(11147.8447, grad_fn=<NegBackward0>) tensor(11147.8467, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11147.8427734375
tensor(11147.8447, grad_fn=<NegBackward0>) tensor(11147.8428, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11147.8427734375
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8428, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11147.84375
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8438, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11147.8427734375
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8428, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11147.84375
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8438, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11147.841796875
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8418, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11147.849609375
tensor(11147.8418, grad_fn=<NegBackward0>) tensor(11147.8496, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11147.84375
tensor(11147.8418, grad_fn=<NegBackward0>) tensor(11147.8438, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11147.8369140625
tensor(11147.8418, grad_fn=<NegBackward0>) tensor(11147.8369, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11147.833984375
tensor(11147.8369, grad_fn=<NegBackward0>) tensor(11147.8340, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11147.8447265625
tensor(11147.8340, grad_fn=<NegBackward0>) tensor(11147.8447, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11147.841796875
tensor(11147.8340, grad_fn=<NegBackward0>) tensor(11147.8418, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11147.833984375
tensor(11147.8340, grad_fn=<NegBackward0>) tensor(11147.8340, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11147.8330078125
tensor(11147.8340, grad_fn=<NegBackward0>) tensor(11147.8330, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11147.83203125
tensor(11147.8330, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11147.83203125
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11147.83203125
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11147.83203125
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11147.83203125
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11147.83203125
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11147.8310546875
tensor(11147.8320, grad_fn=<NegBackward0>) tensor(11147.8311, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11147.83984375
tensor(11147.8311, grad_fn=<NegBackward0>) tensor(11147.8398, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11147.8291015625
tensor(11147.8311, grad_fn=<NegBackward0>) tensor(11147.8291, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11147.8330078125
tensor(11147.8291, grad_fn=<NegBackward0>) tensor(11147.8330, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11147.83203125
tensor(11147.8291, grad_fn=<NegBackward0>) tensor(11147.8320, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11147.8896484375
tensor(11147.8291, grad_fn=<NegBackward0>) tensor(11147.8896, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11147.7939453125
tensor(11147.8291, grad_fn=<NegBackward0>) tensor(11147.7939, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11147.79296875
tensor(11147.7939, grad_fn=<NegBackward0>) tensor(11147.7930, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11147.796875
tensor(11147.7930, grad_fn=<NegBackward0>) tensor(11147.7969, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11147.7890625
tensor(11147.7930, grad_fn=<NegBackward0>) tensor(11147.7891, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11147.771484375
tensor(11147.7891, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11147.783203125
tensor(11147.7715, grad_fn=<NegBackward0>) tensor(11147.7832, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11147.7705078125
tensor(11147.7715, grad_fn=<NegBackward0>) tensor(11147.7705, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11147.78515625
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7852, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11147.771484375
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11147.771484375
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11147.787109375
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7871, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11147.771484375
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7818, 0.2182],
        [0.2348, 0.7652]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5320, 0.4680], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1096],
         [0.7064, 0.3014]],

        [[0.5202, 0.1004],
         [0.5358, 0.5829]],

        [[0.5469, 0.1020],
         [0.7206, 0.6114]],

        [[0.6603, 0.0961],
         [0.5411, 0.6743]],

        [[0.5886, 0.1016],
         [0.5097, 0.7215]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9446717667564802
Average Adjusted Rand Index: 0.9443160478563961
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20728.279296875
inf tensor(20728.2793, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11410.1767578125
tensor(20728.2793, grad_fn=<NegBackward0>) tensor(11410.1768, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11409.650390625
tensor(11410.1768, grad_fn=<NegBackward0>) tensor(11409.6504, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11408.068359375
tensor(11409.6504, grad_fn=<NegBackward0>) tensor(11408.0684, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11407.802734375
tensor(11408.0684, grad_fn=<NegBackward0>) tensor(11407.8027, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11407.3154296875
tensor(11407.8027, grad_fn=<NegBackward0>) tensor(11407.3154, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11406.73828125
tensor(11407.3154, grad_fn=<NegBackward0>) tensor(11406.7383, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11405.5634765625
tensor(11406.7383, grad_fn=<NegBackward0>) tensor(11405.5635, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11404.4716796875
tensor(11405.5635, grad_fn=<NegBackward0>) tensor(11404.4717, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11256.279296875
tensor(11404.4717, grad_fn=<NegBackward0>) tensor(11256.2793, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11175.501953125
tensor(11256.2793, grad_fn=<NegBackward0>) tensor(11175.5020, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11148.6767578125
tensor(11175.5020, grad_fn=<NegBackward0>) tensor(11148.6768, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11148.1220703125
tensor(11148.6768, grad_fn=<NegBackward0>) tensor(11148.1221, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11148.0146484375
tensor(11148.1221, grad_fn=<NegBackward0>) tensor(11148.0146, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11147.9365234375
tensor(11148.0146, grad_fn=<NegBackward0>) tensor(11147.9365, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11147.91015625
tensor(11147.9365, grad_fn=<NegBackward0>) tensor(11147.9102, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11147.8935546875
tensor(11147.9102, grad_fn=<NegBackward0>) tensor(11147.8936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11147.880859375
tensor(11147.8936, grad_fn=<NegBackward0>) tensor(11147.8809, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11147.87109375
tensor(11147.8809, grad_fn=<NegBackward0>) tensor(11147.8711, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11147.8603515625
tensor(11147.8711, grad_fn=<NegBackward0>) tensor(11147.8604, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11147.8427734375
tensor(11147.8604, grad_fn=<NegBackward0>) tensor(11147.8428, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11147.8310546875
tensor(11147.8428, grad_fn=<NegBackward0>) tensor(11147.8311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11147.826171875
tensor(11147.8311, grad_fn=<NegBackward0>) tensor(11147.8262, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11147.8134765625
tensor(11147.8262, grad_fn=<NegBackward0>) tensor(11147.8135, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11147.7919921875
tensor(11147.8135, grad_fn=<NegBackward0>) tensor(11147.7920, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11147.78515625
tensor(11147.7920, grad_fn=<NegBackward0>) tensor(11147.7852, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11147.7841796875
tensor(11147.7852, grad_fn=<NegBackward0>) tensor(11147.7842, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11147.7822265625
tensor(11147.7842, grad_fn=<NegBackward0>) tensor(11147.7822, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11147.78125
tensor(11147.7822, grad_fn=<NegBackward0>) tensor(11147.7812, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11147.7783203125
tensor(11147.7812, grad_fn=<NegBackward0>) tensor(11147.7783, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11147.77734375
tensor(11147.7783, grad_fn=<NegBackward0>) tensor(11147.7773, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11147.775390625
tensor(11147.7773, grad_fn=<NegBackward0>) tensor(11147.7754, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11147.7744140625
tensor(11147.7754, grad_fn=<NegBackward0>) tensor(11147.7744, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11147.7744140625
tensor(11147.7744, grad_fn=<NegBackward0>) tensor(11147.7744, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11147.771484375
tensor(11147.7744, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11147.771484375
tensor(11147.7715, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11147.7705078125
tensor(11147.7715, grad_fn=<NegBackward0>) tensor(11147.7705, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11147.7685546875
tensor(11147.7705, grad_fn=<NegBackward0>) tensor(11147.7686, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11147.7685546875
tensor(11147.7686, grad_fn=<NegBackward0>) tensor(11147.7686, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11147.7666015625
tensor(11147.7686, grad_fn=<NegBackward0>) tensor(11147.7666, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11147.7666015625
tensor(11147.7666, grad_fn=<NegBackward0>) tensor(11147.7666, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11147.765625
tensor(11147.7666, grad_fn=<NegBackward0>) tensor(11147.7656, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11147.76953125
tensor(11147.7656, grad_fn=<NegBackward0>) tensor(11147.7695, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11147.7646484375
tensor(11147.7656, grad_fn=<NegBackward0>) tensor(11147.7646, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11147.765625
tensor(11147.7646, grad_fn=<NegBackward0>) tensor(11147.7656, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11147.7646484375
tensor(11147.7646, grad_fn=<NegBackward0>) tensor(11147.7646, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11147.767578125
tensor(11147.7646, grad_fn=<NegBackward0>) tensor(11147.7676, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11147.765625
tensor(11147.7646, grad_fn=<NegBackward0>) tensor(11147.7656, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11147.763671875
tensor(11147.7646, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11147.7646484375
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7646, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11147.765625
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7656, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11147.763671875
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11147.771484375
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7715, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11147.763671875
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11147.763671875
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11147.763671875
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11147.7626953125
tensor(11147.7637, grad_fn=<NegBackward0>) tensor(11147.7627, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11147.7626953125
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7627, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11147.7626953125
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7627, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11147.775390625
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7754, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11147.763671875
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11147.763671875
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7637, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11147.7607421875
tensor(11147.7627, grad_fn=<NegBackward0>) tensor(11147.7607, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11147.7626953125
tensor(11147.7607, grad_fn=<NegBackward0>) tensor(11147.7627, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11147.76171875
tensor(11147.7607, grad_fn=<NegBackward0>) tensor(11147.7617, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11147.76171875
tensor(11147.7607, grad_fn=<NegBackward0>) tensor(11147.7617, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11147.76171875
tensor(11147.7607, grad_fn=<NegBackward0>) tensor(11147.7617, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11147.7626953125
tensor(11147.7607, grad_fn=<NegBackward0>) tensor(11147.7627, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.7817, 0.2183],
        [0.2358, 0.7642]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5324, 0.4676], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1097],
         [0.6063, 0.3014]],

        [[0.7112, 0.1004],
         [0.5736, 0.5927]],

        [[0.5468, 0.1020],
         [0.5346, 0.6808]],

        [[0.7261, 0.0961],
         [0.5134, 0.7303]],

        [[0.6015, 0.1016],
         [0.6556, 0.6741]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9446717667564802
Average Adjusted Rand Index: 0.9443160478563961
[0.9446717667564802, 0.9446717667564802] [0.9443160478563961, 0.9443160478563961] [11147.771484375, 11147.7626953125]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11405.388075435876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21895.029296875
inf tensor(21895.0293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11755.2822265625
tensor(21895.0293, grad_fn=<NegBackward0>) tensor(11755.2822, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11751.984375
tensor(11755.2822, grad_fn=<NegBackward0>) tensor(11751.9844, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11749.7138671875
tensor(11751.9844, grad_fn=<NegBackward0>) tensor(11749.7139, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11747.3369140625
tensor(11749.7139, grad_fn=<NegBackward0>) tensor(11747.3369, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11482.8798828125
tensor(11747.3369, grad_fn=<NegBackward0>) tensor(11482.8799, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11459.8271484375
tensor(11482.8799, grad_fn=<NegBackward0>) tensor(11459.8271, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11456.9326171875
tensor(11459.8271, grad_fn=<NegBackward0>) tensor(11456.9326, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11456.8076171875
tensor(11456.9326, grad_fn=<NegBackward0>) tensor(11456.8076, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11456.7705078125
tensor(11456.8076, grad_fn=<NegBackward0>) tensor(11456.7705, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11456.7255859375
tensor(11456.7705, grad_fn=<NegBackward0>) tensor(11456.7256, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11456.708984375
tensor(11456.7256, grad_fn=<NegBackward0>) tensor(11456.7090, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11456.69921875
tensor(11456.7090, grad_fn=<NegBackward0>) tensor(11456.6992, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11456.6865234375
tensor(11456.6992, grad_fn=<NegBackward0>) tensor(11456.6865, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11456.6552734375
tensor(11456.6865, grad_fn=<NegBackward0>) tensor(11456.6553, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11456.6435546875
tensor(11456.6553, grad_fn=<NegBackward0>) tensor(11456.6436, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11456.63671875
tensor(11456.6436, grad_fn=<NegBackward0>) tensor(11456.6367, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11456.6298828125
tensor(11456.6367, grad_fn=<NegBackward0>) tensor(11456.6299, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11456.625
tensor(11456.6299, grad_fn=<NegBackward0>) tensor(11456.6250, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11456.623046875
tensor(11456.6250, grad_fn=<NegBackward0>) tensor(11456.6230, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11456.6220703125
tensor(11456.6230, grad_fn=<NegBackward0>) tensor(11456.6221, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11456.6201171875
tensor(11456.6221, grad_fn=<NegBackward0>) tensor(11456.6201, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11456.6181640625
tensor(11456.6201, grad_fn=<NegBackward0>) tensor(11456.6182, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11456.57421875
tensor(11456.6182, grad_fn=<NegBackward0>) tensor(11456.5742, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11456.4892578125
tensor(11456.5742, grad_fn=<NegBackward0>) tensor(11456.4893, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11456.4873046875
tensor(11456.4893, grad_fn=<NegBackward0>) tensor(11456.4873, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11456.4853515625
tensor(11456.4873, grad_fn=<NegBackward0>) tensor(11456.4854, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11456.486328125
tensor(11456.4854, grad_fn=<NegBackward0>) tensor(11456.4863, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11456.484375
tensor(11456.4854, grad_fn=<NegBackward0>) tensor(11456.4844, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11456.4833984375
tensor(11456.4844, grad_fn=<NegBackward0>) tensor(11456.4834, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11456.482421875
tensor(11456.4834, grad_fn=<NegBackward0>) tensor(11456.4824, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11456.4912109375
tensor(11456.4824, grad_fn=<NegBackward0>) tensor(11456.4912, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11456.47265625
tensor(11456.4824, grad_fn=<NegBackward0>) tensor(11456.4727, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11456.4716796875
tensor(11456.4727, grad_fn=<NegBackward0>) tensor(11456.4717, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11456.470703125
tensor(11456.4717, grad_fn=<NegBackward0>) tensor(11456.4707, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11456.4697265625
tensor(11456.4707, grad_fn=<NegBackward0>) tensor(11456.4697, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11456.4697265625
tensor(11456.4697, grad_fn=<NegBackward0>) tensor(11456.4697, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11456.46875
tensor(11456.4697, grad_fn=<NegBackward0>) tensor(11456.4688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11456.46875
tensor(11456.4688, grad_fn=<NegBackward0>) tensor(11456.4688, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11456.4716796875
tensor(11456.4688, grad_fn=<NegBackward0>) tensor(11456.4717, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11456.466796875
tensor(11456.4688, grad_fn=<NegBackward0>) tensor(11456.4668, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11456.46484375
tensor(11456.4668, grad_fn=<NegBackward0>) tensor(11456.4648, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11456.4453125
tensor(11456.4648, grad_fn=<NegBackward0>) tensor(11456.4453, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11456.4375
tensor(11456.4453, grad_fn=<NegBackward0>) tensor(11456.4375, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11456.458984375
tensor(11456.4375, grad_fn=<NegBackward0>) tensor(11456.4590, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11456.4365234375
tensor(11456.4375, grad_fn=<NegBackward0>) tensor(11456.4365, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11456.4365234375
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4365, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11456.4375
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4375, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11456.439453125
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4395, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11456.4384765625
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4385, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11456.4404296875
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4404, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11456.4345703125
tensor(11456.4365, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11456.4375
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4375, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11456.4365234375
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4365, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11456.4345703125
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11456.435546875
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4355, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11456.43359375
tensor(11456.4346, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11456.4345703125
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11456.4345703125
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11456.43359375
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11456.4326171875
tensor(11456.4336, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11456.4326171875
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11456.4345703125
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11456.4326171875
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11456.43359375
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11456.4326171875
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11456.4345703125
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11456.43359375
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11456.4384765625
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4385, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11456.431640625
tensor(11456.4326, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11456.439453125
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4395, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11456.5947265625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.5947, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11456.4326171875
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4326, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11456.43359375
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4336, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11456.4375
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4375, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11456.4345703125
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4346, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11456.431640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4316, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11456.4306640625
tensor(11456.4316, grad_fn=<NegBackward0>) tensor(11456.4307, grad_fn=<NegBackward0>)
pi: tensor([[0.5022, 0.4978],
        [0.5560, 0.4440]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5089, 0.4911], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2916, 0.1085],
         [0.5113, 0.2298]],

        [[0.6375, 0.0989],
         [0.6125, 0.5793]],

        [[0.5058, 0.0866],
         [0.5007, 0.6041]],

        [[0.5808, 0.0958],
         [0.7231, 0.5963]],

        [[0.5301, 0.1003],
         [0.6866, 0.6451]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.882296193749233
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8447743642510657
Global Adjusted Rand Index: 0.3350707548543026
Average Adjusted Rand Index: 0.913573015726356
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20411.330078125
inf tensor(20411.3301, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11755.15234375
tensor(20411.3301, grad_fn=<NegBackward0>) tensor(11755.1523, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11746.736328125
tensor(11755.1523, grad_fn=<NegBackward0>) tensor(11746.7363, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11737.8984375
tensor(11746.7363, grad_fn=<NegBackward0>) tensor(11737.8984, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11734.2509765625
tensor(11737.8984, grad_fn=<NegBackward0>) tensor(11734.2510, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11731.8681640625
tensor(11734.2510, grad_fn=<NegBackward0>) tensor(11731.8682, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11588.8779296875
tensor(11731.8682, grad_fn=<NegBackward0>) tensor(11588.8779, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11530.126953125
tensor(11588.8779, grad_fn=<NegBackward0>) tensor(11530.1270, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11493.69921875
tensor(11530.1270, grad_fn=<NegBackward0>) tensor(11493.6992, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11473.544921875
tensor(11493.6992, grad_fn=<NegBackward0>) tensor(11473.5449, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11473.0478515625
tensor(11473.5449, grad_fn=<NegBackward0>) tensor(11473.0479, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11472.7412109375
tensor(11473.0479, grad_fn=<NegBackward0>) tensor(11472.7412, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11472.7138671875
tensor(11472.7412, grad_fn=<NegBackward0>) tensor(11472.7139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11472.6953125
tensor(11472.7139, grad_fn=<NegBackward0>) tensor(11472.6953, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11472.68359375
tensor(11472.6953, grad_fn=<NegBackward0>) tensor(11472.6836, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11472.669921875
tensor(11472.6836, grad_fn=<NegBackward0>) tensor(11472.6699, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11472.443359375
tensor(11472.6699, grad_fn=<NegBackward0>) tensor(11472.4434, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11472.4033203125
tensor(11472.4434, grad_fn=<NegBackward0>) tensor(11472.4033, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11471.45703125
tensor(11472.4033, grad_fn=<NegBackward0>) tensor(11471.4570, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11471.4150390625
tensor(11471.4570, grad_fn=<NegBackward0>) tensor(11471.4150, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11471.3994140625
tensor(11471.4150, grad_fn=<NegBackward0>) tensor(11471.3994, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11471.3935546875
tensor(11471.3994, grad_fn=<NegBackward0>) tensor(11471.3936, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11471.375
tensor(11471.3936, grad_fn=<NegBackward0>) tensor(11471.3750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11471.3447265625
tensor(11471.3750, grad_fn=<NegBackward0>) tensor(11471.3447, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11471.341796875
tensor(11471.3447, grad_fn=<NegBackward0>) tensor(11471.3418, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11471.33984375
tensor(11471.3418, grad_fn=<NegBackward0>) tensor(11471.3398, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11471.3388671875
tensor(11471.3398, grad_fn=<NegBackward0>) tensor(11471.3389, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11471.33984375
tensor(11471.3389, grad_fn=<NegBackward0>) tensor(11471.3398, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11471.3359375
tensor(11471.3389, grad_fn=<NegBackward0>) tensor(11471.3359, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11471.3359375
tensor(11471.3359, grad_fn=<NegBackward0>) tensor(11471.3359, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11471.333984375
tensor(11471.3359, grad_fn=<NegBackward0>) tensor(11471.3340, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11471.330078125
tensor(11471.3340, grad_fn=<NegBackward0>) tensor(11471.3301, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11471.1865234375
tensor(11471.3301, grad_fn=<NegBackward0>) tensor(11471.1865, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11471.1728515625
tensor(11471.1865, grad_fn=<NegBackward0>) tensor(11471.1729, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11471.1728515625
tensor(11471.1729, grad_fn=<NegBackward0>) tensor(11471.1729, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11471.177734375
tensor(11471.1729, grad_fn=<NegBackward0>) tensor(11471.1777, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11471.1708984375
tensor(11471.1729, grad_fn=<NegBackward0>) tensor(11471.1709, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11471.1689453125
tensor(11471.1709, grad_fn=<NegBackward0>) tensor(11471.1689, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11471.173828125
tensor(11471.1689, grad_fn=<NegBackward0>) tensor(11471.1738, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11471.1669921875
tensor(11471.1689, grad_fn=<NegBackward0>) tensor(11471.1670, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11471.1640625
tensor(11471.1670, grad_fn=<NegBackward0>) tensor(11471.1641, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11471.158203125
tensor(11471.1641, grad_fn=<NegBackward0>) tensor(11471.1582, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11471.15625
tensor(11471.1582, grad_fn=<NegBackward0>) tensor(11471.1562, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11471.1552734375
tensor(11471.1562, grad_fn=<NegBackward0>) tensor(11471.1553, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11471.1611328125
tensor(11471.1553, grad_fn=<NegBackward0>) tensor(11471.1611, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11471.15625
tensor(11471.1553, grad_fn=<NegBackward0>) tensor(11471.1562, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11471.154296875
tensor(11471.1553, grad_fn=<NegBackward0>) tensor(11471.1543, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11471.1552734375
tensor(11471.1543, grad_fn=<NegBackward0>) tensor(11471.1553, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11471.1533203125
tensor(11471.1543, grad_fn=<NegBackward0>) tensor(11471.1533, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11471.1650390625
tensor(11471.1533, grad_fn=<NegBackward0>) tensor(11471.1650, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11471.15234375
tensor(11471.1533, grad_fn=<NegBackward0>) tensor(11471.1523, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11471.1640625
tensor(11471.1523, grad_fn=<NegBackward0>) tensor(11471.1641, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11471.1513671875
tensor(11471.1523, grad_fn=<NegBackward0>) tensor(11471.1514, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11471.15234375
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1523, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11471.15234375
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1523, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11471.1513671875
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1514, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11471.17578125
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1758, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11471.1513671875
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1514, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11471.150390625
tensor(11471.1514, grad_fn=<NegBackward0>) tensor(11471.1504, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11471.1494140625
tensor(11471.1504, grad_fn=<NegBackward0>) tensor(11471.1494, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11467.8251953125
tensor(11471.1494, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11467.8271484375
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8271, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11467.8271484375
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8271, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11467.8251953125
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11467.8251953125
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11467.853515625
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8535, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11467.826171875
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8262, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11467.8251953125
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11467.826171875
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8262, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11467.826171875
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8262, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11467.826171875
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8262, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11467.8251953125
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11467.8251953125
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11467.82421875
tensor(11467.8252, grad_fn=<NegBackward0>) tensor(11467.8242, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11467.8251953125
tensor(11467.8242, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11467.8583984375
tensor(11467.8242, grad_fn=<NegBackward0>) tensor(11467.8584, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11467.8251953125
tensor(11467.8242, grad_fn=<NegBackward0>) tensor(11467.8252, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11467.828125
tensor(11467.8242, grad_fn=<NegBackward0>) tensor(11467.8281, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11467.859375
tensor(11467.8242, grad_fn=<NegBackward0>) tensor(11467.8594, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.6283, 0.3717],
        [0.4314, 0.5686]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4326, 0.5674], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2521, 0.1067],
         [0.5998, 0.2724]],

        [[0.7213, 0.0984],
         [0.6273, 0.5799]],

        [[0.6748, 0.0856],
         [0.5768, 0.7100]],

        [[0.6582, 0.0953],
         [0.5267, 0.6589]],

        [[0.5343, 0.0961],
         [0.5416, 0.5499]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7370098820529168
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.882389682918764
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.882296193749233
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8445229681978799
Global Adjusted Rand Index: 0.03048870062153569
Average Adjusted Rand Index: 0.8534053615453748
[0.3350707548543026, 0.03048870062153569] [0.913573015726356, 0.8534053615453748] [11456.43359375, 11467.859375]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11451.880755507842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22097.63671875
inf tensor(22097.6367, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11854.5224609375
tensor(22097.6367, grad_fn=<NegBackward0>) tensor(11854.5225, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11845.85546875
tensor(11854.5225, grad_fn=<NegBackward0>) tensor(11845.8555, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11841.4794921875
tensor(11845.8555, grad_fn=<NegBackward0>) tensor(11841.4795, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11823.515625
tensor(11841.4795, grad_fn=<NegBackward0>) tensor(11823.5156, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11687.0419921875
tensor(11823.5156, grad_fn=<NegBackward0>) tensor(11687.0420, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11634.12109375
tensor(11687.0420, grad_fn=<NegBackward0>) tensor(11634.1211, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11609.8369140625
tensor(11634.1211, grad_fn=<NegBackward0>) tensor(11609.8369, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11602.0302734375
tensor(11609.8369, grad_fn=<NegBackward0>) tensor(11602.0303, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11596.51953125
tensor(11602.0303, grad_fn=<NegBackward0>) tensor(11596.5195, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11588.21484375
tensor(11596.5195, grad_fn=<NegBackward0>) tensor(11588.2148, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11588.03125
tensor(11588.2148, grad_fn=<NegBackward0>) tensor(11588.0312, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11576.6611328125
tensor(11588.0312, grad_fn=<NegBackward0>) tensor(11576.6611, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11573.8466796875
tensor(11576.6611, grad_fn=<NegBackward0>) tensor(11573.8467, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11569.005859375
tensor(11573.8467, grad_fn=<NegBackward0>) tensor(11569.0059, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11568.7392578125
tensor(11569.0059, grad_fn=<NegBackward0>) tensor(11568.7393, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11561.5400390625
tensor(11568.7393, grad_fn=<NegBackward0>) tensor(11561.5400, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11561.5087890625
tensor(11561.5400, grad_fn=<NegBackward0>) tensor(11561.5088, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11561.4833984375
tensor(11561.5088, grad_fn=<NegBackward0>) tensor(11561.4834, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11561.4609375
tensor(11561.4834, grad_fn=<NegBackward0>) tensor(11561.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11561.3740234375
tensor(11561.4609, grad_fn=<NegBackward0>) tensor(11561.3740, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11561.3232421875
tensor(11561.3740, grad_fn=<NegBackward0>) tensor(11561.3232, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11561.30859375
tensor(11561.3232, grad_fn=<NegBackward0>) tensor(11561.3086, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11561.2978515625
tensor(11561.3086, grad_fn=<NegBackward0>) tensor(11561.2979, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11561.2890625
tensor(11561.2979, grad_fn=<NegBackward0>) tensor(11561.2891, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11561.2822265625
tensor(11561.2891, grad_fn=<NegBackward0>) tensor(11561.2822, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11561.275390625
tensor(11561.2822, grad_fn=<NegBackward0>) tensor(11561.2754, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11561.26953125
tensor(11561.2754, grad_fn=<NegBackward0>) tensor(11561.2695, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11561.2626953125
tensor(11561.2695, grad_fn=<NegBackward0>) tensor(11561.2627, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11556.2138671875
tensor(11561.2627, grad_fn=<NegBackward0>) tensor(11556.2139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11556.1513671875
tensor(11556.2139, grad_fn=<NegBackward0>) tensor(11556.1514, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11556.14453125
tensor(11556.1514, grad_fn=<NegBackward0>) tensor(11556.1445, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11556.140625
tensor(11556.1445, grad_fn=<NegBackward0>) tensor(11556.1406, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11556.13671875
tensor(11556.1406, grad_fn=<NegBackward0>) tensor(11556.1367, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11556.134765625
tensor(11556.1367, grad_fn=<NegBackward0>) tensor(11556.1348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11556.130859375
tensor(11556.1348, grad_fn=<NegBackward0>) tensor(11556.1309, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11556.126953125
tensor(11556.1309, grad_fn=<NegBackward0>) tensor(11556.1270, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11556.1240234375
tensor(11556.1270, grad_fn=<NegBackward0>) tensor(11556.1240, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11556.1220703125
tensor(11556.1240, grad_fn=<NegBackward0>) tensor(11556.1221, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11556.1201171875
tensor(11556.1221, grad_fn=<NegBackward0>) tensor(11556.1201, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11556.1171875
tensor(11556.1201, grad_fn=<NegBackward0>) tensor(11556.1172, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11556.115234375
tensor(11556.1172, grad_fn=<NegBackward0>) tensor(11556.1152, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11556.1142578125
tensor(11556.1152, grad_fn=<NegBackward0>) tensor(11556.1143, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11556.1123046875
tensor(11556.1143, grad_fn=<NegBackward0>) tensor(11556.1123, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11556.111328125
tensor(11556.1123, grad_fn=<NegBackward0>) tensor(11556.1113, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11556.109375
tensor(11556.1113, grad_fn=<NegBackward0>) tensor(11556.1094, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11556.109375
tensor(11556.1094, grad_fn=<NegBackward0>) tensor(11556.1094, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11556.1083984375
tensor(11556.1094, grad_fn=<NegBackward0>) tensor(11556.1084, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11556.111328125
tensor(11556.1084, grad_fn=<NegBackward0>) tensor(11556.1113, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11556.0791015625
tensor(11556.1084, grad_fn=<NegBackward0>) tensor(11556.0791, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11556.068359375
tensor(11556.0791, grad_fn=<NegBackward0>) tensor(11556.0684, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11556.064453125
tensor(11556.0684, grad_fn=<NegBackward0>) tensor(11556.0645, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11556.0517578125
tensor(11556.0645, grad_fn=<NegBackward0>) tensor(11556.0518, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11556.0517578125
tensor(11556.0518, grad_fn=<NegBackward0>) tensor(11556.0518, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11556.05078125
tensor(11556.0518, grad_fn=<NegBackward0>) tensor(11556.0508, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11556.048828125
tensor(11556.0508, grad_fn=<NegBackward0>) tensor(11556.0488, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11556.0654296875
tensor(11556.0488, grad_fn=<NegBackward0>) tensor(11556.0654, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11556.02734375
tensor(11556.0488, grad_fn=<NegBackward0>) tensor(11556.0273, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11556.0068359375
tensor(11556.0273, grad_fn=<NegBackward0>) tensor(11556.0068, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11556.005859375
tensor(11556.0068, grad_fn=<NegBackward0>) tensor(11556.0059, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11556.0048828125
tensor(11556.0059, grad_fn=<NegBackward0>) tensor(11556.0049, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11556.00390625
tensor(11556.0049, grad_fn=<NegBackward0>) tensor(11556.0039, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11556.0107421875
tensor(11556.0039, grad_fn=<NegBackward0>) tensor(11556.0107, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11556.0029296875
tensor(11556.0039, grad_fn=<NegBackward0>) tensor(11556.0029, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11556.0107421875
tensor(11556.0029, grad_fn=<NegBackward0>) tensor(11556.0107, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11556.0
tensor(11556.0029, grad_fn=<NegBackward0>) tensor(11556., grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11556.001953125
tensor(11556., grad_fn=<NegBackward0>) tensor(11556.0020, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11555.994140625
tensor(11556., grad_fn=<NegBackward0>) tensor(11555.9941, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11555.994140625
tensor(11555.9941, grad_fn=<NegBackward0>) tensor(11555.9941, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11555.994140625
tensor(11555.9941, grad_fn=<NegBackward0>) tensor(11555.9941, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11555.9990234375
tensor(11555.9941, grad_fn=<NegBackward0>) tensor(11555.9990, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11555.9775390625
tensor(11555.9941, grad_fn=<NegBackward0>) tensor(11555.9775, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11555.96875
tensor(11555.9775, grad_fn=<NegBackward0>) tensor(11555.9688, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11555.9716796875
tensor(11555.9688, grad_fn=<NegBackward0>) tensor(11555.9717, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11555.9716796875
tensor(11555.9688, grad_fn=<NegBackward0>) tensor(11555.9717, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11555.9609375
tensor(11555.9688, grad_fn=<NegBackward0>) tensor(11555.9609, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11555.958984375
tensor(11555.9609, grad_fn=<NegBackward0>) tensor(11555.9590, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11555.9580078125
tensor(11555.9590, grad_fn=<NegBackward0>) tensor(11555.9580, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11555.958984375
tensor(11555.9580, grad_fn=<NegBackward0>) tensor(11555.9590, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11555.96875
tensor(11555.9580, grad_fn=<NegBackward0>) tensor(11555.9688, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11555.9541015625
tensor(11555.9580, grad_fn=<NegBackward0>) tensor(11555.9541, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11555.927734375
tensor(11555.9541, grad_fn=<NegBackward0>) tensor(11555.9277, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11555.927734375
tensor(11555.9277, grad_fn=<NegBackward0>) tensor(11555.9277, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11555.9267578125
tensor(11555.9277, grad_fn=<NegBackward0>) tensor(11555.9268, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11555.92578125
tensor(11555.9268, grad_fn=<NegBackward0>) tensor(11555.9258, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11555.9306640625
tensor(11555.9258, grad_fn=<NegBackward0>) tensor(11555.9307, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11555.9248046875
tensor(11555.9258, grad_fn=<NegBackward0>) tensor(11555.9248, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11555.9267578125
tensor(11555.9248, grad_fn=<NegBackward0>) tensor(11555.9268, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11555.92578125
tensor(11555.9248, grad_fn=<NegBackward0>) tensor(11555.9258, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11555.92578125
tensor(11555.9248, grad_fn=<NegBackward0>) tensor(11555.9258, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11555.9296875
tensor(11555.9248, grad_fn=<NegBackward0>) tensor(11555.9297, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11555.9267578125
tensor(11555.9248, grad_fn=<NegBackward0>) tensor(11555.9268, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.6018, 0.3982],
        [0.2008, 0.7992]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 1.8163e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1901, 0.1458],
         [0.5597, 0.3048]],

        [[0.6143, 0.0933],
         [0.6105, 0.5376]],

        [[0.7067, 0.0995],
         [0.5847, 0.6011]],

        [[0.7132, 0.1059],
         [0.5462, 0.6252]],

        [[0.5946, 0.1085],
         [0.6859, 0.5361]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.6201631993979153
Average Adjusted Rand Index: 0.7839998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23285.669921875
inf tensor(23285.6699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11639.2744140625
tensor(23285.6699, grad_fn=<NegBackward0>) tensor(11639.2744, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11534.8720703125
tensor(11639.2744, grad_fn=<NegBackward0>) tensor(11534.8721, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11532.626953125
tensor(11534.8721, grad_fn=<NegBackward0>) tensor(11532.6270, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11532.439453125
tensor(11532.6270, grad_fn=<NegBackward0>) tensor(11532.4395, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11531.41796875
tensor(11532.4395, grad_fn=<NegBackward0>) tensor(11531.4180, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11531.3291015625
tensor(11531.4180, grad_fn=<NegBackward0>) tensor(11531.3291, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11531.2734375
tensor(11531.3291, grad_fn=<NegBackward0>) tensor(11531.2734, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11531.2548828125
tensor(11531.2734, grad_fn=<NegBackward0>) tensor(11531.2549, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11531.236328125
tensor(11531.2549, grad_fn=<NegBackward0>) tensor(11531.2363, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11531.224609375
tensor(11531.2363, grad_fn=<NegBackward0>) tensor(11531.2246, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11531.216796875
tensor(11531.2246, grad_fn=<NegBackward0>) tensor(11531.2168, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11531.2109375
tensor(11531.2168, grad_fn=<NegBackward0>) tensor(11531.2109, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11531.2060546875
tensor(11531.2109, grad_fn=<NegBackward0>) tensor(11531.2061, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11531.201171875
tensor(11531.2061, grad_fn=<NegBackward0>) tensor(11531.2012, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11531.19921875
tensor(11531.2012, grad_fn=<NegBackward0>) tensor(11531.1992, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11531.1953125
tensor(11531.1992, grad_fn=<NegBackward0>) tensor(11531.1953, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11531.2041015625
tensor(11531.1953, grad_fn=<NegBackward0>) tensor(11531.2041, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11531.1904296875
tensor(11531.1953, grad_fn=<NegBackward0>) tensor(11531.1904, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11531.1884765625
tensor(11531.1904, grad_fn=<NegBackward0>) tensor(11531.1885, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11531.1865234375
tensor(11531.1885, grad_fn=<NegBackward0>) tensor(11531.1865, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11531.1845703125
tensor(11531.1865, grad_fn=<NegBackward0>) tensor(11531.1846, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11531.1845703125
tensor(11531.1846, grad_fn=<NegBackward0>) tensor(11531.1846, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11531.1806640625
tensor(11531.1846, grad_fn=<NegBackward0>) tensor(11531.1807, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11531.1796875
tensor(11531.1807, grad_fn=<NegBackward0>) tensor(11531.1797, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11531.1787109375
tensor(11531.1797, grad_fn=<NegBackward0>) tensor(11531.1787, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11531.1767578125
tensor(11531.1787, grad_fn=<NegBackward0>) tensor(11531.1768, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11531.1796875
tensor(11531.1768, grad_fn=<NegBackward0>) tensor(11531.1797, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11531.1767578125
tensor(11531.1768, grad_fn=<NegBackward0>) tensor(11531.1768, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11531.1767578125
tensor(11531.1768, grad_fn=<NegBackward0>) tensor(11531.1768, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11531.17578125
tensor(11531.1768, grad_fn=<NegBackward0>) tensor(11531.1758, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11531.1767578125
tensor(11531.1758, grad_fn=<NegBackward0>) tensor(11531.1768, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11531.1748046875
tensor(11531.1758, grad_fn=<NegBackward0>) tensor(11531.1748, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11531.1748046875
tensor(11531.1748, grad_fn=<NegBackward0>) tensor(11531.1748, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11531.173828125
tensor(11531.1748, grad_fn=<NegBackward0>) tensor(11531.1738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11531.1748046875
tensor(11531.1738, grad_fn=<NegBackward0>) tensor(11531.1748, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11531.1748046875
tensor(11531.1738, grad_fn=<NegBackward0>) tensor(11531.1748, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11531.173828125
tensor(11531.1738, grad_fn=<NegBackward0>) tensor(11531.1738, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11531.1728515625
tensor(11531.1738, grad_fn=<NegBackward0>) tensor(11531.1729, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11531.18359375
tensor(11531.1729, grad_fn=<NegBackward0>) tensor(11531.1836, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11531.171875
tensor(11531.1729, grad_fn=<NegBackward0>) tensor(11531.1719, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11531.1728515625
tensor(11531.1719, grad_fn=<NegBackward0>) tensor(11531.1729, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11531.171875
tensor(11531.1719, grad_fn=<NegBackward0>) tensor(11531.1719, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11531.1708984375
tensor(11531.1719, grad_fn=<NegBackward0>) tensor(11531.1709, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11531.171875
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1719, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11531.171875
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1719, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11531.1767578125
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1768, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11531.1708984375
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1709, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11531.1708984375
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1709, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11531.169921875
tensor(11531.1709, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11531.1728515625
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1729, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11531.1708984375
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1709, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11531.1708984375
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1709, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11531.169921875
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11531.169921875
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11531.169921875
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11531.1689453125
tensor(11531.1699, grad_fn=<NegBackward0>) tensor(11531.1689, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11531.1728515625
tensor(11531.1689, grad_fn=<NegBackward0>) tensor(11531.1729, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11531.16796875
tensor(11531.1689, grad_fn=<NegBackward0>) tensor(11531.1680, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11531.173828125
tensor(11531.1680, grad_fn=<NegBackward0>) tensor(11531.1738, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11531.181640625
tensor(11531.1680, grad_fn=<NegBackward0>) tensor(11531.1816, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11531.1669921875
tensor(11531.1680, grad_fn=<NegBackward0>) tensor(11531.1670, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11531.166015625
tensor(11531.1670, grad_fn=<NegBackward0>) tensor(11531.1660, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11531.1669921875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1670, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11531.169921875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11531.166015625
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1660, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11531.166015625
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1660, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11531.16796875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1680, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11531.1669921875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1670, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11531.169921875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1699, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11531.16796875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1680, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11531.16796875
tensor(11531.1660, grad_fn=<NegBackward0>) tensor(11531.1680, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6352, 0.3648],
        [0.3111, 0.6889]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5361, 0.4639], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2809, 0.0869],
         [0.5945, 0.2578]],

        [[0.6262, 0.0929],
         [0.6476, 0.6530]],

        [[0.6562, 0.0993],
         [0.5856, 0.5287]],

        [[0.6400, 0.1045],
         [0.7292, 0.5206]],

        [[0.5899, 0.1059],
         [0.7300, 0.6667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721463199647421
Global Adjusted Rand Index: 0.04475676148722184
Average Adjusted Rand Index: 0.9225747516602535
[0.6201631993979153, 0.04475676148722184] [0.7839998119331364, 0.9225747516602535] [11555.9267578125, 11531.16796875]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11024.680396004685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25113.69921875
inf tensor(25113.6992, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11342.94921875
tensor(25113.6992, grad_fn=<NegBackward0>) tensor(11342.9492, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11341.037109375
tensor(11342.9492, grad_fn=<NegBackward0>) tensor(11341.0371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11338.728515625
tensor(11341.0371, grad_fn=<NegBackward0>) tensor(11338.7285, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11337.9072265625
tensor(11338.7285, grad_fn=<NegBackward0>) tensor(11337.9072, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11337.6123046875
tensor(11337.9072, grad_fn=<NegBackward0>) tensor(11337.6123, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11337.484375
tensor(11337.6123, grad_fn=<NegBackward0>) tensor(11337.4844, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11337.3876953125
tensor(11337.4844, grad_fn=<NegBackward0>) tensor(11337.3877, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11335.2705078125
tensor(11337.3877, grad_fn=<NegBackward0>) tensor(11335.2705, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11333.9912109375
tensor(11335.2705, grad_fn=<NegBackward0>) tensor(11333.9912, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11332.6845703125
tensor(11333.9912, grad_fn=<NegBackward0>) tensor(11332.6846, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11332.51953125
tensor(11332.6846, grad_fn=<NegBackward0>) tensor(11332.5195, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11262.361328125
tensor(11332.5195, grad_fn=<NegBackward0>) tensor(11262.3613, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11193.8916015625
tensor(11262.3613, grad_fn=<NegBackward0>) tensor(11193.8916, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11189.8623046875
tensor(11193.8916, grad_fn=<NegBackward0>) tensor(11189.8623, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11189.6220703125
tensor(11189.8623, grad_fn=<NegBackward0>) tensor(11189.6221, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11188.7568359375
tensor(11189.6221, grad_fn=<NegBackward0>) tensor(11188.7568, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11186.638671875
tensor(11188.7568, grad_fn=<NegBackward0>) tensor(11186.6387, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11185.8662109375
tensor(11186.6387, grad_fn=<NegBackward0>) tensor(11185.8662, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11185.58984375
tensor(11185.8662, grad_fn=<NegBackward0>) tensor(11185.5898, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11185.5693359375
tensor(11185.5898, grad_fn=<NegBackward0>) tensor(11185.5693, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11185.46875
tensor(11185.5693, grad_fn=<NegBackward0>) tensor(11185.4688, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11185.439453125
tensor(11185.4688, grad_fn=<NegBackward0>) tensor(11185.4395, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11185.419921875
tensor(11185.4395, grad_fn=<NegBackward0>) tensor(11185.4199, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11174.2685546875
tensor(11185.4199, grad_fn=<NegBackward0>) tensor(11174.2686, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11162.8154296875
tensor(11174.2686, grad_fn=<NegBackward0>) tensor(11162.8154, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11161.392578125
tensor(11162.8154, grad_fn=<NegBackward0>) tensor(11161.3926, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11161.3544921875
tensor(11161.3926, grad_fn=<NegBackward0>) tensor(11161.3545, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11161.1953125
tensor(11161.3545, grad_fn=<NegBackward0>) tensor(11161.1953, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11161.0712890625
tensor(11161.1953, grad_fn=<NegBackward0>) tensor(11161.0713, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11160.6943359375
tensor(11161.0713, grad_fn=<NegBackward0>) tensor(11160.6943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11160.6796875
tensor(11160.6943, grad_fn=<NegBackward0>) tensor(11160.6797, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11155.888671875
tensor(11160.6797, grad_fn=<NegBackward0>) tensor(11155.8887, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11155.8876953125
tensor(11155.8887, grad_fn=<NegBackward0>) tensor(11155.8877, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11155.8779296875
tensor(11155.8877, grad_fn=<NegBackward0>) tensor(11155.8779, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11155.876953125
tensor(11155.8779, grad_fn=<NegBackward0>) tensor(11155.8770, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11155.876953125
tensor(11155.8770, grad_fn=<NegBackward0>) tensor(11155.8770, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11155.8564453125
tensor(11155.8770, grad_fn=<NegBackward0>) tensor(11155.8564, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11147.4208984375
tensor(11155.8564, grad_fn=<NegBackward0>) tensor(11147.4209, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11146.02734375
tensor(11147.4209, grad_fn=<NegBackward0>) tensor(11146.0273, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11145.314453125
tensor(11146.0273, grad_fn=<NegBackward0>) tensor(11145.3145, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11145.3203125
tensor(11145.3145, grad_fn=<NegBackward0>) tensor(11145.3203, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11145.3154296875
tensor(11145.3145, grad_fn=<NegBackward0>) tensor(11145.3154, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11145.306640625
tensor(11145.3145, grad_fn=<NegBackward0>) tensor(11145.3066, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11144.814453125
tensor(11145.3066, grad_fn=<NegBackward0>) tensor(11144.8145, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11144.8115234375
tensor(11144.8145, grad_fn=<NegBackward0>) tensor(11144.8115, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11144.806640625
tensor(11144.8115, grad_fn=<NegBackward0>) tensor(11144.8066, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11144.80859375
tensor(11144.8066, grad_fn=<NegBackward0>) tensor(11144.8086, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11144.8076171875
tensor(11144.8066, grad_fn=<NegBackward0>) tensor(11144.8076, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11144.7734375
tensor(11144.8066, grad_fn=<NegBackward0>) tensor(11144.7734, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11142.966796875
tensor(11144.7734, grad_fn=<NegBackward0>) tensor(11142.9668, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11142.931640625
tensor(11142.9668, grad_fn=<NegBackward0>) tensor(11142.9316, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11142.921875
tensor(11142.9316, grad_fn=<NegBackward0>) tensor(11142.9219, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11139.3271484375
tensor(11142.9219, grad_fn=<NegBackward0>) tensor(11139.3271, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11139.15625
tensor(11139.3271, grad_fn=<NegBackward0>) tensor(11139.1562, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11139.1552734375
tensor(11139.1562, grad_fn=<NegBackward0>) tensor(11139.1553, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11134.0703125
tensor(11139.1553, grad_fn=<NegBackward0>) tensor(11134.0703, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11134.0498046875
tensor(11134.0703, grad_fn=<NegBackward0>) tensor(11134.0498, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11134.046875
tensor(11134.0498, grad_fn=<NegBackward0>) tensor(11134.0469, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11134.0458984375
tensor(11134.0469, grad_fn=<NegBackward0>) tensor(11134.0459, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11134.03125
tensor(11134.0459, grad_fn=<NegBackward0>) tensor(11134.0312, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11133.9970703125
tensor(11134.0312, grad_fn=<NegBackward0>) tensor(11133.9971, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11133.9970703125
tensor(11133.9971, grad_fn=<NegBackward0>) tensor(11133.9971, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11133.99609375
tensor(11133.9971, grad_fn=<NegBackward0>) tensor(11133.9961, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11133.9951171875
tensor(11133.9961, grad_fn=<NegBackward0>) tensor(11133.9951, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11133.84765625
tensor(11133.9951, grad_fn=<NegBackward0>) tensor(11133.8477, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11130.4462890625
tensor(11133.8477, grad_fn=<NegBackward0>) tensor(11130.4463, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11130.4267578125
tensor(11130.4463, grad_fn=<NegBackward0>) tensor(11130.4268, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11130.380859375
tensor(11130.4268, grad_fn=<NegBackward0>) tensor(11130.3809, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11130.388671875
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3887, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11130.3818359375
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3818, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11130.380859375
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3809, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11130.3828125
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3828, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11130.380859375
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3809, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11130.3818359375
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3818, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11130.37890625
tensor(11130.3809, grad_fn=<NegBackward0>) tensor(11130.3789, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11128.0458984375
tensor(11130.3789, grad_fn=<NegBackward0>) tensor(11128.0459, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11126.787109375
tensor(11128.0459, grad_fn=<NegBackward0>) tensor(11126.7871, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11126.701171875
tensor(11126.7871, grad_fn=<NegBackward0>) tensor(11126.7012, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11126.6982421875
tensor(11126.7012, grad_fn=<NegBackward0>) tensor(11126.6982, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11122.2197265625
tensor(11126.6982, grad_fn=<NegBackward0>) tensor(11122.2197, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11122.21484375
tensor(11122.2197, grad_fn=<NegBackward0>) tensor(11122.2148, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11122.2138671875
tensor(11122.2148, grad_fn=<NegBackward0>) tensor(11122.2139, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11122.2138671875
tensor(11122.2139, grad_fn=<NegBackward0>) tensor(11122.2139, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11122.2109375
tensor(11122.2139, grad_fn=<NegBackward0>) tensor(11122.2109, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11122.208984375
tensor(11122.2109, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11122.208984375
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11122.212890625
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2129, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11122.208984375
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11122.2197265625
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2197, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11122.208984375
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11122.208984375
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11122.2421875
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2422, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11122.208984375
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11122.2090, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11121.2822265625
tensor(11122.2090, grad_fn=<NegBackward0>) tensor(11121.2822, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11121.1796875
tensor(11121.2822, grad_fn=<NegBackward0>) tensor(11121.1797, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11121.1796875
tensor(11121.1797, grad_fn=<NegBackward0>) tensor(11121.1797, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11121.177734375
tensor(11121.1797, grad_fn=<NegBackward0>) tensor(11121.1777, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11121.1748046875
tensor(11121.1777, grad_fn=<NegBackward0>) tensor(11121.1748, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11121.142578125
tensor(11121.1748, grad_fn=<NegBackward0>) tensor(11121.1426, grad_fn=<NegBackward0>)
pi: tensor([[0.4624, 0.5376],
        [0.5262, 0.4738]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5639, 0.4361], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2377, 0.0916],
         [0.6847, 0.2595]],

        [[0.5219, 0.0917],
         [0.5626, 0.6590]],

        [[0.5936, 0.1004],
         [0.6949, 0.6908]],

        [[0.7154, 0.0958],
         [0.5206, 0.6297]],

        [[0.7128, 0.0896],
         [0.6867, 0.6753]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721314419105764
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080965973782139
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7370212140872312
Global Adjusted Rand Index: 0.03969312098002897
Average Adjusted Rand Index: 0.8092251345961738
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22922.1328125
inf tensor(22922.1328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11329.9443359375
tensor(22922.1328, grad_fn=<NegBackward0>) tensor(11329.9443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11219.6396484375
tensor(11329.9443, grad_fn=<NegBackward0>) tensor(11219.6396, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11200.80078125
tensor(11219.6396, grad_fn=<NegBackward0>) tensor(11200.8008, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11199.9697265625
tensor(11200.8008, grad_fn=<NegBackward0>) tensor(11199.9697, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11060.2265625
tensor(11199.9697, grad_fn=<NegBackward0>) tensor(11060.2266, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11012.54296875
tensor(11060.2266, grad_fn=<NegBackward0>) tensor(11012.5430, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11012.400390625
tensor(11012.5430, grad_fn=<NegBackward0>) tensor(11012.4004, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11012.3505859375
tensor(11012.4004, grad_fn=<NegBackward0>) tensor(11012.3506, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11012.3193359375
tensor(11012.3506, grad_fn=<NegBackward0>) tensor(11012.3193, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11012.3037109375
tensor(11012.3193, grad_fn=<NegBackward0>) tensor(11012.3037, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11012.2939453125
tensor(11012.3037, grad_fn=<NegBackward0>) tensor(11012.2939, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11012.287109375
tensor(11012.2939, grad_fn=<NegBackward0>) tensor(11012.2871, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11012.2822265625
tensor(11012.2871, grad_fn=<NegBackward0>) tensor(11012.2822, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11012.27734375
tensor(11012.2822, grad_fn=<NegBackward0>) tensor(11012.2773, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11012.2744140625
tensor(11012.2773, grad_fn=<NegBackward0>) tensor(11012.2744, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11012.271484375
tensor(11012.2744, grad_fn=<NegBackward0>) tensor(11012.2715, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11012.26953125
tensor(11012.2715, grad_fn=<NegBackward0>) tensor(11012.2695, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11012.2666015625
tensor(11012.2695, grad_fn=<NegBackward0>) tensor(11012.2666, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11012.2666015625
tensor(11012.2666, grad_fn=<NegBackward0>) tensor(11012.2666, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11012.265625
tensor(11012.2666, grad_fn=<NegBackward0>) tensor(11012.2656, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11012.2646484375
tensor(11012.2656, grad_fn=<NegBackward0>) tensor(11012.2646, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11012.2626953125
tensor(11012.2646, grad_fn=<NegBackward0>) tensor(11012.2627, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11012.26171875
tensor(11012.2627, grad_fn=<NegBackward0>) tensor(11012.2617, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11012.259765625
tensor(11012.2617, grad_fn=<NegBackward0>) tensor(11012.2598, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11012.259765625
tensor(11012.2598, grad_fn=<NegBackward0>) tensor(11012.2598, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11012.2587890625
tensor(11012.2598, grad_fn=<NegBackward0>) tensor(11012.2588, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11012.2587890625
tensor(11012.2588, grad_fn=<NegBackward0>) tensor(11012.2588, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11012.2578125
tensor(11012.2588, grad_fn=<NegBackward0>) tensor(11012.2578, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11012.2587890625
tensor(11012.2578, grad_fn=<NegBackward0>) tensor(11012.2588, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11012.255859375
tensor(11012.2578, grad_fn=<NegBackward0>) tensor(11012.2559, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11012.255859375
tensor(11012.2559, grad_fn=<NegBackward0>) tensor(11012.2559, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11012.2548828125
tensor(11012.2559, grad_fn=<NegBackward0>) tensor(11012.2549, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11012.255859375
tensor(11012.2549, grad_fn=<NegBackward0>) tensor(11012.2559, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11012.2548828125
tensor(11012.2549, grad_fn=<NegBackward0>) tensor(11012.2549, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11012.2548828125
tensor(11012.2549, grad_fn=<NegBackward0>) tensor(11012.2549, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11012.2578125
tensor(11012.2549, grad_fn=<NegBackward0>) tensor(11012.2578, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11012.251953125
tensor(11012.2549, grad_fn=<NegBackward0>) tensor(11012.2520, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11012.251953125
tensor(11012.2520, grad_fn=<NegBackward0>) tensor(11012.2520, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11012.2529296875
tensor(11012.2520, grad_fn=<NegBackward0>) tensor(11012.2529, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11012.2529296875
tensor(11012.2520, grad_fn=<NegBackward0>) tensor(11012.2529, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11012.2509765625
tensor(11012.2520, grad_fn=<NegBackward0>) tensor(11012.2510, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11012.2529296875
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2529, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11012.2529296875
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2529, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11012.251953125
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2520, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11012.2529296875
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2529, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -11012.2509765625
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2510, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11012.251953125
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2520, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11012.255859375
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2559, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11012.2666015625
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2666, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11012.251953125
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2520, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11012.255859375
tensor(11012.2510, grad_fn=<NegBackward0>) tensor(11012.2559, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.7572, 0.2428],
        [0.2554, 0.7446]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5101, 0.4899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2997, 0.0929],
         [0.6396, 0.1931]],

        [[0.5664, 0.0963],
         [0.5086, 0.6270]],

        [[0.6681, 0.1008],
         [0.5447, 0.5862]],

        [[0.6407, 0.0967],
         [0.6273, 0.6048]],

        [[0.5952, 0.0942],
         [0.6983, 0.5696]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681922769435793
Average Adjusted Rand Index: 0.9681592533255285
[0.03969312098002897, 0.9681922769435793] [0.8092251345961738, 0.9681592533255285] [11121.154296875, 11012.255859375]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11028.959918641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22375.775390625
inf tensor(22375.7754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11342.5810546875
tensor(22375.7754, grad_fn=<NegBackward0>) tensor(11342.5811, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11328.3798828125
tensor(11342.5811, grad_fn=<NegBackward0>) tensor(11328.3799, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10994.1337890625
tensor(11328.3799, grad_fn=<NegBackward0>) tensor(10994.1338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10993.6533203125
tensor(10994.1338, grad_fn=<NegBackward0>) tensor(10993.6533, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10993.5771484375
tensor(10993.6533, grad_fn=<NegBackward0>) tensor(10993.5771, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10993.5380859375
tensor(10993.5771, grad_fn=<NegBackward0>) tensor(10993.5381, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10993.51171875
tensor(10993.5381, grad_fn=<NegBackward0>) tensor(10993.5117, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10993.453125
tensor(10993.5117, grad_fn=<NegBackward0>) tensor(10993.4531, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10993.4423828125
tensor(10993.4531, grad_fn=<NegBackward0>) tensor(10993.4424, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10993.43359375
tensor(10993.4424, grad_fn=<NegBackward0>) tensor(10993.4336, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10993.4287109375
tensor(10993.4336, grad_fn=<NegBackward0>) tensor(10993.4287, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10993.4267578125
tensor(10993.4287, grad_fn=<NegBackward0>) tensor(10993.4268, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10993.4208984375
tensor(10993.4268, grad_fn=<NegBackward0>) tensor(10993.4209, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10993.4189453125
tensor(10993.4209, grad_fn=<NegBackward0>) tensor(10993.4189, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10993.416015625
tensor(10993.4189, grad_fn=<NegBackward0>) tensor(10993.4160, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10993.4140625
tensor(10993.4160, grad_fn=<NegBackward0>) tensor(10993.4141, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10993.4130859375
tensor(10993.4141, grad_fn=<NegBackward0>) tensor(10993.4131, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10993.412109375
tensor(10993.4131, grad_fn=<NegBackward0>) tensor(10993.4121, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10993.41015625
tensor(10993.4121, grad_fn=<NegBackward0>) tensor(10993.4102, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10993.4150390625
tensor(10993.4102, grad_fn=<NegBackward0>) tensor(10993.4150, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -10993.4052734375
tensor(10993.4102, grad_fn=<NegBackward0>) tensor(10993.4053, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10993.40234375
tensor(10993.4053, grad_fn=<NegBackward0>) tensor(10993.4023, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10993.4013671875
tensor(10993.4023, grad_fn=<NegBackward0>) tensor(10993.4014, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10993.4013671875
tensor(10993.4014, grad_fn=<NegBackward0>) tensor(10993.4014, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10993.400390625
tensor(10993.4014, grad_fn=<NegBackward0>) tensor(10993.4004, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10993.3994140625
tensor(10993.4004, grad_fn=<NegBackward0>) tensor(10993.3994, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10993.3984375
tensor(10993.3994, grad_fn=<NegBackward0>) tensor(10993.3984, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10993.3984375
tensor(10993.3984, grad_fn=<NegBackward0>) tensor(10993.3984, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10993.3994140625
tensor(10993.3984, grad_fn=<NegBackward0>) tensor(10993.3994, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10993.3984375
tensor(10993.3984, grad_fn=<NegBackward0>) tensor(10993.3984, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10993.3974609375
tensor(10993.3984, grad_fn=<NegBackward0>) tensor(10993.3975, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10993.3984375
tensor(10993.3975, grad_fn=<NegBackward0>) tensor(10993.3984, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -10993.3974609375
tensor(10993.3975, grad_fn=<NegBackward0>) tensor(10993.3975, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10993.3974609375
tensor(10993.3975, grad_fn=<NegBackward0>) tensor(10993.3975, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10993.3974609375
tensor(10993.3975, grad_fn=<NegBackward0>) tensor(10993.3975, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10993.396484375
tensor(10993.3975, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10993.396484375
tensor(10993.3965, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10993.396484375
tensor(10993.3965, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10993.396484375
tensor(10993.3965, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10993.3955078125
tensor(10993.3965, grad_fn=<NegBackward0>) tensor(10993.3955, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10993.400390625
tensor(10993.3955, grad_fn=<NegBackward0>) tensor(10993.4004, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10993.400390625
tensor(10993.3955, grad_fn=<NegBackward0>) tensor(10993.4004, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10993.3955078125
tensor(10993.3955, grad_fn=<NegBackward0>) tensor(10993.3955, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10993.396484375
tensor(10993.3955, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10993.39453125
tensor(10993.3955, grad_fn=<NegBackward0>) tensor(10993.3945, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10993.4140625
tensor(10993.3945, grad_fn=<NegBackward0>) tensor(10993.4141, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10993.396484375
tensor(10993.3945, grad_fn=<NegBackward0>) tensor(10993.3965, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -10993.4091796875
tensor(10993.3945, grad_fn=<NegBackward0>) tensor(10993.4092, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -10993.3955078125
tensor(10993.3945, grad_fn=<NegBackward0>) tensor(10993.3955, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -10993.3955078125
tensor(10993.3945, grad_fn=<NegBackward0>) tensor(10993.3955, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5000 due to no improvement.
pi: tensor([[0.7457, 0.2543],
        [0.1817, 0.8183]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5317, 0.4683], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3072, 0.0844],
         [0.6884, 0.2045]],

        [[0.5846, 0.1000],
         [0.5501, 0.7133]],

        [[0.5880, 0.1066],
         [0.5676, 0.5507]],

        [[0.6670, 0.0816],
         [0.6166, 0.6854]],

        [[0.5667, 0.1036],
         [0.5042, 0.5995]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8447122004349823
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
Global Adjusted Rand Index: 0.921440307984107
Average Adjusted Rand Index: 0.9225576993621456
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21658.404296875
inf tensor(21658.4043, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11344.251953125
tensor(21658.4043, grad_fn=<NegBackward0>) tensor(11344.2520, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11343.69140625
tensor(11344.2520, grad_fn=<NegBackward0>) tensor(11343.6914, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11342.3583984375
tensor(11343.6914, grad_fn=<NegBackward0>) tensor(11342.3584, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11335.2431640625
tensor(11342.3584, grad_fn=<NegBackward0>) tensor(11335.2432, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11330.849609375
tensor(11335.2432, grad_fn=<NegBackward0>) tensor(11330.8496, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11316.6640625
tensor(11330.8496, grad_fn=<NegBackward0>) tensor(11316.6641, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11314.36328125
tensor(11316.6641, grad_fn=<NegBackward0>) tensor(11314.3633, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11313.548828125
tensor(11314.3633, grad_fn=<NegBackward0>) tensor(11313.5488, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11312.951171875
tensor(11313.5488, grad_fn=<NegBackward0>) tensor(11312.9512, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11312.3515625
tensor(11312.9512, grad_fn=<NegBackward0>) tensor(11312.3516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11310.865234375
tensor(11312.3516, grad_fn=<NegBackward0>) tensor(11310.8652, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11155.19140625
tensor(11310.8652, grad_fn=<NegBackward0>) tensor(11155.1914, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11010.892578125
tensor(11155.1914, grad_fn=<NegBackward0>) tensor(11010.8926, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11010.4248046875
tensor(11010.8926, grad_fn=<NegBackward0>) tensor(11010.4248, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11003.8125
tensor(11010.4248, grad_fn=<NegBackward0>) tensor(11003.8125, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11003.76171875
tensor(11003.8125, grad_fn=<NegBackward0>) tensor(11003.7617, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11003.6923828125
tensor(11003.7617, grad_fn=<NegBackward0>) tensor(11003.6924, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11003.6748046875
tensor(11003.6924, grad_fn=<NegBackward0>) tensor(11003.6748, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11003.6640625
tensor(11003.6748, grad_fn=<NegBackward0>) tensor(11003.6641, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11003.6552734375
tensor(11003.6641, grad_fn=<NegBackward0>) tensor(11003.6553, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11003.6484375
tensor(11003.6553, grad_fn=<NegBackward0>) tensor(11003.6484, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11003.6435546875
tensor(11003.6484, grad_fn=<NegBackward0>) tensor(11003.6436, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11003.638671875
tensor(11003.6436, grad_fn=<NegBackward0>) tensor(11003.6387, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11003.63671875
tensor(11003.6387, grad_fn=<NegBackward0>) tensor(11003.6367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11003.630859375
tensor(11003.6367, grad_fn=<NegBackward0>) tensor(11003.6309, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11003.62890625
tensor(11003.6309, grad_fn=<NegBackward0>) tensor(11003.6289, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11003.625
tensor(11003.6289, grad_fn=<NegBackward0>) tensor(11003.6250, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11003.6220703125
tensor(11003.6250, grad_fn=<NegBackward0>) tensor(11003.6221, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10998.3359375
tensor(11003.6221, grad_fn=<NegBackward0>) tensor(10998.3359, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10998.3115234375
tensor(10998.3359, grad_fn=<NegBackward0>) tensor(10998.3115, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10998.3076171875
tensor(10998.3115, grad_fn=<NegBackward0>) tensor(10998.3076, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10998.2646484375
tensor(10998.3076, grad_fn=<NegBackward0>) tensor(10998.2646, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10998.259765625
tensor(10998.2646, grad_fn=<NegBackward0>) tensor(10998.2598, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10998.2578125
tensor(10998.2598, grad_fn=<NegBackward0>) tensor(10998.2578, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10998.255859375
tensor(10998.2578, grad_fn=<NegBackward0>) tensor(10998.2559, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10998.2548828125
tensor(10998.2559, grad_fn=<NegBackward0>) tensor(10998.2549, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10998.2529296875
tensor(10998.2549, grad_fn=<NegBackward0>) tensor(10998.2529, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10998.25
tensor(10998.2529, grad_fn=<NegBackward0>) tensor(10998.2500, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10998.2490234375
tensor(10998.2500, grad_fn=<NegBackward0>) tensor(10998.2490, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10998.2490234375
tensor(10998.2490, grad_fn=<NegBackward0>) tensor(10998.2490, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10998.2470703125
tensor(10998.2490, grad_fn=<NegBackward0>) tensor(10998.2471, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10998.24609375
tensor(10998.2471, grad_fn=<NegBackward0>) tensor(10998.2461, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10998.236328125
tensor(10998.2461, grad_fn=<NegBackward0>) tensor(10998.2363, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10998.205078125
tensor(10998.2363, grad_fn=<NegBackward0>) tensor(10998.2051, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10998.203125
tensor(10998.2051, grad_fn=<NegBackward0>) tensor(10998.2031, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10998.1748046875
tensor(10998.2031, grad_fn=<NegBackward0>) tensor(10998.1748, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10998.16796875
tensor(10998.1748, grad_fn=<NegBackward0>) tensor(10998.1680, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10998.1669921875
tensor(10998.1680, grad_fn=<NegBackward0>) tensor(10998.1670, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10998.166015625
tensor(10998.1670, grad_fn=<NegBackward0>) tensor(10998.1660, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10998.1669921875
tensor(10998.1660, grad_fn=<NegBackward0>) tensor(10998.1670, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10998.1650390625
tensor(10998.1660, grad_fn=<NegBackward0>) tensor(10998.1650, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10998.1435546875
tensor(10998.1650, grad_fn=<NegBackward0>) tensor(10998.1436, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10998.1396484375
tensor(10998.1436, grad_fn=<NegBackward0>) tensor(10998.1396, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10998.138671875
tensor(10998.1396, grad_fn=<NegBackward0>) tensor(10998.1387, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10998.138671875
tensor(10998.1387, grad_fn=<NegBackward0>) tensor(10998.1387, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10998.1376953125
tensor(10998.1387, grad_fn=<NegBackward0>) tensor(10998.1377, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10998.138671875
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1387, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10998.138671875
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1387, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10998.138671875
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1387, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -10998.1376953125
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1377, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10998.140625
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1406, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10998.1357421875
tensor(10998.1377, grad_fn=<NegBackward0>) tensor(10998.1357, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10998.130859375
tensor(10998.1357, grad_fn=<NegBackward0>) tensor(10998.1309, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10998.130859375
tensor(10998.1309, grad_fn=<NegBackward0>) tensor(10998.1309, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10998.1396484375
tensor(10998.1309, grad_fn=<NegBackward0>) tensor(10998.1396, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10998.1298828125
tensor(10998.1309, grad_fn=<NegBackward0>) tensor(10998.1299, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10998.1298828125
tensor(10998.1299, grad_fn=<NegBackward0>) tensor(10998.1299, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10998.125
tensor(10998.1299, grad_fn=<NegBackward0>) tensor(10998.1250, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10998.1220703125
tensor(10998.1250, grad_fn=<NegBackward0>) tensor(10998.1221, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10998.12890625
tensor(10998.1221, grad_fn=<NegBackward0>) tensor(10998.1289, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10998.12109375
tensor(10998.1221, grad_fn=<NegBackward0>) tensor(10998.1211, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10998.1357421875
tensor(10998.1211, grad_fn=<NegBackward0>) tensor(10998.1357, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10998.1201171875
tensor(10998.1211, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10998.12109375
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1211, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10998.1201171875
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.1201, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10998.083984375
tensor(10998.1201, grad_fn=<NegBackward0>) tensor(10998.0840, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10998.076171875
tensor(10998.0840, grad_fn=<NegBackward0>) tensor(10998.0762, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10998.078125
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0781, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10998.0849609375
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0850, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10998.078125
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0781, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10998.078125
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0781, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -10998.076171875
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0762, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10998.076171875
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0762, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10998.1171875
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.1172, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10998.0751953125
tensor(10998.0762, grad_fn=<NegBackward0>) tensor(10998.0752, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10998.0751953125
tensor(10998.0752, grad_fn=<NegBackward0>) tensor(10998.0752, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10998.0732421875
tensor(10998.0752, grad_fn=<NegBackward0>) tensor(10998.0732, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10998.078125
tensor(10998.0732, grad_fn=<NegBackward0>) tensor(10998.0781, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10998.0732421875
tensor(10998.0732, grad_fn=<NegBackward0>) tensor(10998.0732, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10998.072265625
tensor(10998.0732, grad_fn=<NegBackward0>) tensor(10998.0723, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10998.1669921875
tensor(10998.0723, grad_fn=<NegBackward0>) tensor(10998.1670, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10998.072265625
tensor(10998.0723, grad_fn=<NegBackward0>) tensor(10998.0723, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10998.072265625
tensor(10998.0723, grad_fn=<NegBackward0>) tensor(10998.0723, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10998.07421875
tensor(10998.0723, grad_fn=<NegBackward0>) tensor(10998.0742, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7412, 0.2588],
        [0.1872, 0.8128]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5318, 0.4682], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.0844],
         [0.6315, 0.2046]],

        [[0.5848, 0.1000],
         [0.6589, 0.5744]],

        [[0.7148, 0.1074],
         [0.6575, 0.6500]],

        [[0.5867, 0.0816],
         [0.5033, 0.6048]],

        [[0.7040, 0.1036],
         [0.5640, 0.7176]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8079634347678434
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
Global Adjusted Rand Index: 0.9137610644401934
Average Adjusted Rand Index: 0.9152079462287178
[0.921440307984107, 0.9137610644401934] [0.9225576993621456, 0.9152079462287178] [10993.3955078125, 10998.0732421875]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11131.86956179604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23857.423828125
inf tensor(23857.4238, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11306.671875
tensor(23857.4238, grad_fn=<NegBackward0>) tensor(11306.6719, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11306.0537109375
tensor(11306.6719, grad_fn=<NegBackward0>) tensor(11306.0537, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11305.6005859375
tensor(11306.0537, grad_fn=<NegBackward0>) tensor(11305.6006, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11305.2587890625
tensor(11305.6006, grad_fn=<NegBackward0>) tensor(11305.2588, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11305.033203125
tensor(11305.2588, grad_fn=<NegBackward0>) tensor(11305.0332, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11304.9189453125
tensor(11305.0332, grad_fn=<NegBackward0>) tensor(11304.9189, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11304.869140625
tensor(11304.9189, grad_fn=<NegBackward0>) tensor(11304.8691, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11304.837890625
tensor(11304.8691, grad_fn=<NegBackward0>) tensor(11304.8379, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11304.81640625
tensor(11304.8379, grad_fn=<NegBackward0>) tensor(11304.8164, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11304.7978515625
tensor(11304.8164, grad_fn=<NegBackward0>) tensor(11304.7979, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11304.7861328125
tensor(11304.7979, grad_fn=<NegBackward0>) tensor(11304.7861, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11304.7744140625
tensor(11304.7861, grad_fn=<NegBackward0>) tensor(11304.7744, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11304.765625
tensor(11304.7744, grad_fn=<NegBackward0>) tensor(11304.7656, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11304.7578125
tensor(11304.7656, grad_fn=<NegBackward0>) tensor(11304.7578, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11304.7509765625
tensor(11304.7578, grad_fn=<NegBackward0>) tensor(11304.7510, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11304.744140625
tensor(11304.7510, grad_fn=<NegBackward0>) tensor(11304.7441, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11304.73828125
tensor(11304.7441, grad_fn=<NegBackward0>) tensor(11304.7383, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11304.73046875
tensor(11304.7383, grad_fn=<NegBackward0>) tensor(11304.7305, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11304.7216796875
tensor(11304.7305, grad_fn=<NegBackward0>) tensor(11304.7217, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11304.7080078125
tensor(11304.7217, grad_fn=<NegBackward0>) tensor(11304.7080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11304.689453125
tensor(11304.7080, grad_fn=<NegBackward0>) tensor(11304.6895, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11304.6669921875
tensor(11304.6895, grad_fn=<NegBackward0>) tensor(11304.6670, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11304.6435546875
tensor(11304.6670, grad_fn=<NegBackward0>) tensor(11304.6436, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11304.607421875
tensor(11304.6436, grad_fn=<NegBackward0>) tensor(11304.6074, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11304.552734375
tensor(11304.6074, grad_fn=<NegBackward0>) tensor(11304.5527, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11304.494140625
tensor(11304.5527, grad_fn=<NegBackward0>) tensor(11304.4941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11304.4306640625
tensor(11304.4941, grad_fn=<NegBackward0>) tensor(11304.4307, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11304.3544921875
tensor(11304.4307, grad_fn=<NegBackward0>) tensor(11304.3545, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11304.2900390625
tensor(11304.3545, grad_fn=<NegBackward0>) tensor(11304.2900, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11304.255859375
tensor(11304.2900, grad_fn=<NegBackward0>) tensor(11304.2559, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11304.2412109375
tensor(11304.2559, grad_fn=<NegBackward0>) tensor(11304.2412, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11304.2333984375
tensor(11304.2412, grad_fn=<NegBackward0>) tensor(11304.2334, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11304.2294921875
tensor(11304.2334, grad_fn=<NegBackward0>) tensor(11304.2295, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11304.2265625
tensor(11304.2295, grad_fn=<NegBackward0>) tensor(11304.2266, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11304.224609375
tensor(11304.2266, grad_fn=<NegBackward0>) tensor(11304.2246, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11304.2236328125
tensor(11304.2246, grad_fn=<NegBackward0>) tensor(11304.2236, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11304.22265625
tensor(11304.2236, grad_fn=<NegBackward0>) tensor(11304.2227, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11304.22265625
tensor(11304.2227, grad_fn=<NegBackward0>) tensor(11304.2227, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11304.2294921875
tensor(11304.2227, grad_fn=<NegBackward0>) tensor(11304.2295, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11304.2236328125
tensor(11304.2227, grad_fn=<NegBackward0>) tensor(11304.2236, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -11304.22265625
tensor(11304.2227, grad_fn=<NegBackward0>) tensor(11304.2227, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11304.220703125
tensor(11304.2227, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11304.220703125
tensor(11304.2207, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11304.2197265625
tensor(11304.2207, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11304.2373046875
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2373, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11304.248046875
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2480, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11304.2216796875
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2217, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11304.2197265625
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2197, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11304.2236328125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2236, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11304.224609375
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2246, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11304.220703125
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2207, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11304.2255859375
tensor(11304.2197, grad_fn=<NegBackward0>) tensor(11304.2256, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.2917, 0.7083],
        [0.0266, 0.9734]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9724, 0.0276], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1656, 0.1828],
         [0.6010, 0.1743]],

        [[0.5891, 0.1585],
         [0.6384, 0.6054]],

        [[0.6212, 0.1675],
         [0.6257, 0.6709]],

        [[0.5322, 0.2151],
         [0.6098, 0.5174]],

        [[0.6881, 0.0966],
         [0.5267, 0.6945]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: 0.00451496515570562
Average Adjusted Rand Index: -0.0004540709913559345
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23725.126953125
inf tensor(23725.1270, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11308.3486328125
tensor(23725.1270, grad_fn=<NegBackward0>) tensor(11308.3486, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11307.34375
tensor(11308.3486, grad_fn=<NegBackward0>) tensor(11307.3438, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11306.884765625
tensor(11307.3438, grad_fn=<NegBackward0>) tensor(11306.8848, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11306.5654296875
tensor(11306.8848, grad_fn=<NegBackward0>) tensor(11306.5654, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11306.2763671875
tensor(11306.5654, grad_fn=<NegBackward0>) tensor(11306.2764, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11306.033203125
tensor(11306.2764, grad_fn=<NegBackward0>) tensor(11306.0332, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11305.84375
tensor(11306.0332, grad_fn=<NegBackward0>) tensor(11305.8438, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11305.697265625
tensor(11305.8438, grad_fn=<NegBackward0>) tensor(11305.6973, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11305.564453125
tensor(11305.6973, grad_fn=<NegBackward0>) tensor(11305.5645, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11305.4296875
tensor(11305.5645, grad_fn=<NegBackward0>) tensor(11305.4297, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11305.2841796875
tensor(11305.4297, grad_fn=<NegBackward0>) tensor(11305.2842, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11305.103515625
tensor(11305.2842, grad_fn=<NegBackward0>) tensor(11305.1035, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11304.7890625
tensor(11305.1035, grad_fn=<NegBackward0>) tensor(11304.7891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11301.0048828125
tensor(11304.7891, grad_fn=<NegBackward0>) tensor(11301.0049, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11204.3154296875
tensor(11301.0049, grad_fn=<NegBackward0>) tensor(11204.3154, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11161.890625
tensor(11204.3154, grad_fn=<NegBackward0>) tensor(11161.8906, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11151.0234375
tensor(11161.8906, grad_fn=<NegBackward0>) tensor(11151.0234, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11150.61328125
tensor(11151.0234, grad_fn=<NegBackward0>) tensor(11150.6133, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11150.5380859375
tensor(11150.6133, grad_fn=<NegBackward0>) tensor(11150.5381, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11150.494140625
tensor(11150.5381, grad_fn=<NegBackward0>) tensor(11150.4941, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11150.4638671875
tensor(11150.4941, grad_fn=<NegBackward0>) tensor(11150.4639, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11150.4404296875
tensor(11150.4639, grad_fn=<NegBackward0>) tensor(11150.4404, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11150.4248046875
tensor(11150.4404, grad_fn=<NegBackward0>) tensor(11150.4248, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11150.412109375
tensor(11150.4248, grad_fn=<NegBackward0>) tensor(11150.4121, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11150.40234375
tensor(11150.4121, grad_fn=<NegBackward0>) tensor(11150.4023, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11150.3916015625
tensor(11150.4023, grad_fn=<NegBackward0>) tensor(11150.3916, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11148.7587890625
tensor(11150.3916, grad_fn=<NegBackward0>) tensor(11148.7588, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11148.638671875
tensor(11148.7588, grad_fn=<NegBackward0>) tensor(11148.6387, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11148.62890625
tensor(11148.6387, grad_fn=<NegBackward0>) tensor(11148.6289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11148.6181640625
tensor(11148.6289, grad_fn=<NegBackward0>) tensor(11148.6182, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11148.6044921875
tensor(11148.6182, grad_fn=<NegBackward0>) tensor(11148.6045, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11148.578125
tensor(11148.6045, grad_fn=<NegBackward0>) tensor(11148.5781, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11148.4775390625
tensor(11148.5781, grad_fn=<NegBackward0>) tensor(11148.4775, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11145.7919921875
tensor(11148.4775, grad_fn=<NegBackward0>) tensor(11145.7920, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11145.04296875
tensor(11145.7920, grad_fn=<NegBackward0>) tensor(11145.0430, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11142.337890625
tensor(11145.0430, grad_fn=<NegBackward0>) tensor(11142.3379, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11141.083984375
tensor(11142.3379, grad_fn=<NegBackward0>) tensor(11141.0840, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11137.3486328125
tensor(11141.0840, grad_fn=<NegBackward0>) tensor(11137.3486, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11137.083984375
tensor(11137.3486, grad_fn=<NegBackward0>) tensor(11137.0840, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11136.908203125
tensor(11137.0840, grad_fn=<NegBackward0>) tensor(11136.9082, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11136.900390625
tensor(11136.9082, grad_fn=<NegBackward0>) tensor(11136.9004, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11136.8935546875
tensor(11136.9004, grad_fn=<NegBackward0>) tensor(11136.8936, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11136.8916015625
tensor(11136.8936, grad_fn=<NegBackward0>) tensor(11136.8916, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11136.8876953125
tensor(11136.8916, grad_fn=<NegBackward0>) tensor(11136.8877, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11136.8515625
tensor(11136.8877, grad_fn=<NegBackward0>) tensor(11136.8516, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11136.8427734375
tensor(11136.8516, grad_fn=<NegBackward0>) tensor(11136.8428, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11136.841796875
tensor(11136.8428, grad_fn=<NegBackward0>) tensor(11136.8418, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11136.8408203125
tensor(11136.8418, grad_fn=<NegBackward0>) tensor(11136.8408, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11136.83984375
tensor(11136.8408, grad_fn=<NegBackward0>) tensor(11136.8398, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11136.83984375
tensor(11136.8398, grad_fn=<NegBackward0>) tensor(11136.8398, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11136.8408203125
tensor(11136.8398, grad_fn=<NegBackward0>) tensor(11136.8408, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11136.8388671875
tensor(11136.8398, grad_fn=<NegBackward0>) tensor(11136.8389, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11136.8447265625
tensor(11136.8389, grad_fn=<NegBackward0>) tensor(11136.8447, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11136.8388671875
tensor(11136.8389, grad_fn=<NegBackward0>) tensor(11136.8389, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11136.8388671875
tensor(11136.8389, grad_fn=<NegBackward0>) tensor(11136.8389, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11136.8388671875
tensor(11136.8389, grad_fn=<NegBackward0>) tensor(11136.8389, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11136.8369140625
tensor(11136.8389, grad_fn=<NegBackward0>) tensor(11136.8369, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11136.8359375
tensor(11136.8369, grad_fn=<NegBackward0>) tensor(11136.8359, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11136.837890625
tensor(11136.8359, grad_fn=<NegBackward0>) tensor(11136.8379, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11136.837890625
tensor(11136.8359, grad_fn=<NegBackward0>) tensor(11136.8379, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11136.8359375
tensor(11136.8359, grad_fn=<NegBackward0>) tensor(11136.8359, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11136.8359375
tensor(11136.8359, grad_fn=<NegBackward0>) tensor(11136.8359, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11136.82421875
tensor(11136.8359, grad_fn=<NegBackward0>) tensor(11136.8242, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11136.826171875
tensor(11136.8242, grad_fn=<NegBackward0>) tensor(11136.8262, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11136.822265625
tensor(11136.8242, grad_fn=<NegBackward0>) tensor(11136.8223, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11136.822265625
tensor(11136.8223, grad_fn=<NegBackward0>) tensor(11136.8223, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11136.8212890625
tensor(11136.8223, grad_fn=<NegBackward0>) tensor(11136.8213, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11136.8271484375
tensor(11136.8213, grad_fn=<NegBackward0>) tensor(11136.8271, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11136.8232421875
tensor(11136.8213, grad_fn=<NegBackward0>) tensor(11136.8232, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11136.8203125
tensor(11136.8213, grad_fn=<NegBackward0>) tensor(11136.8203, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11136.822265625
tensor(11136.8203, grad_fn=<NegBackward0>) tensor(11136.8223, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11136.8203125
tensor(11136.8203, grad_fn=<NegBackward0>) tensor(11136.8203, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11136.8193359375
tensor(11136.8203, grad_fn=<NegBackward0>) tensor(11136.8193, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11136.8203125
tensor(11136.8193, grad_fn=<NegBackward0>) tensor(11136.8203, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11136.8193359375
tensor(11136.8193, grad_fn=<NegBackward0>) tensor(11136.8193, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11136.8251953125
tensor(11136.8193, grad_fn=<NegBackward0>) tensor(11136.8252, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11136.826171875
tensor(11136.8193, grad_fn=<NegBackward0>) tensor(11136.8262, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11136.8173828125
tensor(11136.8193, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11136.8173828125
tensor(11136.8174, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11136.8173828125
tensor(11136.8174, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11136.81640625
tensor(11136.8174, grad_fn=<NegBackward0>) tensor(11136.8164, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11136.8173828125
tensor(11136.8164, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11136.83203125
tensor(11136.8164, grad_fn=<NegBackward0>) tensor(11136.8320, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11136.8271484375
tensor(11136.8164, grad_fn=<NegBackward0>) tensor(11136.8271, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11136.8173828125
tensor(11136.8164, grad_fn=<NegBackward0>) tensor(11136.8174, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11136.8212890625
tensor(11136.8164, grad_fn=<NegBackward0>) tensor(11136.8213, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7010, 0.2990],
        [0.3261, 0.6739]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8006, 0.1994], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2049, 0.1056],
         [0.5977, 0.2900]],

        [[0.6919, 0.0919],
         [0.6704, 0.5901]],

        [[0.6403, 0.0994],
         [0.7307, 0.6080]],

        [[0.6150, 0.1012],
         [0.6193, 0.5049]],

        [[0.5190, 0.1034],
         [0.6312, 0.6320]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.08439181663981127
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7366601698966646
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8441155847510795
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4609993639648473
Average Adjusted Rand Index: 0.7020002931355249
[0.00451496515570562, 0.4609993639648473] [-0.0004540709913559345, 0.7020002931355249] [11304.2255859375, 11136.8212890625]
