nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  1%|          | 1/100 [12:40<20:54:01, 760.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  2%|▏         | 2/100 [29:50<25:00:57, 918.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  3%|▎         | 3/100 [43:58<23:53:16, 886.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  4%|▍         | 4/100 [1:02:26<25:58:36, 974.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  5%|▌         | 5/100 [1:10:49<21:13:02, 804.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  6%|▌         | 6/100 [1:29:00<23:32:31, 901.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  7%|▋         | 7/100 [1:47:23<24:59:57, 967.71s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  8%|▊         | 8/100 [2:05:41<25:47:07, 1009.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  9%|▉         | 9/100 [2:22:23<25:27:18, 1007.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 10%|█         | 10/100 [2:37:09<24:14:25, 969.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 11%|█         | 11/100 [2:49:54<22:25:22, 907.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 12%|█▏        | 12/100 [3:00:37<20:12:10, 826.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 13%|█▎        | 13/100 [3:10:08<18:06:31, 749.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 14%|█▍        | 14/100 [3:22:52<18:00:01, 753.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 15%|█▌        | 15/100 [3:32:58<16:44:49, 709.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 16%|█▌        | 16/100 [3:49:00<18:19:22, 785.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 17%|█▋        | 17/100 [3:57:53<16:21:21, 709.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 18%|█▊        | 18/100 [4:12:34<17:19:50, 760.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 19%|█▉        | 19/100 [4:20:28<15:10:57, 674.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 20%|██        | 20/100 [4:33:27<15:41:17, 705.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 21%|██        | 21/100 [4:44:15<15:06:52, 688.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 22%|██▏       | 22/100 [4:53:27<14:01:55, 647.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 23%|██▎       | 23/100 [5:06:55<14:52:44, 695.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 24%|██▍       | 24/100 [5:24:39<17:01:05, 806.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 25%|██▌       | 25/100 [5:40:00<17:30:52, 840.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 26%|██▌       | 26/100 [5:57:33<18:35:26, 904.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 27%|██▋       | 27/100 [6:12:51<18:25:23, 908.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 28%|██▊       | 28/100 [6:24:18<16:50:21, 841.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 29%|██▉       | 29/100 [6:35:10<15:29:06, 785.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 30%|███       | 30/100 [6:51:38<16:27:03, 846.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 31%|███       | 31/100 [7:02:29<15:05:27, 787.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 32%|███▏      | 32/100 [7:16:19<15:06:49, 800.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 33%|███▎      | 33/100 [7:28:24<14:28:20, 777.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 34%|███▍      | 34/100 [7:38:50<13:25:20, 732.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 35%|███▌      | 35/100 [7:51:13<13:16:35, 735.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 36%|███▌      | 36/100 [8:00:20<12:04:15, 678.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 37%|███▋      | 37/100 [8:14:43<12:50:55, 734.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 38%|███▊      | 38/100 [8:33:35<14:42:02, 853.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 39%|███▉      | 39/100 [8:41:28<12:31:33, 739.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -10089.242875480853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22220.845703125
inf tensor(22220.8457, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9948.8857421875
tensor(22220.8457, grad_fn=<NegBackward0>) tensor(9948.8857, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9947.9404296875
tensor(9948.8857, grad_fn=<NegBackward0>) tensor(9947.9404, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9947.1591796875
tensor(9947.9404, grad_fn=<NegBackward0>) tensor(9947.1592, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9946.095703125
tensor(9947.1592, grad_fn=<NegBackward0>) tensor(9946.0957, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9946.0537109375
tensor(9946.0957, grad_fn=<NegBackward0>) tensor(9946.0537, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9946.0439453125
tensor(9946.0537, grad_fn=<NegBackward0>) tensor(9946.0439, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9946.03515625
tensor(9946.0439, grad_fn=<NegBackward0>) tensor(9946.0352, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9946.0234375
tensor(9946.0352, grad_fn=<NegBackward0>) tensor(9946.0234, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9946.001953125
tensor(9946.0234, grad_fn=<NegBackward0>) tensor(9946.0020, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9945.970703125
tensor(9946.0020, grad_fn=<NegBackward0>) tensor(9945.9707, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9945.9267578125
tensor(9945.9707, grad_fn=<NegBackward0>) tensor(9945.9268, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9945.8662109375
tensor(9945.9268, grad_fn=<NegBackward0>) tensor(9945.8662, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9945.755859375
tensor(9945.8662, grad_fn=<NegBackward0>) tensor(9945.7559, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9945.36328125
tensor(9945.7559, grad_fn=<NegBackward0>) tensor(9945.3633, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9944.7822265625
tensor(9945.3633, grad_fn=<NegBackward0>) tensor(9944.7822, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9944.46875
tensor(9944.7822, grad_fn=<NegBackward0>) tensor(9944.4688, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9944.3349609375
tensor(9944.4688, grad_fn=<NegBackward0>) tensor(9944.3350, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9944.26953125
tensor(9944.3350, grad_fn=<NegBackward0>) tensor(9944.2695, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9944.2275390625
tensor(9944.2695, grad_fn=<NegBackward0>) tensor(9944.2275, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9944.193359375
tensor(9944.2275, grad_fn=<NegBackward0>) tensor(9944.1934, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9944.16015625
tensor(9944.1934, grad_fn=<NegBackward0>) tensor(9944.1602, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9944.1171875
tensor(9944.1602, grad_fn=<NegBackward0>) tensor(9944.1172, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9944.060546875
tensor(9944.1172, grad_fn=<NegBackward0>) tensor(9944.0605, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9943.9892578125
tensor(9944.0605, grad_fn=<NegBackward0>) tensor(9943.9893, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9943.9052734375
tensor(9943.9893, grad_fn=<NegBackward0>) tensor(9943.9053, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9943.798828125
tensor(9943.9053, grad_fn=<NegBackward0>) tensor(9943.7988, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9943.6640625
tensor(9943.7988, grad_fn=<NegBackward0>) tensor(9943.6641, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9943.4990234375
tensor(9943.6641, grad_fn=<NegBackward0>) tensor(9943.4990, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9943.3310546875
tensor(9943.4990, grad_fn=<NegBackward0>) tensor(9943.3311, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9943.1904296875
tensor(9943.3311, grad_fn=<NegBackward0>) tensor(9943.1904, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9943.0546875
tensor(9943.1904, grad_fn=<NegBackward0>) tensor(9943.0547, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9942.4970703125
tensor(9943.0547, grad_fn=<NegBackward0>) tensor(9942.4971, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9942.1923828125
tensor(9942.4971, grad_fn=<NegBackward0>) tensor(9942.1924, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9942.1015625
tensor(9942.1924, grad_fn=<NegBackward0>) tensor(9942.1016, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9942.068359375
tensor(9942.1016, grad_fn=<NegBackward0>) tensor(9942.0684, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9942.052734375
tensor(9942.0684, grad_fn=<NegBackward0>) tensor(9942.0527, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9942.0458984375
tensor(9942.0527, grad_fn=<NegBackward0>) tensor(9942.0459, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9942.0400390625
tensor(9942.0459, grad_fn=<NegBackward0>) tensor(9942.0400, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9942.037109375
tensor(9942.0400, grad_fn=<NegBackward0>) tensor(9942.0371, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9942.03515625
tensor(9942.0371, grad_fn=<NegBackward0>) tensor(9942.0352, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9942.033203125
tensor(9942.0352, grad_fn=<NegBackward0>) tensor(9942.0332, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9942.03125
tensor(9942.0332, grad_fn=<NegBackward0>) tensor(9942.0312, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9942.0302734375
tensor(9942.0312, grad_fn=<NegBackward0>) tensor(9942.0303, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9942.029296875
tensor(9942.0303, grad_fn=<NegBackward0>) tensor(9942.0293, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9942.0283203125
tensor(9942.0293, grad_fn=<NegBackward0>) tensor(9942.0283, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9942.0283203125
tensor(9942.0283, grad_fn=<NegBackward0>) tensor(9942.0283, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9942.0263671875
tensor(9942.0283, grad_fn=<NegBackward0>) tensor(9942.0264, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9942.025390625
tensor(9942.0264, grad_fn=<NegBackward0>) tensor(9942.0254, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9942.0263671875
tensor(9942.0254, grad_fn=<NegBackward0>) tensor(9942.0264, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9942.025390625
tensor(9942.0254, grad_fn=<NegBackward0>) tensor(9942.0254, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9942.0224609375
tensor(9942.0254, grad_fn=<NegBackward0>) tensor(9942.0225, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9942.0244140625
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0244, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9942.0234375
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0234, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9942.0234375
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0234, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9942.0234375
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0234, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -9942.0224609375
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0225, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9942.021484375
tensor(9942.0225, grad_fn=<NegBackward0>) tensor(9942.0215, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9942.0205078125
tensor(9942.0215, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9942.0234375
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0234, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9942.0205078125
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9942.021484375
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0215, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9942.0205078125
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9942.0205078125
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9942.01953125
tensor(9942.0205, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9942.01953125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9942.01953125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9942.0205078125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9942.01953125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9942.01953125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9942.01953125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9942.017578125
tensor(9942.0195, grad_fn=<NegBackward0>) tensor(9942.0176, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9942.01953125
tensor(9942.0176, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9942.0205078125
tensor(9942.0176, grad_fn=<NegBackward0>) tensor(9942.0205, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9942.01953125
tensor(9942.0176, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -9942.01953125
tensor(9942.0176, grad_fn=<NegBackward0>) tensor(9942.0195, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -9942.0185546875
tensor(9942.0176, grad_fn=<NegBackward0>) tensor(9942.0186, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[9.9996e-01, 4.4362e-05],
        [4.0197e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0815, 0.9185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0205, 0.1211],
         [0.6087, 0.1391]],

        [[0.7310, 0.0932],
         [0.6738, 0.5592]],

        [[0.7140, 0.1426],
         [0.6153, 0.7074]],

        [[0.6364, 0.1991],
         [0.7207, 0.5092]],

        [[0.6844, 0.1366],
         [0.6800, 0.7066]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.011643993543924371
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0028641099491338354
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0019137235069308285
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002508217918958077
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002508217918958077
Global Adjusted Rand Index: -0.000514859204773055
Average Adjusted Rand Index: -0.004287652567581038
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22217.970703125
inf tensor(22217.9707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9950.1591796875
tensor(22217.9707, grad_fn=<NegBackward0>) tensor(9950.1592, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9948.3408203125
tensor(9950.1592, grad_fn=<NegBackward0>) tensor(9948.3408, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9947.8583984375
tensor(9948.3408, grad_fn=<NegBackward0>) tensor(9947.8584, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9947.6513671875
tensor(9947.8584, grad_fn=<NegBackward0>) tensor(9947.6514, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9947.4970703125
tensor(9947.6514, grad_fn=<NegBackward0>) tensor(9947.4971, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9947.3388671875
tensor(9947.4971, grad_fn=<NegBackward0>) tensor(9947.3389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9947.171875
tensor(9947.3389, grad_fn=<NegBackward0>) tensor(9947.1719, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9947.0048828125
tensor(9947.1719, grad_fn=<NegBackward0>) tensor(9947.0049, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9946.8583984375
tensor(9947.0049, grad_fn=<NegBackward0>) tensor(9946.8584, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9946.73046875
tensor(9946.8584, grad_fn=<NegBackward0>) tensor(9946.7305, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9946.615234375
tensor(9946.7305, grad_fn=<NegBackward0>) tensor(9946.6152, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9946.5087890625
tensor(9946.6152, grad_fn=<NegBackward0>) tensor(9946.5088, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9946.4033203125
tensor(9946.5088, grad_fn=<NegBackward0>) tensor(9946.4033, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9946.2978515625
tensor(9946.4033, grad_fn=<NegBackward0>) tensor(9946.2979, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9946.1865234375
tensor(9946.2979, grad_fn=<NegBackward0>) tensor(9946.1865, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9946.0703125
tensor(9946.1865, grad_fn=<NegBackward0>) tensor(9946.0703, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9945.9541015625
tensor(9946.0703, grad_fn=<NegBackward0>) tensor(9945.9541, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9945.8544921875
tensor(9945.9541, grad_fn=<NegBackward0>) tensor(9945.8545, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9945.7626953125
tensor(9945.8545, grad_fn=<NegBackward0>) tensor(9945.7627, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9945.6494140625
tensor(9945.7627, grad_fn=<NegBackward0>) tensor(9945.6494, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9945.5732421875
tensor(9945.6494, grad_fn=<NegBackward0>) tensor(9945.5732, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9945.5380859375
tensor(9945.5732, grad_fn=<NegBackward0>) tensor(9945.5381, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9945.5146484375
tensor(9945.5381, grad_fn=<NegBackward0>) tensor(9945.5146, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9945.4990234375
tensor(9945.5146, grad_fn=<NegBackward0>) tensor(9945.4990, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9945.4892578125
tensor(9945.4990, grad_fn=<NegBackward0>) tensor(9945.4893, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9945.4814453125
tensor(9945.4893, grad_fn=<NegBackward0>) tensor(9945.4814, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9945.474609375
tensor(9945.4814, grad_fn=<NegBackward0>) tensor(9945.4746, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9945.466796875
tensor(9945.4746, grad_fn=<NegBackward0>) tensor(9945.4668, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9945.4609375
tensor(9945.4668, grad_fn=<NegBackward0>) tensor(9945.4609, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9945.4521484375
tensor(9945.4609, grad_fn=<NegBackward0>) tensor(9945.4521, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9945.4443359375
tensor(9945.4521, grad_fn=<NegBackward0>) tensor(9945.4443, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9945.4345703125
tensor(9945.4443, grad_fn=<NegBackward0>) tensor(9945.4346, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9945.4267578125
tensor(9945.4346, grad_fn=<NegBackward0>) tensor(9945.4268, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9945.4150390625
tensor(9945.4268, grad_fn=<NegBackward0>) tensor(9945.4150, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9945.4052734375
tensor(9945.4150, grad_fn=<NegBackward0>) tensor(9945.4053, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9945.3974609375
tensor(9945.4053, grad_fn=<NegBackward0>) tensor(9945.3975, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9945.390625
tensor(9945.3975, grad_fn=<NegBackward0>) tensor(9945.3906, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9945.3876953125
tensor(9945.3906, grad_fn=<NegBackward0>) tensor(9945.3877, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9945.3876953125
tensor(9945.3877, grad_fn=<NegBackward0>) tensor(9945.3877, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9945.38671875
tensor(9945.3877, grad_fn=<NegBackward0>) tensor(9945.3867, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9945.3857421875
tensor(9945.3867, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9945.384765625
tensor(9945.3857, grad_fn=<NegBackward0>) tensor(9945.3848, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9945.3857421875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9945.3857421875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -9945.384765625
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3848, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9945.38671875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3867, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9945.3857421875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9945.3857421875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9945.384765625
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3848, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9945.3857421875
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9945.3837890625
tensor(9945.3848, grad_fn=<NegBackward0>) tensor(9945.3838, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9945.3876953125
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3877, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9945.384765625
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3848, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9945.3896484375
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3896, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9945.3837890625
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3838, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9945.3916015625
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3916, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9945.3857421875
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3857, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9945.3876953125
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3877, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -9945.384765625
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.3848, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -9945.421875
tensor(9945.3838, grad_fn=<NegBackward0>) tensor(9945.4219, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.0115, 0.9885],
        [0.2167, 0.7833]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1030, 0.8970], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.1586],
         [0.5335, 0.1290]],

        [[0.6030, 0.1486],
         [0.7067, 0.6033]],

        [[0.6114, 0.1459],
         [0.5589, 0.6809]],

        [[0.6130, 0.1698],
         [0.6807, 0.5558]],

        [[0.5348, 0.1603],
         [0.6408, 0.5914]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: 0.0018323442030113322
Average Adjusted Rand Index: 0.00017986817552513484
[-0.000514859204773055, 0.0018323442030113322] [-0.004287652567581038, 0.00017986817552513484] [9942.0185546875, 9945.421875]
-------------------------------------
This iteration is 1
True Objective function: Loss = -9936.157122695342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21772.404296875
inf tensor(21772.4043, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9786.3076171875
tensor(21772.4043, grad_fn=<NegBackward0>) tensor(9786.3076, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9785.849609375
tensor(9786.3076, grad_fn=<NegBackward0>) tensor(9785.8496, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9785.7333984375
tensor(9785.8496, grad_fn=<NegBackward0>) tensor(9785.7334, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9785.67578125
tensor(9785.7334, grad_fn=<NegBackward0>) tensor(9785.6758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9785.64453125
tensor(9785.6758, grad_fn=<NegBackward0>) tensor(9785.6445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9785.625
tensor(9785.6445, grad_fn=<NegBackward0>) tensor(9785.6250, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9785.607421875
tensor(9785.6250, grad_fn=<NegBackward0>) tensor(9785.6074, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9785.591796875
tensor(9785.6074, grad_fn=<NegBackward0>) tensor(9785.5918, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9785.56640625
tensor(9785.5918, grad_fn=<NegBackward0>) tensor(9785.5664, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9785.52734375
tensor(9785.5664, grad_fn=<NegBackward0>) tensor(9785.5273, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9785.4609375
tensor(9785.5273, grad_fn=<NegBackward0>) tensor(9785.4609, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9785.375
tensor(9785.4609, grad_fn=<NegBackward0>) tensor(9785.3750, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9785.3125
tensor(9785.3750, grad_fn=<NegBackward0>) tensor(9785.3125, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9785.251953125
tensor(9785.3125, grad_fn=<NegBackward0>) tensor(9785.2520, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9785.1552734375
tensor(9785.2520, grad_fn=<NegBackward0>) tensor(9785.1553, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9785.01953125
tensor(9785.1553, grad_fn=<NegBackward0>) tensor(9785.0195, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9784.896484375
tensor(9785.0195, grad_fn=<NegBackward0>) tensor(9784.8965, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9784.7958984375
tensor(9784.8965, grad_fn=<NegBackward0>) tensor(9784.7959, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9784.6337890625
tensor(9784.7959, grad_fn=<NegBackward0>) tensor(9784.6338, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9784.1923828125
tensor(9784.6338, grad_fn=<NegBackward0>) tensor(9784.1924, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9783.98046875
tensor(9784.1924, grad_fn=<NegBackward0>) tensor(9783.9805, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9783.8984375
tensor(9783.9805, grad_fn=<NegBackward0>) tensor(9783.8984, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9783.8525390625
tensor(9783.8984, grad_fn=<NegBackward0>) tensor(9783.8525, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9783.8232421875
tensor(9783.8525, grad_fn=<NegBackward0>) tensor(9783.8232, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9783.8017578125
tensor(9783.8232, grad_fn=<NegBackward0>) tensor(9783.8018, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9783.7841796875
tensor(9783.8018, grad_fn=<NegBackward0>) tensor(9783.7842, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9783.7666015625
tensor(9783.7842, grad_fn=<NegBackward0>) tensor(9783.7666, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9783.7509765625
tensor(9783.7666, grad_fn=<NegBackward0>) tensor(9783.7510, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9783.7412109375
tensor(9783.7510, grad_fn=<NegBackward0>) tensor(9783.7412, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9783.73046875
tensor(9783.7412, grad_fn=<NegBackward0>) tensor(9783.7305, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9783.724609375
tensor(9783.7305, grad_fn=<NegBackward0>) tensor(9783.7246, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9783.7177734375
tensor(9783.7246, grad_fn=<NegBackward0>) tensor(9783.7178, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9783.7109375
tensor(9783.7178, grad_fn=<NegBackward0>) tensor(9783.7109, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9783.7060546875
tensor(9783.7109, grad_fn=<NegBackward0>) tensor(9783.7061, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9783.701171875
tensor(9783.7061, grad_fn=<NegBackward0>) tensor(9783.7012, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9783.69921875
tensor(9783.7012, grad_fn=<NegBackward0>) tensor(9783.6992, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9783.6962890625
tensor(9783.6992, grad_fn=<NegBackward0>) tensor(9783.6963, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9783.6943359375
tensor(9783.6963, grad_fn=<NegBackward0>) tensor(9783.6943, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9783.6923828125
tensor(9783.6943, grad_fn=<NegBackward0>) tensor(9783.6924, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9783.689453125
tensor(9783.6924, grad_fn=<NegBackward0>) tensor(9783.6895, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9783.6875
tensor(9783.6895, grad_fn=<NegBackward0>) tensor(9783.6875, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9783.685546875
tensor(9783.6875, grad_fn=<NegBackward0>) tensor(9783.6855, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9783.685546875
tensor(9783.6855, grad_fn=<NegBackward0>) tensor(9783.6855, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9783.68359375
tensor(9783.6855, grad_fn=<NegBackward0>) tensor(9783.6836, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9783.6826171875
tensor(9783.6836, grad_fn=<NegBackward0>) tensor(9783.6826, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9783.6806640625
tensor(9783.6826, grad_fn=<NegBackward0>) tensor(9783.6807, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9783.6806640625
tensor(9783.6807, grad_fn=<NegBackward0>) tensor(9783.6807, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9783.6787109375
tensor(9783.6807, grad_fn=<NegBackward0>) tensor(9783.6787, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9783.677734375
tensor(9783.6787, grad_fn=<NegBackward0>) tensor(9783.6777, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9783.677734375
tensor(9783.6777, grad_fn=<NegBackward0>) tensor(9783.6777, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9783.6767578125
tensor(9783.6777, grad_fn=<NegBackward0>) tensor(9783.6768, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9783.67578125
tensor(9783.6768, grad_fn=<NegBackward0>) tensor(9783.6758, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9783.6748046875
tensor(9783.6758, grad_fn=<NegBackward0>) tensor(9783.6748, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9783.673828125
tensor(9783.6748, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9783.6748046875
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6748, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9783.673828125
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9783.673828125
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9783.673828125
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9783.671875
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9783.6708984375
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9783.6708984375
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9783.671875
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9783.6728515625
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.6729, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -9783.669921875
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9783.6689453125
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6689, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9783.6689453125
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6689, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9783.6689453125
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6689, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9783.6708984375
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9783.6689453125
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6689, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9783.669921875
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9783.6708984375
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9783.669921875
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9783.669921875
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -9783.669921875
tensor(9783.6689, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[9.9937e-01, 6.3177e-04],
        [2.2341e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0419, 0.9581], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1684, 0.1444],
         [0.6986, 0.1338]],

        [[0.5449, 0.1780],
         [0.5684, 0.7291]],

        [[0.6709, 0.1800],
         [0.5904, 0.6668]],

        [[0.5285, 0.1312],
         [0.6940, 0.6496]],

        [[0.5365, 0.0773],
         [0.6367, 0.5021]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.003578912575773538
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.025916162480371957
Global Adjusted Rand Index: 0.0014159336787592516
Average Adjusted Rand Index: -0.005166666898884254
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23120.3046875
inf tensor(23120.3047, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9786.7001953125
tensor(23120.3047, grad_fn=<NegBackward0>) tensor(9786.7002, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9786.1220703125
tensor(9786.7002, grad_fn=<NegBackward0>) tensor(9786.1221, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9785.9873046875
tensor(9786.1221, grad_fn=<NegBackward0>) tensor(9785.9873, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9785.89453125
tensor(9785.9873, grad_fn=<NegBackward0>) tensor(9785.8945, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9785.794921875
tensor(9785.8945, grad_fn=<NegBackward0>) tensor(9785.7949, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9785.66015625
tensor(9785.7949, grad_fn=<NegBackward0>) tensor(9785.6602, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9785.548828125
tensor(9785.6602, grad_fn=<NegBackward0>) tensor(9785.5488, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9785.490234375
tensor(9785.5488, grad_fn=<NegBackward0>) tensor(9785.4902, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9785.44140625
tensor(9785.4902, grad_fn=<NegBackward0>) tensor(9785.4414, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9785.396484375
tensor(9785.4414, grad_fn=<NegBackward0>) tensor(9785.3965, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9785.3505859375
tensor(9785.3965, grad_fn=<NegBackward0>) tensor(9785.3506, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9785.306640625
tensor(9785.3506, grad_fn=<NegBackward0>) tensor(9785.3066, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9785.2626953125
tensor(9785.3066, grad_fn=<NegBackward0>) tensor(9785.2627, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9785.216796875
tensor(9785.2627, grad_fn=<NegBackward0>) tensor(9785.2168, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9785.1728515625
tensor(9785.2168, grad_fn=<NegBackward0>) tensor(9785.1729, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9785.1298828125
tensor(9785.1729, grad_fn=<NegBackward0>) tensor(9785.1299, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9785.0859375
tensor(9785.1299, grad_fn=<NegBackward0>) tensor(9785.0859, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9785.0458984375
tensor(9785.0859, grad_fn=<NegBackward0>) tensor(9785.0459, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9785.00390625
tensor(9785.0459, grad_fn=<NegBackward0>) tensor(9785.0039, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9784.9658203125
tensor(9785.0039, grad_fn=<NegBackward0>) tensor(9784.9658, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9784.9306640625
tensor(9784.9658, grad_fn=<NegBackward0>) tensor(9784.9307, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9784.8935546875
tensor(9784.9307, grad_fn=<NegBackward0>) tensor(9784.8936, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9784.8505859375
tensor(9784.8936, grad_fn=<NegBackward0>) tensor(9784.8506, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9784.7978515625
tensor(9784.8506, grad_fn=<NegBackward0>) tensor(9784.7979, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9784.6923828125
tensor(9784.7979, grad_fn=<NegBackward0>) tensor(9784.6924, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9784.4267578125
tensor(9784.6924, grad_fn=<NegBackward0>) tensor(9784.4268, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9784.171875
tensor(9784.4268, grad_fn=<NegBackward0>) tensor(9784.1719, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9784.0537109375
tensor(9784.1719, grad_fn=<NegBackward0>) tensor(9784.0537, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9783.9853515625
tensor(9784.0537, grad_fn=<NegBackward0>) tensor(9783.9854, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9783.9384765625
tensor(9783.9854, grad_fn=<NegBackward0>) tensor(9783.9385, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9783.9013671875
tensor(9783.9385, grad_fn=<NegBackward0>) tensor(9783.9014, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9783.8720703125
tensor(9783.9014, grad_fn=<NegBackward0>) tensor(9783.8721, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9783.8486328125
tensor(9783.8721, grad_fn=<NegBackward0>) tensor(9783.8486, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9783.8310546875
tensor(9783.8486, grad_fn=<NegBackward0>) tensor(9783.8311, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9783.81640625
tensor(9783.8311, grad_fn=<NegBackward0>) tensor(9783.8164, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9783.8056640625
tensor(9783.8164, grad_fn=<NegBackward0>) tensor(9783.8057, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9783.79296875
tensor(9783.8057, grad_fn=<NegBackward0>) tensor(9783.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9783.783203125
tensor(9783.7930, grad_fn=<NegBackward0>) tensor(9783.7832, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9783.771484375
tensor(9783.7832, grad_fn=<NegBackward0>) tensor(9783.7715, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9783.765625
tensor(9783.7715, grad_fn=<NegBackward0>) tensor(9783.7656, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9783.755859375
tensor(9783.7656, grad_fn=<NegBackward0>) tensor(9783.7559, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9783.75
tensor(9783.7559, grad_fn=<NegBackward0>) tensor(9783.7500, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9783.7412109375
tensor(9783.7500, grad_fn=<NegBackward0>) tensor(9783.7412, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9783.7353515625
tensor(9783.7412, grad_fn=<NegBackward0>) tensor(9783.7354, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9783.7294921875
tensor(9783.7354, grad_fn=<NegBackward0>) tensor(9783.7295, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9783.724609375
tensor(9783.7295, grad_fn=<NegBackward0>) tensor(9783.7246, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9783.7197265625
tensor(9783.7246, grad_fn=<NegBackward0>) tensor(9783.7197, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9783.7158203125
tensor(9783.7197, grad_fn=<NegBackward0>) tensor(9783.7158, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9783.712890625
tensor(9783.7158, grad_fn=<NegBackward0>) tensor(9783.7129, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9783.708984375
tensor(9783.7129, grad_fn=<NegBackward0>) tensor(9783.7090, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9783.705078125
tensor(9783.7090, grad_fn=<NegBackward0>) tensor(9783.7051, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9783.703125
tensor(9783.7051, grad_fn=<NegBackward0>) tensor(9783.7031, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9783.7001953125
tensor(9783.7031, grad_fn=<NegBackward0>) tensor(9783.7002, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9783.6962890625
tensor(9783.7002, grad_fn=<NegBackward0>) tensor(9783.6963, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9783.6962890625
tensor(9783.6963, grad_fn=<NegBackward0>) tensor(9783.6963, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9783.6943359375
tensor(9783.6963, grad_fn=<NegBackward0>) tensor(9783.6943, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9783.6943359375
tensor(9783.6943, grad_fn=<NegBackward0>) tensor(9783.6943, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9783.6904296875
tensor(9783.6943, grad_fn=<NegBackward0>) tensor(9783.6904, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9783.6904296875
tensor(9783.6904, grad_fn=<NegBackward0>) tensor(9783.6904, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9783.6884765625
tensor(9783.6904, grad_fn=<NegBackward0>) tensor(9783.6885, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9783.685546875
tensor(9783.6885, grad_fn=<NegBackward0>) tensor(9783.6855, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9783.685546875
tensor(9783.6855, grad_fn=<NegBackward0>) tensor(9783.6855, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9783.6845703125
tensor(9783.6855, grad_fn=<NegBackward0>) tensor(9783.6846, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9783.6845703125
tensor(9783.6846, grad_fn=<NegBackward0>) tensor(9783.6846, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9783.6826171875
tensor(9783.6846, grad_fn=<NegBackward0>) tensor(9783.6826, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9783.6806640625
tensor(9783.6826, grad_fn=<NegBackward0>) tensor(9783.6807, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9783.6796875
tensor(9783.6807, grad_fn=<NegBackward0>) tensor(9783.6797, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9783.6796875
tensor(9783.6797, grad_fn=<NegBackward0>) tensor(9783.6797, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9783.6796875
tensor(9783.6797, grad_fn=<NegBackward0>) tensor(9783.6797, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9783.6796875
tensor(9783.6797, grad_fn=<NegBackward0>) tensor(9783.6797, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9783.6787109375
tensor(9783.6797, grad_fn=<NegBackward0>) tensor(9783.6787, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9783.677734375
tensor(9783.6787, grad_fn=<NegBackward0>) tensor(9783.6777, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9783.6767578125
tensor(9783.6777, grad_fn=<NegBackward0>) tensor(9783.6768, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9783.6767578125
tensor(9783.6768, grad_fn=<NegBackward0>) tensor(9783.6768, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9783.67578125
tensor(9783.6768, grad_fn=<NegBackward0>) tensor(9783.6758, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9783.67578125
tensor(9783.6758, grad_fn=<NegBackward0>) tensor(9783.6758, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9783.6748046875
tensor(9783.6758, grad_fn=<NegBackward0>) tensor(9783.6748, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9783.67578125
tensor(9783.6748, grad_fn=<NegBackward0>) tensor(9783.6758, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9783.673828125
tensor(9783.6748, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9783.6748046875
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6748, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9783.673828125
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6738, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9783.6728515625
tensor(9783.6738, grad_fn=<NegBackward0>) tensor(9783.6729, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9783.671875
tensor(9783.6729, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9783.693359375
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6934, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9783.681640625
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6816, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9783.6728515625
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6729, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9783.671875
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9783.6728515625
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6729, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9783.6708984375
tensor(9783.6719, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9783.75390625
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.7539, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9783.669921875
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9783.671875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9783.6708984375
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9783.671875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6719, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9783.669921875
tensor(9783.6699, grad_fn=<NegBackward0>) tensor(9783.6699, grad_fn=<NegBackward0>)
pi: tensor([[9.9997e-01, 2.6454e-05],
        [1.4139e-03, 9.9859e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9582, 0.0418], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1341, 0.1445],
         [0.6771, 0.1685]],

        [[0.5574, 0.1781],
         [0.5836, 0.6138]],

        [[0.5918, 0.1800],
         [0.5358, 0.6697]],

        [[0.6692, 0.1312],
         [0.6436, 0.7300]],

        [[0.7173, 0.0773],
         [0.5590, 0.6611]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0009975514204148314
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.003578912575773538
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
Global Adjusted Rand Index: 0.0014159336787592516
Average Adjusted Rand Index: -0.005166666898884254
[0.0014159336787592516, 0.0014159336787592516] [-0.005166666898884254, -0.005166666898884254] [9783.669921875, 9783.6689453125]
-------------------------------------
This iteration is 2
True Objective function: Loss = -9940.856825435876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20130.646484375
inf tensor(20130.6465, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9898.0576171875
tensor(20130.6465, grad_fn=<NegBackward0>) tensor(9898.0576, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9828.4248046875
tensor(9898.0576, grad_fn=<NegBackward0>) tensor(9828.4248, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9820.037109375
tensor(9828.4248, grad_fn=<NegBackward0>) tensor(9820.0371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9818.8818359375
tensor(9820.0371, grad_fn=<NegBackward0>) tensor(9818.8818, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9818.283203125
tensor(9818.8818, grad_fn=<NegBackward0>) tensor(9818.2832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9817.9404296875
tensor(9818.2832, grad_fn=<NegBackward0>) tensor(9817.9404, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9817.716796875
tensor(9817.9404, grad_fn=<NegBackward0>) tensor(9817.7168, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9817.5205078125
tensor(9817.7168, grad_fn=<NegBackward0>) tensor(9817.5205, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9817.263671875
tensor(9817.5205, grad_fn=<NegBackward0>) tensor(9817.2637, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9816.80859375
tensor(9817.2637, grad_fn=<NegBackward0>) tensor(9816.8086, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9816.162109375
tensor(9816.8086, grad_fn=<NegBackward0>) tensor(9816.1621, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9815.921875
tensor(9816.1621, grad_fn=<NegBackward0>) tensor(9815.9219, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9815.7373046875
tensor(9815.9219, grad_fn=<NegBackward0>) tensor(9815.7373, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9815.6103515625
tensor(9815.7373, grad_fn=<NegBackward0>) tensor(9815.6104, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9815.5263671875
tensor(9815.6104, grad_fn=<NegBackward0>) tensor(9815.5264, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9815.4736328125
tensor(9815.5264, grad_fn=<NegBackward0>) tensor(9815.4736, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9815.44140625
tensor(9815.4736, grad_fn=<NegBackward0>) tensor(9815.4414, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9815.4169921875
tensor(9815.4414, grad_fn=<NegBackward0>) tensor(9815.4170, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9815.4013671875
tensor(9815.4170, grad_fn=<NegBackward0>) tensor(9815.4014, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9815.388671875
tensor(9815.4014, grad_fn=<NegBackward0>) tensor(9815.3887, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9815.3798828125
tensor(9815.3887, grad_fn=<NegBackward0>) tensor(9815.3799, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9815.373046875
tensor(9815.3799, grad_fn=<NegBackward0>) tensor(9815.3730, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9815.369140625
tensor(9815.3730, grad_fn=<NegBackward0>) tensor(9815.3691, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9815.36328125
tensor(9815.3691, grad_fn=<NegBackward0>) tensor(9815.3633, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9815.359375
tensor(9815.3633, grad_fn=<NegBackward0>) tensor(9815.3594, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9815.3564453125
tensor(9815.3594, grad_fn=<NegBackward0>) tensor(9815.3564, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9815.35546875
tensor(9815.3564, grad_fn=<NegBackward0>) tensor(9815.3555, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9815.353515625
tensor(9815.3555, grad_fn=<NegBackward0>) tensor(9815.3535, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9815.3515625
tensor(9815.3535, grad_fn=<NegBackward0>) tensor(9815.3516, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9815.3505859375
tensor(9815.3516, grad_fn=<NegBackward0>) tensor(9815.3506, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9815.3486328125
tensor(9815.3506, grad_fn=<NegBackward0>) tensor(9815.3486, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9815.34765625
tensor(9815.3486, grad_fn=<NegBackward0>) tensor(9815.3477, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9815.3466796875
tensor(9815.3477, grad_fn=<NegBackward0>) tensor(9815.3467, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9815.345703125
tensor(9815.3467, grad_fn=<NegBackward0>) tensor(9815.3457, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9815.3447265625
tensor(9815.3457, grad_fn=<NegBackward0>) tensor(9815.3447, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9815.34375
tensor(9815.3447, grad_fn=<NegBackward0>) tensor(9815.3438, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9815.3447265625
tensor(9815.3438, grad_fn=<NegBackward0>) tensor(9815.3447, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9815.3447265625
tensor(9815.3438, grad_fn=<NegBackward0>) tensor(9815.3447, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9815.3427734375
tensor(9815.3438, grad_fn=<NegBackward0>) tensor(9815.3428, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9815.341796875
tensor(9815.3428, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9815.3427734375
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3428, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9815.3427734375
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3428, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -9815.3408203125
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3408, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9815.33984375
tensor(9815.3408, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9815.341796875
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9815.337890625
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9815.3369140625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3369, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9815.3388671875
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -9815.3388671875
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -9815.3388671875
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.9859, 0.0141],
        [0.9959, 0.0041]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0120, 0.9880], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1318, 0.1437],
         [0.5388, 0.1455]],

        [[0.5712, 0.1647],
         [0.6006, 0.7270]],

        [[0.7169, 0.2101],
         [0.6122, 0.7064]],

        [[0.7046, 0.0892],
         [0.6041, 0.5485]],

        [[0.6074, 0.3067],
         [0.6959, 0.5075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: 0.0023835427081554724
Average Adjusted Rand Index: -0.0006936617740552836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22388.025390625
inf tensor(22388.0254, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9824.8359375
tensor(22388.0254, grad_fn=<NegBackward0>) tensor(9824.8359, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9820.626953125
tensor(9824.8359, grad_fn=<NegBackward0>) tensor(9820.6270, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9819.283203125
tensor(9820.6270, grad_fn=<NegBackward0>) tensor(9819.2832, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9818.59765625
tensor(9819.2832, grad_fn=<NegBackward0>) tensor(9818.5977, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9818.197265625
tensor(9818.5977, grad_fn=<NegBackward0>) tensor(9818.1973, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9817.9228515625
tensor(9818.1973, grad_fn=<NegBackward0>) tensor(9817.9229, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9817.712890625
tensor(9817.9229, grad_fn=<NegBackward0>) tensor(9817.7129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9817.533203125
tensor(9817.7129, grad_fn=<NegBackward0>) tensor(9817.5332, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9817.3662109375
tensor(9817.5332, grad_fn=<NegBackward0>) tensor(9817.3662, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9817.201171875
tensor(9817.3662, grad_fn=<NegBackward0>) tensor(9817.2012, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9817.0302734375
tensor(9817.2012, grad_fn=<NegBackward0>) tensor(9817.0303, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9816.84765625
tensor(9817.0303, grad_fn=<NegBackward0>) tensor(9816.8477, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9816.64453125
tensor(9816.8477, grad_fn=<NegBackward0>) tensor(9816.6445, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9816.421875
tensor(9816.6445, grad_fn=<NegBackward0>) tensor(9816.4219, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9816.18359375
tensor(9816.4219, grad_fn=<NegBackward0>) tensor(9816.1836, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9815.958984375
tensor(9816.1836, grad_fn=<NegBackward0>) tensor(9815.9590, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9815.7841796875
tensor(9815.9590, grad_fn=<NegBackward0>) tensor(9815.7842, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9815.6611328125
tensor(9815.7842, grad_fn=<NegBackward0>) tensor(9815.6611, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9815.580078125
tensor(9815.6611, grad_fn=<NegBackward0>) tensor(9815.5801, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9815.5244140625
tensor(9815.5801, grad_fn=<NegBackward0>) tensor(9815.5244, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9815.4853515625
tensor(9815.5244, grad_fn=<NegBackward0>) tensor(9815.4854, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9815.4580078125
tensor(9815.4854, grad_fn=<NegBackward0>) tensor(9815.4580, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9815.4365234375
tensor(9815.4580, grad_fn=<NegBackward0>) tensor(9815.4365, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9815.4208984375
tensor(9815.4365, grad_fn=<NegBackward0>) tensor(9815.4209, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9815.4072265625
tensor(9815.4209, grad_fn=<NegBackward0>) tensor(9815.4072, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9815.3984375
tensor(9815.4072, grad_fn=<NegBackward0>) tensor(9815.3984, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9815.390625
tensor(9815.3984, grad_fn=<NegBackward0>) tensor(9815.3906, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9815.3818359375
tensor(9815.3906, grad_fn=<NegBackward0>) tensor(9815.3818, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9815.376953125
tensor(9815.3818, grad_fn=<NegBackward0>) tensor(9815.3770, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9815.373046875
tensor(9815.3770, grad_fn=<NegBackward0>) tensor(9815.3730, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9815.369140625
tensor(9815.3730, grad_fn=<NegBackward0>) tensor(9815.3691, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9815.3642578125
tensor(9815.3691, grad_fn=<NegBackward0>) tensor(9815.3643, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9815.3623046875
tensor(9815.3643, grad_fn=<NegBackward0>) tensor(9815.3623, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9815.3603515625
tensor(9815.3623, grad_fn=<NegBackward0>) tensor(9815.3604, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9815.357421875
tensor(9815.3604, grad_fn=<NegBackward0>) tensor(9815.3574, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9815.35546875
tensor(9815.3574, grad_fn=<NegBackward0>) tensor(9815.3555, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9815.3544921875
tensor(9815.3555, grad_fn=<NegBackward0>) tensor(9815.3545, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9815.3515625
tensor(9815.3545, grad_fn=<NegBackward0>) tensor(9815.3516, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9815.3515625
tensor(9815.3516, grad_fn=<NegBackward0>) tensor(9815.3516, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9815.3505859375
tensor(9815.3516, grad_fn=<NegBackward0>) tensor(9815.3506, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9815.34765625
tensor(9815.3506, grad_fn=<NegBackward0>) tensor(9815.3477, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9815.3486328125
tensor(9815.3477, grad_fn=<NegBackward0>) tensor(9815.3486, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9815.3466796875
tensor(9815.3477, grad_fn=<NegBackward0>) tensor(9815.3467, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9815.345703125
tensor(9815.3467, grad_fn=<NegBackward0>) tensor(9815.3457, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9815.345703125
tensor(9815.3457, grad_fn=<NegBackward0>) tensor(9815.3457, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9815.3447265625
tensor(9815.3457, grad_fn=<NegBackward0>) tensor(9815.3447, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9815.3447265625
tensor(9815.3447, grad_fn=<NegBackward0>) tensor(9815.3447, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9815.3427734375
tensor(9815.3447, grad_fn=<NegBackward0>) tensor(9815.3428, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9815.34375
tensor(9815.3428, grad_fn=<NegBackward0>) tensor(9815.3438, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9815.341796875
tensor(9815.3428, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9815.341796875
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9815.341796875
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9815.341796875
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3418, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9815.33984375
tensor(9815.3418, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9815.3408203125
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3408, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9815.3408203125
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3408, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9815.33984375
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9815.3388671875
tensor(9815.3398, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9815.33984375
tensor(9815.3389, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9815.3388671875
tensor(9815.3389, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9815.337890625
tensor(9815.3389, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9815.33984375
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -9815.33984375
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -9815.3388671875
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -9815.337890625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9815.3369140625
tensor(9815.3379, grad_fn=<NegBackward0>) tensor(9815.3369, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -9815.3369140625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3369, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9815.3369140625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3369, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9815.3369140625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3369, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9815.3388671875
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9815.337890625
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3379, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9815.3388671875
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3389, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -9815.33984375
tensor(9815.3369, grad_fn=<NegBackward0>) tensor(9815.3398, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.9860, 0.0140],
        [0.9959, 0.0041]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0115, 0.9885], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1320, 0.1438],
         [0.6887, 0.1455]],

        [[0.6222, 0.1647],
         [0.5181, 0.7236]],

        [[0.6690, 0.2101],
         [0.6685, 0.6661]],

        [[0.6587, 0.0892],
         [0.6726, 0.5220]],

        [[0.6142, 0.3067],
         [0.5703, 0.5754]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: 0.0023835427081554724
Average Adjusted Rand Index: -0.0006936617740552836
[0.0023835427081554724, 0.0023835427081554724] [-0.0006936617740552836, -0.0006936617740552836] [9815.3388671875, 9815.33984375]
-------------------------------------
This iteration is 3
True Objective function: Loss = -9942.950780674195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22189.451171875
inf tensor(22189.4512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9835.912109375
tensor(22189.4512, grad_fn=<NegBackward0>) tensor(9835.9121, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9834.7724609375
tensor(9835.9121, grad_fn=<NegBackward0>) tensor(9834.7725, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9834.4521484375
tensor(9834.7725, grad_fn=<NegBackward0>) tensor(9834.4521, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9834.2607421875
tensor(9834.4521, grad_fn=<NegBackward0>) tensor(9834.2607, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9834.1005859375
tensor(9834.2607, grad_fn=<NegBackward0>) tensor(9834.1006, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9833.9501953125
tensor(9834.1006, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9833.8134765625
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.8135, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9833.685546875
tensor(9833.8135, grad_fn=<NegBackward0>) tensor(9833.6855, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9833.556640625
tensor(9833.6855, grad_fn=<NegBackward0>) tensor(9833.5566, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9833.423828125
tensor(9833.5566, grad_fn=<NegBackward0>) tensor(9833.4238, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9833.3037109375
tensor(9833.4238, grad_fn=<NegBackward0>) tensor(9833.3037, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9833.203125
tensor(9833.3037, grad_fn=<NegBackward0>) tensor(9833.2031, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9833.109375
tensor(9833.2031, grad_fn=<NegBackward0>) tensor(9833.1094, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9832.998046875
tensor(9833.1094, grad_fn=<NegBackward0>) tensor(9832.9980, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9832.8408203125
tensor(9832.9980, grad_fn=<NegBackward0>) tensor(9832.8408, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9832.568359375
tensor(9832.8408, grad_fn=<NegBackward0>) tensor(9832.5684, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9832.1064453125
tensor(9832.5684, grad_fn=<NegBackward0>) tensor(9832.1064, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9831.654296875
tensor(9832.1064, grad_fn=<NegBackward0>) tensor(9831.6543, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9831.3974609375
tensor(9831.6543, grad_fn=<NegBackward0>) tensor(9831.3975, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9831.265625
tensor(9831.3975, grad_fn=<NegBackward0>) tensor(9831.2656, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9831.1787109375
tensor(9831.2656, grad_fn=<NegBackward0>) tensor(9831.1787, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9831.115234375
tensor(9831.1787, grad_fn=<NegBackward0>) tensor(9831.1152, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9831.064453125
tensor(9831.1152, grad_fn=<NegBackward0>) tensor(9831.0645, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9831.009765625
tensor(9831.0645, grad_fn=<NegBackward0>) tensor(9831.0098, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9830.8525390625
tensor(9831.0098, grad_fn=<NegBackward0>) tensor(9830.8525, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9830.63671875
tensor(9830.8525, grad_fn=<NegBackward0>) tensor(9830.6367, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9830.3271484375
tensor(9830.6367, grad_fn=<NegBackward0>) tensor(9830.3271, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9830.1416015625
tensor(9830.3271, grad_fn=<NegBackward0>) tensor(9830.1416, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9830.0126953125
tensor(9830.1416, grad_fn=<NegBackward0>) tensor(9830.0127, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9829.9345703125
tensor(9830.0127, grad_fn=<NegBackward0>) tensor(9829.9346, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9829.8818359375
tensor(9829.9346, grad_fn=<NegBackward0>) tensor(9829.8818, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9829.8408203125
tensor(9829.8818, grad_fn=<NegBackward0>) tensor(9829.8408, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9829.8095703125
tensor(9829.8408, grad_fn=<NegBackward0>) tensor(9829.8096, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9829.7861328125
tensor(9829.8096, grad_fn=<NegBackward0>) tensor(9829.7861, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9829.767578125
tensor(9829.7861, grad_fn=<NegBackward0>) tensor(9829.7676, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9829.751953125
tensor(9829.7676, grad_fn=<NegBackward0>) tensor(9829.7520, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9829.73828125
tensor(9829.7520, grad_fn=<NegBackward0>) tensor(9829.7383, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9829.7255859375
tensor(9829.7383, grad_fn=<NegBackward0>) tensor(9829.7256, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9829.7138671875
tensor(9829.7256, grad_fn=<NegBackward0>) tensor(9829.7139, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9829.703125
tensor(9829.7139, grad_fn=<NegBackward0>) tensor(9829.7031, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9829.6904296875
tensor(9829.7031, grad_fn=<NegBackward0>) tensor(9829.6904, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9829.6796875
tensor(9829.6904, grad_fn=<NegBackward0>) tensor(9829.6797, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9829.6640625
tensor(9829.6797, grad_fn=<NegBackward0>) tensor(9829.6641, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9829.6455078125
tensor(9829.6641, grad_fn=<NegBackward0>) tensor(9829.6455, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9829.625
tensor(9829.6455, grad_fn=<NegBackward0>) tensor(9829.6250, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9829.5986328125
tensor(9829.6250, grad_fn=<NegBackward0>) tensor(9829.5986, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9829.576171875
tensor(9829.5986, grad_fn=<NegBackward0>) tensor(9829.5762, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9829.556640625
tensor(9829.5762, grad_fn=<NegBackward0>) tensor(9829.5566, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9829.5390625
tensor(9829.5566, grad_fn=<NegBackward0>) tensor(9829.5391, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9829.52734375
tensor(9829.5391, grad_fn=<NegBackward0>) tensor(9829.5273, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9829.5166015625
tensor(9829.5273, grad_fn=<NegBackward0>) tensor(9829.5166, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9829.5078125
tensor(9829.5166, grad_fn=<NegBackward0>) tensor(9829.5078, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9829.498046875
tensor(9829.5078, grad_fn=<NegBackward0>) tensor(9829.4980, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9829.4912109375
tensor(9829.4980, grad_fn=<NegBackward0>) tensor(9829.4912, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9829.4873046875
tensor(9829.4912, grad_fn=<NegBackward0>) tensor(9829.4873, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9829.4814453125
tensor(9829.4873, grad_fn=<NegBackward0>) tensor(9829.4814, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9829.478515625
tensor(9829.4814, grad_fn=<NegBackward0>) tensor(9829.4785, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9829.47265625
tensor(9829.4785, grad_fn=<NegBackward0>) tensor(9829.4727, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9829.4677734375
tensor(9829.4727, grad_fn=<NegBackward0>) tensor(9829.4678, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9829.4658203125
tensor(9829.4678, grad_fn=<NegBackward0>) tensor(9829.4658, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9829.462890625
tensor(9829.4658, grad_fn=<NegBackward0>) tensor(9829.4629, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9829.4609375
tensor(9829.4629, grad_fn=<NegBackward0>) tensor(9829.4609, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9829.458984375
tensor(9829.4609, grad_fn=<NegBackward0>) tensor(9829.4590, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9829.4580078125
tensor(9829.4590, grad_fn=<NegBackward0>) tensor(9829.4580, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9829.45703125
tensor(9829.4580, grad_fn=<NegBackward0>) tensor(9829.4570, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9829.4560546875
tensor(9829.4570, grad_fn=<NegBackward0>) tensor(9829.4561, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9829.4541015625
tensor(9829.4561, grad_fn=<NegBackward0>) tensor(9829.4541, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9829.4541015625
tensor(9829.4541, grad_fn=<NegBackward0>) tensor(9829.4541, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9829.4521484375
tensor(9829.4541, grad_fn=<NegBackward0>) tensor(9829.4521, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9829.4521484375
tensor(9829.4521, grad_fn=<NegBackward0>) tensor(9829.4521, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9829.4501953125
tensor(9829.4521, grad_fn=<NegBackward0>) tensor(9829.4502, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9829.44921875
tensor(9829.4502, grad_fn=<NegBackward0>) tensor(9829.4492, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9829.44921875
tensor(9829.4492, grad_fn=<NegBackward0>) tensor(9829.4492, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9829.447265625
tensor(9829.4492, grad_fn=<NegBackward0>) tensor(9829.4473, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9829.447265625
tensor(9829.4473, grad_fn=<NegBackward0>) tensor(9829.4473, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9829.4462890625
tensor(9829.4473, grad_fn=<NegBackward0>) tensor(9829.4463, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9829.447265625
tensor(9829.4463, grad_fn=<NegBackward0>) tensor(9829.4473, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9829.4453125
tensor(9829.4463, grad_fn=<NegBackward0>) tensor(9829.4453, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9829.4443359375
tensor(9829.4453, grad_fn=<NegBackward0>) tensor(9829.4443, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9829.4462890625
tensor(9829.4443, grad_fn=<NegBackward0>) tensor(9829.4463, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9829.443359375
tensor(9829.4443, grad_fn=<NegBackward0>) tensor(9829.4434, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9829.44921875
tensor(9829.4434, grad_fn=<NegBackward0>) tensor(9829.4492, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9829.4423828125
tensor(9829.4434, grad_fn=<NegBackward0>) tensor(9829.4424, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9829.44140625
tensor(9829.4424, grad_fn=<NegBackward0>) tensor(9829.4414, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9829.5498046875
tensor(9829.4414, grad_fn=<NegBackward0>) tensor(9829.5498, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9829.439453125
tensor(9829.4414, grad_fn=<NegBackward0>) tensor(9829.4395, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9829.439453125
tensor(9829.4395, grad_fn=<NegBackward0>) tensor(9829.4395, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9829.529296875
tensor(9829.4395, grad_fn=<NegBackward0>) tensor(9829.5293, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9829.439453125
tensor(9829.4395, grad_fn=<NegBackward0>) tensor(9829.4395, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9829.4384765625
tensor(9829.4395, grad_fn=<NegBackward0>) tensor(9829.4385, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9829.4384765625
tensor(9829.4385, grad_fn=<NegBackward0>) tensor(9829.4385, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9829.4365234375
tensor(9829.4385, grad_fn=<NegBackward0>) tensor(9829.4365, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9829.4462890625
tensor(9829.4365, grad_fn=<NegBackward0>) tensor(9829.4463, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9829.4365234375
tensor(9829.4365, grad_fn=<NegBackward0>) tensor(9829.4365, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9829.435546875
tensor(9829.4365, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9829.435546875
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9829.435546875
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9829.43359375
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9829.4384765625
tensor(9829.4336, grad_fn=<NegBackward0>) tensor(9829.4385, grad_fn=<NegBackward0>)
1
pi: tensor([[9.7998e-01, 2.0017e-02],
        [3.6335e-04, 9.9964e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9957, 0.0043], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1357, 0.1534],
         [0.5181, 0.1021]],

        [[0.5874, 0.2251],
         [0.5323, 0.5809]],

        [[0.6102, 0.1491],
         [0.5321, 0.6893]],

        [[0.7297, 0.1698],
         [0.6429, 0.7044]],

        [[0.5959, 0.0779],
         [0.6548, 0.6734]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
Global Adjusted Rand Index: 8.601727269170772e-06
Average Adjusted Rand Index: 0.00042736537927438997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21553.111328125
inf tensor(21553.1113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9834.751953125
tensor(21553.1113, grad_fn=<NegBackward0>) tensor(9834.7520, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9833.98046875
tensor(9834.7520, grad_fn=<NegBackward0>) tensor(9833.9805, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9833.6826171875
tensor(9833.9805, grad_fn=<NegBackward0>) tensor(9833.6826, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9833.40625
tensor(9833.6826, grad_fn=<NegBackward0>) tensor(9833.4062, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9833.150390625
tensor(9833.4062, grad_fn=<NegBackward0>) tensor(9833.1504, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9832.92578125
tensor(9833.1504, grad_fn=<NegBackward0>) tensor(9832.9258, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9832.634765625
tensor(9832.9258, grad_fn=<NegBackward0>) tensor(9832.6348, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9832.1591796875
tensor(9832.6348, grad_fn=<NegBackward0>) tensor(9832.1592, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9831.669921875
tensor(9832.1592, grad_fn=<NegBackward0>) tensor(9831.6699, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9831.392578125
tensor(9831.6699, grad_fn=<NegBackward0>) tensor(9831.3926, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9831.2265625
tensor(9831.3926, grad_fn=<NegBackward0>) tensor(9831.2266, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9831.111328125
tensor(9831.2266, grad_fn=<NegBackward0>) tensor(9831.1113, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9831.0087890625
tensor(9831.1113, grad_fn=<NegBackward0>) tensor(9831.0088, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9830.908203125
tensor(9831.0088, grad_fn=<NegBackward0>) tensor(9830.9082, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9830.771484375
tensor(9830.9082, grad_fn=<NegBackward0>) tensor(9830.7715, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9830.4638671875
tensor(9830.7715, grad_fn=<NegBackward0>) tensor(9830.4639, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9830.2119140625
tensor(9830.4639, grad_fn=<NegBackward0>) tensor(9830.2119, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9830.0693359375
tensor(9830.2119, grad_fn=<NegBackward0>) tensor(9830.0693, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9829.9609375
tensor(9830.0693, grad_fn=<NegBackward0>) tensor(9829.9609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9829.87890625
tensor(9829.9609, grad_fn=<NegBackward0>) tensor(9829.8789, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9829.822265625
tensor(9829.8789, grad_fn=<NegBackward0>) tensor(9829.8223, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9829.78125
tensor(9829.8223, grad_fn=<NegBackward0>) tensor(9829.7812, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9829.7529296875
tensor(9829.7812, grad_fn=<NegBackward0>) tensor(9829.7529, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9829.7294921875
tensor(9829.7529, grad_fn=<NegBackward0>) tensor(9829.7295, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9829.7099609375
tensor(9829.7295, grad_fn=<NegBackward0>) tensor(9829.7100, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9829.693359375
tensor(9829.7100, grad_fn=<NegBackward0>) tensor(9829.6934, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9829.6748046875
tensor(9829.6934, grad_fn=<NegBackward0>) tensor(9829.6748, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9829.6552734375
tensor(9829.6748, grad_fn=<NegBackward0>) tensor(9829.6553, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9829.6298828125
tensor(9829.6553, grad_fn=<NegBackward0>) tensor(9829.6299, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9829.60546875
tensor(9829.6299, grad_fn=<NegBackward0>) tensor(9829.6055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9829.576171875
tensor(9829.6055, grad_fn=<NegBackward0>) tensor(9829.5762, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9829.5537109375
tensor(9829.5762, grad_fn=<NegBackward0>) tensor(9829.5537, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9829.5341796875
tensor(9829.5537, grad_fn=<NegBackward0>) tensor(9829.5342, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9829.5185546875
tensor(9829.5342, grad_fn=<NegBackward0>) tensor(9829.5186, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9829.5078125
tensor(9829.5186, grad_fn=<NegBackward0>) tensor(9829.5078, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9829.498046875
tensor(9829.5078, grad_fn=<NegBackward0>) tensor(9829.4980, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9829.490234375
tensor(9829.4980, grad_fn=<NegBackward0>) tensor(9829.4902, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9829.484375
tensor(9829.4902, grad_fn=<NegBackward0>) tensor(9829.4844, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9829.474609375
tensor(9829.4844, grad_fn=<NegBackward0>) tensor(9829.4746, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9829.4716796875
tensor(9829.4746, grad_fn=<NegBackward0>) tensor(9829.4717, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9829.4677734375
tensor(9829.4717, grad_fn=<NegBackward0>) tensor(9829.4678, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9829.4638671875
tensor(9829.4678, grad_fn=<NegBackward0>) tensor(9829.4639, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9829.4609375
tensor(9829.4639, grad_fn=<NegBackward0>) tensor(9829.4609, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9829.4580078125
tensor(9829.4609, grad_fn=<NegBackward0>) tensor(9829.4580, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9829.455078125
tensor(9829.4580, grad_fn=<NegBackward0>) tensor(9829.4551, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9829.4541015625
tensor(9829.4551, grad_fn=<NegBackward0>) tensor(9829.4541, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9829.451171875
tensor(9829.4541, grad_fn=<NegBackward0>) tensor(9829.4512, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9829.451171875
tensor(9829.4512, grad_fn=<NegBackward0>) tensor(9829.4512, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9829.447265625
tensor(9829.4512, grad_fn=<NegBackward0>) tensor(9829.4473, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9829.4462890625
tensor(9829.4473, grad_fn=<NegBackward0>) tensor(9829.4463, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9829.4462890625
tensor(9829.4463, grad_fn=<NegBackward0>) tensor(9829.4463, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9829.4453125
tensor(9829.4463, grad_fn=<NegBackward0>) tensor(9829.4453, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9829.4423828125
tensor(9829.4453, grad_fn=<NegBackward0>) tensor(9829.4424, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9829.443359375
tensor(9829.4424, grad_fn=<NegBackward0>) tensor(9829.4434, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9829.4423828125
tensor(9829.4424, grad_fn=<NegBackward0>) tensor(9829.4424, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9829.4404296875
tensor(9829.4424, grad_fn=<NegBackward0>) tensor(9829.4404, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9829.44140625
tensor(9829.4404, grad_fn=<NegBackward0>) tensor(9829.4414, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9829.44140625
tensor(9829.4404, grad_fn=<NegBackward0>) tensor(9829.4414, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9829.4404296875
tensor(9829.4404, grad_fn=<NegBackward0>) tensor(9829.4404, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9829.4404296875
tensor(9829.4404, grad_fn=<NegBackward0>) tensor(9829.4404, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9829.439453125
tensor(9829.4404, grad_fn=<NegBackward0>) tensor(9829.4395, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9829.4384765625
tensor(9829.4395, grad_fn=<NegBackward0>) tensor(9829.4385, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9829.4384765625
tensor(9829.4385, grad_fn=<NegBackward0>) tensor(9829.4385, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9829.439453125
tensor(9829.4385, grad_fn=<NegBackward0>) tensor(9829.4395, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9829.4375
tensor(9829.4385, grad_fn=<NegBackward0>) tensor(9829.4375, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9829.4375
tensor(9829.4375, grad_fn=<NegBackward0>) tensor(9829.4375, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9829.4375
tensor(9829.4375, grad_fn=<NegBackward0>) tensor(9829.4375, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9829.4365234375
tensor(9829.4375, grad_fn=<NegBackward0>) tensor(9829.4365, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9829.4365234375
tensor(9829.4365, grad_fn=<NegBackward0>) tensor(9829.4365, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9829.435546875
tensor(9829.4365, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9829.435546875
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9829.435546875
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9829.4345703125
tensor(9829.4355, grad_fn=<NegBackward0>) tensor(9829.4346, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9829.4345703125
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4346, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9829.4365234375
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4365, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9829.435546875
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9829.435546875
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -9829.4345703125
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4346, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9829.4345703125
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4346, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9829.43359375
tensor(9829.4346, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9829.43359375
tensor(9829.4336, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9829.4326171875
tensor(9829.4336, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9829.435546875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9829.4326171875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9829.4326171875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9829.4443359375
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4443, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9829.435546875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9829.43359375
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -9829.435546875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4355, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -9829.4326171875
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9829.43359375
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9829.43359375
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -9829.431640625
tensor(9829.4326, grad_fn=<NegBackward0>) tensor(9829.4316, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9829.4326171875
tensor(9829.4316, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9829.4306640625
tensor(9829.4316, grad_fn=<NegBackward0>) tensor(9829.4307, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9829.43359375
tensor(9829.4307, grad_fn=<NegBackward0>) tensor(9829.4336, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -9829.4306640625
tensor(9829.4307, grad_fn=<NegBackward0>) tensor(9829.4307, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9829.4326171875
tensor(9829.4307, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9829.4326171875
tensor(9829.4307, grad_fn=<NegBackward0>) tensor(9829.4326, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9985e-01, 1.5096e-04],
        [2.0034e-02, 9.7997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0042, 0.9958], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1020, 0.1534],
         [0.6885, 0.1358]],

        [[0.5001, 0.2252],
         [0.5835, 0.5967]],

        [[0.5971, 0.1491],
         [0.6340, 0.5920]],

        [[0.7241, 0.1696],
         [0.5171, 0.6473]],

        [[0.5298, 0.0779],
         [0.5298, 0.6325]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006803382252891437
Global Adjusted Rand Index: 8.601727269170772e-06
Average Adjusted Rand Index: 0.00042736537927438997
[8.601727269170772e-06, 8.601727269170772e-06] [0.00042736537927438997, 0.00042736537927438997] [9829.4326171875, 9829.4296875]
-------------------------------------
This iteration is 4
True Objective function: Loss = -10044.96906855188
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23151.416015625
inf tensor(23151.4160, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9918.2666015625
tensor(23151.4160, grad_fn=<NegBackward0>) tensor(9918.2666, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9917.3408203125
tensor(9918.2666, grad_fn=<NegBackward0>) tensor(9917.3408, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9917.154296875
tensor(9917.3408, grad_fn=<NegBackward0>) tensor(9917.1543, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9917.044921875
tensor(9917.1543, grad_fn=<NegBackward0>) tensor(9917.0449, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9916.9130859375
tensor(9917.0449, grad_fn=<NegBackward0>) tensor(9916.9131, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9916.55859375
tensor(9916.9131, grad_fn=<NegBackward0>) tensor(9916.5586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9915.1962890625
tensor(9916.5586, grad_fn=<NegBackward0>) tensor(9915.1963, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9914.728515625
tensor(9915.1963, grad_fn=<NegBackward0>) tensor(9914.7285, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9914.4443359375
tensor(9914.7285, grad_fn=<NegBackward0>) tensor(9914.4443, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9914.20703125
tensor(9914.4443, grad_fn=<NegBackward0>) tensor(9914.2070, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9913.9970703125
tensor(9914.2070, grad_fn=<NegBackward0>) tensor(9913.9971, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9913.8212890625
tensor(9913.9971, grad_fn=<NegBackward0>) tensor(9913.8213, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9913.6728515625
tensor(9913.8213, grad_fn=<NegBackward0>) tensor(9913.6729, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9913.5556640625
tensor(9913.6729, grad_fn=<NegBackward0>) tensor(9913.5557, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9913.4638671875
tensor(9913.5557, grad_fn=<NegBackward0>) tensor(9913.4639, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9913.3955078125
tensor(9913.4639, grad_fn=<NegBackward0>) tensor(9913.3955, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9913.3447265625
tensor(9913.3955, grad_fn=<NegBackward0>) tensor(9913.3447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9913.306640625
tensor(9913.3447, grad_fn=<NegBackward0>) tensor(9913.3066, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9913.2626953125
tensor(9913.3066, grad_fn=<NegBackward0>) tensor(9913.2627, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9913.205078125
tensor(9913.2627, grad_fn=<NegBackward0>) tensor(9913.2051, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9913.1435546875
tensor(9913.2051, grad_fn=<NegBackward0>) tensor(9913.1436, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9913.0849609375
tensor(9913.1436, grad_fn=<NegBackward0>) tensor(9913.0850, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9913.015625
tensor(9913.0850, grad_fn=<NegBackward0>) tensor(9913.0156, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9912.9326171875
tensor(9913.0156, grad_fn=<NegBackward0>) tensor(9912.9326, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9912.8466796875
tensor(9912.9326, grad_fn=<NegBackward0>) tensor(9912.8467, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9912.7744140625
tensor(9912.8467, grad_fn=<NegBackward0>) tensor(9912.7744, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9912.73046875
tensor(9912.7744, grad_fn=<NegBackward0>) tensor(9912.7305, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9912.7080078125
tensor(9912.7305, grad_fn=<NegBackward0>) tensor(9912.7080, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9912.701171875
tensor(9912.7080, grad_fn=<NegBackward0>) tensor(9912.7012, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9912.6953125
tensor(9912.7012, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9912.7099609375
tensor(9912.6953, grad_fn=<NegBackward0>) tensor(9912.7100, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9912.6943359375
tensor(9912.6953, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9912.6953125
tensor(9912.6943, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9912.6943359375
tensor(9912.6943, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9912.72265625
tensor(9912.6943, grad_fn=<NegBackward0>) tensor(9912.7227, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9912.693359375
tensor(9912.6943, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9912.6923828125
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6924, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9912.6982421875
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6982, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9912.6923828125
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6924, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9912.6943359375
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9912.6953125
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9912.6943359375
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -9912.6943359375
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -9912.693359375
tensor(9912.6924, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4600 due to no improvement.
pi: tensor([[0.0051, 0.9949],
        [0.4321, 0.5679]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1710, 0.8290], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1836, 0.1494],
         [0.5023, 0.1217]],

        [[0.6773, 0.1457],
         [0.7184, 0.5598]],

        [[0.5744, 0.1473],
         [0.5327, 0.7071]],

        [[0.6679, 0.1509],
         [0.6083, 0.6299]],

        [[0.6252, 0.1499],
         [0.6739, 0.7209]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.003904091822363822
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.007842379304084011
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.012633139431831
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.042725975783736325
Global Adjusted Rand Index: 0.010558396699311303
Average Adjusted Rand Index: 0.011405409548101568
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20052.822265625
inf tensor(20052.8223, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9917.4384765625
tensor(20052.8223, grad_fn=<NegBackward0>) tensor(9917.4385, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9917.0419921875
tensor(9917.4385, grad_fn=<NegBackward0>) tensor(9917.0420, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9916.7919921875
tensor(9917.0420, grad_fn=<NegBackward0>) tensor(9916.7920, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9916.1767578125
tensor(9916.7920, grad_fn=<NegBackward0>) tensor(9916.1768, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9914.9443359375
tensor(9916.1768, grad_fn=<NegBackward0>) tensor(9914.9443, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9914.2822265625
tensor(9914.9443, grad_fn=<NegBackward0>) tensor(9914.2822, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9914.005859375
tensor(9914.2822, grad_fn=<NegBackward0>) tensor(9914.0059, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9913.8251953125
tensor(9914.0059, grad_fn=<NegBackward0>) tensor(9913.8252, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9913.6884765625
tensor(9913.8252, grad_fn=<NegBackward0>) tensor(9913.6885, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9913.580078125
tensor(9913.6885, grad_fn=<NegBackward0>) tensor(9913.5801, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9913.4990234375
tensor(9913.5801, grad_fn=<NegBackward0>) tensor(9913.4990, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9913.4345703125
tensor(9913.4990, grad_fn=<NegBackward0>) tensor(9913.4346, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9913.3837890625
tensor(9913.4346, grad_fn=<NegBackward0>) tensor(9913.3838, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9913.337890625
tensor(9913.3838, grad_fn=<NegBackward0>) tensor(9913.3379, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9913.2861328125
tensor(9913.3379, grad_fn=<NegBackward0>) tensor(9913.2861, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9913.212890625
tensor(9913.2861, grad_fn=<NegBackward0>) tensor(9913.2129, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9913.1455078125
tensor(9913.2129, grad_fn=<NegBackward0>) tensor(9913.1455, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9913.0908203125
tensor(9913.1455, grad_fn=<NegBackward0>) tensor(9913.0908, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9913.0390625
tensor(9913.0908, grad_fn=<NegBackward0>) tensor(9913.0391, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9912.966796875
tensor(9913.0391, grad_fn=<NegBackward0>) tensor(9912.9668, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9912.8984375
tensor(9912.9668, grad_fn=<NegBackward0>) tensor(9912.8984, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9912.8330078125
tensor(9912.8984, grad_fn=<NegBackward0>) tensor(9912.8330, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9912.77734375
tensor(9912.8330, grad_fn=<NegBackward0>) tensor(9912.7773, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9912.7392578125
tensor(9912.7773, grad_fn=<NegBackward0>) tensor(9912.7393, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9912.7158203125
tensor(9912.7393, grad_fn=<NegBackward0>) tensor(9912.7158, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9912.703125
tensor(9912.7158, grad_fn=<NegBackward0>) tensor(9912.7031, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9912.7021484375
tensor(9912.7031, grad_fn=<NegBackward0>) tensor(9912.7021, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9912.6953125
tensor(9912.7021, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9912.6953125
tensor(9912.6953, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9912.693359375
tensor(9912.6953, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9912.6953125
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6953, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9912.693359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6934, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -9912.6943359375
tensor(9912.6934, grad_fn=<NegBackward0>) tensor(9912.6943, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.0075, 0.9925],
        [0.4345, 0.5655]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1715, 0.8285], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1834, 0.1493],
         [0.6937, 0.1217]],

        [[0.6970, 0.1456],
         [0.5460, 0.6943]],

        [[0.5726, 0.1472],
         [0.6125, 0.5872]],

        [[0.6225, 0.1508],
         [0.7288, 0.6312]],

        [[0.6209, 0.1498],
         [0.5326, 0.6059]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.006119681923119205
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.007842379304084011
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.018464894137114062
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.042725975783736325
Global Adjusted Rand Index: 0.010537735817384753
Average Adjusted Rand Index: 0.012128642469007104
[0.010558396699311303, 0.010537735817384753] [0.011405409548101568, 0.012128642469007104] [9912.693359375, 9912.6943359375]
-------------------------------------
This iteration is 5
True Objective function: Loss = -10113.397192879685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21171.119140625
inf tensor(21171.1191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9977.2080078125
tensor(21171.1191, grad_fn=<NegBackward0>) tensor(9977.2080, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9975.8359375
tensor(9977.2080, grad_fn=<NegBackward0>) tensor(9975.8359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9975.1689453125
tensor(9975.8359, grad_fn=<NegBackward0>) tensor(9975.1689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9974.9541015625
tensor(9975.1689, grad_fn=<NegBackward0>) tensor(9974.9541, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9974.8896484375
tensor(9974.9541, grad_fn=<NegBackward0>) tensor(9974.8896, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9974.857421875
tensor(9974.8896, grad_fn=<NegBackward0>) tensor(9974.8574, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9974.83984375
tensor(9974.8574, grad_fn=<NegBackward0>) tensor(9974.8398, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9974.8271484375
tensor(9974.8398, grad_fn=<NegBackward0>) tensor(9974.8271, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9974.8173828125
tensor(9974.8271, grad_fn=<NegBackward0>) tensor(9974.8174, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9974.8095703125
tensor(9974.8174, grad_fn=<NegBackward0>) tensor(9974.8096, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9974.802734375
tensor(9974.8096, grad_fn=<NegBackward0>) tensor(9974.8027, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9974.796875
tensor(9974.8027, grad_fn=<NegBackward0>) tensor(9974.7969, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9974.7919921875
tensor(9974.7969, grad_fn=<NegBackward0>) tensor(9974.7920, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9974.78515625
tensor(9974.7920, grad_fn=<NegBackward0>) tensor(9974.7852, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9974.779296875
tensor(9974.7852, grad_fn=<NegBackward0>) tensor(9974.7793, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9974.7744140625
tensor(9974.7793, grad_fn=<NegBackward0>) tensor(9974.7744, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9974.767578125
tensor(9974.7744, grad_fn=<NegBackward0>) tensor(9974.7676, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9974.763671875
tensor(9974.7676, grad_fn=<NegBackward0>) tensor(9974.7637, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9974.7568359375
tensor(9974.7637, grad_fn=<NegBackward0>) tensor(9974.7568, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9974.748046875
tensor(9974.7568, grad_fn=<NegBackward0>) tensor(9974.7480, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9974.7412109375
tensor(9974.7480, grad_fn=<NegBackward0>) tensor(9974.7412, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9974.7294921875
tensor(9974.7412, grad_fn=<NegBackward0>) tensor(9974.7295, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9974.71484375
tensor(9974.7295, grad_fn=<NegBackward0>) tensor(9974.7148, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9974.6953125
tensor(9974.7148, grad_fn=<NegBackward0>) tensor(9974.6953, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9974.6689453125
tensor(9974.6953, grad_fn=<NegBackward0>) tensor(9974.6689, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9974.6279296875
tensor(9974.6689, grad_fn=<NegBackward0>) tensor(9974.6279, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9974.5634765625
tensor(9974.6279, grad_fn=<NegBackward0>) tensor(9974.5635, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9974.478515625
tensor(9974.5635, grad_fn=<NegBackward0>) tensor(9974.4785, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9974.396484375
tensor(9974.4785, grad_fn=<NegBackward0>) tensor(9974.3965, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9974.3369140625
tensor(9974.3965, grad_fn=<NegBackward0>) tensor(9974.3369, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9974.3056640625
tensor(9974.3369, grad_fn=<NegBackward0>) tensor(9974.3057, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9974.2890625
tensor(9974.3057, grad_fn=<NegBackward0>) tensor(9974.2891, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9974.2763671875
tensor(9974.2891, grad_fn=<NegBackward0>) tensor(9974.2764, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9974.265625
tensor(9974.2764, grad_fn=<NegBackward0>) tensor(9974.2656, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9974.2568359375
tensor(9974.2656, grad_fn=<NegBackward0>) tensor(9974.2568, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9974.244140625
tensor(9974.2568, grad_fn=<NegBackward0>) tensor(9974.2441, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9974.224609375
tensor(9974.2441, grad_fn=<NegBackward0>) tensor(9974.2246, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9974.171875
tensor(9974.2246, grad_fn=<NegBackward0>) tensor(9974.1719, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9973.5263671875
tensor(9974.1719, grad_fn=<NegBackward0>) tensor(9973.5264, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9973.158203125
tensor(9973.5264, grad_fn=<NegBackward0>) tensor(9973.1582, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9973.08984375
tensor(9973.1582, grad_fn=<NegBackward0>) tensor(9973.0898, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9973.060546875
tensor(9973.0898, grad_fn=<NegBackward0>) tensor(9973.0605, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9973.04296875
tensor(9973.0605, grad_fn=<NegBackward0>) tensor(9973.0430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9973.0322265625
tensor(9973.0430, grad_fn=<NegBackward0>) tensor(9973.0322, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9973.0244140625
tensor(9973.0322, grad_fn=<NegBackward0>) tensor(9973.0244, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9973.0146484375
tensor(9973.0244, grad_fn=<NegBackward0>) tensor(9973.0146, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9973.0068359375
tensor(9973.0146, grad_fn=<NegBackward0>) tensor(9973.0068, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9972.9951171875
tensor(9973.0068, grad_fn=<NegBackward0>) tensor(9972.9951, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9972.9873046875
tensor(9972.9951, grad_fn=<NegBackward0>) tensor(9972.9873, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9972.98046875
tensor(9972.9873, grad_fn=<NegBackward0>) tensor(9972.9805, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9972.974609375
tensor(9972.9805, grad_fn=<NegBackward0>) tensor(9972.9746, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9972.96875
tensor(9972.9746, grad_fn=<NegBackward0>) tensor(9972.9688, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9972.9638671875
tensor(9972.9688, grad_fn=<NegBackward0>) tensor(9972.9639, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9972.9599609375
tensor(9972.9639, grad_fn=<NegBackward0>) tensor(9972.9600, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9972.955078125
tensor(9972.9600, grad_fn=<NegBackward0>) tensor(9972.9551, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9972.9541015625
tensor(9972.9551, grad_fn=<NegBackward0>) tensor(9972.9541, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9972.953125
tensor(9972.9541, grad_fn=<NegBackward0>) tensor(9972.9531, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9972.94921875
tensor(9972.9531, grad_fn=<NegBackward0>) tensor(9972.9492, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9972.9482421875
tensor(9972.9492, grad_fn=<NegBackward0>) tensor(9972.9482, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9972.947265625
tensor(9972.9482, grad_fn=<NegBackward0>) tensor(9972.9473, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9972.9453125
tensor(9972.9473, grad_fn=<NegBackward0>) tensor(9972.9453, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9972.9443359375
tensor(9972.9453, grad_fn=<NegBackward0>) tensor(9972.9443, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9972.9443359375
tensor(9972.9443, grad_fn=<NegBackward0>) tensor(9972.9443, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9972.943359375
tensor(9972.9443, grad_fn=<NegBackward0>) tensor(9972.9434, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9972.943359375
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9434, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9972.9423828125
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9972.94140625
tensor(9972.9424, grad_fn=<NegBackward0>) tensor(9972.9414, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9972.9423828125
tensor(9972.9414, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9972.9423828125
tensor(9972.9414, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9972.94140625
tensor(9972.9414, grad_fn=<NegBackward0>) tensor(9972.9414, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9972.9423828125
tensor(9972.9414, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9972.9404296875
tensor(9972.9414, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9972.9404296875
tensor(9972.9404, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9972.9404296875
tensor(9972.9404, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9972.9384765625
tensor(9972.9404, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9972.9443359375
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9443, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9972.939453125
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9972.9375
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9972.986328125
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9863, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9972.9384765625
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9972.9375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9972.9375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9972.9384765625
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9972.9384765625
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9972.9375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9972.9375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9972.9375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9972.9365234375
tensor(9972.9375, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9972.939453125
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9972.935546875
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9355, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9972.9443359375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9443, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9972.9443359375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9443, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -9972.9365234375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -9972.9384765625
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -9972.9375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.9993e-01, 6.9331e-05],
        [3.8182e-01, 6.1818e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7768, 0.2232], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1343, 0.1573],
         [0.7052, 0.1842]],

        [[0.5460, 0.1560],
         [0.5504, 0.6088]],

        [[0.6830, 0.1677],
         [0.5758, 0.6705]],

        [[0.6453, 0.1591],
         [0.6498, 0.6192]],

        [[0.6775, 0.0992],
         [0.6764, 0.6694]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005099594640613479
Average Adjusted Rand Index: -9.369493421773029e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21871.7734375
inf tensor(21871.7734, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9976.423828125
tensor(21871.7734, grad_fn=<NegBackward0>) tensor(9976.4238, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9975.6640625
tensor(9976.4238, grad_fn=<NegBackward0>) tensor(9975.6641, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9975.1103515625
tensor(9975.6641, grad_fn=<NegBackward0>) tensor(9975.1104, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9974.9013671875
tensor(9975.1104, grad_fn=<NegBackward0>) tensor(9974.9014, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9974.85546875
tensor(9974.9014, grad_fn=<NegBackward0>) tensor(9974.8555, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9974.8271484375
tensor(9974.8555, grad_fn=<NegBackward0>) tensor(9974.8271, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9974.8046875
tensor(9974.8271, grad_fn=<NegBackward0>) tensor(9974.8047, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9974.787109375
tensor(9974.8047, grad_fn=<NegBackward0>) tensor(9974.7871, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9974.7705078125
tensor(9974.7871, grad_fn=<NegBackward0>) tensor(9974.7705, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9974.755859375
tensor(9974.7705, grad_fn=<NegBackward0>) tensor(9974.7559, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9974.7421875
tensor(9974.7559, grad_fn=<NegBackward0>) tensor(9974.7422, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9974.7255859375
tensor(9974.7422, grad_fn=<NegBackward0>) tensor(9974.7256, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9974.7080078125
tensor(9974.7256, grad_fn=<NegBackward0>) tensor(9974.7080, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9974.6875
tensor(9974.7080, grad_fn=<NegBackward0>) tensor(9974.6875, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9974.66015625
tensor(9974.6875, grad_fn=<NegBackward0>) tensor(9974.6602, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9974.62109375
tensor(9974.6602, grad_fn=<NegBackward0>) tensor(9974.6211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9974.537109375
tensor(9974.6211, grad_fn=<NegBackward0>) tensor(9974.5371, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9974.251953125
tensor(9974.5371, grad_fn=<NegBackward0>) tensor(9974.2520, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9973.7470703125
tensor(9974.2520, grad_fn=<NegBackward0>) tensor(9973.7471, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9973.396484375
tensor(9973.7471, grad_fn=<NegBackward0>) tensor(9973.3965, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9973.2294921875
tensor(9973.3965, grad_fn=<NegBackward0>) tensor(9973.2295, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9973.1552734375
tensor(9973.2295, grad_fn=<NegBackward0>) tensor(9973.1553, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9973.111328125
tensor(9973.1553, grad_fn=<NegBackward0>) tensor(9973.1113, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9973.0830078125
tensor(9973.1113, grad_fn=<NegBackward0>) tensor(9973.0830, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9973.0615234375
tensor(9973.0830, grad_fn=<NegBackward0>) tensor(9973.0615, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9973.048828125
tensor(9973.0615, grad_fn=<NegBackward0>) tensor(9973.0488, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9973.041015625
tensor(9973.0488, grad_fn=<NegBackward0>) tensor(9973.0410, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9973.0322265625
tensor(9973.0410, grad_fn=<NegBackward0>) tensor(9973.0322, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9973.029296875
tensor(9973.0322, grad_fn=<NegBackward0>) tensor(9973.0293, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9973.0234375
tensor(9973.0293, grad_fn=<NegBackward0>) tensor(9973.0234, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9973.0185546875
tensor(9973.0234, grad_fn=<NegBackward0>) tensor(9973.0186, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9973.0146484375
tensor(9973.0186, grad_fn=<NegBackward0>) tensor(9973.0146, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9973.009765625
tensor(9973.0146, grad_fn=<NegBackward0>) tensor(9973.0098, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9973.0048828125
tensor(9973.0098, grad_fn=<NegBackward0>) tensor(9973.0049, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9972.998046875
tensor(9973.0049, grad_fn=<NegBackward0>) tensor(9972.9980, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9972.9873046875
tensor(9972.9980, grad_fn=<NegBackward0>) tensor(9972.9873, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9972.9765625
tensor(9972.9873, grad_fn=<NegBackward0>) tensor(9972.9766, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9972.9697265625
tensor(9972.9766, grad_fn=<NegBackward0>) tensor(9972.9697, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9972.9658203125
tensor(9972.9697, grad_fn=<NegBackward0>) tensor(9972.9658, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9972.9619140625
tensor(9972.9658, grad_fn=<NegBackward0>) tensor(9972.9619, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9972.958984375
tensor(9972.9619, grad_fn=<NegBackward0>) tensor(9972.9590, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9972.9638671875
tensor(9972.9590, grad_fn=<NegBackward0>) tensor(9972.9639, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9972.955078125
tensor(9972.9590, grad_fn=<NegBackward0>) tensor(9972.9551, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9972.95703125
tensor(9972.9551, grad_fn=<NegBackward0>) tensor(9972.9570, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9972.951171875
tensor(9972.9551, grad_fn=<NegBackward0>) tensor(9972.9512, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9972.9609375
tensor(9972.9512, grad_fn=<NegBackward0>) tensor(9972.9609, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9972.9501953125
tensor(9972.9512, grad_fn=<NegBackward0>) tensor(9972.9502, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9972.9501953125
tensor(9972.9502, grad_fn=<NegBackward0>) tensor(9972.9502, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9972.9482421875
tensor(9972.9502, grad_fn=<NegBackward0>) tensor(9972.9482, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9972.9482421875
tensor(9972.9482, grad_fn=<NegBackward0>) tensor(9972.9482, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9972.9482421875
tensor(9972.9482, grad_fn=<NegBackward0>) tensor(9972.9482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9972.9453125
tensor(9972.9482, grad_fn=<NegBackward0>) tensor(9972.9453, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9972.9462890625
tensor(9972.9453, grad_fn=<NegBackward0>) tensor(9972.9463, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9972.9453125
tensor(9972.9453, grad_fn=<NegBackward0>) tensor(9972.9453, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9972.9541015625
tensor(9972.9453, grad_fn=<NegBackward0>) tensor(9972.9541, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9972.943359375
tensor(9972.9453, grad_fn=<NegBackward0>) tensor(9972.9434, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9972.9482421875
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9482, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9972.943359375
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9434, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9972.943359375
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9434, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9972.9423828125
tensor(9972.9434, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9972.9423828125
tensor(9972.9424, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9972.9404296875
tensor(9972.9424, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9972.9423828125
tensor(9972.9404, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9972.939453125
tensor(9972.9404, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9972.9404296875
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9972.939453125
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9972.9404296875
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9972.9404296875
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9972.9404296875
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -9972.939453125
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9972.9404296875
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9404, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9972.9423828125
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9424, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9972.9384765625
tensor(9972.9395, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9972.939453125
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9972.9384765625
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9972.939453125
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9395, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9972.9384765625
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9972.9384765625
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9972.9365234375
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9972.9384765625
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9972.9384765625
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9972.94140625
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9414, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9972.9365234375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9365, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9972.9462890625
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9463, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9972.9375
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9972.935546875
tensor(9972.9365, grad_fn=<NegBackward0>) tensor(9972.9355, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9972.9375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9375, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9972.986328125
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9863, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9972.984375
tensor(9972.9355, grad_fn=<NegBackward0>) tensor(9972.9844, grad_fn=<NegBackward0>)
3
pi: tensor([[6.1857e-01, 3.8143e-01],
        [4.5723e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2231, 0.7769], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1842, 0.1573],
         [0.7247, 0.1344]],

        [[0.6655, 0.1560],
         [0.5319, 0.6422]],

        [[0.5929, 0.1677],
         [0.6742, 0.6059]],

        [[0.6372, 0.1591],
         [0.5342, 0.7153]],

        [[0.6235, 0.0992],
         [0.6013, 0.5193]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005099594640613479
Average Adjusted Rand Index: -9.369493421773029e-05
[-0.0005099594640613479, -0.0005099594640613479] [-9.369493421773029e-05, -9.369493421773029e-05] [9972.9375, 9972.9365234375]
-------------------------------------
This iteration is 6
True Objective function: Loss = -10053.410803183191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21113.5625
inf tensor(21113.5625, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9956.7490234375
tensor(21113.5625, grad_fn=<NegBackward0>) tensor(9956.7490, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9955.5283203125
tensor(9956.7490, grad_fn=<NegBackward0>) tensor(9955.5283, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9955.18359375
tensor(9955.5283, grad_fn=<NegBackward0>) tensor(9955.1836, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9954.9736328125
tensor(9955.1836, grad_fn=<NegBackward0>) tensor(9954.9736, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9954.7216796875
tensor(9954.9736, grad_fn=<NegBackward0>) tensor(9954.7217, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9954.3388671875
tensor(9954.7217, grad_fn=<NegBackward0>) tensor(9954.3389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9953.9365234375
tensor(9954.3389, grad_fn=<NegBackward0>) tensor(9953.9365, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9953.646484375
tensor(9953.9365, grad_fn=<NegBackward0>) tensor(9953.6465, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9953.3828125
tensor(9953.6465, grad_fn=<NegBackward0>) tensor(9953.3828, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9953.1240234375
tensor(9953.3828, grad_fn=<NegBackward0>) tensor(9953.1240, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9952.875
tensor(9953.1240, grad_fn=<NegBackward0>) tensor(9952.8750, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9952.634765625
tensor(9952.8750, grad_fn=<NegBackward0>) tensor(9952.6348, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9952.375
tensor(9952.6348, grad_fn=<NegBackward0>) tensor(9952.3750, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9952.1328125
tensor(9952.3750, grad_fn=<NegBackward0>) tensor(9952.1328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9951.91015625
tensor(9952.1328, grad_fn=<NegBackward0>) tensor(9951.9102, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9951.6953125
tensor(9951.9102, grad_fn=<NegBackward0>) tensor(9951.6953, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9951.44921875
tensor(9951.6953, grad_fn=<NegBackward0>) tensor(9951.4492, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9951.1279296875
tensor(9951.4492, grad_fn=<NegBackward0>) tensor(9951.1279, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9950.7392578125
tensor(9951.1279, grad_fn=<NegBackward0>) tensor(9950.7393, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9950.3681640625
tensor(9950.7393, grad_fn=<NegBackward0>) tensor(9950.3682, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9948.96484375
tensor(9950.3682, grad_fn=<NegBackward0>) tensor(9948.9648, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9942.2998046875
tensor(9948.9648, grad_fn=<NegBackward0>) tensor(9942.2998, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9942.111328125
tensor(9942.2998, grad_fn=<NegBackward0>) tensor(9942.1113, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9942.0576171875
tensor(9942.1113, grad_fn=<NegBackward0>) tensor(9942.0576, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9942.0283203125
tensor(9942.0576, grad_fn=<NegBackward0>) tensor(9942.0283, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9942.0107421875
tensor(9942.0283, grad_fn=<NegBackward0>) tensor(9942.0107, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9941.99609375
tensor(9942.0107, grad_fn=<NegBackward0>) tensor(9941.9961, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9941.982421875
tensor(9941.9961, grad_fn=<NegBackward0>) tensor(9941.9824, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9941.9716796875
tensor(9941.9824, grad_fn=<NegBackward0>) tensor(9941.9717, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9941.9619140625
tensor(9941.9717, grad_fn=<NegBackward0>) tensor(9941.9619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9941.951171875
tensor(9941.9619, grad_fn=<NegBackward0>) tensor(9941.9512, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9941.943359375
tensor(9941.9512, grad_fn=<NegBackward0>) tensor(9941.9434, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9941.9296875
tensor(9941.9434, grad_fn=<NegBackward0>) tensor(9941.9297, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9941.921875
tensor(9941.9297, grad_fn=<NegBackward0>) tensor(9941.9219, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9941.912109375
tensor(9941.9219, grad_fn=<NegBackward0>) tensor(9941.9121, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9941.90625
tensor(9941.9121, grad_fn=<NegBackward0>) tensor(9941.9062, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9941.9013671875
tensor(9941.9062, grad_fn=<NegBackward0>) tensor(9941.9014, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9941.8955078125
tensor(9941.9014, grad_fn=<NegBackward0>) tensor(9941.8955, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9941.9130859375
tensor(9941.8955, grad_fn=<NegBackward0>) tensor(9941.9131, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9941.8876953125
tensor(9941.8955, grad_fn=<NegBackward0>) tensor(9941.8877, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9941.884765625
tensor(9941.8877, grad_fn=<NegBackward0>) tensor(9941.8848, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9941.8828125
tensor(9941.8848, grad_fn=<NegBackward0>) tensor(9941.8828, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9941.8798828125
tensor(9941.8828, grad_fn=<NegBackward0>) tensor(9941.8799, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9941.8779296875
tensor(9941.8799, grad_fn=<NegBackward0>) tensor(9941.8779, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9941.876953125
tensor(9941.8779, grad_fn=<NegBackward0>) tensor(9941.8770, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9941.8740234375
tensor(9941.8770, grad_fn=<NegBackward0>) tensor(9941.8740, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9941.873046875
tensor(9941.8740, grad_fn=<NegBackward0>) tensor(9941.8730, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9941.87109375
tensor(9941.8730, grad_fn=<NegBackward0>) tensor(9941.8711, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9941.8701171875
tensor(9941.8711, grad_fn=<NegBackward0>) tensor(9941.8701, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9941.8701171875
tensor(9941.8701, grad_fn=<NegBackward0>) tensor(9941.8701, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9941.8681640625
tensor(9941.8701, grad_fn=<NegBackward0>) tensor(9941.8682, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9941.8671875
tensor(9941.8682, grad_fn=<NegBackward0>) tensor(9941.8672, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9941.8720703125
tensor(9941.8672, grad_fn=<NegBackward0>) tensor(9941.8721, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9941.8671875
tensor(9941.8672, grad_fn=<NegBackward0>) tensor(9941.8672, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9941.8662109375
tensor(9941.8672, grad_fn=<NegBackward0>) tensor(9941.8662, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9941.8642578125
tensor(9941.8662, grad_fn=<NegBackward0>) tensor(9941.8643, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9941.865234375
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8652, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9941.8671875
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8672, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9941.8642578125
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8643, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9941.86328125
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8633, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9941.8623046875
tensor(9941.8633, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9941.861328125
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9941.8603515625
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8604, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9941.861328125
tensor(9941.8604, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9941.861328125
tensor(9941.8604, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9941.8603515625
tensor(9941.8604, grad_fn=<NegBackward0>) tensor(9941.8604, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9941.859375
tensor(9941.8604, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9941.86328125
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8633, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9941.859375
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9941.865234375
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8652, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9941.8583984375
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9941.859375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9941.859375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9941.8662109375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8662, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9941.857421875
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9941.8603515625
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8604, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9941.8583984375
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -9941.8779296875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8779, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -9941.8583984375
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9941.859375
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9941.861328125
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9941.8837890625
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8838, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9941.8564453125
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8564, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9941.857421875
tensor(9941.8564, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
1
pi: tensor([[7.8121e-01, 2.1879e-01],
        [5.5674e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 6.5858e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1306, 0.1179],
         [0.5614, 0.1945]],

        [[0.5826, 0.1528],
         [0.6921, 0.6235]],

        [[0.5857, 0.1375],
         [0.6822, 0.5442]],

        [[0.6278, 0.1062],
         [0.6693, 0.5285]],

        [[0.6907, 0.1023],
         [0.7036, 0.6736]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.003677385461885144
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 66
Adjusted Rand Index: 0.09338528260639611
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 87
Adjusted Rand Index: 0.5430303030303031
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 86
Adjusted Rand Index: 0.5128023649170617
Global Adjusted Rand Index: 0.13858302883851104
Average Adjusted Rand Index: 0.22910811301837514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22690.013671875
inf tensor(22690.0137, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9957.306640625
tensor(22690.0137, grad_fn=<NegBackward0>) tensor(9957.3066, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9955.669921875
tensor(9957.3066, grad_fn=<NegBackward0>) tensor(9955.6699, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9955.208984375
tensor(9955.6699, grad_fn=<NegBackward0>) tensor(9955.2090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9954.8896484375
tensor(9955.2090, grad_fn=<NegBackward0>) tensor(9954.8896, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9954.5009765625
tensor(9954.8896, grad_fn=<NegBackward0>) tensor(9954.5010, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9954.1337890625
tensor(9954.5010, grad_fn=<NegBackward0>) tensor(9954.1338, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9953.87109375
tensor(9954.1338, grad_fn=<NegBackward0>) tensor(9953.8711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9953.640625
tensor(9953.8711, grad_fn=<NegBackward0>) tensor(9953.6406, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9953.41796875
tensor(9953.6406, grad_fn=<NegBackward0>) tensor(9953.4180, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9953.1923828125
tensor(9953.4180, grad_fn=<NegBackward0>) tensor(9953.1924, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9952.9580078125
tensor(9953.1924, grad_fn=<NegBackward0>) tensor(9952.9580, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9952.7158203125
tensor(9952.9580, grad_fn=<NegBackward0>) tensor(9952.7158, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9952.4736328125
tensor(9952.7158, grad_fn=<NegBackward0>) tensor(9952.4736, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9952.228515625
tensor(9952.4736, grad_fn=<NegBackward0>) tensor(9952.2285, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9951.9951171875
tensor(9952.2285, grad_fn=<NegBackward0>) tensor(9951.9951, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9951.7744140625
tensor(9951.9951, grad_fn=<NegBackward0>) tensor(9951.7744, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9951.548828125
tensor(9951.7744, grad_fn=<NegBackward0>) tensor(9951.5488, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9951.2802734375
tensor(9951.5488, grad_fn=<NegBackward0>) tensor(9951.2803, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9950.95703125
tensor(9951.2803, grad_fn=<NegBackward0>) tensor(9950.9570, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9950.634765625
tensor(9950.9570, grad_fn=<NegBackward0>) tensor(9950.6348, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9950.310546875
tensor(9950.6348, grad_fn=<NegBackward0>) tensor(9950.3105, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9947.9208984375
tensor(9950.3105, grad_fn=<NegBackward0>) tensor(9947.9209, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9942.318359375
tensor(9947.9209, grad_fn=<NegBackward0>) tensor(9942.3184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9942.1669921875
tensor(9942.3184, grad_fn=<NegBackward0>) tensor(9942.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9942.1064453125
tensor(9942.1670, grad_fn=<NegBackward0>) tensor(9942.1064, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9942.0703125
tensor(9942.1064, grad_fn=<NegBackward0>) tensor(9942.0703, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9942.0458984375
tensor(9942.0703, grad_fn=<NegBackward0>) tensor(9942.0459, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9942.0283203125
tensor(9942.0459, grad_fn=<NegBackward0>) tensor(9942.0283, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9942.013671875
tensor(9942.0283, grad_fn=<NegBackward0>) tensor(9942.0137, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9942.0009765625
tensor(9942.0137, grad_fn=<NegBackward0>) tensor(9942.0010, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9941.98828125
tensor(9942.0010, grad_fn=<NegBackward0>) tensor(9941.9883, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9941.978515625
tensor(9941.9883, grad_fn=<NegBackward0>) tensor(9941.9785, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9941.9677734375
tensor(9941.9785, grad_fn=<NegBackward0>) tensor(9941.9678, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9941.95703125
tensor(9941.9678, grad_fn=<NegBackward0>) tensor(9941.9570, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9941.947265625
tensor(9941.9570, grad_fn=<NegBackward0>) tensor(9941.9473, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9941.9365234375
tensor(9941.9473, grad_fn=<NegBackward0>) tensor(9941.9365, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9941.927734375
tensor(9941.9365, grad_fn=<NegBackward0>) tensor(9941.9277, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9941.919921875
tensor(9941.9277, grad_fn=<NegBackward0>) tensor(9941.9199, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9941.9130859375
tensor(9941.9199, grad_fn=<NegBackward0>) tensor(9941.9131, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9941.9072265625
tensor(9941.9131, grad_fn=<NegBackward0>) tensor(9941.9072, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9941.90234375
tensor(9941.9072, grad_fn=<NegBackward0>) tensor(9941.9023, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9941.8974609375
tensor(9941.9023, grad_fn=<NegBackward0>) tensor(9941.8975, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9941.8935546875
tensor(9941.8975, grad_fn=<NegBackward0>) tensor(9941.8936, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9941.8896484375
tensor(9941.8936, grad_fn=<NegBackward0>) tensor(9941.8896, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9941.8876953125
tensor(9941.8896, grad_fn=<NegBackward0>) tensor(9941.8877, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9941.884765625
tensor(9941.8877, grad_fn=<NegBackward0>) tensor(9941.8848, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9941.8818359375
tensor(9941.8848, grad_fn=<NegBackward0>) tensor(9941.8818, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9941.8798828125
tensor(9941.8818, grad_fn=<NegBackward0>) tensor(9941.8799, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9941.87890625
tensor(9941.8799, grad_fn=<NegBackward0>) tensor(9941.8789, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9941.876953125
tensor(9941.8789, grad_fn=<NegBackward0>) tensor(9941.8770, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9941.875
tensor(9941.8770, grad_fn=<NegBackward0>) tensor(9941.8750, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9941.873046875
tensor(9941.8750, grad_fn=<NegBackward0>) tensor(9941.8730, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9941.87109375
tensor(9941.8730, grad_fn=<NegBackward0>) tensor(9941.8711, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9941.8701171875
tensor(9941.8711, grad_fn=<NegBackward0>) tensor(9941.8701, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9941.8720703125
tensor(9941.8701, grad_fn=<NegBackward0>) tensor(9941.8721, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9941.869140625
tensor(9941.8701, grad_fn=<NegBackward0>) tensor(9941.8691, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9941.869140625
tensor(9941.8691, grad_fn=<NegBackward0>) tensor(9941.8691, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9941.8681640625
tensor(9941.8691, grad_fn=<NegBackward0>) tensor(9941.8682, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9941.8671875
tensor(9941.8682, grad_fn=<NegBackward0>) tensor(9941.8672, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9941.8662109375
tensor(9941.8672, grad_fn=<NegBackward0>) tensor(9941.8662, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9941.865234375
tensor(9941.8662, grad_fn=<NegBackward0>) tensor(9941.8652, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9941.87109375
tensor(9941.8652, grad_fn=<NegBackward0>) tensor(9941.8711, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9941.8642578125
tensor(9941.8652, grad_fn=<NegBackward0>) tensor(9941.8643, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9941.8642578125
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8643, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9941.8642578125
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8643, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9941.8623046875
tensor(9941.8643, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9941.86328125
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8633, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9941.8623046875
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9941.861328125
tensor(9941.8623, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9941.861328125
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9941.865234375
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8652, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9941.859375
tensor(9941.8613, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9941.861328125
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8613, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9941.859375
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9941.8603515625
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8604, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9941.8583984375
tensor(9941.8594, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9941.859375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9941.859375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9941.8623046875
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8623, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9941.859375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8594, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -9941.8701171875
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8701, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9941.8798828125
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8799, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9941.8583984375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8584, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9941.8701171875
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8701, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9941.8681640625
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8682, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9941.8681640625
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8682, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -9941.8740234375
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8740, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -9941.857421875
tensor(9941.8584, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9941.8779296875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8779, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9941.857421875
tensor(9941.8574, grad_fn=<NegBackward0>) tensor(9941.8574, grad_fn=<NegBackward0>)
pi: tensor([[7.8123e-01, 2.1877e-01],
        [6.5017e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.1843e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1310, 0.1308],
         [0.7019, 0.1944]],

        [[0.7254, 0.1527],
         [0.7065, 0.6234]],

        [[0.6727, 0.1375],
         [0.7258, 0.7024]],

        [[0.6952, 0.1060],
         [0.5448, 0.5040]],

        [[0.5750, 0.1023],
         [0.6943, 0.7080]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0020727689972979977
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 66
Adjusted Rand Index: 0.09338528260639611
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 87
Adjusted Rand Index: 0.5430303030303031
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 86
Adjusted Rand Index: 0.5128023649170617
Global Adjusted Rand Index: 0.14164296209324317
Average Adjusted Rand Index: 0.23025814391021177
[0.13858302883851104, 0.14164296209324317] [0.22910811301837514, 0.23025814391021177] [9941.8564453125, 9941.865234375]
-------------------------------------
This iteration is 7
True Objective function: Loss = -9964.218174084708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22439.8828125
inf tensor(22439.8828, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9811.568359375
tensor(22439.8828, grad_fn=<NegBackward0>) tensor(9811.5684, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9810.46875
tensor(9811.5684, grad_fn=<NegBackward0>) tensor(9810.4688, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9810.11328125
tensor(9810.4688, grad_fn=<NegBackward0>) tensor(9810.1133, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9809.86328125
tensor(9810.1133, grad_fn=<NegBackward0>) tensor(9809.8633, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9809.498046875
tensor(9809.8633, grad_fn=<NegBackward0>) tensor(9809.4980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9809.169921875
tensor(9809.4980, grad_fn=<NegBackward0>) tensor(9809.1699, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9809.0693359375
tensor(9809.1699, grad_fn=<NegBackward0>) tensor(9809.0693, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9809.0029296875
tensor(9809.0693, grad_fn=<NegBackward0>) tensor(9809.0029, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9808.9482421875
tensor(9809.0029, grad_fn=<NegBackward0>) tensor(9808.9482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9808.8994140625
tensor(9808.9482, grad_fn=<NegBackward0>) tensor(9808.8994, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9808.8525390625
tensor(9808.8994, grad_fn=<NegBackward0>) tensor(9808.8525, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9808.8046875
tensor(9808.8525, grad_fn=<NegBackward0>) tensor(9808.8047, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9808.7509765625
tensor(9808.8047, grad_fn=<NegBackward0>) tensor(9808.7510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9808.69140625
tensor(9808.7510, grad_fn=<NegBackward0>) tensor(9808.6914, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9808.62890625
tensor(9808.6914, grad_fn=<NegBackward0>) tensor(9808.6289, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9808.5478515625
tensor(9808.6289, grad_fn=<NegBackward0>) tensor(9808.5479, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9808.4599609375
tensor(9808.5479, grad_fn=<NegBackward0>) tensor(9808.4600, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9808.3642578125
tensor(9808.4600, grad_fn=<NegBackward0>) tensor(9808.3643, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9808.267578125
tensor(9808.3643, grad_fn=<NegBackward0>) tensor(9808.2676, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9808.1728515625
tensor(9808.2676, grad_fn=<NegBackward0>) tensor(9808.1729, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9808.0849609375
tensor(9808.1729, grad_fn=<NegBackward0>) tensor(9808.0850, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9808.0302734375
tensor(9808.0850, grad_fn=<NegBackward0>) tensor(9808.0303, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9808.001953125
tensor(9808.0303, grad_fn=<NegBackward0>) tensor(9808.0020, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9807.9794921875
tensor(9808.0020, grad_fn=<NegBackward0>) tensor(9807.9795, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9807.935546875
tensor(9807.9795, grad_fn=<NegBackward0>) tensor(9807.9355, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9807.849609375
tensor(9807.9355, grad_fn=<NegBackward0>) tensor(9807.8496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9807.8408203125
tensor(9807.8496, grad_fn=<NegBackward0>) tensor(9807.8408, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9807.82421875
tensor(9807.8408, grad_fn=<NegBackward0>) tensor(9807.8242, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9807.8310546875
tensor(9807.8242, grad_fn=<NegBackward0>) tensor(9807.8311, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9807.81640625
tensor(9807.8242, grad_fn=<NegBackward0>) tensor(9807.8164, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9807.8154296875
tensor(9807.8164, grad_fn=<NegBackward0>) tensor(9807.8154, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9807.8134765625
tensor(9807.8154, grad_fn=<NegBackward0>) tensor(9807.8135, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9807.8115234375
tensor(9807.8135, grad_fn=<NegBackward0>) tensor(9807.8115, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9807.810546875
tensor(9807.8115, grad_fn=<NegBackward0>) tensor(9807.8105, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9807.8095703125
tensor(9807.8105, grad_fn=<NegBackward0>) tensor(9807.8096, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9807.8095703125
tensor(9807.8096, grad_fn=<NegBackward0>) tensor(9807.8096, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9807.8046875
tensor(9807.8096, grad_fn=<NegBackward0>) tensor(9807.8047, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9807.8046875
tensor(9807.8047, grad_fn=<NegBackward0>) tensor(9807.8047, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9807.802734375
tensor(9807.8047, grad_fn=<NegBackward0>) tensor(9807.8027, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9807.80078125
tensor(9807.8027, grad_fn=<NegBackward0>) tensor(9807.8008, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9807.7978515625
tensor(9807.8008, grad_fn=<NegBackward0>) tensor(9807.7979, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9807.794921875
tensor(9807.7979, grad_fn=<NegBackward0>) tensor(9807.7949, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9807.796875
tensor(9807.7949, grad_fn=<NegBackward0>) tensor(9807.7969, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9807.7841796875
tensor(9807.7949, grad_fn=<NegBackward0>) tensor(9807.7842, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9807.7763671875
tensor(9807.7842, grad_fn=<NegBackward0>) tensor(9807.7764, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9807.75390625
tensor(9807.7764, grad_fn=<NegBackward0>) tensor(9807.7539, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9807.708984375
tensor(9807.7539, grad_fn=<NegBackward0>) tensor(9807.7090, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9807.60546875
tensor(9807.7090, grad_fn=<NegBackward0>) tensor(9807.6055, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9807.3447265625
tensor(9807.6055, grad_fn=<NegBackward0>) tensor(9807.3447, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9807.22265625
tensor(9807.3447, grad_fn=<NegBackward0>) tensor(9807.2227, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9807.18359375
tensor(9807.2227, grad_fn=<NegBackward0>) tensor(9807.1836, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9807.1494140625
tensor(9807.1836, grad_fn=<NegBackward0>) tensor(9807.1494, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9807.0263671875
tensor(9807.1494, grad_fn=<NegBackward0>) tensor(9807.0264, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9806.8212890625
tensor(9807.0264, grad_fn=<NegBackward0>) tensor(9806.8213, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9806.6484375
tensor(9806.8213, grad_fn=<NegBackward0>) tensor(9806.6484, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9805.291015625
tensor(9806.6484, grad_fn=<NegBackward0>) tensor(9805.2910, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9805.26171875
tensor(9805.2910, grad_fn=<NegBackward0>) tensor(9805.2617, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9805.25390625
tensor(9805.2617, grad_fn=<NegBackward0>) tensor(9805.2539, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9805.2490234375
tensor(9805.2539, grad_fn=<NegBackward0>) tensor(9805.2490, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9805.2451171875
tensor(9805.2490, grad_fn=<NegBackward0>) tensor(9805.2451, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9805.2431640625
tensor(9805.2451, grad_fn=<NegBackward0>) tensor(9805.2432, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9805.2412109375
tensor(9805.2432, grad_fn=<NegBackward0>) tensor(9805.2412, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9805.240234375
tensor(9805.2412, grad_fn=<NegBackward0>) tensor(9805.2402, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9805.23828125
tensor(9805.2402, grad_fn=<NegBackward0>) tensor(9805.2383, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9805.2373046875
tensor(9805.2383, grad_fn=<NegBackward0>) tensor(9805.2373, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9805.236328125
tensor(9805.2373, grad_fn=<NegBackward0>) tensor(9805.2363, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9805.236328125
tensor(9805.2363, grad_fn=<NegBackward0>) tensor(9805.2363, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9805.236328125
tensor(9805.2363, grad_fn=<NegBackward0>) tensor(9805.2363, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9805.2353515625
tensor(9805.2363, grad_fn=<NegBackward0>) tensor(9805.2354, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9805.2333984375
tensor(9805.2354, grad_fn=<NegBackward0>) tensor(9805.2334, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9805.2333984375
tensor(9805.2334, grad_fn=<NegBackward0>) tensor(9805.2334, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9805.232421875
tensor(9805.2334, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9805.232421875
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9805.232421875
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9805.232421875
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9805.232421875
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9805.2314453125
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2314, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9805.23046875
tensor(9805.2314, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9805.23046875
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9805.2353515625
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.2354, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9805.2294921875
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.2295, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9805.2294921875
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2295, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9805.2294921875
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2295, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9805.23046875
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9805.228515625
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2285, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9805.2626953125
tensor(9805.2285, grad_fn=<NegBackward0>) tensor(9805.2627, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9804.736328125
tensor(9805.2285, grad_fn=<NegBackward0>) tensor(9804.7363, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9804.7412109375
tensor(9804.7363, grad_fn=<NegBackward0>) tensor(9804.7412, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9804.73046875
tensor(9804.7363, grad_fn=<NegBackward0>) tensor(9804.7305, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9804.7548828125
tensor(9804.7305, grad_fn=<NegBackward0>) tensor(9804.7549, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9804.7294921875
tensor(9804.7305, grad_fn=<NegBackward0>) tensor(9804.7295, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9804.728515625
tensor(9804.7295, grad_fn=<NegBackward0>) tensor(9804.7285, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9804.7275390625
tensor(9804.7285, grad_fn=<NegBackward0>) tensor(9804.7275, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9804.7275390625
tensor(9804.7275, grad_fn=<NegBackward0>) tensor(9804.7275, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9804.728515625
tensor(9804.7275, grad_fn=<NegBackward0>) tensor(9804.7285, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9804.7578125
tensor(9804.7275, grad_fn=<NegBackward0>) tensor(9804.7578, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -9804.765625
tensor(9804.7275, grad_fn=<NegBackward0>) tensor(9804.7656, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -9804.7265625
tensor(9804.7275, grad_fn=<NegBackward0>) tensor(9804.7266, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9804.7265625
tensor(9804.7266, grad_fn=<NegBackward0>) tensor(9804.7266, grad_fn=<NegBackward0>)
pi: tensor([[5.4449e-06, 9.9999e-01],
        [9.9998e-01, 2.2357e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0404, 0.9596], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1408, 0.1871],
         [0.5270, 0.1291]],

        [[0.6104, 0.1144],
         [0.6762, 0.5616]],

        [[0.6832, 0.1373],
         [0.7267, 0.6072]],

        [[0.6491, 0.1119],
         [0.6762, 0.5022]],

        [[0.6176, 0.2126],
         [0.5167, 0.7211]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.008844375641612072
Global Adjusted Rand Index: 0.0037969074003396143
Average Adjusted Rand Index: 0.004515000632232144
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22917.00390625
inf tensor(22917.0039, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9811.5048828125
tensor(22917.0039, grad_fn=<NegBackward0>) tensor(9811.5049, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9810.2802734375
tensor(9811.5049, grad_fn=<NegBackward0>) tensor(9810.2803, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9810.01953125
tensor(9810.2803, grad_fn=<NegBackward0>) tensor(9810.0195, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9809.8955078125
tensor(9810.0195, grad_fn=<NegBackward0>) tensor(9809.8955, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9809.779296875
tensor(9809.8955, grad_fn=<NegBackward0>) tensor(9809.7793, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9809.6318359375
tensor(9809.7793, grad_fn=<NegBackward0>) tensor(9809.6318, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9809.4365234375
tensor(9809.6318, grad_fn=<NegBackward0>) tensor(9809.4365, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9809.2578125
tensor(9809.4365, grad_fn=<NegBackward0>) tensor(9809.2578, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9809.14453125
tensor(9809.2578, grad_fn=<NegBackward0>) tensor(9809.1445, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9809.0576171875
tensor(9809.1445, grad_fn=<NegBackward0>) tensor(9809.0576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9808.9853515625
tensor(9809.0576, grad_fn=<NegBackward0>) tensor(9808.9854, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9808.9189453125
tensor(9808.9854, grad_fn=<NegBackward0>) tensor(9808.9189, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9808.859375
tensor(9808.9189, grad_fn=<NegBackward0>) tensor(9808.8594, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9808.806640625
tensor(9808.8594, grad_fn=<NegBackward0>) tensor(9808.8066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9808.7568359375
tensor(9808.8066, grad_fn=<NegBackward0>) tensor(9808.7568, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9808.7060546875
tensor(9808.7568, grad_fn=<NegBackward0>) tensor(9808.7061, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9808.6533203125
tensor(9808.7061, grad_fn=<NegBackward0>) tensor(9808.6533, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9808.59765625
tensor(9808.6533, grad_fn=<NegBackward0>) tensor(9808.5977, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9808.5341796875
tensor(9808.5977, grad_fn=<NegBackward0>) tensor(9808.5342, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9808.466796875
tensor(9808.5342, grad_fn=<NegBackward0>) tensor(9808.4668, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9808.39453125
tensor(9808.4668, grad_fn=<NegBackward0>) tensor(9808.3945, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9808.322265625
tensor(9808.3945, grad_fn=<NegBackward0>) tensor(9808.3223, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9808.2529296875
tensor(9808.3223, grad_fn=<NegBackward0>) tensor(9808.2529, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9808.1943359375
tensor(9808.2529, grad_fn=<NegBackward0>) tensor(9808.1943, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9808.14453125
tensor(9808.1943, grad_fn=<NegBackward0>) tensor(9808.1445, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9808.103515625
tensor(9808.1445, grad_fn=<NegBackward0>) tensor(9808.1035, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9808.07421875
tensor(9808.1035, grad_fn=<NegBackward0>) tensor(9808.0742, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9808.0517578125
tensor(9808.0742, grad_fn=<NegBackward0>) tensor(9808.0518, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9808.0322265625
tensor(9808.0518, grad_fn=<NegBackward0>) tensor(9808.0322, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9808.015625
tensor(9808.0322, grad_fn=<NegBackward0>) tensor(9808.0156, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9807.998046875
tensor(9808.0156, grad_fn=<NegBackward0>) tensor(9807.9980, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9807.9580078125
tensor(9807.9980, grad_fn=<NegBackward0>) tensor(9807.9580, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9807.873046875
tensor(9807.9580, grad_fn=<NegBackward0>) tensor(9807.8730, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9807.8447265625
tensor(9807.8730, grad_fn=<NegBackward0>) tensor(9807.8447, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9807.828125
tensor(9807.8447, grad_fn=<NegBackward0>) tensor(9807.8281, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9807.818359375
tensor(9807.8281, grad_fn=<NegBackward0>) tensor(9807.8184, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9807.8095703125
tensor(9807.8184, grad_fn=<NegBackward0>) tensor(9807.8096, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9807.80078125
tensor(9807.8096, grad_fn=<NegBackward0>) tensor(9807.8008, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9807.7900390625
tensor(9807.8008, grad_fn=<NegBackward0>) tensor(9807.7900, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9807.77734375
tensor(9807.7900, grad_fn=<NegBackward0>) tensor(9807.7773, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9807.7587890625
tensor(9807.7773, grad_fn=<NegBackward0>) tensor(9807.7588, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9807.724609375
tensor(9807.7588, grad_fn=<NegBackward0>) tensor(9807.7246, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9807.66015625
tensor(9807.7246, grad_fn=<NegBackward0>) tensor(9807.6602, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9807.533203125
tensor(9807.6602, grad_fn=<NegBackward0>) tensor(9807.5332, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9807.3916015625
tensor(9807.5332, grad_fn=<NegBackward0>) tensor(9807.3916, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9807.30078125
tensor(9807.3916, grad_fn=<NegBackward0>) tensor(9807.3008, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9807.2509765625
tensor(9807.3008, grad_fn=<NegBackward0>) tensor(9807.2510, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9807.22265625
tensor(9807.2510, grad_fn=<NegBackward0>) tensor(9807.2227, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9807.2041015625
tensor(9807.2227, grad_fn=<NegBackward0>) tensor(9807.2041, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9807.189453125
tensor(9807.2041, grad_fn=<NegBackward0>) tensor(9807.1895, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9807.1748046875
tensor(9807.1895, grad_fn=<NegBackward0>) tensor(9807.1748, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9807.154296875
tensor(9807.1748, grad_fn=<NegBackward0>) tensor(9807.1543, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9807.1142578125
tensor(9807.1543, grad_fn=<NegBackward0>) tensor(9807.1143, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9807.01171875
tensor(9807.1143, grad_fn=<NegBackward0>) tensor(9807.0117, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9806.89453125
tensor(9807.0117, grad_fn=<NegBackward0>) tensor(9806.8945, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9806.814453125
tensor(9806.8945, grad_fn=<NegBackward0>) tensor(9806.8145, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9806.7412109375
tensor(9806.8145, grad_fn=<NegBackward0>) tensor(9806.7412, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9805.5654296875
tensor(9806.7412, grad_fn=<NegBackward0>) tensor(9805.5654, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9805.3017578125
tensor(9805.5654, grad_fn=<NegBackward0>) tensor(9805.3018, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9805.2822265625
tensor(9805.3018, grad_fn=<NegBackward0>) tensor(9805.2822, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9805.2734375
tensor(9805.2822, grad_fn=<NegBackward0>) tensor(9805.2734, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9805.265625
tensor(9805.2734, grad_fn=<NegBackward0>) tensor(9805.2656, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9805.2626953125
tensor(9805.2656, grad_fn=<NegBackward0>) tensor(9805.2627, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9805.2578125
tensor(9805.2627, grad_fn=<NegBackward0>) tensor(9805.2578, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9805.2548828125
tensor(9805.2578, grad_fn=<NegBackward0>) tensor(9805.2549, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9805.2529296875
tensor(9805.2549, grad_fn=<NegBackward0>) tensor(9805.2529, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9805.25
tensor(9805.2529, grad_fn=<NegBackward0>) tensor(9805.2500, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9805.248046875
tensor(9805.2500, grad_fn=<NegBackward0>) tensor(9805.2480, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9805.24609375
tensor(9805.2480, grad_fn=<NegBackward0>) tensor(9805.2461, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9805.2451171875
tensor(9805.2461, grad_fn=<NegBackward0>) tensor(9805.2451, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9805.244140625
tensor(9805.2451, grad_fn=<NegBackward0>) tensor(9805.2441, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9805.2421875
tensor(9805.2441, grad_fn=<NegBackward0>) tensor(9805.2422, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9805.2412109375
tensor(9805.2422, grad_fn=<NegBackward0>) tensor(9805.2412, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9805.240234375
tensor(9805.2412, grad_fn=<NegBackward0>) tensor(9805.2402, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9805.23828125
tensor(9805.2402, grad_fn=<NegBackward0>) tensor(9805.2383, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9805.2373046875
tensor(9805.2383, grad_fn=<NegBackward0>) tensor(9805.2373, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9805.23828125
tensor(9805.2373, grad_fn=<NegBackward0>) tensor(9805.2383, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9805.2373046875
tensor(9805.2373, grad_fn=<NegBackward0>) tensor(9805.2373, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9805.236328125
tensor(9805.2373, grad_fn=<NegBackward0>) tensor(9805.2363, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9805.236328125
tensor(9805.2363, grad_fn=<NegBackward0>) tensor(9805.2363, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9805.234375
tensor(9805.2363, grad_fn=<NegBackward0>) tensor(9805.2344, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9805.234375
tensor(9805.2344, grad_fn=<NegBackward0>) tensor(9805.2344, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9805.2353515625
tensor(9805.2344, grad_fn=<NegBackward0>) tensor(9805.2354, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9805.2412109375
tensor(9805.2344, grad_fn=<NegBackward0>) tensor(9805.2412, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9805.2333984375
tensor(9805.2344, grad_fn=<NegBackward0>) tensor(9805.2334, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9805.2412109375
tensor(9805.2334, grad_fn=<NegBackward0>) tensor(9805.2412, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9805.232421875
tensor(9805.2334, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9805.2333984375
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2334, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9805.2314453125
tensor(9805.2324, grad_fn=<NegBackward0>) tensor(9805.2314, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9805.3017578125
tensor(9805.2314, grad_fn=<NegBackward0>) tensor(9805.3018, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9805.2314453125
tensor(9805.2314, grad_fn=<NegBackward0>) tensor(9805.2314, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9805.240234375
tensor(9805.2314, grad_fn=<NegBackward0>) tensor(9805.2402, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9805.23046875
tensor(9805.2314, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9805.232421875
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.2324, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9805.2294921875
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.2295, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9805.2353515625
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2354, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -9805.2294921875
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2295, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9805.3603515625
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.3604, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9805.23046875
tensor(9805.2295, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
2
pi: tensor([[2.2256e-05, 9.9998e-01],
        [9.9969e-01, 3.0670e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0712, 0.9288], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1441, 0.1586],
         [0.5541, 0.1303]],

        [[0.5072, 0.0970],
         [0.6417, 0.6895]],

        [[0.6933, 0.1134],
         [0.5586, 0.6076]],

        [[0.6008, 0.1060],
         [0.6862, 0.6352]],

        [[0.7065, 0.1858],
         [0.6297, 0.6732]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0019209323236274677
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05107700779685254
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.007272876449007818
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.004682108332720131
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0036350439655890437
Global Adjusted Rand Index: 0.004420403096533698
Average Adjusted Rand Index: 0.008167226931417235
[0.0037969074003396143, 0.004420403096533698] [0.004515000632232144, 0.008167226931417235] [9804.7314453125, 9805.228515625]
-------------------------------------
This iteration is 8
True Objective function: Loss = -10288.182656987165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23474.634765625
inf tensor(23474.6348, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10200.126953125
tensor(23474.6348, grad_fn=<NegBackward0>) tensor(10200.1270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10197.3896484375
tensor(10200.1270, grad_fn=<NegBackward0>) tensor(10197.3896, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10194.3369140625
tensor(10197.3896, grad_fn=<NegBackward0>) tensor(10194.3369, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10192.8623046875
tensor(10194.3369, grad_fn=<NegBackward0>) tensor(10192.8623, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10191.3203125
tensor(10192.8623, grad_fn=<NegBackward0>) tensor(10191.3203, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10189.76953125
tensor(10191.3203, grad_fn=<NegBackward0>) tensor(10189.7695, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10189.19140625
tensor(10189.7695, grad_fn=<NegBackward0>) tensor(10189.1914, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10188.9501953125
tensor(10189.1914, grad_fn=<NegBackward0>) tensor(10188.9502, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10188.8359375
tensor(10188.9502, grad_fn=<NegBackward0>) tensor(10188.8359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10188.7763671875
tensor(10188.8359, grad_fn=<NegBackward0>) tensor(10188.7764, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10188.7392578125
tensor(10188.7764, grad_fn=<NegBackward0>) tensor(10188.7393, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10188.7138671875
tensor(10188.7393, grad_fn=<NegBackward0>) tensor(10188.7139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10188.697265625
tensor(10188.7139, grad_fn=<NegBackward0>) tensor(10188.6973, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10188.68359375
tensor(10188.6973, grad_fn=<NegBackward0>) tensor(10188.6836, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10188.6748046875
tensor(10188.6836, grad_fn=<NegBackward0>) tensor(10188.6748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10188.66796875
tensor(10188.6748, grad_fn=<NegBackward0>) tensor(10188.6680, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10188.6630859375
tensor(10188.6680, grad_fn=<NegBackward0>) tensor(10188.6631, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10188.6572265625
tensor(10188.6631, grad_fn=<NegBackward0>) tensor(10188.6572, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10188.6533203125
tensor(10188.6572, grad_fn=<NegBackward0>) tensor(10188.6533, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10188.650390625
tensor(10188.6533, grad_fn=<NegBackward0>) tensor(10188.6504, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10188.6484375
tensor(10188.6504, grad_fn=<NegBackward0>) tensor(10188.6484, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10188.646484375
tensor(10188.6484, grad_fn=<NegBackward0>) tensor(10188.6465, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10188.64453125
tensor(10188.6465, grad_fn=<NegBackward0>) tensor(10188.6445, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10188.6416015625
tensor(10188.6445, grad_fn=<NegBackward0>) tensor(10188.6416, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10188.6396484375
tensor(10188.6416, grad_fn=<NegBackward0>) tensor(10188.6396, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10188.638671875
tensor(10188.6396, grad_fn=<NegBackward0>) tensor(10188.6387, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10188.6376953125
tensor(10188.6387, grad_fn=<NegBackward0>) tensor(10188.6377, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10188.6357421875
tensor(10188.6377, grad_fn=<NegBackward0>) tensor(10188.6357, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10188.6357421875
tensor(10188.6357, grad_fn=<NegBackward0>) tensor(10188.6357, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10188.634765625
tensor(10188.6357, grad_fn=<NegBackward0>) tensor(10188.6348, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10188.6337890625
tensor(10188.6348, grad_fn=<NegBackward0>) tensor(10188.6338, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10188.6328125
tensor(10188.6338, grad_fn=<NegBackward0>) tensor(10188.6328, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10188.6318359375
tensor(10188.6328, grad_fn=<NegBackward0>) tensor(10188.6318, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10188.6318359375
tensor(10188.6318, grad_fn=<NegBackward0>) tensor(10188.6318, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10188.630859375
tensor(10188.6318, grad_fn=<NegBackward0>) tensor(10188.6309, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10188.630859375
tensor(10188.6309, grad_fn=<NegBackward0>) tensor(10188.6309, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10188.630859375
tensor(10188.6309, grad_fn=<NegBackward0>) tensor(10188.6309, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10188.6298828125
tensor(10188.6309, grad_fn=<NegBackward0>) tensor(10188.6299, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10188.6279296875
tensor(10188.6299, grad_fn=<NegBackward0>) tensor(10188.6279, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10188.62890625
tensor(10188.6279, grad_fn=<NegBackward0>) tensor(10188.6289, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10188.6279296875
tensor(10188.6279, grad_fn=<NegBackward0>) tensor(10188.6279, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10188.626953125
tensor(10188.6279, grad_fn=<NegBackward0>) tensor(10188.6270, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10188.6279296875
tensor(10188.6270, grad_fn=<NegBackward0>) tensor(10188.6279, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10188.6259765625
tensor(10188.6270, grad_fn=<NegBackward0>) tensor(10188.6260, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10188.6259765625
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6260, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10188.626953125
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6270, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10188.626953125
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6270, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -10188.6259765625
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6260, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10188.626953125
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6270, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10188.6259765625
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6260, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10188.625
tensor(10188.6260, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10188.625
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10188.625
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10188.625
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10188.6259765625
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6260, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10188.625
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10188.6240234375
tensor(10188.6250, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10188.6240234375
tensor(10188.6240, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10188.6240234375
tensor(10188.6240, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10188.623046875
tensor(10188.6240, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10188.625
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10188.6240234375
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10188.6240234375
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10188.6279296875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6279, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10188.625
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6250, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10188.6240234375
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10188.623046875
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10188.6220703125
tensor(10188.6230, grad_fn=<NegBackward0>) tensor(10188.6221, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10188.623046875
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10188.6240234375
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6240, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10188.6220703125
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6221, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10188.623046875
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10188.6337890625
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6338, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10188.623046875
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10188.623046875
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6230, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10188.6298828125
tensor(10188.6221, grad_fn=<NegBackward0>) tensor(10188.6299, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[3.8912e-01, 6.1088e-01],
        [6.0123e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1831, 0.8169], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1578, 0.0861],
         [0.6879, 0.1487]],

        [[0.5446, 0.0708],
         [0.5459, 0.5021]],

        [[0.7277, 0.1998],
         [0.6756, 0.5919]],

        [[0.6151, 0.2923],
         [0.5133, 0.5737]],

        [[0.6443, 0.1414],
         [0.6315, 0.5848]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12080808080808081
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.035323271006983556
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.017795508661327984
Average Adjusted Rand Index: 0.029499776003867433
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22710.232421875
inf tensor(22710.2324, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10203.259765625
tensor(22710.2324, grad_fn=<NegBackward0>) tensor(10203.2598, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10199.6865234375
tensor(10203.2598, grad_fn=<NegBackward0>) tensor(10199.6865, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10197.4560546875
tensor(10199.6865, grad_fn=<NegBackward0>) tensor(10197.4561, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10196.0615234375
tensor(10197.4561, grad_fn=<NegBackward0>) tensor(10196.0615, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10195.2685546875
tensor(10196.0615, grad_fn=<NegBackward0>) tensor(10195.2686, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10194.7041015625
tensor(10195.2686, grad_fn=<NegBackward0>) tensor(10194.7041, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10194.2255859375
tensor(10194.7041, grad_fn=<NegBackward0>) tensor(10194.2256, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10193.8212890625
tensor(10194.2256, grad_fn=<NegBackward0>) tensor(10193.8213, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10193.5009765625
tensor(10193.8213, grad_fn=<NegBackward0>) tensor(10193.5010, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10193.26171875
tensor(10193.5010, grad_fn=<NegBackward0>) tensor(10193.2617, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10193.0908203125
tensor(10193.2617, grad_fn=<NegBackward0>) tensor(10193.0908, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10192.9599609375
tensor(10193.0908, grad_fn=<NegBackward0>) tensor(10192.9600, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10192.8564453125
tensor(10192.9600, grad_fn=<NegBackward0>) tensor(10192.8564, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10192.748046875
tensor(10192.8564, grad_fn=<NegBackward0>) tensor(10192.7480, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10192.470703125
tensor(10192.7480, grad_fn=<NegBackward0>) tensor(10192.4707, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10192.1962890625
tensor(10192.4707, grad_fn=<NegBackward0>) tensor(10192.1963, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10192.0732421875
tensor(10192.1963, grad_fn=<NegBackward0>) tensor(10192.0732, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10191.9794921875
tensor(10192.0732, grad_fn=<NegBackward0>) tensor(10191.9795, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10191.90625
tensor(10191.9795, grad_fn=<NegBackward0>) tensor(10191.9062, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10191.859375
tensor(10191.9062, grad_fn=<NegBackward0>) tensor(10191.8594, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10191.830078125
tensor(10191.8594, grad_fn=<NegBackward0>) tensor(10191.8301, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10191.8076171875
tensor(10191.8301, grad_fn=<NegBackward0>) tensor(10191.8076, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10191.7841796875
tensor(10191.8076, grad_fn=<NegBackward0>) tensor(10191.7842, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10191.75390625
tensor(10191.7842, grad_fn=<NegBackward0>) tensor(10191.7539, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10191.7109375
tensor(10191.7539, grad_fn=<NegBackward0>) tensor(10191.7109, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10191.6494140625
tensor(10191.7109, grad_fn=<NegBackward0>) tensor(10191.6494, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10191.546875
tensor(10191.6494, grad_fn=<NegBackward0>) tensor(10191.5469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10191.3359375
tensor(10191.5469, grad_fn=<NegBackward0>) tensor(10191.3359, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10190.466796875
tensor(10191.3359, grad_fn=<NegBackward0>) tensor(10190.4668, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10182.7333984375
tensor(10190.4668, grad_fn=<NegBackward0>) tensor(10182.7334, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10171.84375
tensor(10182.7334, grad_fn=<NegBackward0>) tensor(10171.8438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10171.765625
tensor(10171.8438, grad_fn=<NegBackward0>) tensor(10171.7656, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10171.7490234375
tensor(10171.7656, grad_fn=<NegBackward0>) tensor(10171.7490, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10171.7412109375
tensor(10171.7490, grad_fn=<NegBackward0>) tensor(10171.7412, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10171.7373046875
tensor(10171.7412, grad_fn=<NegBackward0>) tensor(10171.7373, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10171.734375
tensor(10171.7373, grad_fn=<NegBackward0>) tensor(10171.7344, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10171.732421875
tensor(10171.7344, grad_fn=<NegBackward0>) tensor(10171.7324, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10171.7314453125
tensor(10171.7324, grad_fn=<NegBackward0>) tensor(10171.7314, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10171.7294921875
tensor(10171.7314, grad_fn=<NegBackward0>) tensor(10171.7295, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10171.7294921875
tensor(10171.7295, grad_fn=<NegBackward0>) tensor(10171.7295, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10171.7294921875
tensor(10171.7295, grad_fn=<NegBackward0>) tensor(10171.7295, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10171.7333984375
tensor(10171.7295, grad_fn=<NegBackward0>) tensor(10171.7334, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10171.728515625
tensor(10171.7295, grad_fn=<NegBackward0>) tensor(10171.7285, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10171.728515625
tensor(10171.7285, grad_fn=<NegBackward0>) tensor(10171.7285, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10171.7314453125
tensor(10171.7285, grad_fn=<NegBackward0>) tensor(10171.7314, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10171.734375
tensor(10171.7285, grad_fn=<NegBackward0>) tensor(10171.7344, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10171.7275390625
tensor(10171.7285, grad_fn=<NegBackward0>) tensor(10171.7275, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10171.7275390625
tensor(10171.7275, grad_fn=<NegBackward0>) tensor(10171.7275, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10171.7265625
tensor(10171.7275, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10171.7314453125
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7314, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10171.728515625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7285, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10171.73046875
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7305, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10171.7265625
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10171.7255859375
tensor(10171.7266, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10171.732421875
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7324, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10171.7626953125
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7627, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10171.7783203125
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7783, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10171.732421875
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7324, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -10171.7392578125
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7393, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10171.7490234375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7490, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10171.7255859375
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7256, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10171.728515625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7285, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -10171.7265625
tensor(10171.7256, grad_fn=<NegBackward0>) tensor(10171.7266, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7170, 0.2830],
        [0.1903, 0.8097]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4251, 0.5749], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1565, 0.0871],
         [0.5380, 0.2021]],

        [[0.6352, 0.0991],
         [0.6228, 0.6990]],

        [[0.6144, 0.1134],
         [0.6932, 0.6753]],

        [[0.5788, 0.0987],
         [0.5573, 0.6672]],

        [[0.6405, 0.0962],
         [0.5214, 0.5314]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 82
Adjusted Rand Index: 0.4036363636363636
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 79
Adjusted Rand Index: 0.32962955646269254
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 83
Adjusted Rand Index: 0.42995705855246824
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5731727929990932
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 87
Adjusted Rand Index: 0.5430303030303031
Global Adjusted Rand Index: 0.4557271388132552
Average Adjusted Rand Index: 0.4558852149361841
[0.017795508661327984, 0.4557271388132552] [0.029499776003867433, 0.4558852149361841] [10188.6298828125, 10171.7265625]
-------------------------------------
This iteration is 9
True Objective function: Loss = -10219.736408972329
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21395.189453125
inf tensor(21395.1895, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10131.373046875
tensor(21395.1895, grad_fn=<NegBackward0>) tensor(10131.3730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10130.3505859375
tensor(10131.3730, grad_fn=<NegBackward0>) tensor(10130.3506, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10130.0107421875
tensor(10130.3506, grad_fn=<NegBackward0>) tensor(10130.0107, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10129.763671875
tensor(10130.0107, grad_fn=<NegBackward0>) tensor(10129.7637, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10129.6005859375
tensor(10129.7637, grad_fn=<NegBackward0>) tensor(10129.6006, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10129.4033203125
tensor(10129.6006, grad_fn=<NegBackward0>) tensor(10129.4033, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10128.9375
tensor(10129.4033, grad_fn=<NegBackward0>) tensor(10128.9375, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10128.4931640625
tensor(10128.9375, grad_fn=<NegBackward0>) tensor(10128.4932, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10127.873046875
tensor(10128.4932, grad_fn=<NegBackward0>) tensor(10127.8730, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10126.638671875
tensor(10127.8730, grad_fn=<NegBackward0>) tensor(10126.6387, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10125.9111328125
tensor(10126.6387, grad_fn=<NegBackward0>) tensor(10125.9111, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10125.626953125
tensor(10125.9111, grad_fn=<NegBackward0>) tensor(10125.6270, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10125.47265625
tensor(10125.6270, grad_fn=<NegBackward0>) tensor(10125.4727, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10125.2578125
tensor(10125.4727, grad_fn=<NegBackward0>) tensor(10125.2578, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10125.1171875
tensor(10125.2578, grad_fn=<NegBackward0>) tensor(10125.1172, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10124.70703125
tensor(10125.1172, grad_fn=<NegBackward0>) tensor(10124.7070, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10120.08203125
tensor(10124.7070, grad_fn=<NegBackward0>) tensor(10120.0820, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10119.3388671875
tensor(10120.0820, grad_fn=<NegBackward0>) tensor(10119.3389, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10119.2353515625
tensor(10119.3389, grad_fn=<NegBackward0>) tensor(10119.2354, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10119.1943359375
tensor(10119.2354, grad_fn=<NegBackward0>) tensor(10119.1943, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10119.1748046875
tensor(10119.1943, grad_fn=<NegBackward0>) tensor(10119.1748, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10119.1640625
tensor(10119.1748, grad_fn=<NegBackward0>) tensor(10119.1641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10119.1552734375
tensor(10119.1641, grad_fn=<NegBackward0>) tensor(10119.1553, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10119.1474609375
tensor(10119.1553, grad_fn=<NegBackward0>) tensor(10119.1475, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10119.142578125
tensor(10119.1475, grad_fn=<NegBackward0>) tensor(10119.1426, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10119.1328125
tensor(10119.1426, grad_fn=<NegBackward0>) tensor(10119.1328, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10119.123046875
tensor(10119.1328, grad_fn=<NegBackward0>) tensor(10119.1230, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10119.119140625
tensor(10119.1230, grad_fn=<NegBackward0>) tensor(10119.1191, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10119.115234375
tensor(10119.1191, grad_fn=<NegBackward0>) tensor(10119.1152, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10119.103515625
tensor(10119.1152, grad_fn=<NegBackward0>) tensor(10119.1035, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10119.1015625
tensor(10119.1035, grad_fn=<NegBackward0>) tensor(10119.1016, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10119.0986328125
tensor(10119.1016, grad_fn=<NegBackward0>) tensor(10119.0986, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10119.09765625
tensor(10119.0986, grad_fn=<NegBackward0>) tensor(10119.0977, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10119.095703125
tensor(10119.0977, grad_fn=<NegBackward0>) tensor(10119.0957, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10119.095703125
tensor(10119.0957, grad_fn=<NegBackward0>) tensor(10119.0957, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10119.0966796875
tensor(10119.0957, grad_fn=<NegBackward0>) tensor(10119.0967, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10119.0947265625
tensor(10119.0957, grad_fn=<NegBackward0>) tensor(10119.0947, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10119.0927734375
tensor(10119.0947, grad_fn=<NegBackward0>) tensor(10119.0928, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10119.09375
tensor(10119.0928, grad_fn=<NegBackward0>) tensor(10119.0938, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10119.0927734375
tensor(10119.0928, grad_fn=<NegBackward0>) tensor(10119.0928, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10119.0908203125
tensor(10119.0928, grad_fn=<NegBackward0>) tensor(10119.0908, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10119.091796875
tensor(10119.0908, grad_fn=<NegBackward0>) tensor(10119.0918, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10119.08984375
tensor(10119.0908, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10119.091796875
tensor(10119.0898, grad_fn=<NegBackward0>) tensor(10119.0918, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10119.0908203125
tensor(10119.0898, grad_fn=<NegBackward0>) tensor(10119.0908, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10119.08984375
tensor(10119.0898, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10119.08984375
tensor(10119.0898, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10119.0888671875
tensor(10119.0898, grad_fn=<NegBackward0>) tensor(10119.0889, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10119.0888671875
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0889, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10119.08984375
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10119.0888671875
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0889, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10119.0888671875
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0889, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10119.08984375
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10119.087890625
tensor(10119.0889, grad_fn=<NegBackward0>) tensor(10119.0879, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10119.08984375
tensor(10119.0879, grad_fn=<NegBackward0>) tensor(10119.0898, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10119.0859375
tensor(10119.0879, grad_fn=<NegBackward0>) tensor(10119.0859, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10119.083984375
tensor(10119.0859, grad_fn=<NegBackward0>) tensor(10119.0840, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10119.0859375
tensor(10119.0840, grad_fn=<NegBackward0>) tensor(10119.0859, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10119.083984375
tensor(10119.0840, grad_fn=<NegBackward0>) tensor(10119.0840, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10119.078125
tensor(10119.0840, grad_fn=<NegBackward0>) tensor(10119.0781, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10119.076171875
tensor(10119.0781, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10119.0771484375
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0771, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10119.0771484375
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0771, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10119.0771484375
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0771, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -10119.076171875
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10119.076171875
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10119.076171875
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10119.076171875
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10119.0771484375
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0771, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10119.0751953125
tensor(10119.0762, grad_fn=<NegBackward0>) tensor(10119.0752, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10119.0751953125
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0752, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10119.076171875
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10119.0751953125
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0752, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10119.076171875
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10119.0859375
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0859, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10119.076171875
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0762, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10119.0751953125
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0752, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10119.0263671875
tensor(10119.0752, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10119.0263671875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10119.0263671875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10119.27734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.2773, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -10119.0263671875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10119.0283203125
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0283, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10119.0283203125
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0283, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10119.0263671875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10119.0263671875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0264, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10119.0283203125
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0283, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10119.02734375
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0273, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -10119.0341796875
tensor(10119.0264, grad_fn=<NegBackward0>) tensor(10119.0342, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7382, 0.2618],
        [0.0574, 0.9426]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.9925e-04, 9.9970e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5005, 0.1818],
         [0.5967, 0.1380]],

        [[0.5488, 0.1800],
         [0.6708, 0.6674]],

        [[0.5160, 0.1442],
         [0.6704, 0.5714]],

        [[0.5942, 0.1423],
         [0.7116, 0.5418]],

        [[0.5982, 0.1410],
         [0.6414, 0.6554]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.022102326722050213
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.019501517337445897
Global Adjusted Rand Index: 0.00565596952472256
Average Adjusted Rand Index: 0.002895369545605033
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20283.255859375
inf tensor(20283.2559, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10131.513671875
tensor(20283.2559, grad_fn=<NegBackward0>) tensor(10131.5137, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10131.1171875
tensor(10131.5137, grad_fn=<NegBackward0>) tensor(10131.1172, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10130.9248046875
tensor(10131.1172, grad_fn=<NegBackward0>) tensor(10130.9248, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10130.5380859375
tensor(10130.9248, grad_fn=<NegBackward0>) tensor(10130.5381, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10129.2822265625
tensor(10130.5381, grad_fn=<NegBackward0>) tensor(10129.2822, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10128.6474609375
tensor(10129.2822, grad_fn=<NegBackward0>) tensor(10128.6475, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10128.423828125
tensor(10128.6475, grad_fn=<NegBackward0>) tensor(10128.4238, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10128.2587890625
tensor(10128.4238, grad_fn=<NegBackward0>) tensor(10128.2588, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10128.1083984375
tensor(10128.2588, grad_fn=<NegBackward0>) tensor(10128.1084, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10127.96484375
tensor(10128.1084, grad_fn=<NegBackward0>) tensor(10127.9648, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10127.8359375
tensor(10127.9648, grad_fn=<NegBackward0>) tensor(10127.8359, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10127.724609375
tensor(10127.8359, grad_fn=<NegBackward0>) tensor(10127.7246, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10127.630859375
tensor(10127.7246, grad_fn=<NegBackward0>) tensor(10127.6309, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10127.5576171875
tensor(10127.6309, grad_fn=<NegBackward0>) tensor(10127.5576, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10127.5009765625
tensor(10127.5576, grad_fn=<NegBackward0>) tensor(10127.5010, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10127.4560546875
tensor(10127.5010, grad_fn=<NegBackward0>) tensor(10127.4561, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10127.4189453125
tensor(10127.4561, grad_fn=<NegBackward0>) tensor(10127.4189, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10127.392578125
tensor(10127.4189, grad_fn=<NegBackward0>) tensor(10127.3926, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10127.37109375
tensor(10127.3926, grad_fn=<NegBackward0>) tensor(10127.3711, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10127.35546875
tensor(10127.3711, grad_fn=<NegBackward0>) tensor(10127.3555, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10127.3447265625
tensor(10127.3555, grad_fn=<NegBackward0>) tensor(10127.3447, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10127.3349609375
tensor(10127.3447, grad_fn=<NegBackward0>) tensor(10127.3350, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10127.3271484375
tensor(10127.3350, grad_fn=<NegBackward0>) tensor(10127.3271, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10127.3212890625
tensor(10127.3271, grad_fn=<NegBackward0>) tensor(10127.3213, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10127.3173828125
tensor(10127.3213, grad_fn=<NegBackward0>) tensor(10127.3174, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10127.3134765625
tensor(10127.3174, grad_fn=<NegBackward0>) tensor(10127.3135, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10127.3095703125
tensor(10127.3135, grad_fn=<NegBackward0>) tensor(10127.3096, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10127.3095703125
tensor(10127.3096, grad_fn=<NegBackward0>) tensor(10127.3096, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10127.306640625
tensor(10127.3096, grad_fn=<NegBackward0>) tensor(10127.3066, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10127.3046875
tensor(10127.3066, grad_fn=<NegBackward0>) tensor(10127.3047, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10127.3037109375
tensor(10127.3047, grad_fn=<NegBackward0>) tensor(10127.3037, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10127.3037109375
tensor(10127.3037, grad_fn=<NegBackward0>) tensor(10127.3037, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10127.3037109375
tensor(10127.3037, grad_fn=<NegBackward0>) tensor(10127.3037, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10127.302734375
tensor(10127.3037, grad_fn=<NegBackward0>) tensor(10127.3027, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10127.3017578125
tensor(10127.3027, grad_fn=<NegBackward0>) tensor(10127.3018, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10127.2998046875
tensor(10127.3018, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10127.30078125
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.3008, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10127.30078125
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.3008, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -10127.2998046875
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10127.30078125
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.3008, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10127.2998046875
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10127.2998046875
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10127.298828125
tensor(10127.2998, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10127.2998046875
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10127.2998046875
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10127.298828125
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10127.2978515625
tensor(10127.2988, grad_fn=<NegBackward0>) tensor(10127.2979, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10127.2998046875
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10127.298828125
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10127.298828125
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10127.2978515625
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2979, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10127.2998046875
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10127.298828125
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10127.2978515625
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2979, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10127.2998046875
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10127.298828125
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10127.2998046875
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10127.2998046875
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2998, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -10127.298828125
tensor(10127.2979, grad_fn=<NegBackward0>) tensor(10127.2988, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[9.7379e-01, 2.6207e-02],
        [9.9994e-01, 6.3883e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9978, 0.0022], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1410, 0.1821],
         [0.6035, 0.4902]],

        [[0.6734, 0.2159],
         [0.5949, 0.6280]],

        [[0.7095, 0.1955],
         [0.6910, 0.6508]],

        [[0.6153, 0.0554],
         [0.6469, 0.5752]],

        [[0.6050, 0.2241],
         [0.5287, 0.6286]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
Global Adjusted Rand Index: -0.0003881239101021502
Average Adjusted Rand Index: -0.0002260269516544653
[0.00565596952472256, -0.0003881239101021502] [0.002895369545605033, -0.0002260269516544653] [10119.0341796875, 10127.298828125]
-------------------------------------
This iteration is 10
True Objective function: Loss = -9856.14440685901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21380.30078125
inf tensor(21380.3008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9714.8037109375
tensor(21380.3008, grad_fn=<NegBackward0>) tensor(9714.8037, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9713.6611328125
tensor(9714.8037, grad_fn=<NegBackward0>) tensor(9713.6611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9712.7177734375
tensor(9713.6611, grad_fn=<NegBackward0>) tensor(9712.7178, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9711.912109375
tensor(9712.7178, grad_fn=<NegBackward0>) tensor(9711.9121, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9711.4072265625
tensor(9711.9121, grad_fn=<NegBackward0>) tensor(9711.4072, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9711.0556640625
tensor(9711.4072, grad_fn=<NegBackward0>) tensor(9711.0557, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9710.7861328125
tensor(9711.0557, grad_fn=<NegBackward0>) tensor(9710.7861, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9710.5546875
tensor(9710.7861, grad_fn=<NegBackward0>) tensor(9710.5547, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9710.3701171875
tensor(9710.5547, grad_fn=<NegBackward0>) tensor(9710.3701, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9710.236328125
tensor(9710.3701, grad_fn=<NegBackward0>) tensor(9710.2363, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9710.1435546875
tensor(9710.2363, grad_fn=<NegBackward0>) tensor(9710.1436, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9710.0791015625
tensor(9710.1436, grad_fn=<NegBackward0>) tensor(9710.0791, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9710.0341796875
tensor(9710.0791, grad_fn=<NegBackward0>) tensor(9710.0342, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9710.00390625
tensor(9710.0342, grad_fn=<NegBackward0>) tensor(9710.0039, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9709.98046875
tensor(9710.0039, grad_fn=<NegBackward0>) tensor(9709.9805, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9709.9638671875
tensor(9709.9805, grad_fn=<NegBackward0>) tensor(9709.9639, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9709.9521484375
tensor(9709.9639, grad_fn=<NegBackward0>) tensor(9709.9521, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9709.9423828125
tensor(9709.9521, grad_fn=<NegBackward0>) tensor(9709.9424, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9709.9345703125
tensor(9709.9424, grad_fn=<NegBackward0>) tensor(9709.9346, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9709.931640625
tensor(9709.9346, grad_fn=<NegBackward0>) tensor(9709.9316, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9709.9296875
tensor(9709.9316, grad_fn=<NegBackward0>) tensor(9709.9297, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9709.9248046875
tensor(9709.9297, grad_fn=<NegBackward0>) tensor(9709.9248, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9709.9248046875
tensor(9709.9248, grad_fn=<NegBackward0>) tensor(9709.9248, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9709.923828125
tensor(9709.9248, grad_fn=<NegBackward0>) tensor(9709.9238, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9709.923828125
tensor(9709.9238, grad_fn=<NegBackward0>) tensor(9709.9238, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9709.9228515625
tensor(9709.9238, grad_fn=<NegBackward0>) tensor(9709.9229, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9709.9228515625
tensor(9709.9229, grad_fn=<NegBackward0>) tensor(9709.9229, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9709.921875
tensor(9709.9229, grad_fn=<NegBackward0>) tensor(9709.9219, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9709.919921875
tensor(9709.9219, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9709.921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9219, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9709.9208984375
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9709.9208984375
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9709.9208984375
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9709.9208984375
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9709.9208984375
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9709.919921875
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9709.9189453125
tensor(9709.9199, grad_fn=<NegBackward0>) tensor(9709.9189, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9709.9208984375
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9709.9208984375
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -9709.9189453125
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9189, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9709.9189453125
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9189, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9709.9462890625
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9463, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9709.9189453125
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9189, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9709.9189453125
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9709.919921875
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9199, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9709.9208984375
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9709.9208984375
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -9709.9208984375
tensor(9709.9189, grad_fn=<NegBackward0>) tensor(9709.9209, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.8722, 0.1278],
        [0.8004, 0.1996]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9954, 0.0046], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1272, 0.1187],
         [0.7204, 0.1863]],

        [[0.5547, 0.1767],
         [0.7161, 0.6947]],

        [[0.6041, 0.1661],
         [0.6156, 0.5214]],

        [[0.5787, 0.1397],
         [0.6075, 0.6776]],

        [[0.7049, 0.1344],
         [0.5730, 0.6757]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.010101010101010102
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004247437425626213
Average Adjusted Rand Index: 0.0003224041291391217
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20128.044921875
inf tensor(20128.0449, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9715.3173828125
tensor(20128.0449, grad_fn=<NegBackward0>) tensor(9715.3174, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9714.6689453125
tensor(9715.3174, grad_fn=<NegBackward0>) tensor(9714.6689, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9714.380859375
tensor(9714.6689, grad_fn=<NegBackward0>) tensor(9714.3809, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9713.71875
tensor(9714.3809, grad_fn=<NegBackward0>) tensor(9713.7188, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9713.2841796875
tensor(9713.7188, grad_fn=<NegBackward0>) tensor(9713.2842, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9711.9638671875
tensor(9713.2842, grad_fn=<NegBackward0>) tensor(9711.9639, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9711.8037109375
tensor(9711.9639, grad_fn=<NegBackward0>) tensor(9711.8037, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9711.7431640625
tensor(9711.8037, grad_fn=<NegBackward0>) tensor(9711.7432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9711.6689453125
tensor(9711.7432, grad_fn=<NegBackward0>) tensor(9711.6689, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9711.591796875
tensor(9711.6689, grad_fn=<NegBackward0>) tensor(9711.5918, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9711.544921875
tensor(9711.5918, grad_fn=<NegBackward0>) tensor(9711.5449, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9711.51953125
tensor(9711.5449, grad_fn=<NegBackward0>) tensor(9711.5195, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9711.498046875
tensor(9711.5195, grad_fn=<NegBackward0>) tensor(9711.4980, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9711.470703125
tensor(9711.4980, grad_fn=<NegBackward0>) tensor(9711.4707, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9711.4296875
tensor(9711.4707, grad_fn=<NegBackward0>) tensor(9711.4297, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9711.3642578125
tensor(9711.4297, grad_fn=<NegBackward0>) tensor(9711.3643, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9711.21875
tensor(9711.3643, grad_fn=<NegBackward0>) tensor(9711.2188, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9710.7548828125
tensor(9711.2188, grad_fn=<NegBackward0>) tensor(9710.7549, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9710.30859375
tensor(9710.7549, grad_fn=<NegBackward0>) tensor(9710.3086, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9710.224609375
tensor(9710.3086, grad_fn=<NegBackward0>) tensor(9710.2246, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9710.14453125
tensor(9710.2246, grad_fn=<NegBackward0>) tensor(9710.1445, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9710.1083984375
tensor(9710.1445, grad_fn=<NegBackward0>) tensor(9710.1084, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9710.0791015625
tensor(9710.1084, grad_fn=<NegBackward0>) tensor(9710.0791, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9710.07421875
tensor(9710.0791, grad_fn=<NegBackward0>) tensor(9710.0742, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9710.0791015625
tensor(9710.0742, grad_fn=<NegBackward0>) tensor(9710.0791, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -9710.0654296875
tensor(9710.0742, grad_fn=<NegBackward0>) tensor(9710.0654, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9710.0703125
tensor(9710.0654, grad_fn=<NegBackward0>) tensor(9710.0703, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9710.0615234375
tensor(9710.0654, grad_fn=<NegBackward0>) tensor(9710.0615, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9710.060546875
tensor(9710.0615, grad_fn=<NegBackward0>) tensor(9710.0605, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9710.05859375
tensor(9710.0605, grad_fn=<NegBackward0>) tensor(9710.0586, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9710.05859375
tensor(9710.0586, grad_fn=<NegBackward0>) tensor(9710.0586, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9710.056640625
tensor(9710.0586, grad_fn=<NegBackward0>) tensor(9710.0566, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9710.0556640625
tensor(9710.0566, grad_fn=<NegBackward0>) tensor(9710.0557, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9710.0537109375
tensor(9710.0557, grad_fn=<NegBackward0>) tensor(9710.0537, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9710.06640625
tensor(9710.0537, grad_fn=<NegBackward0>) tensor(9710.0664, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9710.052734375
tensor(9710.0537, grad_fn=<NegBackward0>) tensor(9710.0527, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9710.056640625
tensor(9710.0527, grad_fn=<NegBackward0>) tensor(9710.0566, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9710.056640625
tensor(9710.0527, grad_fn=<NegBackward0>) tensor(9710.0566, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9710.05078125
tensor(9710.0527, grad_fn=<NegBackward0>) tensor(9710.0508, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9710.05078125
tensor(9710.0508, grad_fn=<NegBackward0>) tensor(9710.0508, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9710.056640625
tensor(9710.0508, grad_fn=<NegBackward0>) tensor(9710.0566, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9710.0517578125
tensor(9710.0508, grad_fn=<NegBackward0>) tensor(9710.0518, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -9710.05078125
tensor(9710.0508, grad_fn=<NegBackward0>) tensor(9710.0508, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9710.048828125
tensor(9710.0508, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9710.048828125
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9710.0498046875
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0498, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9710.048828125
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9710.048828125
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9710.0498046875
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0498, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9710.0478515625
tensor(9710.0488, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9710.0478515625
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9710.048828125
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9710.0537109375
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0537, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9710.0478515625
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9710.0478515625
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9710.0478515625
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9710.0478515625
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0479, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9710.0458984375
tensor(9710.0479, grad_fn=<NegBackward0>) tensor(9710.0459, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9710.0498046875
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0498, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9710.0458984375
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0459, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9710.046875
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0469, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9710.0712890625
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0713, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9710.0458984375
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0459, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9710.046875
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0469, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9710.0595703125
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0596, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -9710.048828125
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0488, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -9710.046875
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0469, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -9710.05078125
tensor(9710.0459, grad_fn=<NegBackward0>) tensor(9710.0508, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[7.1688e-01, 2.8312e-01],
        [9.9900e-01, 9.9662e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1273, 0.8727], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1359, 0.1309],
         [0.5325, 0.1245]],

        [[0.5755, 0.2076],
         [0.7089, 0.6321]],

        [[0.6084, 0.1431],
         [0.6548, 0.6695]],

        [[0.6396, 0.1211],
         [0.6987, 0.5904]],

        [[0.5966, 0.1226],
         [0.6010, 0.6824]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001794567017790883
Average Adjusted Rand Index: -0.0014545454545454545
[0.004247437425626213, -0.001794567017790883] [0.0003224041291391217, -0.0014545454545454545] [9709.9208984375, 9710.05078125]
-------------------------------------
This iteration is 11
True Objective function: Loss = -9910.092464332021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20302.9609375
inf tensor(20302.9609, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9747.2646484375
tensor(20302.9609, grad_fn=<NegBackward0>) tensor(9747.2646, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9746.8310546875
tensor(9747.2646, grad_fn=<NegBackward0>) tensor(9746.8311, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9746.7255859375
tensor(9746.8311, grad_fn=<NegBackward0>) tensor(9746.7256, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9746.669921875
tensor(9746.7256, grad_fn=<NegBackward0>) tensor(9746.6699, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9746.6279296875
tensor(9746.6699, grad_fn=<NegBackward0>) tensor(9746.6279, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9746.5927734375
tensor(9746.6279, grad_fn=<NegBackward0>) tensor(9746.5928, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9746.5576171875
tensor(9746.5928, grad_fn=<NegBackward0>) tensor(9746.5576, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9746.5263671875
tensor(9746.5576, grad_fn=<NegBackward0>) tensor(9746.5264, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9746.5029296875
tensor(9746.5264, grad_fn=<NegBackward0>) tensor(9746.5029, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9746.482421875
tensor(9746.5029, grad_fn=<NegBackward0>) tensor(9746.4824, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9746.4638671875
tensor(9746.4824, grad_fn=<NegBackward0>) tensor(9746.4639, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9746.447265625
tensor(9746.4639, grad_fn=<NegBackward0>) tensor(9746.4473, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9746.4326171875
tensor(9746.4473, grad_fn=<NegBackward0>) tensor(9746.4326, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9746.4189453125
tensor(9746.4326, grad_fn=<NegBackward0>) tensor(9746.4189, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9746.40625
tensor(9746.4189, grad_fn=<NegBackward0>) tensor(9746.4062, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9746.3935546875
tensor(9746.4062, grad_fn=<NegBackward0>) tensor(9746.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9746.3828125
tensor(9746.3936, grad_fn=<NegBackward0>) tensor(9746.3828, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9746.373046875
tensor(9746.3828, grad_fn=<NegBackward0>) tensor(9746.3730, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9746.361328125
tensor(9746.3730, grad_fn=<NegBackward0>) tensor(9746.3613, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9746.3525390625
tensor(9746.3613, grad_fn=<NegBackward0>) tensor(9746.3525, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9746.345703125
tensor(9746.3525, grad_fn=<NegBackward0>) tensor(9746.3457, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9746.337890625
tensor(9746.3457, grad_fn=<NegBackward0>) tensor(9746.3379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9746.330078125
tensor(9746.3379, grad_fn=<NegBackward0>) tensor(9746.3301, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9746.3251953125
tensor(9746.3301, grad_fn=<NegBackward0>) tensor(9746.3252, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9746.3212890625
tensor(9746.3252, grad_fn=<NegBackward0>) tensor(9746.3213, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9746.3173828125
tensor(9746.3213, grad_fn=<NegBackward0>) tensor(9746.3174, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9746.3134765625
tensor(9746.3174, grad_fn=<NegBackward0>) tensor(9746.3135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9746.310546875
tensor(9746.3135, grad_fn=<NegBackward0>) tensor(9746.3105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9746.310546875
tensor(9746.3105, grad_fn=<NegBackward0>) tensor(9746.3105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9746.3095703125
tensor(9746.3105, grad_fn=<NegBackward0>) tensor(9746.3096, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9746.3095703125
tensor(9746.3096, grad_fn=<NegBackward0>) tensor(9746.3096, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9746.3076171875
tensor(9746.3096, grad_fn=<NegBackward0>) tensor(9746.3076, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9746.3076171875
tensor(9746.3076, grad_fn=<NegBackward0>) tensor(9746.3076, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9746.306640625
tensor(9746.3076, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9746.3076171875
tensor(9746.3066, grad_fn=<NegBackward0>) tensor(9746.3076, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9746.3056640625
tensor(9746.3066, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9746.306640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9746.306640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9746.3056640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9746.306640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9746.306640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9746.3056640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9746.3056640625
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9746.3046875
tensor(9746.3057, grad_fn=<NegBackward0>) tensor(9746.3047, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9746.3056640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -9746.3046875
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3047, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9746.306640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3066, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -9746.3056640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -9746.3056640625
tensor(9746.3047, grad_fn=<NegBackward0>) tensor(9746.3057, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.0065, 0.9935],
        [0.0646, 0.9354]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0705, 0.9295], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.1551],
         [0.5643, 0.1301]],

        [[0.6427, 0.1585],
         [0.6493, 0.6848]],

        [[0.6475, 0.1673],
         [0.6437, 0.5107]],

        [[0.6429, 0.1527],
         [0.5538, 0.5554]],

        [[0.6636, 0.1700],
         [0.5355, 0.6538]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21869.365234375
inf tensor(21869.3652, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9747.0615234375
tensor(21869.3652, grad_fn=<NegBackward0>) tensor(9747.0615, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9746.7138671875
tensor(9747.0615, grad_fn=<NegBackward0>) tensor(9746.7139, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9746.6357421875
tensor(9746.7139, grad_fn=<NegBackward0>) tensor(9746.6357, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9746.6025390625
tensor(9746.6357, grad_fn=<NegBackward0>) tensor(9746.6025, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9746.5810546875
tensor(9746.6025, grad_fn=<NegBackward0>) tensor(9746.5811, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9746.564453125
tensor(9746.5811, grad_fn=<NegBackward0>) tensor(9746.5645, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9746.5517578125
tensor(9746.5645, grad_fn=<NegBackward0>) tensor(9746.5518, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9746.541015625
tensor(9746.5518, grad_fn=<NegBackward0>) tensor(9746.5410, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9746.53125
tensor(9746.5410, grad_fn=<NegBackward0>) tensor(9746.5312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9746.5234375
tensor(9746.5312, grad_fn=<NegBackward0>) tensor(9746.5234, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9746.5166015625
tensor(9746.5234, grad_fn=<NegBackward0>) tensor(9746.5166, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9746.5107421875
tensor(9746.5166, grad_fn=<NegBackward0>) tensor(9746.5107, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9746.505859375
tensor(9746.5107, grad_fn=<NegBackward0>) tensor(9746.5059, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9746.501953125
tensor(9746.5059, grad_fn=<NegBackward0>) tensor(9746.5020, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9746.498046875
tensor(9746.5020, grad_fn=<NegBackward0>) tensor(9746.4980, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9746.4931640625
tensor(9746.4980, grad_fn=<NegBackward0>) tensor(9746.4932, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9746.4912109375
tensor(9746.4932, grad_fn=<NegBackward0>) tensor(9746.4912, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9746.48828125
tensor(9746.4912, grad_fn=<NegBackward0>) tensor(9746.4883, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9746.4853515625
tensor(9746.4883, grad_fn=<NegBackward0>) tensor(9746.4854, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9746.4833984375
tensor(9746.4854, grad_fn=<NegBackward0>) tensor(9746.4834, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9746.4794921875
tensor(9746.4834, grad_fn=<NegBackward0>) tensor(9746.4795, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9746.478515625
tensor(9746.4795, grad_fn=<NegBackward0>) tensor(9746.4785, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9746.4755859375
tensor(9746.4785, grad_fn=<NegBackward0>) tensor(9746.4756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9746.4736328125
tensor(9746.4756, grad_fn=<NegBackward0>) tensor(9746.4736, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9746.4697265625
tensor(9746.4736, grad_fn=<NegBackward0>) tensor(9746.4697, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9746.46875
tensor(9746.4697, grad_fn=<NegBackward0>) tensor(9746.4688, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9746.462890625
tensor(9746.4688, grad_fn=<NegBackward0>) tensor(9746.4629, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9746.458984375
tensor(9746.4629, grad_fn=<NegBackward0>) tensor(9746.4590, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9746.4482421875
tensor(9746.4590, grad_fn=<NegBackward0>) tensor(9746.4482, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9746.4033203125
tensor(9746.4482, grad_fn=<NegBackward0>) tensor(9746.4033, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9744.8828125
tensor(9746.4033, grad_fn=<NegBackward0>) tensor(9744.8828, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9744.0390625
tensor(9744.8828, grad_fn=<NegBackward0>) tensor(9744.0391, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9743.8388671875
tensor(9744.0391, grad_fn=<NegBackward0>) tensor(9743.8389, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9743.791015625
tensor(9743.8389, grad_fn=<NegBackward0>) tensor(9743.7910, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9743.77734375
tensor(9743.7910, grad_fn=<NegBackward0>) tensor(9743.7773, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9743.771484375
tensor(9743.7773, grad_fn=<NegBackward0>) tensor(9743.7715, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9743.767578125
tensor(9743.7715, grad_fn=<NegBackward0>) tensor(9743.7676, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9743.765625
tensor(9743.7676, grad_fn=<NegBackward0>) tensor(9743.7656, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9743.763671875
tensor(9743.7656, grad_fn=<NegBackward0>) tensor(9743.7637, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9743.763671875
tensor(9743.7637, grad_fn=<NegBackward0>) tensor(9743.7637, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9743.7626953125
tensor(9743.7637, grad_fn=<NegBackward0>) tensor(9743.7627, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9743.7626953125
tensor(9743.7627, grad_fn=<NegBackward0>) tensor(9743.7627, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9743.76171875
tensor(9743.7627, grad_fn=<NegBackward0>) tensor(9743.7617, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9743.76171875
tensor(9743.7617, grad_fn=<NegBackward0>) tensor(9743.7617, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9743.7607421875
tensor(9743.7617, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9743.7607421875
tensor(9743.7607, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9743.76171875
tensor(9743.7607, grad_fn=<NegBackward0>) tensor(9743.7617, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9743.7607421875
tensor(9743.7607, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9743.7607421875
tensor(9743.7607, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9743.7587890625
tensor(9743.7607, grad_fn=<NegBackward0>) tensor(9743.7588, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9743.7587890625
tensor(9743.7588, grad_fn=<NegBackward0>) tensor(9743.7588, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9743.7578125
tensor(9743.7588, grad_fn=<NegBackward0>) tensor(9743.7578, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9743.759765625
tensor(9743.7578, grad_fn=<NegBackward0>) tensor(9743.7598, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9743.7607421875
tensor(9743.7578, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9743.7607421875
tensor(9743.7578, grad_fn=<NegBackward0>) tensor(9743.7607, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9743.7587890625
tensor(9743.7578, grad_fn=<NegBackward0>) tensor(9743.7588, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9743.7587890625
tensor(9743.7578, grad_fn=<NegBackward0>) tensor(9743.7588, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[4.7232e-04, 9.9953e-01],
        [1.2447e-02, 9.8755e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.8860e-04, 9.9981e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1332, 0.1263],
         [0.6808, 0.1345]],

        [[0.7218, 0.0246],
         [0.6117, 0.7245]],

        [[0.5598, 0.1847],
         [0.5350, 0.6644]],

        [[0.5477, 0.1013],
         [0.6406, 0.5904]],

        [[0.6548, 0.1058],
         [0.5692, 0.5178]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005650385942344435
Average Adjusted Rand Index: 0.0
[0.0, -0.0005650385942344435] [0.0, 0.0] [9746.3056640625, 9743.7587890625]
-------------------------------------
This iteration is 12
True Objective function: Loss = -10046.331722144521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22311.669921875
inf tensor(22311.6699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9919.552734375
tensor(22311.6699, grad_fn=<NegBackward0>) tensor(9919.5527, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9918.94921875
tensor(9919.5527, grad_fn=<NegBackward0>) tensor(9918.9492, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9918.759765625
tensor(9918.9492, grad_fn=<NegBackward0>) tensor(9918.7598, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9918.625
tensor(9918.7598, grad_fn=<NegBackward0>) tensor(9918.6250, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9918.52734375
tensor(9918.6250, grad_fn=<NegBackward0>) tensor(9918.5273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9918.4365234375
tensor(9918.5273, grad_fn=<NegBackward0>) tensor(9918.4365, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9918.3330078125
tensor(9918.4365, grad_fn=<NegBackward0>) tensor(9918.3330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9918.177734375
tensor(9918.3330, grad_fn=<NegBackward0>) tensor(9918.1777, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9917.8837890625
tensor(9918.1777, grad_fn=<NegBackward0>) tensor(9917.8838, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9917.26953125
tensor(9917.8838, grad_fn=<NegBackward0>) tensor(9917.2695, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9916.6650390625
tensor(9917.2695, grad_fn=<NegBackward0>) tensor(9916.6650, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9916.1640625
tensor(9916.6650, grad_fn=<NegBackward0>) tensor(9916.1641, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9915.7568359375
tensor(9916.1641, grad_fn=<NegBackward0>) tensor(9915.7568, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9915.4716796875
tensor(9915.7568, grad_fn=<NegBackward0>) tensor(9915.4717, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9915.30859375
tensor(9915.4717, grad_fn=<NegBackward0>) tensor(9915.3086, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9915.2109375
tensor(9915.3086, grad_fn=<NegBackward0>) tensor(9915.2109, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9915.1708984375
tensor(9915.2109, grad_fn=<NegBackward0>) tensor(9915.1709, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9915.1533203125
tensor(9915.1709, grad_fn=<NegBackward0>) tensor(9915.1533, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9915.140625
tensor(9915.1533, grad_fn=<NegBackward0>) tensor(9915.1406, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9915.134765625
tensor(9915.1406, grad_fn=<NegBackward0>) tensor(9915.1348, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9915.12890625
tensor(9915.1348, grad_fn=<NegBackward0>) tensor(9915.1289, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9915.1376953125
tensor(9915.1289, grad_fn=<NegBackward0>) tensor(9915.1377, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9915.119140625
tensor(9915.1289, grad_fn=<NegBackward0>) tensor(9915.1191, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9915.1142578125
tensor(9915.1191, grad_fn=<NegBackward0>) tensor(9915.1143, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9915.1083984375
tensor(9915.1143, grad_fn=<NegBackward0>) tensor(9915.1084, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9915.1025390625
tensor(9915.1084, grad_fn=<NegBackward0>) tensor(9915.1025, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9915.09375
tensor(9915.1025, grad_fn=<NegBackward0>) tensor(9915.0938, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9915.0888671875
tensor(9915.0938, grad_fn=<NegBackward0>) tensor(9915.0889, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9915.0869140625
tensor(9915.0889, grad_fn=<NegBackward0>) tensor(9915.0869, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9915.0791015625
tensor(9915.0869, grad_fn=<NegBackward0>) tensor(9915.0791, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9915.0791015625
tensor(9915.0791, grad_fn=<NegBackward0>) tensor(9915.0791, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9915.07421875
tensor(9915.0791, grad_fn=<NegBackward0>) tensor(9915.0742, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9915.087890625
tensor(9915.0742, grad_fn=<NegBackward0>) tensor(9915.0879, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9915.072265625
tensor(9915.0742, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9915.076171875
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0762, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9915.078125
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0781, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9915.0712890625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0713, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9915.0712890625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0713, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9915.0732421875
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9915.0732421875
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.1909, 0.8091],
        [0.1538, 0.8462]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7900, 0.2100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1471, 0.1404],
         [0.6925, 0.1340]],

        [[0.7043, 0.1429],
         [0.5773, 0.5990]],

        [[0.5892, 0.1146],
         [0.6807, 0.5244]],

        [[0.6771, 0.1526],
         [0.6775, 0.6034]],

        [[0.6635, 0.1493],
         [0.5436, 0.6548]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0009308826084851337
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24001.818359375
inf tensor(24001.8184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9920.2607421875
tensor(24001.8184, grad_fn=<NegBackward0>) tensor(9920.2607, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9919.1611328125
tensor(9920.2607, grad_fn=<NegBackward0>) tensor(9919.1611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9918.8740234375
tensor(9919.1611, grad_fn=<NegBackward0>) tensor(9918.8740, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9918.6962890625
tensor(9918.8740, grad_fn=<NegBackward0>) tensor(9918.6963, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9918.552734375
tensor(9918.6963, grad_fn=<NegBackward0>) tensor(9918.5527, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9918.416015625
tensor(9918.5527, grad_fn=<NegBackward0>) tensor(9918.4160, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9918.255859375
tensor(9918.4160, grad_fn=<NegBackward0>) tensor(9918.2559, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9918.052734375
tensor(9918.2559, grad_fn=<NegBackward0>) tensor(9918.0527, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9917.7880859375
tensor(9918.0527, grad_fn=<NegBackward0>) tensor(9917.7881, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9917.49609375
tensor(9917.7881, grad_fn=<NegBackward0>) tensor(9917.4961, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9917.197265625
tensor(9917.4961, grad_fn=<NegBackward0>) tensor(9917.1973, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9916.8701171875
tensor(9917.1973, grad_fn=<NegBackward0>) tensor(9916.8701, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9916.505859375
tensor(9916.8701, grad_fn=<NegBackward0>) tensor(9916.5059, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9916.1328125
tensor(9916.5059, grad_fn=<NegBackward0>) tensor(9916.1328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9915.7978515625
tensor(9916.1328, grad_fn=<NegBackward0>) tensor(9915.7979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9915.5322265625
tensor(9915.7979, grad_fn=<NegBackward0>) tensor(9915.5322, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9915.3623046875
tensor(9915.5322, grad_fn=<NegBackward0>) tensor(9915.3623, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9915.2587890625
tensor(9915.3623, grad_fn=<NegBackward0>) tensor(9915.2588, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9915.19921875
tensor(9915.2588, grad_fn=<NegBackward0>) tensor(9915.1992, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9915.16796875
tensor(9915.1992, grad_fn=<NegBackward0>) tensor(9915.1680, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9915.150390625
tensor(9915.1680, grad_fn=<NegBackward0>) tensor(9915.1504, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9915.1396484375
tensor(9915.1504, grad_fn=<NegBackward0>) tensor(9915.1396, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9915.1328125
tensor(9915.1396, grad_fn=<NegBackward0>) tensor(9915.1328, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9915.1259765625
tensor(9915.1328, grad_fn=<NegBackward0>) tensor(9915.1260, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9915.1201171875
tensor(9915.1260, grad_fn=<NegBackward0>) tensor(9915.1201, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9915.1142578125
tensor(9915.1201, grad_fn=<NegBackward0>) tensor(9915.1143, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9915.1103515625
tensor(9915.1143, grad_fn=<NegBackward0>) tensor(9915.1104, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9915.103515625
tensor(9915.1104, grad_fn=<NegBackward0>) tensor(9915.1035, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9915.099609375
tensor(9915.1035, grad_fn=<NegBackward0>) tensor(9915.0996, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9915.0927734375
tensor(9915.0996, grad_fn=<NegBackward0>) tensor(9915.0928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9915.0869140625
tensor(9915.0928, grad_fn=<NegBackward0>) tensor(9915.0869, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9915.0830078125
tensor(9915.0869, grad_fn=<NegBackward0>) tensor(9915.0830, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9915.078125
tensor(9915.0830, grad_fn=<NegBackward0>) tensor(9915.0781, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9915.0771484375
tensor(9915.0781, grad_fn=<NegBackward0>) tensor(9915.0771, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9915.0751953125
tensor(9915.0771, grad_fn=<NegBackward0>) tensor(9915.0752, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9915.07421875
tensor(9915.0752, grad_fn=<NegBackward0>) tensor(9915.0742, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9915.07421875
tensor(9915.0742, grad_fn=<NegBackward0>) tensor(9915.0742, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9915.072265625
tensor(9915.0742, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9915.0732421875
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9915.0732421875
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9915.072265625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9915.0712890625
tensor(9915.0723, grad_fn=<NegBackward0>) tensor(9915.0713, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9915.0732421875
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9915.0712890625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0713, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9915.0712890625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0713, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9915.072265625
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0723, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -9915.0732421875
tensor(9915.0713, grad_fn=<NegBackward0>) tensor(9915.0732, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.1904, 0.8096],
        [0.1539, 0.8461]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7852, 0.2148], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1472, 0.1405],
         [0.5107, 0.1340]],

        [[0.7055, 0.1430],
         [0.5641, 0.6802]],

        [[0.5737, 0.1147],
         [0.5780, 0.5108]],

        [[0.6564, 0.1526],
         [0.7040, 0.6450]],

        [[0.5833, 0.1493],
         [0.6475, 0.5518]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0009308826084851337
Average Adjusted Rand Index: 0.0
[0.0009308826084851337, 0.0009308826084851337] [0.0, 0.0] [9915.0732421875, 9915.0732421875]
-------------------------------------
This iteration is 13
True Objective function: Loss = -10161.157811924195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24680.267578125
inf tensor(24680.2676, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10008.5810546875
tensor(24680.2676, grad_fn=<NegBackward0>) tensor(10008.5811, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10005.861328125
tensor(10008.5811, grad_fn=<NegBackward0>) tensor(10005.8613, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10005.5927734375
tensor(10005.8613, grad_fn=<NegBackward0>) tensor(10005.5928, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10005.2734375
tensor(10005.5928, grad_fn=<NegBackward0>) tensor(10005.2734, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10004.9609375
tensor(10005.2734, grad_fn=<NegBackward0>) tensor(10004.9609, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10004.7275390625
tensor(10004.9609, grad_fn=<NegBackward0>) tensor(10004.7275, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10004.6015625
tensor(10004.7275, grad_fn=<NegBackward0>) tensor(10004.6016, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10004.5400390625
tensor(10004.6016, grad_fn=<NegBackward0>) tensor(10004.5400, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10004.505859375
tensor(10004.5400, grad_fn=<NegBackward0>) tensor(10004.5059, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10004.4853515625
tensor(10004.5059, grad_fn=<NegBackward0>) tensor(10004.4854, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10004.4716796875
tensor(10004.4854, grad_fn=<NegBackward0>) tensor(10004.4717, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10004.458984375
tensor(10004.4717, grad_fn=<NegBackward0>) tensor(10004.4590, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10004.44921875
tensor(10004.4590, grad_fn=<NegBackward0>) tensor(10004.4492, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10004.4423828125
tensor(10004.4492, grad_fn=<NegBackward0>) tensor(10004.4424, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10004.431640625
tensor(10004.4424, grad_fn=<NegBackward0>) tensor(10004.4316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10004.419921875
tensor(10004.4316, grad_fn=<NegBackward0>) tensor(10004.4199, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10004.3984375
tensor(10004.4199, grad_fn=<NegBackward0>) tensor(10004.3984, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10004.3671875
tensor(10004.3984, grad_fn=<NegBackward0>) tensor(10004.3672, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10004.318359375
tensor(10004.3672, grad_fn=<NegBackward0>) tensor(10004.3184, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10004.2685546875
tensor(10004.3184, grad_fn=<NegBackward0>) tensor(10004.2686, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10004.22265625
tensor(10004.2686, grad_fn=<NegBackward0>) tensor(10004.2227, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10004.1865234375
tensor(10004.2227, grad_fn=<NegBackward0>) tensor(10004.1865, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10004.1630859375
tensor(10004.1865, grad_fn=<NegBackward0>) tensor(10004.1631, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10004.146484375
tensor(10004.1631, grad_fn=<NegBackward0>) tensor(10004.1465, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10004.138671875
tensor(10004.1465, grad_fn=<NegBackward0>) tensor(10004.1387, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10004.1328125
tensor(10004.1387, grad_fn=<NegBackward0>) tensor(10004.1328, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10004.1279296875
tensor(10004.1328, grad_fn=<NegBackward0>) tensor(10004.1279, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10004.1240234375
tensor(10004.1279, grad_fn=<NegBackward0>) tensor(10004.1240, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10004.119140625
tensor(10004.1240, grad_fn=<NegBackward0>) tensor(10004.1191, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10004.1103515625
tensor(10004.1191, grad_fn=<NegBackward0>) tensor(10004.1104, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10004.068359375
tensor(10004.1104, grad_fn=<NegBackward0>) tensor(10004.0684, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10004.0146484375
tensor(10004.0684, grad_fn=<NegBackward0>) tensor(10004.0146, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10004.001953125
tensor(10004.0146, grad_fn=<NegBackward0>) tensor(10004.0020, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10003.9970703125
tensor(10004.0020, grad_fn=<NegBackward0>) tensor(10003.9971, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10003.99609375
tensor(10003.9971, grad_fn=<NegBackward0>) tensor(10003.9961, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10003.9951171875
tensor(10003.9961, grad_fn=<NegBackward0>) tensor(10003.9951, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10003.9951171875
tensor(10003.9951, grad_fn=<NegBackward0>) tensor(10003.9951, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10003.9951171875
tensor(10003.9951, grad_fn=<NegBackward0>) tensor(10003.9951, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10003.9951171875
tensor(10003.9951, grad_fn=<NegBackward0>) tensor(10003.9951, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10003.994140625
tensor(10003.9951, grad_fn=<NegBackward0>) tensor(10003.9941, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10003.9951171875
tensor(10003.9941, grad_fn=<NegBackward0>) tensor(10003.9951, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10004.001953125
tensor(10003.9941, grad_fn=<NegBackward0>) tensor(10004.0020, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10003.9931640625
tensor(10003.9941, grad_fn=<NegBackward0>) tensor(10003.9932, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10003.99609375
tensor(10003.9932, grad_fn=<NegBackward0>) tensor(10003.9961, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10003.9931640625
tensor(10003.9932, grad_fn=<NegBackward0>) tensor(10003.9932, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10003.9931640625
tensor(10003.9932, grad_fn=<NegBackward0>) tensor(10003.9932, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10003.9912109375
tensor(10003.9932, grad_fn=<NegBackward0>) tensor(10003.9912, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10003.98828125
tensor(10003.9912, grad_fn=<NegBackward0>) tensor(10003.9883, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10003.970703125
tensor(10003.9883, grad_fn=<NegBackward0>) tensor(10003.9707, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10003.9130859375
tensor(10003.9707, grad_fn=<NegBackward0>) tensor(10003.9131, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10003.904296875
tensor(10003.9131, grad_fn=<NegBackward0>) tensor(10003.9043, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10003.8994140625
tensor(10003.9043, grad_fn=<NegBackward0>) tensor(10003.8994, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10003.8984375
tensor(10003.8994, grad_fn=<NegBackward0>) tensor(10003.8984, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10003.8974609375
tensor(10003.8984, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10003.896484375
tensor(10003.8975, grad_fn=<NegBackward0>) tensor(10003.8965, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10003.8994140625
tensor(10003.8965, grad_fn=<NegBackward0>) tensor(10003.8994, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10003.8955078125
tensor(10003.8965, grad_fn=<NegBackward0>) tensor(10003.8955, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10003.8984375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8984, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10003.8955078125
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8955, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -10003.8974609375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8975, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -10003.896484375
tensor(10003.8955, grad_fn=<NegBackward0>) tensor(10003.8965, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.1546, 0.8454],
        [0.0216, 0.9784]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1202, 0.8798], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0873, 0.1201],
         [0.6461, 0.1437]],

        [[0.6351, 0.0627],
         [0.6091, 0.6103]],

        [[0.6394, 0.0930],
         [0.5352, 0.5880]],

        [[0.6995, 0.0765],
         [0.7214, 0.6845]],

        [[0.6494, 0.0846],
         [0.6656, 0.5563]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00036156365775701495
Average Adjusted Rand Index: 0.0004884135958389755
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19926.6640625
inf tensor(19926.6641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10007.171875
tensor(19926.6641, grad_fn=<NegBackward0>) tensor(10007.1719, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10006.7041015625
tensor(10007.1719, grad_fn=<NegBackward0>) tensor(10006.7041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10006.576171875
tensor(10006.7041, grad_fn=<NegBackward0>) tensor(10006.5762, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10006.4794921875
tensor(10006.5762, grad_fn=<NegBackward0>) tensor(10006.4795, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10006.384765625
tensor(10006.4795, grad_fn=<NegBackward0>) tensor(10006.3848, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10006.2470703125
tensor(10006.3848, grad_fn=<NegBackward0>) tensor(10006.2471, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10006.017578125
tensor(10006.2471, grad_fn=<NegBackward0>) tensor(10006.0176, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10005.8095703125
tensor(10006.0176, grad_fn=<NegBackward0>) tensor(10005.8096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10005.7236328125
tensor(10005.8096, grad_fn=<NegBackward0>) tensor(10005.7236, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10005.6796875
tensor(10005.7236, grad_fn=<NegBackward0>) tensor(10005.6797, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10005.650390625
tensor(10005.6797, grad_fn=<NegBackward0>) tensor(10005.6504, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10005.6240234375
tensor(10005.6504, grad_fn=<NegBackward0>) tensor(10005.6240, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10005.603515625
tensor(10005.6240, grad_fn=<NegBackward0>) tensor(10005.6035, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10005.583984375
tensor(10005.6035, grad_fn=<NegBackward0>) tensor(10005.5840, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10005.5654296875
tensor(10005.5840, grad_fn=<NegBackward0>) tensor(10005.5654, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10005.5458984375
tensor(10005.5654, grad_fn=<NegBackward0>) tensor(10005.5459, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10005.5283203125
tensor(10005.5459, grad_fn=<NegBackward0>) tensor(10005.5283, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10005.5068359375
tensor(10005.5283, grad_fn=<NegBackward0>) tensor(10005.5068, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10005.4814453125
tensor(10005.5068, grad_fn=<NegBackward0>) tensor(10005.4814, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10005.4501953125
tensor(10005.4814, grad_fn=<NegBackward0>) tensor(10005.4502, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10005.4033203125
tensor(10005.4502, grad_fn=<NegBackward0>) tensor(10005.4033, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10005.3505859375
tensor(10005.4033, grad_fn=<NegBackward0>) tensor(10005.3506, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10005.2998046875
tensor(10005.3506, grad_fn=<NegBackward0>) tensor(10005.2998, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10005.2412109375
tensor(10005.2998, grad_fn=<NegBackward0>) tensor(10005.2412, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10005.173828125
tensor(10005.2412, grad_fn=<NegBackward0>) tensor(10005.1738, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10005.095703125
tensor(10005.1738, grad_fn=<NegBackward0>) tensor(10005.0957, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10005.01171875
tensor(10005.0957, grad_fn=<NegBackward0>) tensor(10005.0117, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10004.9296875
tensor(10005.0117, grad_fn=<NegBackward0>) tensor(10004.9297, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10004.8515625
tensor(10004.9297, grad_fn=<NegBackward0>) tensor(10004.8516, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10004.77734375
tensor(10004.8516, grad_fn=<NegBackward0>) tensor(10004.7773, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10004.708984375
tensor(10004.7773, grad_fn=<NegBackward0>) tensor(10004.7090, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10004.6513671875
tensor(10004.7090, grad_fn=<NegBackward0>) tensor(10004.6514, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10004.6015625
tensor(10004.6514, grad_fn=<NegBackward0>) tensor(10004.6016, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10004.5634765625
tensor(10004.6016, grad_fn=<NegBackward0>) tensor(10004.5635, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10004.533203125
tensor(10004.5635, grad_fn=<NegBackward0>) tensor(10004.5332, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10004.5078125
tensor(10004.5332, grad_fn=<NegBackward0>) tensor(10004.5078, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10004.4892578125
tensor(10004.5078, grad_fn=<NegBackward0>) tensor(10004.4893, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10004.47265625
tensor(10004.4893, grad_fn=<NegBackward0>) tensor(10004.4727, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10004.4609375
tensor(10004.4727, grad_fn=<NegBackward0>) tensor(10004.4609, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10004.451171875
tensor(10004.4609, grad_fn=<NegBackward0>) tensor(10004.4512, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10004.4443359375
tensor(10004.4512, grad_fn=<NegBackward0>) tensor(10004.4443, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10004.4384765625
tensor(10004.4443, grad_fn=<NegBackward0>) tensor(10004.4385, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10004.48046875
tensor(10004.4385, grad_fn=<NegBackward0>) tensor(10004.4805, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10004.431640625
tensor(10004.4385, grad_fn=<NegBackward0>) tensor(10004.4316, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10004.4287109375
tensor(10004.4316, grad_fn=<NegBackward0>) tensor(10004.4287, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10004.4267578125
tensor(10004.4287, grad_fn=<NegBackward0>) tensor(10004.4268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10004.4248046875
tensor(10004.4268, grad_fn=<NegBackward0>) tensor(10004.4248, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10004.4306640625
tensor(10004.4248, grad_fn=<NegBackward0>) tensor(10004.4307, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10004.423828125
tensor(10004.4248, grad_fn=<NegBackward0>) tensor(10004.4238, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10004.421875
tensor(10004.4238, grad_fn=<NegBackward0>) tensor(10004.4219, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10004.421875
tensor(10004.4219, grad_fn=<NegBackward0>) tensor(10004.4219, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10004.421875
tensor(10004.4219, grad_fn=<NegBackward0>) tensor(10004.4219, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10004.4208984375
tensor(10004.4219, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10004.421875
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4219, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10004.4208984375
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10004.4287109375
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4287, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10004.4208984375
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10004.4208984375
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10004.4208984375
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10004.4228515625
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4229, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10004.4189453125
tensor(10004.4209, grad_fn=<NegBackward0>) tensor(10004.4189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10004.4208984375
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10004.431640625
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4316, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10004.419921875
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4199, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -10004.4208984375
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -10004.4189453125
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4189, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10004.4208984375
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10004.4208984375
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10004.4208984375
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4209, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10004.419921875
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4199, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10004.4326171875
tensor(10004.4189, grad_fn=<NegBackward0>) tensor(10004.4326, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6715, 0.3285],
        [0.8143, 0.1857]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6307, 0.3693], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1559, 0.1311],
         [0.5981, 0.1051]],

        [[0.6958, 0.1247],
         [0.5325, 0.6455]],

        [[0.6321, 0.1325],
         [0.7050, 0.6244]],

        [[0.5723, 0.1298],
         [0.5955, 0.5645]],

        [[0.6350, 0.1237],
         [0.5192, 0.7104]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.007361487795428129
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.012713900284584396
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0029826068632377496
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0017536168347216134
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0045595124595072724
Global Adjusted Rand Index: -0.0021621853953358427
Average Adjusted Rand Index: -0.005172778113607186
[-0.00036156365775701495, -0.0021621853953358427] [0.0004884135958389755, -0.005172778113607186] [10003.896484375, 10004.4326171875]
-------------------------------------
This iteration is 14
True Objective function: Loss = -10046.579871638682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24852.46484375
inf tensor(24852.4648, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9927.5009765625
tensor(24852.4648, grad_fn=<NegBackward0>) tensor(9927.5010, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9918.322265625
tensor(9927.5010, grad_fn=<NegBackward0>) tensor(9918.3223, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9918.2861328125
tensor(9918.3223, grad_fn=<NegBackward0>) tensor(9918.2861, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9918.2001953125
tensor(9918.2861, grad_fn=<NegBackward0>) tensor(9918.2002, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9918.0302734375
tensor(9918.2002, grad_fn=<NegBackward0>) tensor(9918.0303, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9917.892578125
tensor(9918.0303, grad_fn=<NegBackward0>) tensor(9917.8926, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9917.751953125
tensor(9917.8926, grad_fn=<NegBackward0>) tensor(9917.7520, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9916.91015625
tensor(9917.7520, grad_fn=<NegBackward0>) tensor(9916.9102, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9915.4150390625
tensor(9916.9102, grad_fn=<NegBackward0>) tensor(9915.4150, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9915.009765625
tensor(9915.4150, grad_fn=<NegBackward0>) tensor(9915.0098, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9914.9345703125
tensor(9915.0098, grad_fn=<NegBackward0>) tensor(9914.9346, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9914.9072265625
tensor(9914.9346, grad_fn=<NegBackward0>) tensor(9914.9072, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9914.892578125
tensor(9914.9072, grad_fn=<NegBackward0>) tensor(9914.8926, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9914.8828125
tensor(9914.8926, grad_fn=<NegBackward0>) tensor(9914.8828, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9914.876953125
tensor(9914.8828, grad_fn=<NegBackward0>) tensor(9914.8770, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9914.8720703125
tensor(9914.8770, grad_fn=<NegBackward0>) tensor(9914.8721, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9914.869140625
tensor(9914.8721, grad_fn=<NegBackward0>) tensor(9914.8691, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9914.8671875
tensor(9914.8691, grad_fn=<NegBackward0>) tensor(9914.8672, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9914.8642578125
tensor(9914.8672, grad_fn=<NegBackward0>) tensor(9914.8643, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9914.8623046875
tensor(9914.8643, grad_fn=<NegBackward0>) tensor(9914.8623, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9914.861328125
tensor(9914.8623, grad_fn=<NegBackward0>) tensor(9914.8613, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9914.8818359375
tensor(9914.8613, grad_fn=<NegBackward0>) tensor(9914.8818, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9914.8583984375
tensor(9914.8613, grad_fn=<NegBackward0>) tensor(9914.8584, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9914.8583984375
tensor(9914.8584, grad_fn=<NegBackward0>) tensor(9914.8584, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9914.857421875
tensor(9914.8584, grad_fn=<NegBackward0>) tensor(9914.8574, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9914.8564453125
tensor(9914.8574, grad_fn=<NegBackward0>) tensor(9914.8564, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9914.85546875
tensor(9914.8564, grad_fn=<NegBackward0>) tensor(9914.8555, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9914.85546875
tensor(9914.8555, grad_fn=<NegBackward0>) tensor(9914.8555, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9914.8544921875
tensor(9914.8555, grad_fn=<NegBackward0>) tensor(9914.8545, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9914.85546875
tensor(9914.8545, grad_fn=<NegBackward0>) tensor(9914.8555, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9914.853515625
tensor(9914.8545, grad_fn=<NegBackward0>) tensor(9914.8535, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9914.8681640625
tensor(9914.8535, grad_fn=<NegBackward0>) tensor(9914.8682, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9914.8525390625
tensor(9914.8535, grad_fn=<NegBackward0>) tensor(9914.8525, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9914.853515625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8535, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9914.853515625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8535, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -9914.8525390625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8525, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9914.861328125
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8613, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9914.8525390625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8525, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9914.8525390625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8525, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9914.8515625
tensor(9914.8525, grad_fn=<NegBackward0>) tensor(9914.8516, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9914.8525390625
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8525, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9914.8564453125
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8564, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -9914.8515625
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8516, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9914.8583984375
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8584, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9914.8515625
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8516, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9914.8515625
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8516, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9914.849609375
tensor(9914.8516, grad_fn=<NegBackward0>) tensor(9914.8496, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9914.8603515625
tensor(9914.8496, grad_fn=<NegBackward0>) tensor(9914.8604, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9914.8515625
tensor(9914.8496, grad_fn=<NegBackward0>) tensor(9914.8516, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9914.8505859375
tensor(9914.8496, grad_fn=<NegBackward0>) tensor(9914.8506, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -9914.8505859375
tensor(9914.8496, grad_fn=<NegBackward0>) tensor(9914.8506, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -9914.8505859375
tensor(9914.8496, grad_fn=<NegBackward0>) tensor(9914.8506, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.0013, 0.9987],
        [0.5074, 0.4926]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0411, 0.9589], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1212, 0.0585],
         [0.7261, 0.1468]],

        [[0.6635, 0.1271],
         [0.5199, 0.5758]],

        [[0.7069, 0.1343],
         [0.6246, 0.6041]],

        [[0.6336, 0.1457],
         [0.6261, 0.5985]],

        [[0.6068, 0.1265],
         [0.6863, 0.6711]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 63
Adjusted Rand Index: 0.05827405947381952
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.047116115001612856
Global Adjusted Rand Index: 0.020956786649975044
Average Adjusted Rand Index: 0.022235638678045115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23133.55078125
inf tensor(23133.5508, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9922.5087890625
tensor(23133.5508, grad_fn=<NegBackward0>) tensor(9922.5088, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9920.5986328125
tensor(9922.5088, grad_fn=<NegBackward0>) tensor(9920.5986, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9919.8505859375
tensor(9920.5986, grad_fn=<NegBackward0>) tensor(9919.8506, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9919.6513671875
tensor(9919.8506, grad_fn=<NegBackward0>) tensor(9919.6514, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9919.5498046875
tensor(9919.6514, grad_fn=<NegBackward0>) tensor(9919.5498, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9919.486328125
tensor(9919.5498, grad_fn=<NegBackward0>) tensor(9919.4863, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9919.443359375
tensor(9919.4863, grad_fn=<NegBackward0>) tensor(9919.4434, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9919.408203125
tensor(9919.4434, grad_fn=<NegBackward0>) tensor(9919.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9919.3779296875
tensor(9919.4082, grad_fn=<NegBackward0>) tensor(9919.3779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9919.34765625
tensor(9919.3779, grad_fn=<NegBackward0>) tensor(9919.3477, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9919.3173828125
tensor(9919.3477, grad_fn=<NegBackward0>) tensor(9919.3174, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9919.2822265625
tensor(9919.3174, grad_fn=<NegBackward0>) tensor(9919.2822, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9919.2412109375
tensor(9919.2822, grad_fn=<NegBackward0>) tensor(9919.2412, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9919.193359375
tensor(9919.2412, grad_fn=<NegBackward0>) tensor(9919.1934, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9919.134765625
tensor(9919.1934, grad_fn=<NegBackward0>) tensor(9919.1348, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9919.0703125
tensor(9919.1348, grad_fn=<NegBackward0>) tensor(9919.0703, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9919.0029296875
tensor(9919.0703, grad_fn=<NegBackward0>) tensor(9919.0029, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9918.9306640625
tensor(9919.0029, grad_fn=<NegBackward0>) tensor(9918.9307, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9918.8505859375
tensor(9918.9307, grad_fn=<NegBackward0>) tensor(9918.8506, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9918.7587890625
tensor(9918.8506, grad_fn=<NegBackward0>) tensor(9918.7588, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9918.658203125
tensor(9918.7588, grad_fn=<NegBackward0>) tensor(9918.6582, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9918.556640625
tensor(9918.6582, grad_fn=<NegBackward0>) tensor(9918.5566, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9918.4697265625
tensor(9918.5566, grad_fn=<NegBackward0>) tensor(9918.4697, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9918.4072265625
tensor(9918.4697, grad_fn=<NegBackward0>) tensor(9918.4072, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9918.3583984375
tensor(9918.4072, grad_fn=<NegBackward0>) tensor(9918.3584, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9918.31640625
tensor(9918.3584, grad_fn=<NegBackward0>) tensor(9918.3164, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9918.2822265625
tensor(9918.3164, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9918.2578125
tensor(9918.2822, grad_fn=<NegBackward0>) tensor(9918.2578, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9918.2373046875
tensor(9918.2578, grad_fn=<NegBackward0>) tensor(9918.2373, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9918.2216796875
tensor(9918.2373, grad_fn=<NegBackward0>) tensor(9918.2217, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9918.20703125
tensor(9918.2217, grad_fn=<NegBackward0>) tensor(9918.2070, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9918.1923828125
tensor(9918.2070, grad_fn=<NegBackward0>) tensor(9918.1924, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9918.17578125
tensor(9918.1924, grad_fn=<NegBackward0>) tensor(9918.1758, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9918.15234375
tensor(9918.1758, grad_fn=<NegBackward0>) tensor(9918.1523, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9918.1162109375
tensor(9918.1523, grad_fn=<NegBackward0>) tensor(9918.1162, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9918.0439453125
tensor(9918.1162, grad_fn=<NegBackward0>) tensor(9918.0439, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9917.9287109375
tensor(9918.0439, grad_fn=<NegBackward0>) tensor(9917.9287, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9917.828125
tensor(9917.9287, grad_fn=<NegBackward0>) tensor(9917.8281, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9917.681640625
tensor(9917.8281, grad_fn=<NegBackward0>) tensor(9917.6816, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9917.5849609375
tensor(9917.6816, grad_fn=<NegBackward0>) tensor(9917.5850, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9917.5244140625
tensor(9917.5850, grad_fn=<NegBackward0>) tensor(9917.5244, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9917.4853515625
tensor(9917.5244, grad_fn=<NegBackward0>) tensor(9917.4854, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9917.46484375
tensor(9917.4854, grad_fn=<NegBackward0>) tensor(9917.4648, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9917.45703125
tensor(9917.4648, grad_fn=<NegBackward0>) tensor(9917.4570, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9917.4560546875
tensor(9917.4570, grad_fn=<NegBackward0>) tensor(9917.4561, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9917.4541015625
tensor(9917.4561, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9917.453125
tensor(9917.4541, grad_fn=<NegBackward0>) tensor(9917.4531, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9917.453125
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4531, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9917.453125
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4531, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9917.453125
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4531, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9917.4794921875
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4795, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9917.4541015625
tensor(9917.4531, grad_fn=<NegBackward0>) tensor(9917.4541, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.4417, 0.5583],
        [0.9954, 0.0046]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9825, 0.0175], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1382, 0.2275],
         [0.7061, 0.1336]],

        [[0.6303, 0.1281],
         [0.6903, 0.5230]],

        [[0.5628, 0.1409],
         [0.6316, 0.5219]],

        [[0.6753, 0.1476],
         [0.6769, 0.6879]],

        [[0.7128, 0.1313],
         [0.6612, 0.7023]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014711239185679125
Average Adjusted Rand Index: -0.00046630525784062936
[0.020956786649975044, 0.0014711239185679125] [0.022235638678045115, -0.00046630525784062936] [9914.8505859375, 9917.4541015625]
-------------------------------------
This iteration is 15
True Objective function: Loss = -10209.723446850014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21358.9140625
inf tensor(21358.9141, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10107.435546875
tensor(21358.9141, grad_fn=<NegBackward0>) tensor(10107.4355, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10104.22265625
tensor(10107.4355, grad_fn=<NegBackward0>) tensor(10104.2227, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10102.931640625
tensor(10104.2227, grad_fn=<NegBackward0>) tensor(10102.9316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10102.353515625
tensor(10102.9316, grad_fn=<NegBackward0>) tensor(10102.3535, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10102.072265625
tensor(10102.3535, grad_fn=<NegBackward0>) tensor(10102.0723, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10101.8798828125
tensor(10102.0723, grad_fn=<NegBackward0>) tensor(10101.8799, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10101.7294921875
tensor(10101.8799, grad_fn=<NegBackward0>) tensor(10101.7295, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10101.59765625
tensor(10101.7295, grad_fn=<NegBackward0>) tensor(10101.5977, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10101.462890625
tensor(10101.5977, grad_fn=<NegBackward0>) tensor(10101.4629, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10101.1904296875
tensor(10101.4629, grad_fn=<NegBackward0>) tensor(10101.1904, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10100.134765625
tensor(10101.1904, grad_fn=<NegBackward0>) tensor(10100.1348, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10098.7763671875
tensor(10100.1348, grad_fn=<NegBackward0>) tensor(10098.7764, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10098.10546875
tensor(10098.7764, grad_fn=<NegBackward0>) tensor(10098.1055, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10097.2646484375
tensor(10098.1055, grad_fn=<NegBackward0>) tensor(10097.2646, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10092.55078125
tensor(10097.2646, grad_fn=<NegBackward0>) tensor(10092.5508, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10088.6103515625
tensor(10092.5508, grad_fn=<NegBackward0>) tensor(10088.6104, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10087.416015625
tensor(10088.6104, grad_fn=<NegBackward0>) tensor(10087.4160, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10086.966796875
tensor(10087.4160, grad_fn=<NegBackward0>) tensor(10086.9668, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10086.7880859375
tensor(10086.9668, grad_fn=<NegBackward0>) tensor(10086.7881, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10086.7255859375
tensor(10086.7881, grad_fn=<NegBackward0>) tensor(10086.7256, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10086.6806640625
tensor(10086.7256, grad_fn=<NegBackward0>) tensor(10086.6807, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10086.650390625
tensor(10086.6807, grad_fn=<NegBackward0>) tensor(10086.6504, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10086.560546875
tensor(10086.6504, grad_fn=<NegBackward0>) tensor(10086.5605, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10086.0673828125
tensor(10086.5605, grad_fn=<NegBackward0>) tensor(10086.0674, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10086.0546875
tensor(10086.0674, grad_fn=<NegBackward0>) tensor(10086.0547, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10086.0458984375
tensor(10086.0547, grad_fn=<NegBackward0>) tensor(10086.0459, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10086.001953125
tensor(10086.0459, grad_fn=<NegBackward0>) tensor(10086.0020, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10085.9794921875
tensor(10086.0020, grad_fn=<NegBackward0>) tensor(10085.9795, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10085.97265625
tensor(10085.9795, grad_fn=<NegBackward0>) tensor(10085.9727, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10085.8876953125
tensor(10085.9727, grad_fn=<NegBackward0>) tensor(10085.8877, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10085.8681640625
tensor(10085.8877, grad_fn=<NegBackward0>) tensor(10085.8682, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10085.8642578125
tensor(10085.8682, grad_fn=<NegBackward0>) tensor(10085.8643, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10085.8486328125
tensor(10085.8643, grad_fn=<NegBackward0>) tensor(10085.8486, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10085.8466796875
tensor(10085.8486, grad_fn=<NegBackward0>) tensor(10085.8467, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10085.8427734375
tensor(10085.8467, grad_fn=<NegBackward0>) tensor(10085.8428, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10085.8095703125
tensor(10085.8428, grad_fn=<NegBackward0>) tensor(10085.8096, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10085.806640625
tensor(10085.8096, grad_fn=<NegBackward0>) tensor(10085.8066, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10085.7314453125
tensor(10085.8066, grad_fn=<NegBackward0>) tensor(10085.7314, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10085.7314453125
tensor(10085.7314, grad_fn=<NegBackward0>) tensor(10085.7314, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10085.7294921875
tensor(10085.7314, grad_fn=<NegBackward0>) tensor(10085.7295, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10085.7294921875
tensor(10085.7295, grad_fn=<NegBackward0>) tensor(10085.7295, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10085.7109375
tensor(10085.7295, grad_fn=<NegBackward0>) tensor(10085.7109, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10085.708984375
tensor(10085.7109, grad_fn=<NegBackward0>) tensor(10085.7090, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10085.7080078125
tensor(10085.7090, grad_fn=<NegBackward0>) tensor(10085.7080, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10085.7060546875
tensor(10085.7080, grad_fn=<NegBackward0>) tensor(10085.7061, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10085.705078125
tensor(10085.7061, grad_fn=<NegBackward0>) tensor(10085.7051, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10085.70703125
tensor(10085.7051, grad_fn=<NegBackward0>) tensor(10085.7070, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10085.703125
tensor(10085.7051, grad_fn=<NegBackward0>) tensor(10085.7031, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10085.7021484375
tensor(10085.7031, grad_fn=<NegBackward0>) tensor(10085.7021, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10085.701171875
tensor(10085.7021, grad_fn=<NegBackward0>) tensor(10085.7012, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10085.69921875
tensor(10085.7012, grad_fn=<NegBackward0>) tensor(10085.6992, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10085.7080078125
tensor(10085.6992, grad_fn=<NegBackward0>) tensor(10085.7080, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10085.681640625
tensor(10085.6992, grad_fn=<NegBackward0>) tensor(10085.6816, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10085.6806640625
tensor(10085.6816, grad_fn=<NegBackward0>) tensor(10085.6807, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10085.6806640625
tensor(10085.6807, grad_fn=<NegBackward0>) tensor(10085.6807, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10085.6787109375
tensor(10085.6807, grad_fn=<NegBackward0>) tensor(10085.6787, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10085.666015625
tensor(10085.6787, grad_fn=<NegBackward0>) tensor(10085.6660, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10085.6650390625
tensor(10085.6660, grad_fn=<NegBackward0>) tensor(10085.6650, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10085.6650390625
tensor(10085.6650, grad_fn=<NegBackward0>) tensor(10085.6650, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10085.6650390625
tensor(10085.6650, grad_fn=<NegBackward0>) tensor(10085.6650, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10085.66796875
tensor(10085.6650, grad_fn=<NegBackward0>) tensor(10085.6680, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10085.6630859375
tensor(10085.6650, grad_fn=<NegBackward0>) tensor(10085.6631, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10085.6494140625
tensor(10085.6631, grad_fn=<NegBackward0>) tensor(10085.6494, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10085.6474609375
tensor(10085.6494, grad_fn=<NegBackward0>) tensor(10085.6475, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10085.6533203125
tensor(10085.6475, grad_fn=<NegBackward0>) tensor(10085.6533, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10085.642578125
tensor(10085.6475, grad_fn=<NegBackward0>) tensor(10085.6426, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10085.615234375
tensor(10085.6426, grad_fn=<NegBackward0>) tensor(10085.6152, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10085.609375
tensor(10085.6152, grad_fn=<NegBackward0>) tensor(10085.6094, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10085.6064453125
tensor(10085.6094, grad_fn=<NegBackward0>) tensor(10085.6064, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10085.6044921875
tensor(10085.6064, grad_fn=<NegBackward0>) tensor(10085.6045, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10085.6025390625
tensor(10085.6045, grad_fn=<NegBackward0>) tensor(10085.6025, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10085.5849609375
tensor(10085.6025, grad_fn=<NegBackward0>) tensor(10085.5850, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10085.5830078125
tensor(10085.5850, grad_fn=<NegBackward0>) tensor(10085.5830, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10085.5703125
tensor(10085.5830, grad_fn=<NegBackward0>) tensor(10085.5703, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10085.5693359375
tensor(10085.5703, grad_fn=<NegBackward0>) tensor(10085.5693, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10085.5751953125
tensor(10085.5693, grad_fn=<NegBackward0>) tensor(10085.5752, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10085.5693359375
tensor(10085.5693, grad_fn=<NegBackward0>) tensor(10085.5693, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10085.5537109375
tensor(10085.5693, grad_fn=<NegBackward0>) tensor(10085.5537, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10085.5400390625
tensor(10085.5537, grad_fn=<NegBackward0>) tensor(10085.5400, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10085.5390625
tensor(10085.5400, grad_fn=<NegBackward0>) tensor(10085.5391, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10085.537109375
tensor(10085.5391, grad_fn=<NegBackward0>) tensor(10085.5371, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10085.537109375
tensor(10085.5371, grad_fn=<NegBackward0>) tensor(10085.5371, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10085.537109375
tensor(10085.5371, grad_fn=<NegBackward0>) tensor(10085.5371, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10085.53515625
tensor(10085.5371, grad_fn=<NegBackward0>) tensor(10085.5352, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10085.501953125
tensor(10085.5352, grad_fn=<NegBackward0>) tensor(10085.5020, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10085.5029296875
tensor(10085.5020, grad_fn=<NegBackward0>) tensor(10085.5029, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10085.5009765625
tensor(10085.5020, grad_fn=<NegBackward0>) tensor(10085.5010, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10085.5
tensor(10085.5010, grad_fn=<NegBackward0>) tensor(10085.5000, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10085.5
tensor(10085.5000, grad_fn=<NegBackward0>) tensor(10085.5000, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10085.50390625
tensor(10085.5000, grad_fn=<NegBackward0>) tensor(10085.5039, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10085.515625
tensor(10085.5000, grad_fn=<NegBackward0>) tensor(10085.5156, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10085.4990234375
tensor(10085.5000, grad_fn=<NegBackward0>) tensor(10085.4990, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10085.501953125
tensor(10085.4990, grad_fn=<NegBackward0>) tensor(10085.5020, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10085.4951171875
tensor(10085.4990, grad_fn=<NegBackward0>) tensor(10085.4951, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10085.4931640625
tensor(10085.4951, grad_fn=<NegBackward0>) tensor(10085.4932, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10085.490234375
tensor(10085.4932, grad_fn=<NegBackward0>) tensor(10085.4902, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10085.4892578125
tensor(10085.4902, grad_fn=<NegBackward0>) tensor(10085.4893, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10085.4921875
tensor(10085.4893, grad_fn=<NegBackward0>) tensor(10085.4922, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10085.4814453125
tensor(10085.4893, grad_fn=<NegBackward0>) tensor(10085.4814, grad_fn=<NegBackward0>)
pi: tensor([[8.6418e-01, 1.3582e-01],
        [1.6811e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9140, 0.0860], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1321, 0.1584],
         [0.5121, 0.2236]],

        [[0.6755, 0.1681],
         [0.6625, 0.6072]],

        [[0.6096, 0.1177],
         [0.5334, 0.7011]],

        [[0.5965, 0.1336],
         [0.7278, 0.6512]],

        [[0.5639, 0.1048],
         [0.6580, 0.7244]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.021191995813393903
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 74
Adjusted Rand Index: 0.22316194085694444
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 73
Adjusted Rand Index: 0.2037923394253957
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 82
Adjusted Rand Index: 0.4036460016807809
Global Adjusted Rand Index: 0.11854722570760401
Average Adjusted Rand Index: 0.16951599220728078
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25344.716796875
inf tensor(25344.7168, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10104.9541015625
tensor(25344.7168, grad_fn=<NegBackward0>) tensor(10104.9541, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10103.3154296875
tensor(10104.9541, grad_fn=<NegBackward0>) tensor(10103.3154, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10102.99609375
tensor(10103.3154, grad_fn=<NegBackward0>) tensor(10102.9961, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10102.6669921875
tensor(10102.9961, grad_fn=<NegBackward0>) tensor(10102.6670, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10102.1904296875
tensor(10102.6670, grad_fn=<NegBackward0>) tensor(10102.1904, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10101.84765625
tensor(10102.1904, grad_fn=<NegBackward0>) tensor(10101.8477, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10101.6357421875
tensor(10101.8477, grad_fn=<NegBackward0>) tensor(10101.6357, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10101.4970703125
tensor(10101.6357, grad_fn=<NegBackward0>) tensor(10101.4971, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10101.2509765625
tensor(10101.4971, grad_fn=<NegBackward0>) tensor(10101.2510, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10100.4599609375
tensor(10101.2510, grad_fn=<NegBackward0>) tensor(10100.4600, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10098.8037109375
tensor(10100.4600, grad_fn=<NegBackward0>) tensor(10098.8037, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10098.001953125
tensor(10098.8037, grad_fn=<NegBackward0>) tensor(10098.0020, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10097.603515625
tensor(10098.0020, grad_fn=<NegBackward0>) tensor(10097.6035, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10097.1708984375
tensor(10097.6035, grad_fn=<NegBackward0>) tensor(10097.1709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10096.6318359375
tensor(10097.1709, grad_fn=<NegBackward0>) tensor(10096.6318, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10096.12109375
tensor(10096.6318, grad_fn=<NegBackward0>) tensor(10096.1211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10095.7783203125
tensor(10096.1211, grad_fn=<NegBackward0>) tensor(10095.7783, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10095.5712890625
tensor(10095.7783, grad_fn=<NegBackward0>) tensor(10095.5713, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10095.4296875
tensor(10095.5713, grad_fn=<NegBackward0>) tensor(10095.4297, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10095.333984375
tensor(10095.4297, grad_fn=<NegBackward0>) tensor(10095.3340, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10095.263671875
tensor(10095.3340, grad_fn=<NegBackward0>) tensor(10095.2637, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10095.2119140625
tensor(10095.2637, grad_fn=<NegBackward0>) tensor(10095.2119, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10095.1806640625
tensor(10095.2119, grad_fn=<NegBackward0>) tensor(10095.1807, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10095.16015625
tensor(10095.1807, grad_fn=<NegBackward0>) tensor(10095.1602, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10095.146484375
tensor(10095.1602, grad_fn=<NegBackward0>) tensor(10095.1465, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10095.1337890625
tensor(10095.1465, grad_fn=<NegBackward0>) tensor(10095.1338, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10095.125
tensor(10095.1338, grad_fn=<NegBackward0>) tensor(10095.1250, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10095.119140625
tensor(10095.1250, grad_fn=<NegBackward0>) tensor(10095.1191, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10095.1142578125
tensor(10095.1191, grad_fn=<NegBackward0>) tensor(10095.1143, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10095.1103515625
tensor(10095.1143, grad_fn=<NegBackward0>) tensor(10095.1104, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10095.1064453125
tensor(10095.1104, grad_fn=<NegBackward0>) tensor(10095.1064, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10095.1025390625
tensor(10095.1064, grad_fn=<NegBackward0>) tensor(10095.1025, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10095.0986328125
tensor(10095.1025, grad_fn=<NegBackward0>) tensor(10095.0986, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10095.0966796875
tensor(10095.0986, grad_fn=<NegBackward0>) tensor(10095.0967, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10095.0927734375
tensor(10095.0967, grad_fn=<NegBackward0>) tensor(10095.0928, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10095.0908203125
tensor(10095.0928, grad_fn=<NegBackward0>) tensor(10095.0908, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10095.08984375
tensor(10095.0908, grad_fn=<NegBackward0>) tensor(10095.0898, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10095.087890625
tensor(10095.0898, grad_fn=<NegBackward0>) tensor(10095.0879, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10095.0869140625
tensor(10095.0879, grad_fn=<NegBackward0>) tensor(10095.0869, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10095.0849609375
tensor(10095.0869, grad_fn=<NegBackward0>) tensor(10095.0850, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10095.083984375
tensor(10095.0850, grad_fn=<NegBackward0>) tensor(10095.0840, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10095.08203125
tensor(10095.0840, grad_fn=<NegBackward0>) tensor(10095.0820, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10095.080078125
tensor(10095.0820, grad_fn=<NegBackward0>) tensor(10095.0801, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10095.080078125
tensor(10095.0801, grad_fn=<NegBackward0>) tensor(10095.0801, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10095.078125
tensor(10095.0801, grad_fn=<NegBackward0>) tensor(10095.0781, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10095.07421875
tensor(10095.0781, grad_fn=<NegBackward0>) tensor(10095.0742, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10095.072265625
tensor(10095.0742, grad_fn=<NegBackward0>) tensor(10095.0723, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10095.0712890625
tensor(10095.0723, grad_fn=<NegBackward0>) tensor(10095.0713, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10095.0693359375
tensor(10095.0713, grad_fn=<NegBackward0>) tensor(10095.0693, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10095.0693359375
tensor(10095.0693, grad_fn=<NegBackward0>) tensor(10095.0693, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10095.068359375
tensor(10095.0693, grad_fn=<NegBackward0>) tensor(10095.0684, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10095.068359375
tensor(10095.0684, grad_fn=<NegBackward0>) tensor(10095.0684, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10095.068359375
tensor(10095.0684, grad_fn=<NegBackward0>) tensor(10095.0684, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10095.0673828125
tensor(10095.0684, grad_fn=<NegBackward0>) tensor(10095.0674, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10095.0673828125
tensor(10095.0674, grad_fn=<NegBackward0>) tensor(10095.0674, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10095.0654296875
tensor(10095.0674, grad_fn=<NegBackward0>) tensor(10095.0654, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10095.0654296875
tensor(10095.0654, grad_fn=<NegBackward0>) tensor(10095.0654, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10095.0654296875
tensor(10095.0654, grad_fn=<NegBackward0>) tensor(10095.0654, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10095.064453125
tensor(10095.0654, grad_fn=<NegBackward0>) tensor(10095.0645, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10095.064453125
tensor(10095.0645, grad_fn=<NegBackward0>) tensor(10095.0645, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10095.0634765625
tensor(10095.0645, grad_fn=<NegBackward0>) tensor(10095.0635, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10095.064453125
tensor(10095.0635, grad_fn=<NegBackward0>) tensor(10095.0645, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10095.0625
tensor(10095.0635, grad_fn=<NegBackward0>) tensor(10095.0625, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10095.0634765625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0635, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10095.0634765625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0635, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10095.0634765625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0635, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10095.0634765625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0635, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -10095.0625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0625, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10095.0625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0625, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10095.0625
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0625, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10095.060546875
tensor(10095.0625, grad_fn=<NegBackward0>) tensor(10095.0605, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10095.0615234375
tensor(10095.0605, grad_fn=<NegBackward0>) tensor(10095.0615, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10095.0615234375
tensor(10095.0605, grad_fn=<NegBackward0>) tensor(10095.0615, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10095.0615234375
tensor(10095.0605, grad_fn=<NegBackward0>) tensor(10095.0615, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -10095.0615234375
tensor(10095.0605, grad_fn=<NegBackward0>) tensor(10095.0615, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -10095.0615234375
tensor(10095.0605, grad_fn=<NegBackward0>) tensor(10095.0615, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[9.9983e-01, 1.6975e-04],
        [4.3429e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0371, 0.9629], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1047, 0.1561],
         [0.6671, 0.1386]],

        [[0.5295, 0.2455],
         [0.6041, 0.5601]],

        [[0.6025, 0.1276],
         [0.5653, 0.5587]],

        [[0.5264, 0.1601],
         [0.7170, 0.6742]],

        [[0.5418, 0.2347],
         [0.5193, 0.5813]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
Global Adjusted Rand Index: -0.002385729049420206
Average Adjusted Rand Index: -0.00356746465591987
[0.11854722570760401, -0.002385729049420206] [0.16951599220728078, -0.00356746465591987] [10085.482421875, 10095.0615234375]
-------------------------------------
This iteration is 16
True Objective function: Loss = -10142.842690094172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23484.056640625
inf tensor(23484.0566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10033.921875
tensor(23484.0566, grad_fn=<NegBackward0>) tensor(10033.9219, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10032.4384765625
tensor(10033.9219, grad_fn=<NegBackward0>) tensor(10032.4385, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10031.958984375
tensor(10032.4385, grad_fn=<NegBackward0>) tensor(10031.9590, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10031.6630859375
tensor(10031.9590, grad_fn=<NegBackward0>) tensor(10031.6631, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10031.349609375
tensor(10031.6631, grad_fn=<NegBackward0>) tensor(10031.3496, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10030.9599609375
tensor(10031.3496, grad_fn=<NegBackward0>) tensor(10030.9600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10030.6318359375
tensor(10030.9600, grad_fn=<NegBackward0>) tensor(10030.6318, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10030.384765625
tensor(10030.6318, grad_fn=<NegBackward0>) tensor(10030.3848, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10030.1787109375
tensor(10030.3848, grad_fn=<NegBackward0>) tensor(10030.1787, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10030.001953125
tensor(10030.1787, grad_fn=<NegBackward0>) tensor(10030.0020, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10029.84765625
tensor(10030.0020, grad_fn=<NegBackward0>) tensor(10029.8477, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10029.701171875
tensor(10029.8477, grad_fn=<NegBackward0>) tensor(10029.7012, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10029.556640625
tensor(10029.7012, grad_fn=<NegBackward0>) tensor(10029.5566, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10029.4111328125
tensor(10029.5566, grad_fn=<NegBackward0>) tensor(10029.4111, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10029.2587890625
tensor(10029.4111, grad_fn=<NegBackward0>) tensor(10029.2588, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10029.107421875
tensor(10029.2588, grad_fn=<NegBackward0>) tensor(10029.1074, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10028.955078125
tensor(10029.1074, grad_fn=<NegBackward0>) tensor(10028.9551, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10028.80859375
tensor(10028.9551, grad_fn=<NegBackward0>) tensor(10028.8086, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10028.6689453125
tensor(10028.8086, grad_fn=<NegBackward0>) tensor(10028.6689, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10028.544921875
tensor(10028.6689, grad_fn=<NegBackward0>) tensor(10028.5449, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10028.4404296875
tensor(10028.5449, grad_fn=<NegBackward0>) tensor(10028.4404, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10028.3603515625
tensor(10028.4404, grad_fn=<NegBackward0>) tensor(10028.3604, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10028.3076171875
tensor(10028.3604, grad_fn=<NegBackward0>) tensor(10028.3076, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10028.2744140625
tensor(10028.3076, grad_fn=<NegBackward0>) tensor(10028.2744, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10028.26171875
tensor(10028.2744, grad_fn=<NegBackward0>) tensor(10028.2617, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10028.236328125
tensor(10028.2617, grad_fn=<NegBackward0>) tensor(10028.2363, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10028.2158203125
tensor(10028.2363, grad_fn=<NegBackward0>) tensor(10028.2158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10028.193359375
tensor(10028.2158, grad_fn=<NegBackward0>) tensor(10028.1934, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10028.1611328125
tensor(10028.1934, grad_fn=<NegBackward0>) tensor(10028.1611, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10028.1103515625
tensor(10028.1611, grad_fn=<NegBackward0>) tensor(10028.1104, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10028.0234375
tensor(10028.1104, grad_fn=<NegBackward0>) tensor(10028.0234, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10027.8603515625
tensor(10028.0234, grad_fn=<NegBackward0>) tensor(10027.8604, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10027.5029296875
tensor(10027.8604, grad_fn=<NegBackward0>) tensor(10027.5029, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10026.7314453125
tensor(10027.5029, grad_fn=<NegBackward0>) tensor(10026.7314, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10025.861328125
tensor(10026.7314, grad_fn=<NegBackward0>) tensor(10025.8613, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10025.5947265625
tensor(10025.8613, grad_fn=<NegBackward0>) tensor(10025.5947, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10025.5498046875
tensor(10025.5947, grad_fn=<NegBackward0>) tensor(10025.5498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10025.5380859375
tensor(10025.5498, grad_fn=<NegBackward0>) tensor(10025.5381, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10025.537109375
tensor(10025.5381, grad_fn=<NegBackward0>) tensor(10025.5371, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10025.537109375
tensor(10025.5371, grad_fn=<NegBackward0>) tensor(10025.5371, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10025.5361328125
tensor(10025.5371, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10025.53515625
tensor(10025.5361, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10025.53515625
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10025.537109375
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5371, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10025.537109375
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5371, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10025.53515625
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10025.53515625
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10025.53515625
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10025.5341796875
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5342, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10025.53515625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10025.5361328125
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10025.53515625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -10025.53515625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -10025.537109375
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5371, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.7956, 0.2044],
        [0.4353, 0.5647]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7280, 0.2720], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1635, 0.1268],
         [0.5708, 0.0990]],

        [[0.7305, 0.1163],
         [0.6408, 0.6873]],

        [[0.5373, 0.1252],
         [0.7215, 0.5571]],

        [[0.6857, 0.1226],
         [0.7162, 0.6312]],

        [[0.5763, 0.1228],
         [0.7300, 0.5002]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.06225390001178435
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 26
Adjusted Rand Index: 0.22262626262626262
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03281965310464891
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.08269491587551776
Global Adjusted Rand Index: 0.0839627586348888
Average Adjusted Rand Index: 0.0880667251628966
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22474.78515625
inf tensor(22474.7852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10033.009765625
tensor(22474.7852, grad_fn=<NegBackward0>) tensor(10033.0098, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10032.146484375
tensor(10033.0098, grad_fn=<NegBackward0>) tensor(10032.1465, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10031.8955078125
tensor(10032.1465, grad_fn=<NegBackward0>) tensor(10031.8955, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10031.70703125
tensor(10031.8955, grad_fn=<NegBackward0>) tensor(10031.7070, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10031.37890625
tensor(10031.7070, grad_fn=<NegBackward0>) tensor(10031.3789, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10030.7685546875
tensor(10031.3789, grad_fn=<NegBackward0>) tensor(10030.7686, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10030.2783203125
tensor(10030.7686, grad_fn=<NegBackward0>) tensor(10030.2783, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10030.009765625
tensor(10030.2783, grad_fn=<NegBackward0>) tensor(10030.0098, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10029.794921875
tensor(10030.0098, grad_fn=<NegBackward0>) tensor(10029.7949, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10029.61328125
tensor(10029.7949, grad_fn=<NegBackward0>) tensor(10029.6133, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10029.435546875
tensor(10029.6133, grad_fn=<NegBackward0>) tensor(10029.4355, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10029.25390625
tensor(10029.4355, grad_fn=<NegBackward0>) tensor(10029.2539, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10029.0712890625
tensor(10029.2539, grad_fn=<NegBackward0>) tensor(10029.0713, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10028.88671875
tensor(10029.0713, grad_fn=<NegBackward0>) tensor(10028.8867, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10028.7109375
tensor(10028.8867, grad_fn=<NegBackward0>) tensor(10028.7109, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10028.556640625
tensor(10028.7109, grad_fn=<NegBackward0>) tensor(10028.5566, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10028.4345703125
tensor(10028.5566, grad_fn=<NegBackward0>) tensor(10028.4346, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10028.349609375
tensor(10028.4346, grad_fn=<NegBackward0>) tensor(10028.3496, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10028.294921875
tensor(10028.3496, grad_fn=<NegBackward0>) tensor(10028.2949, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10028.2587890625
tensor(10028.2949, grad_fn=<NegBackward0>) tensor(10028.2588, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10028.2333984375
tensor(10028.2588, grad_fn=<NegBackward0>) tensor(10028.2334, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10028.205078125
tensor(10028.2334, grad_fn=<NegBackward0>) tensor(10028.2051, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10028.1689453125
tensor(10028.2051, grad_fn=<NegBackward0>) tensor(10028.1689, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10028.1142578125
tensor(10028.1689, grad_fn=<NegBackward0>) tensor(10028.1143, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10028.0146484375
tensor(10028.1143, grad_fn=<NegBackward0>) tensor(10028.0146, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10027.8193359375
tensor(10028.0146, grad_fn=<NegBackward0>) tensor(10027.8193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10027.357421875
tensor(10027.8193, grad_fn=<NegBackward0>) tensor(10027.3574, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10026.41796875
tensor(10027.3574, grad_fn=<NegBackward0>) tensor(10026.4180, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10025.7275390625
tensor(10026.4180, grad_fn=<NegBackward0>) tensor(10025.7275, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10025.5712890625
tensor(10025.7275, grad_fn=<NegBackward0>) tensor(10025.5713, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10025.54296875
tensor(10025.5713, grad_fn=<NegBackward0>) tensor(10025.5430, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10025.5361328125
tensor(10025.5430, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10025.53515625
tensor(10025.5361, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10025.5361328125
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10025.5341796875
tensor(10025.5352, grad_fn=<NegBackward0>) tensor(10025.5342, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10025.5361328125
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10025.5341796875
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5342, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10025.5361328125
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10025.53515625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -10025.53515625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5352, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -10025.5400390625
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5400, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -10025.5361328125
tensor(10025.5342, grad_fn=<NegBackward0>) tensor(10025.5361, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.5630, 0.4370],
        [0.2044, 0.7956]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2721, 0.7279], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0990, 0.1268],
         [0.5708, 0.1635]],

        [[0.5641, 0.1163],
         [0.6556, 0.7228]],

        [[0.7193, 0.1252],
         [0.5456, 0.6031]],

        [[0.6040, 0.1226],
         [0.6746, 0.6581]],

        [[0.5183, 0.1228],
         [0.7279, 0.6205]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 63
Adjusted Rand Index: 0.06225390001178435
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 74
Adjusted Rand Index: 0.22262626262626262
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03281965310464891
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 65
Adjusted Rand Index: 0.08269491587551776
Global Adjusted Rand Index: 0.0839627586348888
Average Adjusted Rand Index: 0.0880667251628966
[0.0839627586348888, 0.0839627586348888] [0.0880667251628966, 0.0880667251628966] [10025.537109375, 10025.5361328125]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10088.265418513682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22500.421875
inf tensor(22500.4219, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10014.96484375
tensor(22500.4219, grad_fn=<NegBackward0>) tensor(10014.9648, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10013.927734375
tensor(10014.9648, grad_fn=<NegBackward0>) tensor(10013.9277, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10013.53515625
tensor(10013.9277, grad_fn=<NegBackward0>) tensor(10013.5352, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10013.3046875
tensor(10013.5352, grad_fn=<NegBackward0>) tensor(10013.3047, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10013.140625
tensor(10013.3047, grad_fn=<NegBackward0>) tensor(10013.1406, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10013.0029296875
tensor(10013.1406, grad_fn=<NegBackward0>) tensor(10013.0029, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10012.8583984375
tensor(10013.0029, grad_fn=<NegBackward0>) tensor(10012.8584, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10012.6904296875
tensor(10012.8584, grad_fn=<NegBackward0>) tensor(10012.6904, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10012.50390625
tensor(10012.6904, grad_fn=<NegBackward0>) tensor(10012.5039, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10012.353515625
tensor(10012.5039, grad_fn=<NegBackward0>) tensor(10012.3535, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10012.23828125
tensor(10012.3535, grad_fn=<NegBackward0>) tensor(10012.2383, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10012.1357421875
tensor(10012.2383, grad_fn=<NegBackward0>) tensor(10012.1357, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10012.0380859375
tensor(10012.1357, grad_fn=<NegBackward0>) tensor(10012.0381, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10011.9384765625
tensor(10012.0381, grad_fn=<NegBackward0>) tensor(10011.9385, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10011.8408203125
tensor(10011.9385, grad_fn=<NegBackward0>) tensor(10011.8408, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10011.744140625
tensor(10011.8408, grad_fn=<NegBackward0>) tensor(10011.7441, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10011.6513671875
tensor(10011.7441, grad_fn=<NegBackward0>) tensor(10011.6514, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10011.5546875
tensor(10011.6514, grad_fn=<NegBackward0>) tensor(10011.5547, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10011.439453125
tensor(10011.5547, grad_fn=<NegBackward0>) tensor(10011.4395, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10011.2666015625
tensor(10011.4395, grad_fn=<NegBackward0>) tensor(10011.2666, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10010.85546875
tensor(10011.2666, grad_fn=<NegBackward0>) tensor(10010.8555, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10009.9267578125
tensor(10010.8555, grad_fn=<NegBackward0>) tensor(10009.9268, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10009.0947265625
tensor(10009.9268, grad_fn=<NegBackward0>) tensor(10009.0947, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10007.3525390625
tensor(10009.0947, grad_fn=<NegBackward0>) tensor(10007.3525, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9983.8603515625
tensor(10007.3525, grad_fn=<NegBackward0>) tensor(9983.8604, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9977.888671875
tensor(9983.8604, grad_fn=<NegBackward0>) tensor(9977.8887, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9977.5400390625
tensor(9977.8887, grad_fn=<NegBackward0>) tensor(9977.5400, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9977.458984375
tensor(9977.5400, grad_fn=<NegBackward0>) tensor(9977.4590, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9977.23828125
tensor(9977.4590, grad_fn=<NegBackward0>) tensor(9977.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9977.224609375
tensor(9977.2383, grad_fn=<NegBackward0>) tensor(9977.2246, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9977.2109375
tensor(9977.2246, grad_fn=<NegBackward0>) tensor(9977.2109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9977.173828125
tensor(9977.2109, grad_fn=<NegBackward0>) tensor(9977.1738, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9977.1513671875
tensor(9977.1738, grad_fn=<NegBackward0>) tensor(9977.1514, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9977.1474609375
tensor(9977.1514, grad_fn=<NegBackward0>) tensor(9977.1475, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9977.1455078125
tensor(9977.1475, grad_fn=<NegBackward0>) tensor(9977.1455, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9977.142578125
tensor(9977.1455, grad_fn=<NegBackward0>) tensor(9977.1426, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9977.1396484375
tensor(9977.1426, grad_fn=<NegBackward0>) tensor(9977.1396, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9977.1376953125
tensor(9977.1396, grad_fn=<NegBackward0>) tensor(9977.1377, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9977.13671875
tensor(9977.1377, grad_fn=<NegBackward0>) tensor(9977.1367, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9977.1376953125
tensor(9977.1367, grad_fn=<NegBackward0>) tensor(9977.1377, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9977.134765625
tensor(9977.1367, grad_fn=<NegBackward0>) tensor(9977.1348, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9977.1337890625
tensor(9977.1348, grad_fn=<NegBackward0>) tensor(9977.1338, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9977.1328125
tensor(9977.1338, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9977.1318359375
tensor(9977.1328, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9977.1337890625
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1338, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9977.130859375
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9977.130859375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9977.130859375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9977.1298828125
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9977.1298828125
tensor(9977.1299, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9977.130859375
tensor(9977.1299, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9977.12890625
tensor(9977.1299, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9977.130859375
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9977.12890625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9977.12890625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9977.1279296875
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1279, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9977.126953125
tensor(9977.1279, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9977.1318359375
tensor(9977.1270, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9977.126953125
tensor(9977.1270, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9977.126953125
tensor(9977.1270, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9977.126953125
tensor(9977.1270, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9977.1259765625
tensor(9977.1270, grad_fn=<NegBackward0>) tensor(9977.1260, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9977.126953125
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9977.126953125
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9977.12890625
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -9977.1259765625
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1260, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9977.1279296875
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1279, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9977.126953125
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1270, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9977.1259765625
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1260, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9977.125
tensor(9977.1260, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9977.125
tensor(9977.1250, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9977.1240234375
tensor(9977.1250, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9977.1240234375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9977.1337890625
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1338, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9977.125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -9977.1240234375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9977.1240234375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9977.1240234375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9977.125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9977.1708984375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1709, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9977.1240234375
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1240, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9977.125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9977.125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9977.125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1250, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9977.1328125
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -9977.1259765625
tensor(9977.1240, grad_fn=<NegBackward0>) tensor(9977.1260, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.8651, 0.1349],
        [0.1973, 0.8027]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5176, 0.4824], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.0976],
         [0.5651, 0.1526]],

        [[0.6067, 0.1043],
         [0.5404, 0.5608]],

        [[0.6391, 0.1024],
         [0.6329, 0.6810]],

        [[0.6801, 0.0948],
         [0.5085, 0.6496]],

        [[0.5764, 0.0879],
         [0.6931, 0.5796]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.5733065849980433
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 13
Adjusted Rand Index: 0.5429927016951992
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 20
Adjusted Rand Index: 0.3534948257546111
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7019372117711894
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6363636363636364
Global Adjusted Rand Index: 0.5586161259978316
Average Adjusted Rand Index: 0.5616189921165359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22801.294921875
inf tensor(22801.2949, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10014.15234375
tensor(22801.2949, grad_fn=<NegBackward0>) tensor(10014.1523, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10013.3388671875
tensor(10014.1523, grad_fn=<NegBackward0>) tensor(10013.3389, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10012.9912109375
tensor(10013.3389, grad_fn=<NegBackward0>) tensor(10012.9912, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10012.705078125
tensor(10012.9912, grad_fn=<NegBackward0>) tensor(10012.7051, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10012.4892578125
tensor(10012.7051, grad_fn=<NegBackward0>) tensor(10012.4893, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10012.3583984375
tensor(10012.4893, grad_fn=<NegBackward0>) tensor(10012.3584, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10012.25390625
tensor(10012.3584, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10012.14453125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.1445, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10012.0400390625
tensor(10012.1445, grad_fn=<NegBackward0>) tensor(10012.0400, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10011.939453125
tensor(10012.0400, grad_fn=<NegBackward0>) tensor(10011.9395, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10011.8408203125
tensor(10011.9395, grad_fn=<NegBackward0>) tensor(10011.8408, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10011.7490234375
tensor(10011.8408, grad_fn=<NegBackward0>) tensor(10011.7490, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10011.6533203125
tensor(10011.7490, grad_fn=<NegBackward0>) tensor(10011.6533, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10011.5302734375
tensor(10011.6533, grad_fn=<NegBackward0>) tensor(10011.5303, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10011.3017578125
tensor(10011.5303, grad_fn=<NegBackward0>) tensor(10011.3018, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10010.2958984375
tensor(10011.3018, grad_fn=<NegBackward0>) tensor(10010.2959, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10009.0302734375
tensor(10010.2959, grad_fn=<NegBackward0>) tensor(10009.0303, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9993.7744140625
tensor(10009.0303, grad_fn=<NegBackward0>) tensor(9993.7744, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9989.826171875
tensor(9993.7744, grad_fn=<NegBackward0>) tensor(9989.8262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9989.6708984375
tensor(9989.8262, grad_fn=<NegBackward0>) tensor(9989.6709, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9989.59375
tensor(9989.6709, grad_fn=<NegBackward0>) tensor(9989.5938, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9989.57421875
tensor(9989.5938, grad_fn=<NegBackward0>) tensor(9989.5742, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9989.5673828125
tensor(9989.5742, grad_fn=<NegBackward0>) tensor(9989.5674, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9989.5556640625
tensor(9989.5674, grad_fn=<NegBackward0>) tensor(9989.5557, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9989.552734375
tensor(9989.5557, grad_fn=<NegBackward0>) tensor(9989.5527, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9989.5498046875
tensor(9989.5527, grad_fn=<NegBackward0>) tensor(9989.5498, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9989.546875
tensor(9989.5498, grad_fn=<NegBackward0>) tensor(9989.5469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9989.546875
tensor(9989.5469, grad_fn=<NegBackward0>) tensor(9989.5469, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9989.529296875
tensor(9989.5469, grad_fn=<NegBackward0>) tensor(9989.5293, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9988.7763671875
tensor(9989.5293, grad_fn=<NegBackward0>) tensor(9988.7764, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9977.45703125
tensor(9988.7764, grad_fn=<NegBackward0>) tensor(9977.4570, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9977.32421875
tensor(9977.4570, grad_fn=<NegBackward0>) tensor(9977.3242, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9977.177734375
tensor(9977.3242, grad_fn=<NegBackward0>) tensor(9977.1777, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9977.1552734375
tensor(9977.1777, grad_fn=<NegBackward0>) tensor(9977.1553, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9977.1484375
tensor(9977.1553, grad_fn=<NegBackward0>) tensor(9977.1484, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9977.1474609375
tensor(9977.1484, grad_fn=<NegBackward0>) tensor(9977.1475, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9977.1435546875
tensor(9977.1475, grad_fn=<NegBackward0>) tensor(9977.1436, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9977.140625
tensor(9977.1436, grad_fn=<NegBackward0>) tensor(9977.1406, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9977.142578125
tensor(9977.1406, grad_fn=<NegBackward0>) tensor(9977.1426, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9977.1435546875
tensor(9977.1406, grad_fn=<NegBackward0>) tensor(9977.1436, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9977.1396484375
tensor(9977.1406, grad_fn=<NegBackward0>) tensor(9977.1396, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9977.1376953125
tensor(9977.1396, grad_fn=<NegBackward0>) tensor(9977.1377, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9977.1357421875
tensor(9977.1377, grad_fn=<NegBackward0>) tensor(9977.1357, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9977.1328125
tensor(9977.1357, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9977.1318359375
tensor(9977.1328, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9977.1318359375
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9977.1328125
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9977.1318359375
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9977.1328125
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9977.1328125
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1328, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -9977.130859375
tensor(9977.1318, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9977.1318359375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9977.130859375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9977.1318359375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9977.130859375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9977.1572265625
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1572, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9977.130859375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9977.1484375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1484, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9977.1318359375
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9977.1298828125
tensor(9977.1309, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9977.1298828125
tensor(9977.1299, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9977.12890625
tensor(9977.1299, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9977.1357421875
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1357, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9977.12890625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9977.1318359375
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1318, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9977.1298828125
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9977.12890625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9977.1298828125
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9977.12890625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1289, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9977.1298828125
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9977.1298828125
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9977.1298828125
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1299, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9977.140625
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1406, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -9977.130859375
tensor(9977.1289, grad_fn=<NegBackward0>) tensor(9977.1309, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.8021, 0.1979],
        [0.1348, 0.8652]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4841, 0.5159], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1526, 0.0976],
         [0.5894, 0.2000]],

        [[0.5308, 0.1043],
         [0.6151, 0.6712]],

        [[0.6860, 0.1024],
         [0.5001, 0.6564]],

        [[0.7228, 0.0948],
         [0.6899, 0.5330]],

        [[0.7142, 0.0879],
         [0.5753, 0.5304]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733065849980433
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 87
Adjusted Rand Index: 0.5429927016951992
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 80
Adjusted Rand Index: 0.3534948257546111
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7019372117711894
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363636363636364
Global Adjusted Rand Index: 0.5586161259978316
Average Adjusted Rand Index: 0.5616189921165359
[0.5586161259978316, 0.5586161259978316] [0.5616189921165359, 0.5616189921165359] [9977.1259765625, 9977.130859375]
-------------------------------------
This iteration is 18
True Objective function: Loss = -9983.763219102699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20159.623046875
inf tensor(20159.6230, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9840.4111328125
tensor(20159.6230, grad_fn=<NegBackward0>) tensor(9840.4111, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9839.1513671875
tensor(9840.4111, grad_fn=<NegBackward0>) tensor(9839.1514, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9837.548828125
tensor(9839.1514, grad_fn=<NegBackward0>) tensor(9837.5488, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9836.3994140625
tensor(9837.5488, grad_fn=<NegBackward0>) tensor(9836.3994, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9835.484375
tensor(9836.3994, grad_fn=<NegBackward0>) tensor(9835.4844, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9834.7998046875
tensor(9835.4844, grad_fn=<NegBackward0>) tensor(9834.7998, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9834.126953125
tensor(9834.7998, grad_fn=<NegBackward0>) tensor(9834.1270, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9833.5048828125
tensor(9834.1270, grad_fn=<NegBackward0>) tensor(9833.5049, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9833.119140625
tensor(9833.5049, grad_fn=<NegBackward0>) tensor(9833.1191, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9832.884765625
tensor(9833.1191, grad_fn=<NegBackward0>) tensor(9832.8848, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9832.6923828125
tensor(9832.8848, grad_fn=<NegBackward0>) tensor(9832.6924, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9832.5927734375
tensor(9832.6924, grad_fn=<NegBackward0>) tensor(9832.5928, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9832.5654296875
tensor(9832.5928, grad_fn=<NegBackward0>) tensor(9832.5654, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9832.5546875
tensor(9832.5654, grad_fn=<NegBackward0>) tensor(9832.5547, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9832.548828125
tensor(9832.5547, grad_fn=<NegBackward0>) tensor(9832.5488, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9832.5439453125
tensor(9832.5488, grad_fn=<NegBackward0>) tensor(9832.5439, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9832.5400390625
tensor(9832.5439, grad_fn=<NegBackward0>) tensor(9832.5400, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9832.5380859375
tensor(9832.5400, grad_fn=<NegBackward0>) tensor(9832.5381, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9832.5361328125
tensor(9832.5381, grad_fn=<NegBackward0>) tensor(9832.5361, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9832.53515625
tensor(9832.5361, grad_fn=<NegBackward0>) tensor(9832.5352, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9832.533203125
tensor(9832.5352, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
2
Iteration 2400: Loss = -9832.533203125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9832.533203125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9832.5322265625
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9832.533203125
tensor(9832.5322, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9832.5322265625
tensor(9832.5322, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9832.53125
tensor(9832.5322, grad_fn=<NegBackward0>) tensor(9832.5312, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9832.5322265625
tensor(9832.5312, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9832.5322265625
tensor(9832.5312, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -9832.5322265625
tensor(9832.5312, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -9832.5322265625
tensor(9832.5312, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
4
Iteration 3400: Loss = -9832.5302734375
tensor(9832.5312, grad_fn=<NegBackward0>) tensor(9832.5303, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9832.53125
tensor(9832.5303, grad_fn=<NegBackward0>) tensor(9832.5312, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9832.5322265625
tensor(9832.5303, grad_fn=<NegBackward0>) tensor(9832.5322, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9832.53125
tensor(9832.5303, grad_fn=<NegBackward0>) tensor(9832.5312, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -9832.53125
tensor(9832.5303, grad_fn=<NegBackward0>) tensor(9832.5312, grad_fn=<NegBackward0>)
4
Iteration 3900: Loss = -9832.53125
tensor(9832.5303, grad_fn=<NegBackward0>) tensor(9832.5312, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3900 due to no improvement.
pi: tensor([[0.6098, 0.3902],
        [0.1743, 0.8257]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0025, 0.9975], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1880, 0.1415],
         [0.5529, 0.1239]],

        [[0.6038, 0.1583],
         [0.6314, 0.5260]],

        [[0.6868, 0.1569],
         [0.6850, 0.5136]],

        [[0.6373, 0.1572],
         [0.6659, 0.7237]],

        [[0.6748, 0.1409],
         [0.5096, 0.5171]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.01644068827141728
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.010537225773017793
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
Global Adjusted Rand Index: 0.010104347693189044
Average Adjusted Rand Index: 0.003604934923922322
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24532.375
inf tensor(24532.3750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9841.005859375
tensor(24532.3750, grad_fn=<NegBackward0>) tensor(9841.0059, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9839.7861328125
tensor(9841.0059, grad_fn=<NegBackward0>) tensor(9839.7861, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9839.2646484375
tensor(9839.7861, grad_fn=<NegBackward0>) tensor(9839.2646, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9838.94921875
tensor(9839.2646, grad_fn=<NegBackward0>) tensor(9838.9492, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9838.1611328125
tensor(9838.9492, grad_fn=<NegBackward0>) tensor(9838.1611, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9835.326171875
tensor(9838.1611, grad_fn=<NegBackward0>) tensor(9835.3262, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9832.64453125
tensor(9835.3262, grad_fn=<NegBackward0>) tensor(9832.6445, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9832.578125
tensor(9832.6445, grad_fn=<NegBackward0>) tensor(9832.5781, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9832.5615234375
tensor(9832.5781, grad_fn=<NegBackward0>) tensor(9832.5615, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9832.5537109375
tensor(9832.5615, grad_fn=<NegBackward0>) tensor(9832.5537, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9832.5478515625
tensor(9832.5537, grad_fn=<NegBackward0>) tensor(9832.5479, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9832.5439453125
tensor(9832.5479, grad_fn=<NegBackward0>) tensor(9832.5439, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9832.5419921875
tensor(9832.5439, grad_fn=<NegBackward0>) tensor(9832.5420, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9832.541015625
tensor(9832.5420, grad_fn=<NegBackward0>) tensor(9832.5410, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9832.5390625
tensor(9832.5410, grad_fn=<NegBackward0>) tensor(9832.5391, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9832.5380859375
tensor(9832.5391, grad_fn=<NegBackward0>) tensor(9832.5381, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9832.537109375
tensor(9832.5381, grad_fn=<NegBackward0>) tensor(9832.5371, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9832.537109375
tensor(9832.5371, grad_fn=<NegBackward0>) tensor(9832.5371, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9832.5361328125
tensor(9832.5371, grad_fn=<NegBackward0>) tensor(9832.5361, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9832.5361328125
tensor(9832.5361, grad_fn=<NegBackward0>) tensor(9832.5361, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9832.53515625
tensor(9832.5361, grad_fn=<NegBackward0>) tensor(9832.5352, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9832.537109375
tensor(9832.5352, grad_fn=<NegBackward0>) tensor(9832.5371, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9832.53515625
tensor(9832.5352, grad_fn=<NegBackward0>) tensor(9832.5352, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9832.5341796875
tensor(9832.5352, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9832.5341796875
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9832.53515625
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5352, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -9832.53515625
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5352, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -9832.5341796875
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9832.5341796875
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9832.533203125
tensor(9832.5342, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9832.533203125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9832.533203125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
4
Iteration 3800: Loss = -9832.533203125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5332, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -9832.5341796875
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5342, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -9832.578125
tensor(9832.5332, grad_fn=<NegBackward0>) tensor(9832.5781, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.6105, 0.3895],
        [0.1763, 0.8237]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.9368e-04, 9.9931e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1881, 0.1471],
         [0.5633, 0.1239]],

        [[0.5070, 0.1585],
         [0.6519, 0.5929]],

        [[0.6576, 0.1570],
         [0.6665, 0.5814]],

        [[0.5884, 0.1573],
         [0.5659, 0.6461]],

        [[0.6099, 0.1409],
         [0.6797, 0.6633]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.01644068827141728
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.010537225773017793
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: 0.009300469022722309
Average Adjusted Rand Index: 0.0023928137118011095
[0.010104347693189044, 0.009300469022722309] [0.003604934923922322, 0.0023928137118011095] [9832.53125, 9832.578125]
-------------------------------------
This iteration is 19
True Objective function: Loss = -9996.195815766367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22368.498046875
inf tensor(22368.4980, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9843.0654296875
tensor(22368.4980, grad_fn=<NegBackward0>) tensor(9843.0654, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9839.73046875
tensor(9843.0654, grad_fn=<NegBackward0>) tensor(9839.7305, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9837.9482421875
tensor(9839.7305, grad_fn=<NegBackward0>) tensor(9837.9482, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9836.9619140625
tensor(9837.9482, grad_fn=<NegBackward0>) tensor(9836.9619, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9836.177734375
tensor(9836.9619, grad_fn=<NegBackward0>) tensor(9836.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9835.4375
tensor(9836.1777, grad_fn=<NegBackward0>) tensor(9835.4375, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9834.841796875
tensor(9835.4375, grad_fn=<NegBackward0>) tensor(9834.8418, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9834.5986328125
tensor(9834.8418, grad_fn=<NegBackward0>) tensor(9834.5986, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9834.466796875
tensor(9834.5986, grad_fn=<NegBackward0>) tensor(9834.4668, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9834.380859375
tensor(9834.4668, grad_fn=<NegBackward0>) tensor(9834.3809, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9834.3115234375
tensor(9834.3809, grad_fn=<NegBackward0>) tensor(9834.3115, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9834.25390625
tensor(9834.3115, grad_fn=<NegBackward0>) tensor(9834.2539, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9834.19921875
tensor(9834.2539, grad_fn=<NegBackward0>) tensor(9834.1992, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9834.1494140625
tensor(9834.1992, grad_fn=<NegBackward0>) tensor(9834.1494, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9834.1044921875
tensor(9834.1494, grad_fn=<NegBackward0>) tensor(9834.1045, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9834.0673828125
tensor(9834.1045, grad_fn=<NegBackward0>) tensor(9834.0674, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9834.0390625
tensor(9834.0674, grad_fn=<NegBackward0>) tensor(9834.0391, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9834.0166015625
tensor(9834.0391, grad_fn=<NegBackward0>) tensor(9834.0166, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9834.0009765625
tensor(9834.0166, grad_fn=<NegBackward0>) tensor(9834.0010, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9833.9912109375
tensor(9834.0010, grad_fn=<NegBackward0>) tensor(9833.9912, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9833.984375
tensor(9833.9912, grad_fn=<NegBackward0>) tensor(9833.9844, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9833.978515625
tensor(9833.9844, grad_fn=<NegBackward0>) tensor(9833.9785, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9833.9736328125
tensor(9833.9785, grad_fn=<NegBackward0>) tensor(9833.9736, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9833.9697265625
tensor(9833.9736, grad_fn=<NegBackward0>) tensor(9833.9697, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9833.9677734375
tensor(9833.9697, grad_fn=<NegBackward0>) tensor(9833.9678, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9833.9638671875
tensor(9833.9678, grad_fn=<NegBackward0>) tensor(9833.9639, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9833.962890625
tensor(9833.9639, grad_fn=<NegBackward0>) tensor(9833.9629, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9833.9609375
tensor(9833.9629, grad_fn=<NegBackward0>) tensor(9833.9609, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9833.9599609375
tensor(9833.9609, grad_fn=<NegBackward0>) tensor(9833.9600, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9833.9599609375
tensor(9833.9600, grad_fn=<NegBackward0>) tensor(9833.9600, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9833.9580078125
tensor(9833.9600, grad_fn=<NegBackward0>) tensor(9833.9580, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9833.9580078125
tensor(9833.9580, grad_fn=<NegBackward0>) tensor(9833.9580, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9833.95703125
tensor(9833.9580, grad_fn=<NegBackward0>) tensor(9833.9570, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9833.9560546875
tensor(9833.9570, grad_fn=<NegBackward0>) tensor(9833.9561, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9833.9541015625
tensor(9833.9561, grad_fn=<NegBackward0>) tensor(9833.9541, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9833.9541015625
tensor(9833.9541, grad_fn=<NegBackward0>) tensor(9833.9541, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9833.953125
tensor(9833.9541, grad_fn=<NegBackward0>) tensor(9833.9531, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9833.951171875
tensor(9833.9531, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9833.9521484375
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9521, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9833.9521484375
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9521, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9833.951171875
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9833.9501953125
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9833.951171875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9833.951171875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -9833.9501953125
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9833.94921875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9833.94921875
tensor(9833.9492, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9833.9482421875
tensor(9833.9492, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9833.9482421875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9833.9482421875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9833.94921875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9833.9482421875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9833.94921875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9833.9482421875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9833.9462890625
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9463, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9833.94921875
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9833.94921875
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9833.947265625
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -9833.947265625
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -9833.9482421875
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[9.7303e-01, 2.6972e-02],
        [9.9956e-01, 4.3731e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8837, 0.1163], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1307, 0.1519],
         [0.6801, 0.5311]],

        [[0.6498, 0.1974],
         [0.6753, 0.5161]],

        [[0.5439, 0.2507],
         [0.6509, 0.6570]],

        [[0.6617, 0.2192],
         [0.6910, 0.5313]],

        [[0.5411, 0.1906],
         [0.6127, 0.6313]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.09333333333333334
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005698251975296894
Average Adjusted Rand Index: 0.016357248489374414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21743.4296875
inf tensor(21743.4297, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9843.0693359375
tensor(21743.4297, grad_fn=<NegBackward0>) tensor(9843.0693, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9838.5498046875
tensor(9843.0693, grad_fn=<NegBackward0>) tensor(9838.5498, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9836.77734375
tensor(9838.5498, grad_fn=<NegBackward0>) tensor(9836.7773, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9836.0087890625
tensor(9836.7773, grad_fn=<NegBackward0>) tensor(9836.0088, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9835.3427734375
tensor(9836.0088, grad_fn=<NegBackward0>) tensor(9835.3428, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9834.8974609375
tensor(9835.3428, grad_fn=<NegBackward0>) tensor(9834.8975, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9834.6552734375
tensor(9834.8975, grad_fn=<NegBackward0>) tensor(9834.6553, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9834.5126953125
tensor(9834.6553, grad_fn=<NegBackward0>) tensor(9834.5127, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9834.4150390625
tensor(9834.5127, grad_fn=<NegBackward0>) tensor(9834.4150, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9834.3427734375
tensor(9834.4150, grad_fn=<NegBackward0>) tensor(9834.3428, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9834.279296875
tensor(9834.3428, grad_fn=<NegBackward0>) tensor(9834.2793, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9834.2216796875
tensor(9834.2793, grad_fn=<NegBackward0>) tensor(9834.2217, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9834.1650390625
tensor(9834.2217, grad_fn=<NegBackward0>) tensor(9834.1650, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9834.1123046875
tensor(9834.1650, grad_fn=<NegBackward0>) tensor(9834.1123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9834.0693359375
tensor(9834.1123, grad_fn=<NegBackward0>) tensor(9834.0693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9834.037109375
tensor(9834.0693, grad_fn=<NegBackward0>) tensor(9834.0371, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9834.013671875
tensor(9834.0371, grad_fn=<NegBackward0>) tensor(9834.0137, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9833.998046875
tensor(9834.0137, grad_fn=<NegBackward0>) tensor(9833.9980, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9833.9873046875
tensor(9833.9980, grad_fn=<NegBackward0>) tensor(9833.9873, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9833.98046875
tensor(9833.9873, grad_fn=<NegBackward0>) tensor(9833.9805, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9833.974609375
tensor(9833.9805, grad_fn=<NegBackward0>) tensor(9833.9746, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9833.9716796875
tensor(9833.9746, grad_fn=<NegBackward0>) tensor(9833.9717, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9833.9677734375
tensor(9833.9717, grad_fn=<NegBackward0>) tensor(9833.9678, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9833.96484375
tensor(9833.9678, grad_fn=<NegBackward0>) tensor(9833.9648, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9833.96484375
tensor(9833.9648, grad_fn=<NegBackward0>) tensor(9833.9648, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9833.9619140625
tensor(9833.9648, grad_fn=<NegBackward0>) tensor(9833.9619, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9833.9609375
tensor(9833.9619, grad_fn=<NegBackward0>) tensor(9833.9609, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9833.958984375
tensor(9833.9609, grad_fn=<NegBackward0>) tensor(9833.9590, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9833.958984375
tensor(9833.9590, grad_fn=<NegBackward0>) tensor(9833.9590, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9833.95703125
tensor(9833.9590, grad_fn=<NegBackward0>) tensor(9833.9570, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9833.95703125
tensor(9833.9570, grad_fn=<NegBackward0>) tensor(9833.9570, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9833.95703125
tensor(9833.9570, grad_fn=<NegBackward0>) tensor(9833.9570, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9833.9541015625
tensor(9833.9570, grad_fn=<NegBackward0>) tensor(9833.9541, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9833.955078125
tensor(9833.9541, grad_fn=<NegBackward0>) tensor(9833.9551, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9833.9541015625
tensor(9833.9541, grad_fn=<NegBackward0>) tensor(9833.9541, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9833.953125
tensor(9833.9541, grad_fn=<NegBackward0>) tensor(9833.9531, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9833.953125
tensor(9833.9531, grad_fn=<NegBackward0>) tensor(9833.9531, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9833.953125
tensor(9833.9531, grad_fn=<NegBackward0>) tensor(9833.9531, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9833.951171875
tensor(9833.9531, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9833.9521484375
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9521, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9833.951171875
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9833.9521484375
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9521, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9833.951171875
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9833.9501953125
tensor(9833.9512, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9833.951171875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9833.951171875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9833.9501953125
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9833.94921875
tensor(9833.9502, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9833.9501953125
tensor(9833.9492, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9833.94921875
tensor(9833.9492, grad_fn=<NegBackward0>) tensor(9833.9492, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9833.9482421875
tensor(9833.9492, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9833.9482421875
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9833.947265625
tensor(9833.9482, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9833.9501953125
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -9833.951171875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -9833.947265625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9833.947265625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9833.9541015625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9541, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9833.947265625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9833.947265625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9833.9501953125
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9502, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9833.9482421875
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -9833.9462890625
tensor(9833.9473, grad_fn=<NegBackward0>) tensor(9833.9463, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9833.947265625
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9833.9462890625
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9463, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9833.9482421875
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9833.951171875
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9512, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9833.9453125
tensor(9833.9463, grad_fn=<NegBackward0>) tensor(9833.9453, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9833.947265625
tensor(9833.9453, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9833.947265625
tensor(9833.9453, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -9833.9482421875
tensor(9833.9453, grad_fn=<NegBackward0>) tensor(9833.9482, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -9833.947265625
tensor(9833.9453, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -9833.947265625
tensor(9833.9453, grad_fn=<NegBackward0>) tensor(9833.9473, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[9.7317e-01, 2.6827e-02],
        [9.9985e-01, 1.4705e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8839, 0.1161], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1307, 0.1519],
         [0.5524, 0.5313]],

        [[0.5714, 0.1975],
         [0.6978, 0.6826]],

        [[0.6493, 0.2508],
         [0.7185, 0.5178]],

        [[0.5748, 0.2192],
         [0.5685, 0.5495]],

        [[0.5202, 0.1906],
         [0.5751, 0.5779]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.09333333333333334
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005698251975296894
Average Adjusted Rand Index: 0.016357248489374414
[-0.0005698251975296894, -0.0005698251975296894] [0.016357248489374414, 0.016357248489374414] [9833.9482421875, 9833.947265625]
-------------------------------------
This iteration is 20
True Objective function: Loss = -10045.66383548985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23663.1484375
inf tensor(23663.1484, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9908.802734375
tensor(23663.1484, grad_fn=<NegBackward0>) tensor(9908.8027, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9907.6435546875
tensor(9908.8027, grad_fn=<NegBackward0>) tensor(9907.6436, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9907.287109375
tensor(9907.6436, grad_fn=<NegBackward0>) tensor(9907.2871, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9907.0703125
tensor(9907.2871, grad_fn=<NegBackward0>) tensor(9907.0703, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9906.8818359375
tensor(9907.0703, grad_fn=<NegBackward0>) tensor(9906.8818, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9906.72265625
tensor(9906.8818, grad_fn=<NegBackward0>) tensor(9906.7227, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9906.5703125
tensor(9906.7227, grad_fn=<NegBackward0>) tensor(9906.5703, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9906.4228515625
tensor(9906.5703, grad_fn=<NegBackward0>) tensor(9906.4229, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9906.2880859375
tensor(9906.4229, grad_fn=<NegBackward0>) tensor(9906.2881, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9906.1708984375
tensor(9906.2881, grad_fn=<NegBackward0>) tensor(9906.1709, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9906.0654296875
tensor(9906.1709, grad_fn=<NegBackward0>) tensor(9906.0654, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9905.96875
tensor(9906.0654, grad_fn=<NegBackward0>) tensor(9905.9688, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9905.8759765625
tensor(9905.9688, grad_fn=<NegBackward0>) tensor(9905.8760, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9905.7900390625
tensor(9905.8760, grad_fn=<NegBackward0>) tensor(9905.7900, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9905.716796875
tensor(9905.7900, grad_fn=<NegBackward0>) tensor(9905.7168, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9905.666015625
tensor(9905.7168, grad_fn=<NegBackward0>) tensor(9905.6660, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9905.6337890625
tensor(9905.6660, grad_fn=<NegBackward0>) tensor(9905.6338, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9905.611328125
tensor(9905.6338, grad_fn=<NegBackward0>) tensor(9905.6113, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9905.6015625
tensor(9905.6113, grad_fn=<NegBackward0>) tensor(9905.6016, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9905.59765625
tensor(9905.6016, grad_fn=<NegBackward0>) tensor(9905.5977, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9905.59375
tensor(9905.5977, grad_fn=<NegBackward0>) tensor(9905.5938, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9905.5927734375
tensor(9905.5938, grad_fn=<NegBackward0>) tensor(9905.5928, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9905.591796875
tensor(9905.5928, grad_fn=<NegBackward0>) tensor(9905.5918, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9905.5908203125
tensor(9905.5918, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9905.58984375
tensor(9905.5908, grad_fn=<NegBackward0>) tensor(9905.5898, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9905.5908203125
tensor(9905.5898, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -9905.5888671875
tensor(9905.5898, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9905.5888671875
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9905.5888671875
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9905.5908203125
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9905.58984375
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5898, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -9905.58984375
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5898, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -9905.587890625
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9905.5888671875
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9905.5888671875
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9905.595703125
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5957, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9905.5869140625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9905.5888671875
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9905.5859375
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5859, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.0202, 0.9798],
        [0.0183, 0.9817]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1886, 0.8114], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1943, 0.1579],
         [0.6799, 0.1338]],

        [[0.5334, 0.1887],
         [0.5490, 0.5420]],

        [[0.5267, 0.2139],
         [0.5460, 0.6395]],

        [[0.5122, 0.2321],
         [0.6473, 0.6387]],

        [[0.6737, 0.1430],
         [0.5626, 0.5255]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005860107247257204
Average Adjusted Rand Index: 0.0010224091060634035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21877.333984375
inf tensor(21877.3340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9909.1640625
tensor(21877.3340, grad_fn=<NegBackward0>) tensor(9909.1641, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9907.9462890625
tensor(9909.1641, grad_fn=<NegBackward0>) tensor(9907.9463, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9907.6572265625
tensor(9907.9463, grad_fn=<NegBackward0>) tensor(9907.6572, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9907.4775390625
tensor(9907.6572, grad_fn=<NegBackward0>) tensor(9907.4775, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9907.2841796875
tensor(9907.4775, grad_fn=<NegBackward0>) tensor(9907.2842, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9907.0693359375
tensor(9907.2842, grad_fn=<NegBackward0>) tensor(9907.0693, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9906.8427734375
tensor(9907.0693, grad_fn=<NegBackward0>) tensor(9906.8428, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9906.6376953125
tensor(9906.8428, grad_fn=<NegBackward0>) tensor(9906.6377, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9906.4638671875
tensor(9906.6377, grad_fn=<NegBackward0>) tensor(9906.4639, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9906.3173828125
tensor(9906.4639, grad_fn=<NegBackward0>) tensor(9906.3174, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9906.1962890625
tensor(9906.3174, grad_fn=<NegBackward0>) tensor(9906.1963, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9906.091796875
tensor(9906.1963, grad_fn=<NegBackward0>) tensor(9906.0918, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9905.9970703125
tensor(9906.0918, grad_fn=<NegBackward0>) tensor(9905.9971, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9905.912109375
tensor(9905.9971, grad_fn=<NegBackward0>) tensor(9905.9121, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9905.837890625
tensor(9905.9121, grad_fn=<NegBackward0>) tensor(9905.8379, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9905.7744140625
tensor(9905.8379, grad_fn=<NegBackward0>) tensor(9905.7744, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9905.7236328125
tensor(9905.7744, grad_fn=<NegBackward0>) tensor(9905.7236, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9905.6875
tensor(9905.7236, grad_fn=<NegBackward0>) tensor(9905.6875, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9905.65234375
tensor(9905.6875, grad_fn=<NegBackward0>) tensor(9905.6523, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9905.6318359375
tensor(9905.6523, grad_fn=<NegBackward0>) tensor(9905.6318, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9905.6171875
tensor(9905.6318, grad_fn=<NegBackward0>) tensor(9905.6172, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9905.609375
tensor(9905.6172, grad_fn=<NegBackward0>) tensor(9905.6094, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9905.60546875
tensor(9905.6094, grad_fn=<NegBackward0>) tensor(9905.6055, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9905.6005859375
tensor(9905.6055, grad_fn=<NegBackward0>) tensor(9905.6006, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9905.5986328125
tensor(9905.6006, grad_fn=<NegBackward0>) tensor(9905.5986, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9905.5986328125
tensor(9905.5986, grad_fn=<NegBackward0>) tensor(9905.5986, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9905.5966796875
tensor(9905.5986, grad_fn=<NegBackward0>) tensor(9905.5967, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9905.5947265625
tensor(9905.5967, grad_fn=<NegBackward0>) tensor(9905.5947, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9905.595703125
tensor(9905.5947, grad_fn=<NegBackward0>) tensor(9905.5957, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9905.5947265625
tensor(9905.5947, grad_fn=<NegBackward0>) tensor(9905.5947, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9905.59375
tensor(9905.5947, grad_fn=<NegBackward0>) tensor(9905.5938, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9905.591796875
tensor(9905.5938, grad_fn=<NegBackward0>) tensor(9905.5918, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9905.59375
tensor(9905.5918, grad_fn=<NegBackward0>) tensor(9905.5938, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9905.5927734375
tensor(9905.5918, grad_fn=<NegBackward0>) tensor(9905.5928, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -9905.591796875
tensor(9905.5918, grad_fn=<NegBackward0>) tensor(9905.5918, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9905.5908203125
tensor(9905.5918, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9905.591796875
tensor(9905.5908, grad_fn=<NegBackward0>) tensor(9905.5918, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9905.5888671875
tensor(9905.5908, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9905.58984375
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5898, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9905.5908203125
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9905.5888671875
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9905.5908203125
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5908, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9905.58984375
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5898, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9905.5888671875
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9905.587890625
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9905.5888671875
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9905.587890625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9905.5888671875
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9905.5869140625
tensor(9905.5879, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9905.5966796875
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5967, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9905.5869140625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9905.587890625
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5879, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9905.5859375
tensor(9905.5869, grad_fn=<NegBackward0>) tensor(9905.5859, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -9905.5869140625
tensor(9905.5859, grad_fn=<NegBackward0>) tensor(9905.5869, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.9817, 0.0183],
        [0.9788, 0.0212]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8119, 0.1881], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1338, 0.1578],
         [0.6921, 0.1943]],

        [[0.5671, 0.1885],
         [0.6915, 0.7029]],

        [[0.6059, 0.2139],
         [0.7195, 0.6904]],

        [[0.5046, 0.2320],
         [0.5697, 0.7020]],

        [[0.6029, 0.1431],
         [0.6918, 0.6460]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005860107247257204
Average Adjusted Rand Index: 0.0010224091060634035
[0.0005860107247257204, 0.0005860107247257204] [0.0010224091060634035, 0.0010224091060634035] [9905.5869140625, 9905.5869140625]
-------------------------------------
This iteration is 21
True Objective function: Loss = -9966.861244781672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24033.662109375
inf tensor(24033.6621, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9843.1552734375
tensor(24033.6621, grad_fn=<NegBackward0>) tensor(9843.1553, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9840.5107421875
tensor(9843.1553, grad_fn=<NegBackward0>) tensor(9840.5107, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9839.3154296875
tensor(9840.5107, grad_fn=<NegBackward0>) tensor(9839.3154, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9838.6103515625
tensor(9839.3154, grad_fn=<NegBackward0>) tensor(9838.6104, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9838.2509765625
tensor(9838.6104, grad_fn=<NegBackward0>) tensor(9838.2510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9838.0068359375
tensor(9838.2510, grad_fn=<NegBackward0>) tensor(9838.0068, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9837.8095703125
tensor(9838.0068, grad_fn=<NegBackward0>) tensor(9837.8096, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9837.6298828125
tensor(9837.8096, grad_fn=<NegBackward0>) tensor(9837.6299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9837.4560546875
tensor(9837.6299, grad_fn=<NegBackward0>) tensor(9837.4561, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9837.28515625
tensor(9837.4561, grad_fn=<NegBackward0>) tensor(9837.2852, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9837.1083984375
tensor(9837.2852, grad_fn=<NegBackward0>) tensor(9837.1084, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9836.92578125
tensor(9837.1084, grad_fn=<NegBackward0>) tensor(9836.9258, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9836.7392578125
tensor(9836.9258, grad_fn=<NegBackward0>) tensor(9836.7393, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9836.55859375
tensor(9836.7393, grad_fn=<NegBackward0>) tensor(9836.5586, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9836.3896484375
tensor(9836.5586, grad_fn=<NegBackward0>) tensor(9836.3896, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9836.2421875
tensor(9836.3896, grad_fn=<NegBackward0>) tensor(9836.2422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9836.12109375
tensor(9836.2422, grad_fn=<NegBackward0>) tensor(9836.1211, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9836.0302734375
tensor(9836.1211, grad_fn=<NegBackward0>) tensor(9836.0303, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9835.966796875
tensor(9836.0303, grad_fn=<NegBackward0>) tensor(9835.9668, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9835.9248046875
tensor(9835.9668, grad_fn=<NegBackward0>) tensor(9835.9248, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9835.8916015625
tensor(9835.9248, grad_fn=<NegBackward0>) tensor(9835.8916, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9835.8701171875
tensor(9835.8916, grad_fn=<NegBackward0>) tensor(9835.8701, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9835.8544921875
tensor(9835.8701, grad_fn=<NegBackward0>) tensor(9835.8545, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9835.841796875
tensor(9835.8545, grad_fn=<NegBackward0>) tensor(9835.8418, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9835.830078125
tensor(9835.8418, grad_fn=<NegBackward0>) tensor(9835.8301, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9835.8193359375
tensor(9835.8301, grad_fn=<NegBackward0>) tensor(9835.8193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9835.80859375
tensor(9835.8193, grad_fn=<NegBackward0>) tensor(9835.8086, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9835.798828125
tensor(9835.8086, grad_fn=<NegBackward0>) tensor(9835.7988, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9835.787109375
tensor(9835.7988, grad_fn=<NegBackward0>) tensor(9835.7871, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9835.77734375
tensor(9835.7871, grad_fn=<NegBackward0>) tensor(9835.7773, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9835.7783203125
tensor(9835.7773, grad_fn=<NegBackward0>) tensor(9835.7783, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9835.763671875
tensor(9835.7773, grad_fn=<NegBackward0>) tensor(9835.7637, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9835.7607421875
tensor(9835.7637, grad_fn=<NegBackward0>) tensor(9835.7607, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9835.7587890625
tensor(9835.7607, grad_fn=<NegBackward0>) tensor(9835.7588, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9835.7568359375
tensor(9835.7588, grad_fn=<NegBackward0>) tensor(9835.7568, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9835.755859375
tensor(9835.7568, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9835.755859375
tensor(9835.7559, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9835.7548828125
tensor(9835.7559, grad_fn=<NegBackward0>) tensor(9835.7549, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9835.76171875
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7617, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9835.755859375
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9835.7548828125
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7549, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9835.7548828125
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7549, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9835.7548828125
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7549, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9835.755859375
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9835.755859375
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9835.7685546875
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7686, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9835.7607421875
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7607, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9835.755859375
tensor(9835.7549, grad_fn=<NegBackward0>) tensor(9835.7559, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.7420, 0.2580],
        [0.8596, 0.1404]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7617, 0.2383], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1176, 0.1524],
         [0.6318, 0.2080]],

        [[0.6546, 0.1589],
         [0.5540, 0.7064]],

        [[0.6659, 0.1517],
         [0.5441, 0.5311]],

        [[0.6223, 0.1595],
         [0.7041, 0.5837]],

        [[0.6108, 0.1554],
         [0.5757, 0.6049]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.015172214275382118
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002678093197643278
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.00575069963675942
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.033695242120345935
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.043550744307015006
Global Adjusted Rand Index: 0.0135649896826492
Average Adjusted Rand Index: 0.016797881573668072
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22630.95703125
inf tensor(22630.9570, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9842.9052734375
tensor(22630.9570, grad_fn=<NegBackward0>) tensor(9842.9053, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9841.9560546875
tensor(9842.9053, grad_fn=<NegBackward0>) tensor(9841.9561, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9841.6328125
tensor(9841.9561, grad_fn=<NegBackward0>) tensor(9841.6328, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9841.4375
tensor(9841.6328, grad_fn=<NegBackward0>) tensor(9841.4375, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9841.3115234375
tensor(9841.4375, grad_fn=<NegBackward0>) tensor(9841.3115, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9841.228515625
tensor(9841.3115, grad_fn=<NegBackward0>) tensor(9841.2285, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9841.1650390625
tensor(9841.2285, grad_fn=<NegBackward0>) tensor(9841.1650, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9841.1083984375
tensor(9841.1650, grad_fn=<NegBackward0>) tensor(9841.1084, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9841.052734375
tensor(9841.1084, grad_fn=<NegBackward0>) tensor(9841.0527, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9840.9970703125
tensor(9841.0527, grad_fn=<NegBackward0>) tensor(9840.9971, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9840.9462890625
tensor(9840.9971, grad_fn=<NegBackward0>) tensor(9840.9463, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9840.9150390625
tensor(9840.9463, grad_fn=<NegBackward0>) tensor(9840.9150, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9840.8984375
tensor(9840.9150, grad_fn=<NegBackward0>) tensor(9840.8984, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9840.888671875
tensor(9840.8984, grad_fn=<NegBackward0>) tensor(9840.8887, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9840.880859375
tensor(9840.8887, grad_fn=<NegBackward0>) tensor(9840.8809, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9840.875
tensor(9840.8809, grad_fn=<NegBackward0>) tensor(9840.8750, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9840.8701171875
tensor(9840.8750, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9840.8671875
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8672, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9840.8623046875
tensor(9840.8672, grad_fn=<NegBackward0>) tensor(9840.8623, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9840.8564453125
tensor(9840.8623, grad_fn=<NegBackward0>) tensor(9840.8564, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9840.8466796875
tensor(9840.8564, grad_fn=<NegBackward0>) tensor(9840.8467, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9840.837890625
tensor(9840.8467, grad_fn=<NegBackward0>) tensor(9840.8379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9840.826171875
tensor(9840.8379, grad_fn=<NegBackward0>) tensor(9840.8262, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9840.8134765625
tensor(9840.8262, grad_fn=<NegBackward0>) tensor(9840.8135, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9840.8046875
tensor(9840.8135, grad_fn=<NegBackward0>) tensor(9840.8047, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9840.7998046875
tensor(9840.8047, grad_fn=<NegBackward0>) tensor(9840.7998, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9840.7978515625
tensor(9840.7998, grad_fn=<NegBackward0>) tensor(9840.7979, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9840.796875
tensor(9840.7979, grad_fn=<NegBackward0>) tensor(9840.7969, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9840.7958984375
tensor(9840.7969, grad_fn=<NegBackward0>) tensor(9840.7959, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9840.7958984375
tensor(9840.7959, grad_fn=<NegBackward0>) tensor(9840.7959, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9840.7958984375
tensor(9840.7959, grad_fn=<NegBackward0>) tensor(9840.7959, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9840.7958984375
tensor(9840.7959, grad_fn=<NegBackward0>) tensor(9840.7959, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9840.7939453125
tensor(9840.7959, grad_fn=<NegBackward0>) tensor(9840.7939, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9840.794921875
tensor(9840.7939, grad_fn=<NegBackward0>) tensor(9840.7949, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9840.7939453125
tensor(9840.7939, grad_fn=<NegBackward0>) tensor(9840.7939, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9840.79296875
tensor(9840.7939, grad_fn=<NegBackward0>) tensor(9840.7930, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9840.7919921875
tensor(9840.7930, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9840.79296875
tensor(9840.7920, grad_fn=<NegBackward0>) tensor(9840.7930, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9840.7919921875
tensor(9840.7920, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9840.791015625
tensor(9840.7920, grad_fn=<NegBackward0>) tensor(9840.7910, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9840.79296875
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7930, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9840.7919921875
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -9840.79296875
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7930, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -9840.791015625
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7910, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9840.791015625
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7910, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9840.7900390625
tensor(9840.7910, grad_fn=<NegBackward0>) tensor(9840.7900, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9840.791015625
tensor(9840.7900, grad_fn=<NegBackward0>) tensor(9840.7910, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9840.7919921875
tensor(9840.7900, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9840.7919921875
tensor(9840.7900, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -9840.7919921875
tensor(9840.7900, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -9840.7919921875
tensor(9840.7900, grad_fn=<NegBackward0>) tensor(9840.7920, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.9843, 0.0157],
        [0.8322, 0.1678]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9916, 0.0084], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1348, 0.0805],
         [0.6506, 0.2590]],

        [[0.5201, 0.0639],
         [0.5490, 0.6226]],

        [[0.5586, 0.2181],
         [0.7090, 0.6055]],

        [[0.5886, 0.1949],
         [0.5280, 0.6706]],

        [[0.6404, 0.2373],
         [0.5990, 0.5333]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -6.336374590047627e-05
Average Adjusted Rand Index: -0.0002540087674690285
[0.0135649896826492, -6.336374590047627e-05] [0.016797881573668072, -0.0002540087674690285] [9835.755859375, 9840.7919921875]
-------------------------------------
This iteration is 22
True Objective function: Loss = -10011.198158844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23624.220703125
inf tensor(23624.2207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9901.578125
tensor(23624.2207, grad_fn=<NegBackward0>) tensor(9901.5781, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9900.283203125
tensor(9901.5781, grad_fn=<NegBackward0>) tensor(9900.2832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9900.1015625
tensor(9900.2832, grad_fn=<NegBackward0>) tensor(9900.1016, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9899.953125
tensor(9900.1016, grad_fn=<NegBackward0>) tensor(9899.9531, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9899.7578125
tensor(9899.9531, grad_fn=<NegBackward0>) tensor(9899.7578, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9899.4921875
tensor(9899.7578, grad_fn=<NegBackward0>) tensor(9899.4922, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9899.2587890625
tensor(9899.4922, grad_fn=<NegBackward0>) tensor(9899.2588, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9899.1298828125
tensor(9899.2588, grad_fn=<NegBackward0>) tensor(9899.1299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9899.0556640625
tensor(9899.1299, grad_fn=<NegBackward0>) tensor(9899.0557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9898.9912109375
tensor(9899.0557, grad_fn=<NegBackward0>) tensor(9898.9912, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9898.951171875
tensor(9898.9912, grad_fn=<NegBackward0>) tensor(9898.9512, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9898.916015625
tensor(9898.9512, grad_fn=<NegBackward0>) tensor(9898.9160, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9898.8837890625
tensor(9898.9160, grad_fn=<NegBackward0>) tensor(9898.8838, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9898.85546875
tensor(9898.8838, grad_fn=<NegBackward0>) tensor(9898.8555, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9898.8310546875
tensor(9898.8555, grad_fn=<NegBackward0>) tensor(9898.8311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9898.8095703125
tensor(9898.8311, grad_fn=<NegBackward0>) tensor(9898.8096, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9898.7939453125
tensor(9898.8096, grad_fn=<NegBackward0>) tensor(9898.7939, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9898.7783203125
tensor(9898.7939, grad_fn=<NegBackward0>) tensor(9898.7783, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9898.767578125
tensor(9898.7783, grad_fn=<NegBackward0>) tensor(9898.7676, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9898.7587890625
tensor(9898.7676, grad_fn=<NegBackward0>) tensor(9898.7588, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9898.7509765625
tensor(9898.7588, grad_fn=<NegBackward0>) tensor(9898.7510, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9898.74609375
tensor(9898.7510, grad_fn=<NegBackward0>) tensor(9898.7461, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9898.7421875
tensor(9898.7461, grad_fn=<NegBackward0>) tensor(9898.7422, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9898.73828125
tensor(9898.7422, grad_fn=<NegBackward0>) tensor(9898.7383, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9898.7353515625
tensor(9898.7383, grad_fn=<NegBackward0>) tensor(9898.7354, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9898.734375
tensor(9898.7354, grad_fn=<NegBackward0>) tensor(9898.7344, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9898.732421875
tensor(9898.7344, grad_fn=<NegBackward0>) tensor(9898.7324, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9898.73046875
tensor(9898.7324, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9898.73046875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9898.73046875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9898.7294921875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9898.728515625
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9898.728515625
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9898.728515625
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9898.7275390625
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9898.7275390625
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9898.7265625
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9898.7275390625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9898.7275390625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9898.7275390625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9898.7255859375
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9898.7255859375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9898.7255859375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9898.7255859375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9898.724609375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7246, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9898.7255859375
tensor(9898.7246, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9898.7265625
tensor(9898.7246, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9898.724609375
tensor(9898.7246, grad_fn=<NegBackward0>) tensor(9898.7246, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9898.7236328125
tensor(9898.7246, grad_fn=<NegBackward0>) tensor(9898.7236, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9898.7255859375
tensor(9898.7236, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9898.724609375
tensor(9898.7236, grad_fn=<NegBackward0>) tensor(9898.7246, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -9898.7255859375
tensor(9898.7236, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -9898.724609375
tensor(9898.7236, grad_fn=<NegBackward0>) tensor(9898.7246, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -9898.7275390625
tensor(9898.7236, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[8.7473e-05, 9.9991e-01],
        [2.9601e-02, 9.7040e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0279, 0.9721], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2606, 0.1851],
         [0.5370, 0.1362]],

        [[0.5512, 0.2059],
         [0.6013, 0.6222]],

        [[0.6010, 0.0588],
         [0.5831, 0.7110]],

        [[0.5479, 0.1537],
         [0.5471, 0.6229]],

        [[0.5365, 0.1688],
         [0.5229, 0.5700]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.342917731397605e-05
Average Adjusted Rand Index: 0.0003250722991824782
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23408.01171875
inf tensor(23408.0117, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9901.916015625
tensor(23408.0117, grad_fn=<NegBackward0>) tensor(9901.9160, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9900.65625
tensor(9901.9160, grad_fn=<NegBackward0>) tensor(9900.6562, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9900.36328125
tensor(9900.6562, grad_fn=<NegBackward0>) tensor(9900.3633, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9900.2177734375
tensor(9900.3633, grad_fn=<NegBackward0>) tensor(9900.2178, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9900.1171875
tensor(9900.2178, grad_fn=<NegBackward0>) tensor(9900.1172, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9900.029296875
tensor(9900.1172, grad_fn=<NegBackward0>) tensor(9900.0293, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9899.9521484375
tensor(9900.0293, grad_fn=<NegBackward0>) tensor(9899.9521, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9899.8818359375
tensor(9899.9521, grad_fn=<NegBackward0>) tensor(9899.8818, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9899.8251953125
tensor(9899.8818, grad_fn=<NegBackward0>) tensor(9899.8252, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9899.765625
tensor(9899.8252, grad_fn=<NegBackward0>) tensor(9899.7656, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9899.6845703125
tensor(9899.7656, grad_fn=<NegBackward0>) tensor(9899.6846, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9899.5625
tensor(9899.6846, grad_fn=<NegBackward0>) tensor(9899.5625, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9899.3623046875
tensor(9899.5625, grad_fn=<NegBackward0>) tensor(9899.3623, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9899.0361328125
tensor(9899.3623, grad_fn=<NegBackward0>) tensor(9899.0361, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9898.890625
tensor(9899.0361, grad_fn=<NegBackward0>) tensor(9898.8906, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9898.845703125
tensor(9898.8906, grad_fn=<NegBackward0>) tensor(9898.8457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9898.822265625
tensor(9898.8457, grad_fn=<NegBackward0>) tensor(9898.8223, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9898.8076171875
tensor(9898.8223, grad_fn=<NegBackward0>) tensor(9898.8076, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9898.798828125
tensor(9898.8076, grad_fn=<NegBackward0>) tensor(9898.7988, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9898.7890625
tensor(9898.7988, grad_fn=<NegBackward0>) tensor(9898.7891, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9898.7822265625
tensor(9898.7891, grad_fn=<NegBackward0>) tensor(9898.7822, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9898.7763671875
tensor(9898.7822, grad_fn=<NegBackward0>) tensor(9898.7764, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9898.7724609375
tensor(9898.7764, grad_fn=<NegBackward0>) tensor(9898.7725, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9898.7685546875
tensor(9898.7725, grad_fn=<NegBackward0>) tensor(9898.7686, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9898.7646484375
tensor(9898.7686, grad_fn=<NegBackward0>) tensor(9898.7646, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9898.76171875
tensor(9898.7646, grad_fn=<NegBackward0>) tensor(9898.7617, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9898.7587890625
tensor(9898.7617, grad_fn=<NegBackward0>) tensor(9898.7588, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9898.7568359375
tensor(9898.7588, grad_fn=<NegBackward0>) tensor(9898.7568, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9898.7548828125
tensor(9898.7568, grad_fn=<NegBackward0>) tensor(9898.7549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9898.751953125
tensor(9898.7549, grad_fn=<NegBackward0>) tensor(9898.7520, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9898.751953125
tensor(9898.7520, grad_fn=<NegBackward0>) tensor(9898.7520, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9898.7490234375
tensor(9898.7520, grad_fn=<NegBackward0>) tensor(9898.7490, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9898.74609375
tensor(9898.7490, grad_fn=<NegBackward0>) tensor(9898.7461, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9898.74609375
tensor(9898.7461, grad_fn=<NegBackward0>) tensor(9898.7461, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9898.744140625
tensor(9898.7461, grad_fn=<NegBackward0>) tensor(9898.7441, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9898.7421875
tensor(9898.7441, grad_fn=<NegBackward0>) tensor(9898.7422, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9898.7421875
tensor(9898.7422, grad_fn=<NegBackward0>) tensor(9898.7422, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9898.740234375
tensor(9898.7422, grad_fn=<NegBackward0>) tensor(9898.7402, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9898.7392578125
tensor(9898.7402, grad_fn=<NegBackward0>) tensor(9898.7393, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9898.7392578125
tensor(9898.7393, grad_fn=<NegBackward0>) tensor(9898.7393, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9898.7373046875
tensor(9898.7393, grad_fn=<NegBackward0>) tensor(9898.7373, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9898.736328125
tensor(9898.7373, grad_fn=<NegBackward0>) tensor(9898.7363, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9898.7353515625
tensor(9898.7363, grad_fn=<NegBackward0>) tensor(9898.7354, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9898.7353515625
tensor(9898.7354, grad_fn=<NegBackward0>) tensor(9898.7354, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9898.736328125
tensor(9898.7354, grad_fn=<NegBackward0>) tensor(9898.7363, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9898.734375
tensor(9898.7354, grad_fn=<NegBackward0>) tensor(9898.7344, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9898.7333984375
tensor(9898.7344, grad_fn=<NegBackward0>) tensor(9898.7334, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9898.734375
tensor(9898.7334, grad_fn=<NegBackward0>) tensor(9898.7344, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9898.734375
tensor(9898.7334, grad_fn=<NegBackward0>) tensor(9898.7344, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9898.7314453125
tensor(9898.7334, grad_fn=<NegBackward0>) tensor(9898.7314, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9898.7333984375
tensor(9898.7314, grad_fn=<NegBackward0>) tensor(9898.7334, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9898.73046875
tensor(9898.7314, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9898.7314453125
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7314, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9898.73046875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9898.73046875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9898.7294921875
tensor(9898.7305, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9898.73046875
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7305, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9898.7294921875
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9898.7294921875
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9898.7294921875
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9898.728515625
tensor(9898.7295, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9898.7294921875
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9898.728515625
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9898.7275390625
tensor(9898.7285, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9898.7294921875
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7295, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9898.7275390625
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9898.728515625
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7285, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9898.7265625
tensor(9898.7275, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9898.7275390625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9898.7275390625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7275, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9898.7265625
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9898.7255859375
tensor(9898.7266, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -9898.7255859375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7256, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9898.740234375
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7402, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9898.736328125
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7363, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -9898.7265625
tensor(9898.7256, grad_fn=<NegBackward0>) tensor(9898.7266, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[1.7863e-04, 9.9982e-01],
        [2.9282e-02, 9.7072e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0276, 0.9724], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2606, 0.1851],
         [0.5744, 0.1362]],

        [[0.5838, 0.2059],
         [0.6742, 0.5603]],

        [[0.6521, 0.0588],
         [0.6041, 0.5867]],

        [[0.6858, 0.1538],
         [0.7144, 0.5450]],

        [[0.6086, 0.1688],
         [0.6376, 0.6324]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.342917731397605e-05
Average Adjusted Rand Index: 0.0003250722991824782
[-6.342917731397605e-05, -6.342917731397605e-05] [0.0003250722991824782, 0.0003250722991824782] [9898.7275390625, 9898.7265625]
-------------------------------------
This iteration is 23
True Objective function: Loss = -10154.551263992535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21481.11328125
inf tensor(21481.1133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10060.03125
tensor(21481.1133, grad_fn=<NegBackward0>) tensor(10060.0312, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10059.125
tensor(10060.0312, grad_fn=<NegBackward0>) tensor(10059.1250, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10058.794921875
tensor(10059.1250, grad_fn=<NegBackward0>) tensor(10058.7949, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10058.5830078125
tensor(10058.7949, grad_fn=<NegBackward0>) tensor(10058.5830, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10058.4208984375
tensor(10058.5830, grad_fn=<NegBackward0>) tensor(10058.4209, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10058.2919921875
tensor(10058.4209, grad_fn=<NegBackward0>) tensor(10058.2920, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10058.1806640625
tensor(10058.2920, grad_fn=<NegBackward0>) tensor(10058.1807, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10058.0771484375
tensor(10058.1807, grad_fn=<NegBackward0>) tensor(10058.0771, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10057.9794921875
tensor(10058.0771, grad_fn=<NegBackward0>) tensor(10057.9795, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10057.8818359375
tensor(10057.9795, grad_fn=<NegBackward0>) tensor(10057.8818, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10057.7626953125
tensor(10057.8818, grad_fn=<NegBackward0>) tensor(10057.7627, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10057.5380859375
tensor(10057.7627, grad_fn=<NegBackward0>) tensor(10057.5381, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10056.966796875
tensor(10057.5381, grad_fn=<NegBackward0>) tensor(10056.9668, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10056.345703125
tensor(10056.9668, grad_fn=<NegBackward0>) tensor(10056.3457, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10055.8330078125
tensor(10056.3457, grad_fn=<NegBackward0>) tensor(10055.8330, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10055.3056640625
tensor(10055.8330, grad_fn=<NegBackward0>) tensor(10055.3057, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10054.8896484375
tensor(10055.3057, grad_fn=<NegBackward0>) tensor(10054.8896, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10054.2568359375
tensor(10054.8896, grad_fn=<NegBackward0>) tensor(10054.2568, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10053.265625
tensor(10054.2568, grad_fn=<NegBackward0>) tensor(10053.2656, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10051.3896484375
tensor(10053.2656, grad_fn=<NegBackward0>) tensor(10051.3896, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10049.068359375
tensor(10051.3896, grad_fn=<NegBackward0>) tensor(10049.0684, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10047.19140625
tensor(10049.0684, grad_fn=<NegBackward0>) tensor(10047.1914, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10046.7919921875
tensor(10047.1914, grad_fn=<NegBackward0>) tensor(10046.7920, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10046.6796875
tensor(10046.7920, grad_fn=<NegBackward0>) tensor(10046.6797, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10046.4228515625
tensor(10046.6797, grad_fn=<NegBackward0>) tensor(10046.4229, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10046.3798828125
tensor(10046.4229, grad_fn=<NegBackward0>) tensor(10046.3799, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10046.3564453125
tensor(10046.3799, grad_fn=<NegBackward0>) tensor(10046.3564, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10046.3095703125
tensor(10046.3564, grad_fn=<NegBackward0>) tensor(10046.3096, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10046.2451171875
tensor(10046.3096, grad_fn=<NegBackward0>) tensor(10046.2451, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10046.2216796875
tensor(10046.2451, grad_fn=<NegBackward0>) tensor(10046.2217, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10046.205078125
tensor(10046.2217, grad_fn=<NegBackward0>) tensor(10046.2051, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10046.123046875
tensor(10046.2051, grad_fn=<NegBackward0>) tensor(10046.1230, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10045.8984375
tensor(10046.1230, grad_fn=<NegBackward0>) tensor(10045.8984, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10043.4296875
tensor(10045.8984, grad_fn=<NegBackward0>) tensor(10043.4297, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10040.533203125
tensor(10043.4297, grad_fn=<NegBackward0>) tensor(10040.5332, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10039.5537109375
tensor(10040.5332, grad_fn=<NegBackward0>) tensor(10039.5537, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10038.9833984375
tensor(10039.5537, grad_fn=<NegBackward0>) tensor(10038.9834, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10038.6806640625
tensor(10038.9834, grad_fn=<NegBackward0>) tensor(10038.6807, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10038.5693359375
tensor(10038.6807, grad_fn=<NegBackward0>) tensor(10038.5693, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10038.484375
tensor(10038.5693, grad_fn=<NegBackward0>) tensor(10038.4844, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10038.4775390625
tensor(10038.4844, grad_fn=<NegBackward0>) tensor(10038.4775, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10038.4755859375
tensor(10038.4775, grad_fn=<NegBackward0>) tensor(10038.4756, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10038.4755859375
tensor(10038.4756, grad_fn=<NegBackward0>) tensor(10038.4756, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10038.4736328125
tensor(10038.4756, grad_fn=<NegBackward0>) tensor(10038.4736, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10038.4716796875
tensor(10038.4736, grad_fn=<NegBackward0>) tensor(10038.4717, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10038.4716796875
tensor(10038.4717, grad_fn=<NegBackward0>) tensor(10038.4717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10038.4697265625
tensor(10038.4717, grad_fn=<NegBackward0>) tensor(10038.4697, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10038.470703125
tensor(10038.4697, grad_fn=<NegBackward0>) tensor(10038.4707, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10038.4697265625
tensor(10038.4697, grad_fn=<NegBackward0>) tensor(10038.4697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10038.46875
tensor(10038.4697, grad_fn=<NegBackward0>) tensor(10038.4688, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10038.46875
tensor(10038.4688, grad_fn=<NegBackward0>) tensor(10038.4688, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10038.46875
tensor(10038.4688, grad_fn=<NegBackward0>) tensor(10038.4688, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10038.46875
tensor(10038.4688, grad_fn=<NegBackward0>) tensor(10038.4688, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10038.4677734375
tensor(10038.4688, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10038.4677734375
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10038.4677734375
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10038.4697265625
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4697, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10038.4677734375
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10038.4677734375
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10038.4677734375
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10038.466796875
tensor(10038.4678, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10038.484375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4844, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10038.46875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4688, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10038.4755859375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4756, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10038.478515625
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4785, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10038.466796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4668, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10038.4677734375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4678, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10038.470703125
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4707, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10038.490234375
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4902, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10038.4716796875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4717, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10038.482421875
tensor(10038.4668, grad_fn=<NegBackward0>) tensor(10038.4824, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.8591, 0.1409],
        [0.2096, 0.7904]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1268, 0.8732], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2112, 0.1606],
         [0.6761, 0.1357]],

        [[0.5220, 0.1498],
         [0.5211, 0.6402]],

        [[0.6556, 0.1260],
         [0.6527, 0.6254]],

        [[0.5378, 0.0869],
         [0.5936, 0.6001]],

        [[0.5305, 0.1072],
         [0.6024, 0.7112]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.03154805575935436
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 31
Adjusted Rand Index: 0.13553403708433695
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 14
Adjusted Rand Index: 0.5135203436947182
Global Adjusted Rand Index: 0.18825002556084452
Average Adjusted Rand Index: 0.29757815639422347
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22240.96484375
inf tensor(22240.9648, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10059.990234375
tensor(22240.9648, grad_fn=<NegBackward0>) tensor(10059.9902, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10058.9560546875
tensor(10059.9902, grad_fn=<NegBackward0>) tensor(10058.9561, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10058.638671875
tensor(10058.9561, grad_fn=<NegBackward0>) tensor(10058.6387, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10058.4814453125
tensor(10058.6387, grad_fn=<NegBackward0>) tensor(10058.4814, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10058.3779296875
tensor(10058.4814, grad_fn=<NegBackward0>) tensor(10058.3779, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10058.291015625
tensor(10058.3779, grad_fn=<NegBackward0>) tensor(10058.2910, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10058.197265625
tensor(10058.2910, grad_fn=<NegBackward0>) tensor(10058.1973, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10058.09375
tensor(10058.1973, grad_fn=<NegBackward0>) tensor(10058.0938, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10058.0087890625
tensor(10058.0938, grad_fn=<NegBackward0>) tensor(10058.0088, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10057.9501953125
tensor(10058.0088, grad_fn=<NegBackward0>) tensor(10057.9502, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10057.9013671875
tensor(10057.9502, grad_fn=<NegBackward0>) tensor(10057.9014, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10057.8583984375
tensor(10057.9014, grad_fn=<NegBackward0>) tensor(10057.8584, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10057.8232421875
tensor(10057.8584, grad_fn=<NegBackward0>) tensor(10057.8232, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10057.7880859375
tensor(10057.8232, grad_fn=<NegBackward0>) tensor(10057.7881, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10057.74609375
tensor(10057.7881, grad_fn=<NegBackward0>) tensor(10057.7461, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10057.6943359375
tensor(10057.7461, grad_fn=<NegBackward0>) tensor(10057.6943, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10057.6298828125
tensor(10057.6943, grad_fn=<NegBackward0>) tensor(10057.6299, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10057.5478515625
tensor(10057.6299, grad_fn=<NegBackward0>) tensor(10057.5479, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10057.4599609375
tensor(10057.5479, grad_fn=<NegBackward0>) tensor(10057.4600, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10057.2919921875
tensor(10057.4600, grad_fn=<NegBackward0>) tensor(10057.2920, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10057.05078125
tensor(10057.2920, grad_fn=<NegBackward0>) tensor(10057.0508, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10056.7861328125
tensor(10057.0508, grad_fn=<NegBackward0>) tensor(10056.7861, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10056.0185546875
tensor(10056.7861, grad_fn=<NegBackward0>) tensor(10056.0186, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10055.0810546875
tensor(10056.0186, grad_fn=<NegBackward0>) tensor(10055.0811, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10053.8017578125
tensor(10055.0811, grad_fn=<NegBackward0>) tensor(10053.8018, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10053.052734375
tensor(10053.8018, grad_fn=<NegBackward0>) tensor(10053.0527, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10052.787109375
tensor(10053.0527, grad_fn=<NegBackward0>) tensor(10052.7871, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10052.70703125
tensor(10052.7871, grad_fn=<NegBackward0>) tensor(10052.7070, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10052.6630859375
tensor(10052.7070, grad_fn=<NegBackward0>) tensor(10052.6631, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10052.5947265625
tensor(10052.6631, grad_fn=<NegBackward0>) tensor(10052.5947, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10052.5830078125
tensor(10052.5947, grad_fn=<NegBackward0>) tensor(10052.5830, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10052.5732421875
tensor(10052.5830, grad_fn=<NegBackward0>) tensor(10052.5732, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10052.5654296875
tensor(10052.5732, grad_fn=<NegBackward0>) tensor(10052.5654, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10052.5615234375
tensor(10052.5654, grad_fn=<NegBackward0>) tensor(10052.5615, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10052.5576171875
tensor(10052.5615, grad_fn=<NegBackward0>) tensor(10052.5576, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10052.5537109375
tensor(10052.5576, grad_fn=<NegBackward0>) tensor(10052.5537, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10052.5517578125
tensor(10052.5537, grad_fn=<NegBackward0>) tensor(10052.5518, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10052.548828125
tensor(10052.5518, grad_fn=<NegBackward0>) tensor(10052.5488, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10052.541015625
tensor(10052.5488, grad_fn=<NegBackward0>) tensor(10052.5410, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10052.53515625
tensor(10052.5410, grad_fn=<NegBackward0>) tensor(10052.5352, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10052.5322265625
tensor(10052.5352, grad_fn=<NegBackward0>) tensor(10052.5322, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10052.5302734375
tensor(10052.5322, grad_fn=<NegBackward0>) tensor(10052.5303, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10052.529296875
tensor(10052.5303, grad_fn=<NegBackward0>) tensor(10052.5293, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10052.5283203125
tensor(10052.5293, grad_fn=<NegBackward0>) tensor(10052.5283, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10052.5283203125
tensor(10052.5283, grad_fn=<NegBackward0>) tensor(10052.5283, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10052.52734375
tensor(10052.5283, grad_fn=<NegBackward0>) tensor(10052.5273, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10052.5234375
tensor(10052.5273, grad_fn=<NegBackward0>) tensor(10052.5234, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10052.5078125
tensor(10052.5234, grad_fn=<NegBackward0>) tensor(10052.5078, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10052.4775390625
tensor(10052.5078, grad_fn=<NegBackward0>) tensor(10052.4775, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10052.4794921875
tensor(10052.4775, grad_fn=<NegBackward0>) tensor(10052.4795, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10052.4775390625
tensor(10052.4775, grad_fn=<NegBackward0>) tensor(10052.4775, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10052.4755859375
tensor(10052.4775, grad_fn=<NegBackward0>) tensor(10052.4756, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10052.4755859375
tensor(10052.4756, grad_fn=<NegBackward0>) tensor(10052.4756, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10052.474609375
tensor(10052.4756, grad_fn=<NegBackward0>) tensor(10052.4746, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10052.474609375
tensor(10052.4746, grad_fn=<NegBackward0>) tensor(10052.4746, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10052.4736328125
tensor(10052.4746, grad_fn=<NegBackward0>) tensor(10052.4736, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10052.4736328125
tensor(10052.4736, grad_fn=<NegBackward0>) tensor(10052.4736, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10052.4736328125
tensor(10052.4736, grad_fn=<NegBackward0>) tensor(10052.4736, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10052.4716796875
tensor(10052.4736, grad_fn=<NegBackward0>) tensor(10052.4717, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10052.470703125
tensor(10052.4717, grad_fn=<NegBackward0>) tensor(10052.4707, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10052.470703125
tensor(10052.4707, grad_fn=<NegBackward0>) tensor(10052.4707, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10052.4716796875
tensor(10052.4707, grad_fn=<NegBackward0>) tensor(10052.4717, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10052.46875
tensor(10052.4707, grad_fn=<NegBackward0>) tensor(10052.4688, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10052.46875
tensor(10052.4688, grad_fn=<NegBackward0>) tensor(10052.4688, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10052.4658203125
tensor(10052.4688, grad_fn=<NegBackward0>) tensor(10052.4658, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10052.466796875
tensor(10052.4658, grad_fn=<NegBackward0>) tensor(10052.4668, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10052.4658203125
tensor(10052.4658, grad_fn=<NegBackward0>) tensor(10052.4658, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10052.4677734375
tensor(10052.4658, grad_fn=<NegBackward0>) tensor(10052.4678, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10052.4658203125
tensor(10052.4658, grad_fn=<NegBackward0>) tensor(10052.4658, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10052.46484375
tensor(10052.4658, grad_fn=<NegBackward0>) tensor(10052.4648, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10052.4658203125
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4658, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10052.4677734375
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4678, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10052.466796875
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4668, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10052.466796875
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4668, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -10052.46484375
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4648, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10052.46484375
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4648, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10052.46484375
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.4648, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10052.3642578125
tensor(10052.4648, grad_fn=<NegBackward0>) tensor(10052.3643, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10052.3623046875
tensor(10052.3643, grad_fn=<NegBackward0>) tensor(10052.3623, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10052.244140625
tensor(10052.3623, grad_fn=<NegBackward0>) tensor(10052.2441, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10052.2373046875
tensor(10052.2441, grad_fn=<NegBackward0>) tensor(10052.2373, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10052.236328125
tensor(10052.2373, grad_fn=<NegBackward0>) tensor(10052.2363, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10052.2353515625
tensor(10052.2363, grad_fn=<NegBackward0>) tensor(10052.2354, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10052.234375
tensor(10052.2354, grad_fn=<NegBackward0>) tensor(10052.2344, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10052.234375
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2344, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10052.2353515625
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2354, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10052.2431640625
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2432, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10052.240234375
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2402, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -10052.234375
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2344, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10052.234375
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2344, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10052.2333984375
tensor(10052.2344, grad_fn=<NegBackward0>) tensor(10052.2334, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10052.2353515625
tensor(10052.2334, grad_fn=<NegBackward0>) tensor(10052.2354, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10052.078125
tensor(10052.2334, grad_fn=<NegBackward0>) tensor(10052.0781, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10051.9287109375
tensor(10052.0781, grad_fn=<NegBackward0>) tensor(10051.9287, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10051.92578125
tensor(10051.9287, grad_fn=<NegBackward0>) tensor(10051.9258, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10051.9248046875
tensor(10051.9258, grad_fn=<NegBackward0>) tensor(10051.9248, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10051.923828125
tensor(10051.9248, grad_fn=<NegBackward0>) tensor(10051.9238, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10051.9248046875
tensor(10051.9238, grad_fn=<NegBackward0>) tensor(10051.9248, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -10051.9140625
tensor(10051.9238, grad_fn=<NegBackward0>) tensor(10051.9141, grad_fn=<NegBackward0>)
pi: tensor([[9.9998e-01, 1.8225e-05],
        [3.3371e-04, 9.9967e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1631, 0.8369], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2541, 0.1609],
         [0.5675, 0.1339]],

        [[0.7152, 0.1660],
         [0.6192, 0.5473]],

        [[0.6965, 0.1422],
         [0.5115, 0.5767]],

        [[0.6057, 0.1461],
         [0.6755, 0.7157]],

        [[0.6130, 0.1221],
         [0.7279, 0.5893]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.004873591227535791
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0017599824001759982
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.011877446349034957
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0266305739473973
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.011439885601143989
Global Adjusted Rand Index: 0.0069962651915641515
Average Adjusted Rand Index: 0.0065653173654436235
[0.18825002556084452, 0.0069962651915641515] [0.29757815639422347, 0.0065653173654436235] [10038.482421875, 10051.9130859375]
-------------------------------------
This iteration is 24
True Objective function: Loss = -10123.028031730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21747.943359375
inf tensor(21747.9434, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9993.87109375
tensor(21747.9434, grad_fn=<NegBackward0>) tensor(9993.8711, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9993.2099609375
tensor(9993.8711, grad_fn=<NegBackward0>) tensor(9993.2100, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9992.64453125
tensor(9993.2100, grad_fn=<NegBackward0>) tensor(9992.6445, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9992.5341796875
tensor(9992.6445, grad_fn=<NegBackward0>) tensor(9992.5342, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9992.4755859375
tensor(9992.5342, grad_fn=<NegBackward0>) tensor(9992.4756, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9992.43359375
tensor(9992.4756, grad_fn=<NegBackward0>) tensor(9992.4336, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9992.3994140625
tensor(9992.4336, grad_fn=<NegBackward0>) tensor(9992.3994, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9992.3662109375
tensor(9992.3994, grad_fn=<NegBackward0>) tensor(9992.3662, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9992.33203125
tensor(9992.3662, grad_fn=<NegBackward0>) tensor(9992.3320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9992.287109375
tensor(9992.3320, grad_fn=<NegBackward0>) tensor(9992.2871, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9992.2333984375
tensor(9992.2871, grad_fn=<NegBackward0>) tensor(9992.2334, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9992.1826171875
tensor(9992.2334, grad_fn=<NegBackward0>) tensor(9992.1826, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9992.1474609375
tensor(9992.1826, grad_fn=<NegBackward0>) tensor(9992.1475, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9992.12109375
tensor(9992.1475, grad_fn=<NegBackward0>) tensor(9992.1211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9992.09765625
tensor(9992.1211, grad_fn=<NegBackward0>) tensor(9992.0977, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9992.072265625
tensor(9992.0977, grad_fn=<NegBackward0>) tensor(9992.0723, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9992.046875
tensor(9992.0723, grad_fn=<NegBackward0>) tensor(9992.0469, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9992.025390625
tensor(9992.0469, grad_fn=<NegBackward0>) tensor(9992.0254, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9992.009765625
tensor(9992.0254, grad_fn=<NegBackward0>) tensor(9992.0098, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9992.0029296875
tensor(9992.0098, grad_fn=<NegBackward0>) tensor(9992.0029, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9991.9970703125
tensor(9992.0029, grad_fn=<NegBackward0>) tensor(9991.9971, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9991.9921875
tensor(9991.9971, grad_fn=<NegBackward0>) tensor(9991.9922, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9991.990234375
tensor(9991.9922, grad_fn=<NegBackward0>) tensor(9991.9902, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9991.98828125
tensor(9991.9902, grad_fn=<NegBackward0>) tensor(9991.9883, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9991.986328125
tensor(9991.9883, grad_fn=<NegBackward0>) tensor(9991.9863, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9991.9833984375
tensor(9991.9863, grad_fn=<NegBackward0>) tensor(9991.9834, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9991.982421875
tensor(9991.9834, grad_fn=<NegBackward0>) tensor(9991.9824, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9991.98046875
tensor(9991.9824, grad_fn=<NegBackward0>) tensor(9991.9805, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9991.9794921875
tensor(9991.9805, grad_fn=<NegBackward0>) tensor(9991.9795, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9991.9775390625
tensor(9991.9795, grad_fn=<NegBackward0>) tensor(9991.9775, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9991.9765625
tensor(9991.9775, grad_fn=<NegBackward0>) tensor(9991.9766, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9991.9755859375
tensor(9991.9766, grad_fn=<NegBackward0>) tensor(9991.9756, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9991.974609375
tensor(9991.9756, grad_fn=<NegBackward0>) tensor(9991.9746, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9991.974609375
tensor(9991.9746, grad_fn=<NegBackward0>) tensor(9991.9746, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9991.9736328125
tensor(9991.9746, grad_fn=<NegBackward0>) tensor(9991.9736, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9991.97265625
tensor(9991.9736, grad_fn=<NegBackward0>) tensor(9991.9727, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9991.9716796875
tensor(9991.9727, grad_fn=<NegBackward0>) tensor(9991.9717, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9991.9716796875
tensor(9991.9717, grad_fn=<NegBackward0>) tensor(9991.9717, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9991.970703125
tensor(9991.9717, grad_fn=<NegBackward0>) tensor(9991.9707, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9991.9697265625
tensor(9991.9707, grad_fn=<NegBackward0>) tensor(9991.9697, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9991.9697265625
tensor(9991.9697, grad_fn=<NegBackward0>) tensor(9991.9697, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9991.9697265625
tensor(9991.9697, grad_fn=<NegBackward0>) tensor(9991.9697, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9991.9677734375
tensor(9991.9697, grad_fn=<NegBackward0>) tensor(9991.9678, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9991.9677734375
tensor(9991.9678, grad_fn=<NegBackward0>) tensor(9991.9678, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9991.966796875
tensor(9991.9678, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9991.9658203125
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9991.96484375
tensor(9991.9658, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9991.9658203125
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9991.9658203125
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9991.9658203125
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9991.9638671875
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9991.96484375
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9991.96484375
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -9991.96484375
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -9991.9658203125
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -9991.9638671875
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9991.96484375
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9991.962890625
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9991.96484375
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9991.9638671875
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -9991.962890625
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9991.9619140625
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -9991.9619140625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -9991.9658203125
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[6.0248e-04, 9.9940e-01],
        [1.5942e-02, 9.8406e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0938, 0.9062], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2422, 0.1818],
         [0.5862, 0.1371]],

        [[0.7283, 0.2174],
         [0.5874, 0.7055]],

        [[0.6758, 0.1543],
         [0.5925, 0.6366]],

        [[0.5329, 0.1541],
         [0.6394, 0.6916]],

        [[0.5947, 0.1068],
         [0.5303, 0.5588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0012502748628180822
Average Adjusted Rand Index: -0.0020207530416798307
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24311.11328125
inf tensor(24311.1133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9993.828125
tensor(24311.1133, grad_fn=<NegBackward0>) tensor(9993.8281, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9992.2958984375
tensor(9993.8281, grad_fn=<NegBackward0>) tensor(9992.2959, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9992.203125
tensor(9992.2959, grad_fn=<NegBackward0>) tensor(9992.2031, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9992.1572265625
tensor(9992.2031, grad_fn=<NegBackward0>) tensor(9992.1572, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9992.1240234375
tensor(9992.1572, grad_fn=<NegBackward0>) tensor(9992.1240, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9992.0986328125
tensor(9992.1240, grad_fn=<NegBackward0>) tensor(9992.0986, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9992.0751953125
tensor(9992.0986, grad_fn=<NegBackward0>) tensor(9992.0752, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9992.05859375
tensor(9992.0752, grad_fn=<NegBackward0>) tensor(9992.0586, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9992.046875
tensor(9992.0586, grad_fn=<NegBackward0>) tensor(9992.0469, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9992.0361328125
tensor(9992.0469, grad_fn=<NegBackward0>) tensor(9992.0361, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9992.0302734375
tensor(9992.0361, grad_fn=<NegBackward0>) tensor(9992.0303, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9992.0244140625
tensor(9992.0303, grad_fn=<NegBackward0>) tensor(9992.0244, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9992.0185546875
tensor(9992.0244, grad_fn=<NegBackward0>) tensor(9992.0186, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9992.01171875
tensor(9992.0186, grad_fn=<NegBackward0>) tensor(9992.0117, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9992.0078125
tensor(9992.0117, grad_fn=<NegBackward0>) tensor(9992.0078, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9992.005859375
tensor(9992.0078, grad_fn=<NegBackward0>) tensor(9992.0059, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9992.0009765625
tensor(9992.0059, grad_fn=<NegBackward0>) tensor(9992.0010, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9991.9970703125
tensor(9992.0010, grad_fn=<NegBackward0>) tensor(9991.9971, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9991.994140625
tensor(9991.9971, grad_fn=<NegBackward0>) tensor(9991.9941, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9991.9912109375
tensor(9991.9941, grad_fn=<NegBackward0>) tensor(9991.9912, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9991.9892578125
tensor(9991.9912, grad_fn=<NegBackward0>) tensor(9991.9893, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9991.9853515625
tensor(9991.9893, grad_fn=<NegBackward0>) tensor(9991.9854, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9991.984375
tensor(9991.9854, grad_fn=<NegBackward0>) tensor(9991.9844, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9991.982421875
tensor(9991.9844, grad_fn=<NegBackward0>) tensor(9991.9824, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9991.98046875
tensor(9991.9824, grad_fn=<NegBackward0>) tensor(9991.9805, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9991.9794921875
tensor(9991.9805, grad_fn=<NegBackward0>) tensor(9991.9795, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9991.9775390625
tensor(9991.9795, grad_fn=<NegBackward0>) tensor(9991.9775, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9991.9765625
tensor(9991.9775, grad_fn=<NegBackward0>) tensor(9991.9766, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9991.974609375
tensor(9991.9766, grad_fn=<NegBackward0>) tensor(9991.9746, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9991.9736328125
tensor(9991.9746, grad_fn=<NegBackward0>) tensor(9991.9736, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9991.97265625
tensor(9991.9736, grad_fn=<NegBackward0>) tensor(9991.9727, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9991.97265625
tensor(9991.9727, grad_fn=<NegBackward0>) tensor(9991.9727, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9991.970703125
tensor(9991.9727, grad_fn=<NegBackward0>) tensor(9991.9707, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9991.9716796875
tensor(9991.9707, grad_fn=<NegBackward0>) tensor(9991.9717, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9991.970703125
tensor(9991.9707, grad_fn=<NegBackward0>) tensor(9991.9707, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9991.970703125
tensor(9991.9707, grad_fn=<NegBackward0>) tensor(9991.9707, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9991.96875
tensor(9991.9707, grad_fn=<NegBackward0>) tensor(9991.9688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9991.96875
tensor(9991.9688, grad_fn=<NegBackward0>) tensor(9991.9688, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9991.96875
tensor(9991.9688, grad_fn=<NegBackward0>) tensor(9991.9688, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9991.966796875
tensor(9991.9688, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9991.9677734375
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9678, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9991.9677734375
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9678, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9991.966796875
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9668, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9991.9658203125
tensor(9991.9668, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9991.9658203125
tensor(9991.9658, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9991.96484375
tensor(9991.9658, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9991.9658203125
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9991.9658203125
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9991.96484375
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9991.9638671875
tensor(9991.9648, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9991.9638671875
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9991.962890625
tensor(9991.9639, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9991.96484375
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9991.96484375
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9991.9638671875
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9991.962890625
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9991.96484375
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9648, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9991.9638671875
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9991.9677734375
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9678, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9991.9619140625
tensor(9991.9629, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9991.9619140625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9991.9658203125
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9658, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -9991.9619140625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9991.9619140625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -9991.9619140625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9619, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9991.962890625
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9991.9638671875
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9639, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9991.9609375
tensor(9991.9619, grad_fn=<NegBackward0>) tensor(9991.9609, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9991.962890625
tensor(9991.9609, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9991.962890625
tensor(9991.9609, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9991.962890625
tensor(9991.9609, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -9991.962890625
tensor(9991.9609, grad_fn=<NegBackward0>) tensor(9991.9629, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -9991.9814453125
tensor(9991.9609, grad_fn=<NegBackward0>) tensor(9991.9814, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.8389e-01, 1.6113e-02],
        [9.9967e-01, 3.3424e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9052, 0.0948], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1368, 0.1819],
         [0.5671, 0.2424]],

        [[0.7023, 0.2176],
         [0.5105, 0.6601]],

        [[0.7107, 0.1543],
         [0.5280, 0.6357]],

        [[0.5145, 0.1540],
         [0.5965, 0.6976]],

        [[0.6065, 0.1068],
         [0.6769, 0.5072]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0012502748628180822
Average Adjusted Rand Index: -0.0020207530416798307
[0.0012502748628180822, 0.0012502748628180822] [-0.0020207530416798307, -0.0020207530416798307] [9991.962890625, 9991.9814453125]
-------------------------------------
This iteration is 25
True Objective function: Loss = -10286.400204662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22609.400390625
inf tensor(22609.4004, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10163.119140625
tensor(22609.4004, grad_fn=<NegBackward0>) tensor(10163.1191, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10162.052734375
tensor(10163.1191, grad_fn=<NegBackward0>) tensor(10162.0527, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10161.6865234375
tensor(10162.0527, grad_fn=<NegBackward0>) tensor(10161.6865, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10161.5
tensor(10161.6865, grad_fn=<NegBackward0>) tensor(10161.5000, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10161.3193359375
tensor(10161.5000, grad_fn=<NegBackward0>) tensor(10161.3193, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10161.08984375
tensor(10161.3193, grad_fn=<NegBackward0>) tensor(10161.0898, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10160.865234375
tensor(10161.0898, grad_fn=<NegBackward0>) tensor(10160.8652, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10160.638671875
tensor(10160.8652, grad_fn=<NegBackward0>) tensor(10160.6387, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10160.380859375
tensor(10160.6387, grad_fn=<NegBackward0>) tensor(10160.3809, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10160.0458984375
tensor(10160.3809, grad_fn=<NegBackward0>) tensor(10160.0459, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10159.669921875
tensor(10160.0459, grad_fn=<NegBackward0>) tensor(10159.6699, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10159.31640625
tensor(10159.6699, grad_fn=<NegBackward0>) tensor(10159.3164, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10158.998046875
tensor(10159.3164, grad_fn=<NegBackward0>) tensor(10158.9980, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10158.6728515625
tensor(10158.9980, grad_fn=<NegBackward0>) tensor(10158.6729, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10158.3037109375
tensor(10158.6729, grad_fn=<NegBackward0>) tensor(10158.3037, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10157.8935546875
tensor(10158.3037, grad_fn=<NegBackward0>) tensor(10157.8936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10157.529296875
tensor(10157.8936, grad_fn=<NegBackward0>) tensor(10157.5293, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10157.2802734375
tensor(10157.5293, grad_fn=<NegBackward0>) tensor(10157.2803, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10157.0927734375
tensor(10157.2803, grad_fn=<NegBackward0>) tensor(10157.0928, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10156.9619140625
tensor(10157.0928, grad_fn=<NegBackward0>) tensor(10156.9619, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10156.859375
tensor(10156.9619, grad_fn=<NegBackward0>) tensor(10156.8594, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10156.771484375
tensor(10156.8594, grad_fn=<NegBackward0>) tensor(10156.7715, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10156.6884765625
tensor(10156.7715, grad_fn=<NegBackward0>) tensor(10156.6885, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10156.619140625
tensor(10156.6885, grad_fn=<NegBackward0>) tensor(10156.6191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10156.5537109375
tensor(10156.6191, grad_fn=<NegBackward0>) tensor(10156.5537, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10156.482421875
tensor(10156.5537, grad_fn=<NegBackward0>) tensor(10156.4824, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10156.39453125
tensor(10156.4824, grad_fn=<NegBackward0>) tensor(10156.3945, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10156.30078125
tensor(10156.3945, grad_fn=<NegBackward0>) tensor(10156.3008, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10156.1865234375
tensor(10156.3008, grad_fn=<NegBackward0>) tensor(10156.1865, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10156.1083984375
tensor(10156.1865, grad_fn=<NegBackward0>) tensor(10156.1084, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10156.0361328125
tensor(10156.1084, grad_fn=<NegBackward0>) tensor(10156.0361, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10155.896484375
tensor(10156.0361, grad_fn=<NegBackward0>) tensor(10155.8965, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10153.3291015625
tensor(10155.8965, grad_fn=<NegBackward0>) tensor(10153.3291, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10152.6494140625
tensor(10153.3291, grad_fn=<NegBackward0>) tensor(10152.6494, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10152.51171875
tensor(10152.6494, grad_fn=<NegBackward0>) tensor(10152.5117, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10152.3671875
tensor(10152.5117, grad_fn=<NegBackward0>) tensor(10152.3672, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10152.3447265625
tensor(10152.3672, grad_fn=<NegBackward0>) tensor(10152.3447, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10152.2939453125
tensor(10152.3447, grad_fn=<NegBackward0>) tensor(10152.2939, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10152.16796875
tensor(10152.2939, grad_fn=<NegBackward0>) tensor(10152.1680, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10152.158203125
tensor(10152.1680, grad_fn=<NegBackward0>) tensor(10152.1582, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10152.15234375
tensor(10152.1582, grad_fn=<NegBackward0>) tensor(10152.1523, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10152.1474609375
tensor(10152.1523, grad_fn=<NegBackward0>) tensor(10152.1475, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10152.1435546875
tensor(10152.1475, grad_fn=<NegBackward0>) tensor(10152.1436, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10152.12890625
tensor(10152.1436, grad_fn=<NegBackward0>) tensor(10152.1289, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10152.125
tensor(10152.1289, grad_fn=<NegBackward0>) tensor(10152.1250, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10152.126953125
tensor(10152.1250, grad_fn=<NegBackward0>) tensor(10152.1270, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10152.1181640625
tensor(10152.1250, grad_fn=<NegBackward0>) tensor(10152.1182, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10152.115234375
tensor(10152.1182, grad_fn=<NegBackward0>) tensor(10152.1152, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10152.1142578125
tensor(10152.1152, grad_fn=<NegBackward0>) tensor(10152.1143, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10152.11328125
tensor(10152.1143, grad_fn=<NegBackward0>) tensor(10152.1133, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10152.1103515625
tensor(10152.1133, grad_fn=<NegBackward0>) tensor(10152.1104, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10152.1083984375
tensor(10152.1104, grad_fn=<NegBackward0>) tensor(10152.1084, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10152.1083984375
tensor(10152.1084, grad_fn=<NegBackward0>) tensor(10152.1084, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10152.1083984375
tensor(10152.1084, grad_fn=<NegBackward0>) tensor(10152.1084, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10152.1044921875
tensor(10152.1084, grad_fn=<NegBackward0>) tensor(10152.1045, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10152.1103515625
tensor(10152.1045, grad_fn=<NegBackward0>) tensor(10152.1104, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10152.103515625
tensor(10152.1045, grad_fn=<NegBackward0>) tensor(10152.1035, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10152.1015625
tensor(10152.1035, grad_fn=<NegBackward0>) tensor(10152.1016, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10152.095703125
tensor(10152.1016, grad_fn=<NegBackward0>) tensor(10152.0957, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10152.0966796875
tensor(10152.0957, grad_fn=<NegBackward0>) tensor(10152.0967, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10152.095703125
tensor(10152.0957, grad_fn=<NegBackward0>) tensor(10152.0957, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10152.0947265625
tensor(10152.0957, grad_fn=<NegBackward0>) tensor(10152.0947, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10152.0947265625
tensor(10152.0947, grad_fn=<NegBackward0>) tensor(10152.0947, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10152.0927734375
tensor(10152.0947, grad_fn=<NegBackward0>) tensor(10152.0928, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10152.091796875
tensor(10152.0928, grad_fn=<NegBackward0>) tensor(10152.0918, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10152.0927734375
tensor(10152.0918, grad_fn=<NegBackward0>) tensor(10152.0928, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10152.0908203125
tensor(10152.0918, grad_fn=<NegBackward0>) tensor(10152.0908, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10152.0908203125
tensor(10152.0908, grad_fn=<NegBackward0>) tensor(10152.0908, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10152.08984375
tensor(10152.0908, grad_fn=<NegBackward0>) tensor(10152.0898, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10152.0908203125
tensor(10152.0898, grad_fn=<NegBackward0>) tensor(10152.0908, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10152.08984375
tensor(10152.0898, grad_fn=<NegBackward0>) tensor(10152.0898, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10152.08984375
tensor(10152.0898, grad_fn=<NegBackward0>) tensor(10152.0898, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10152.087890625
tensor(10152.0898, grad_fn=<NegBackward0>) tensor(10152.0879, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10152.0908203125
tensor(10152.0879, grad_fn=<NegBackward0>) tensor(10152.0908, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10152.0869140625
tensor(10152.0879, grad_fn=<NegBackward0>) tensor(10152.0869, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10152.0849609375
tensor(10152.0869, grad_fn=<NegBackward0>) tensor(10152.0850, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10152.0830078125
tensor(10152.0850, grad_fn=<NegBackward0>) tensor(10152.0830, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10152.0830078125
tensor(10152.0830, grad_fn=<NegBackward0>) tensor(10152.0830, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10152.0830078125
tensor(10152.0830, grad_fn=<NegBackward0>) tensor(10152.0830, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10152.0830078125
tensor(10152.0830, grad_fn=<NegBackward0>) tensor(10152.0830, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10152.08203125
tensor(10152.0830, grad_fn=<NegBackward0>) tensor(10152.0820, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10152.1044921875
tensor(10152.0820, grad_fn=<NegBackward0>) tensor(10152.1045, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10152.08203125
tensor(10152.0820, grad_fn=<NegBackward0>) tensor(10152.0820, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10152.0830078125
tensor(10152.0820, grad_fn=<NegBackward0>) tensor(10152.0830, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -10152.0810546875
tensor(10152.0820, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10152.1064453125
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.1064, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10152.0810546875
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10152.0810546875
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10152.083984375
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0840, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10152.0810546875
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10152.0810546875
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10152.0810546875
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10152.0869140625
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0869, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10152.080078125
tensor(10152.0811, grad_fn=<NegBackward0>) tensor(10152.0801, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10152.359375
tensor(10152.0801, grad_fn=<NegBackward0>) tensor(10152.3594, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -10152.080078125
tensor(10152.0801, grad_fn=<NegBackward0>) tensor(10152.0801, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10152.0810546875
tensor(10152.0801, grad_fn=<NegBackward0>) tensor(10152.0811, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10152.080078125
tensor(10152.0801, grad_fn=<NegBackward0>) tensor(10152.0801, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10152.080078125
tensor(10152.0801, grad_fn=<NegBackward0>) tensor(10152.0801, grad_fn=<NegBackward0>)
pi: tensor([[9.9996e-01, 3.9942e-05],
        [1.5447e-01, 8.4553e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0105, 0.9895], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2226, 0.2654],
         [0.5528, 0.1371]],

        [[0.6231, 0.1412],
         [0.6961, 0.6654]],

        [[0.5069, 0.1235],
         [0.6259, 0.6648]],

        [[0.7253, 0.1202],
         [0.6142, 0.6078]],

        [[0.5760, 0.1257],
         [0.6953, 0.6198]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.043550744307015006
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 28
Adjusted Rand Index: 0.18628863074177288
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 25
Adjusted Rand Index: 0.2421540437802863
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 26
Adjusted Rand Index: 0.2226749911728827
Global Adjusted Rand Index: 0.1137709596583105
Average Adjusted Rand Index: 0.1392671275509773
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20169.359375
inf tensor(20169.3594, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10162.021484375
tensor(20169.3594, grad_fn=<NegBackward0>) tensor(10162.0215, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10161.6455078125
tensor(10162.0215, grad_fn=<NegBackward0>) tensor(10161.6455, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10161.4912109375
tensor(10161.6455, grad_fn=<NegBackward0>) tensor(10161.4912, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10161.27734375
tensor(10161.4912, grad_fn=<NegBackward0>) tensor(10161.2773, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10160.9990234375
tensor(10161.2773, grad_fn=<NegBackward0>) tensor(10160.9990, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10160.7587890625
tensor(10160.9990, grad_fn=<NegBackward0>) tensor(10160.7588, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10160.5556640625
tensor(10160.7588, grad_fn=<NegBackward0>) tensor(10160.5557, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10160.1083984375
tensor(10160.5557, grad_fn=<NegBackward0>) tensor(10160.1084, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10158.947265625
tensor(10160.1084, grad_fn=<NegBackward0>) tensor(10158.9473, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10158.2470703125
tensor(10158.9473, grad_fn=<NegBackward0>) tensor(10158.2471, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10157.9443359375
tensor(10158.2471, grad_fn=<NegBackward0>) tensor(10157.9443, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10157.8115234375
tensor(10157.9443, grad_fn=<NegBackward0>) tensor(10157.8115, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10157.728515625
tensor(10157.8115, grad_fn=<NegBackward0>) tensor(10157.7285, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10157.673828125
tensor(10157.7285, grad_fn=<NegBackward0>) tensor(10157.6738, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10157.63671875
tensor(10157.6738, grad_fn=<NegBackward0>) tensor(10157.6367, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10157.609375
tensor(10157.6367, grad_fn=<NegBackward0>) tensor(10157.6094, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10157.5888671875
tensor(10157.6094, grad_fn=<NegBackward0>) tensor(10157.5889, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10157.5712890625
tensor(10157.5889, grad_fn=<NegBackward0>) tensor(10157.5713, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10157.556640625
tensor(10157.5713, grad_fn=<NegBackward0>) tensor(10157.5566, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10157.541015625
tensor(10157.5566, grad_fn=<NegBackward0>) tensor(10157.5410, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10157.5234375
tensor(10157.5410, grad_fn=<NegBackward0>) tensor(10157.5234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10157.4921875
tensor(10157.5234, grad_fn=<NegBackward0>) tensor(10157.4922, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10157.4140625
tensor(10157.4922, grad_fn=<NegBackward0>) tensor(10157.4141, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10157.177734375
tensor(10157.4141, grad_fn=<NegBackward0>) tensor(10157.1777, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10156.9169921875
tensor(10157.1777, grad_fn=<NegBackward0>) tensor(10156.9170, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10156.8076171875
tensor(10156.9170, grad_fn=<NegBackward0>) tensor(10156.8076, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10156.759765625
tensor(10156.8076, grad_fn=<NegBackward0>) tensor(10156.7598, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10156.73046875
tensor(10156.7598, grad_fn=<NegBackward0>) tensor(10156.7305, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10156.7138671875
tensor(10156.7305, grad_fn=<NegBackward0>) tensor(10156.7139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10156.701171875
tensor(10156.7139, grad_fn=<NegBackward0>) tensor(10156.7012, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10156.6923828125
tensor(10156.7012, grad_fn=<NegBackward0>) tensor(10156.6924, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10156.685546875
tensor(10156.6924, grad_fn=<NegBackward0>) tensor(10156.6855, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10156.6796875
tensor(10156.6855, grad_fn=<NegBackward0>) tensor(10156.6797, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10156.6767578125
tensor(10156.6797, grad_fn=<NegBackward0>) tensor(10156.6768, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10156.6748046875
tensor(10156.6768, grad_fn=<NegBackward0>) tensor(10156.6748, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10156.6708984375
tensor(10156.6748, grad_fn=<NegBackward0>) tensor(10156.6709, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10156.669921875
tensor(10156.6709, grad_fn=<NegBackward0>) tensor(10156.6699, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10156.6689453125
tensor(10156.6699, grad_fn=<NegBackward0>) tensor(10156.6689, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10156.66796875
tensor(10156.6689, grad_fn=<NegBackward0>) tensor(10156.6680, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10156.6669921875
tensor(10156.6680, grad_fn=<NegBackward0>) tensor(10156.6670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10156.666015625
tensor(10156.6670, grad_fn=<NegBackward0>) tensor(10156.6660, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10156.6640625
tensor(10156.6660, grad_fn=<NegBackward0>) tensor(10156.6641, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10156.6630859375
tensor(10156.6641, grad_fn=<NegBackward0>) tensor(10156.6631, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10156.6640625
tensor(10156.6631, grad_fn=<NegBackward0>) tensor(10156.6641, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10156.6640625
tensor(10156.6631, grad_fn=<NegBackward0>) tensor(10156.6641, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10156.662109375
tensor(10156.6631, grad_fn=<NegBackward0>) tensor(10156.6621, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10156.66015625
tensor(10156.6621, grad_fn=<NegBackward0>) tensor(10156.6602, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10156.6611328125
tensor(10156.6602, grad_fn=<NegBackward0>) tensor(10156.6611, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10156.66015625
tensor(10156.6602, grad_fn=<NegBackward0>) tensor(10156.6602, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10156.66015625
tensor(10156.6602, grad_fn=<NegBackward0>) tensor(10156.6602, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10156.6591796875
tensor(10156.6602, grad_fn=<NegBackward0>) tensor(10156.6592, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10156.6591796875
tensor(10156.6592, grad_fn=<NegBackward0>) tensor(10156.6592, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10156.6591796875
tensor(10156.6592, grad_fn=<NegBackward0>) tensor(10156.6592, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10156.658203125
tensor(10156.6592, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10156.658203125
tensor(10156.6582, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10156.6572265625
tensor(10156.6582, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10156.658203125
tensor(10156.6572, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10156.65625
tensor(10156.6572, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10156.6591796875
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6592, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10156.658203125
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10156.658203125
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10156.66796875
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6680, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10156.6728515625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6729, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -10156.71484375
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.7148, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -10156.65625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10156.6572265625
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10156.658203125
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10156.6552734375
tensor(10156.6562, grad_fn=<NegBackward0>) tensor(10156.6553, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10156.6572265625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10156.6552734375
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6553, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10156.6767578125
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6768, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10156.6572265625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6572, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10156.658203125
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6582, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10156.65625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -10156.6552734375
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6553, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10156.65625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10156.65625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -10156.662109375
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6621, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -10156.65625
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.6562, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -10156.7880859375
tensor(10156.6553, grad_fn=<NegBackward0>) tensor(10156.7881, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.9660, 0.0340],
        [0.9952, 0.0048]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9908, 0.0092], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1428, 0.2601],
         [0.6636, 0.1159]],

        [[0.7117, 0.0646],
         [0.6137, 0.6013]],

        [[0.5220, 0.0670],
         [0.5414, 0.5733]],

        [[0.6544, 0.1810],
         [0.5159, 0.6050]],

        [[0.5799, 0.1812],
         [0.6928, 0.7159]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 1.4478937888384089e-05
Average Adjusted Rand Index: 0.0002143774429470895
[0.1137709596583105, 1.4478937888384089e-05] [0.1392671275509773, 0.0002143774429470895] [10152.080078125, 10156.7880859375]
-------------------------------------
This iteration is 26
True Objective function: Loss = -10205.708511078867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25892.54296875
inf tensor(25892.5430, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10047.2939453125
tensor(25892.5430, grad_fn=<NegBackward0>) tensor(10047.2939, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10046.63671875
tensor(10047.2939, grad_fn=<NegBackward0>) tensor(10046.6367, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10046.517578125
tensor(10046.6367, grad_fn=<NegBackward0>) tensor(10046.5176, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10046.4462890625
tensor(10046.5176, grad_fn=<NegBackward0>) tensor(10046.4463, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10046.388671875
tensor(10046.4463, grad_fn=<NegBackward0>) tensor(10046.3887, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10046.3232421875
tensor(10046.3887, grad_fn=<NegBackward0>) tensor(10046.3232, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10046.22265625
tensor(10046.3232, grad_fn=<NegBackward0>) tensor(10046.2227, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10046.01171875
tensor(10046.2227, grad_fn=<NegBackward0>) tensor(10046.0117, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10045.6826171875
tensor(10046.0117, grad_fn=<NegBackward0>) tensor(10045.6826, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10045.5185546875
tensor(10045.6826, grad_fn=<NegBackward0>) tensor(10045.5186, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10045.3828125
tensor(10045.5186, grad_fn=<NegBackward0>) tensor(10045.3828, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10044.78515625
tensor(10045.3828, grad_fn=<NegBackward0>) tensor(10044.7852, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10044.2412109375
tensor(10044.7852, grad_fn=<NegBackward0>) tensor(10044.2412, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10044.205078125
tensor(10044.2412, grad_fn=<NegBackward0>) tensor(10044.2051, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10044.1904296875
tensor(10044.2051, grad_fn=<NegBackward0>) tensor(10044.1904, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10044.1845703125
tensor(10044.1904, grad_fn=<NegBackward0>) tensor(10044.1846, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10044.1796875
tensor(10044.1846, grad_fn=<NegBackward0>) tensor(10044.1797, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10044.1787109375
tensor(10044.1797, grad_fn=<NegBackward0>) tensor(10044.1787, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10044.17578125
tensor(10044.1787, grad_fn=<NegBackward0>) tensor(10044.1758, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10044.17578125
tensor(10044.1758, grad_fn=<NegBackward0>) tensor(10044.1758, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10044.1748046875
tensor(10044.1758, grad_fn=<NegBackward0>) tensor(10044.1748, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10044.1728515625
tensor(10044.1748, grad_fn=<NegBackward0>) tensor(10044.1729, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10044.171875
tensor(10044.1729, grad_fn=<NegBackward0>) tensor(10044.1719, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10044.1708984375
tensor(10044.1719, grad_fn=<NegBackward0>) tensor(10044.1709, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10044.169921875
tensor(10044.1709, grad_fn=<NegBackward0>) tensor(10044.1699, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10044.1689453125
tensor(10044.1699, grad_fn=<NegBackward0>) tensor(10044.1689, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10044.169921875
tensor(10044.1689, grad_fn=<NegBackward0>) tensor(10044.1699, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10044.16796875
tensor(10044.1689, grad_fn=<NegBackward0>) tensor(10044.1680, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10044.16796875
tensor(10044.1680, grad_fn=<NegBackward0>) tensor(10044.1680, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10044.1669921875
tensor(10044.1680, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10044.1669921875
tensor(10044.1670, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10044.1669921875
tensor(10044.1670, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10044.16796875
tensor(10044.1670, grad_fn=<NegBackward0>) tensor(10044.1680, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10044.1669921875
tensor(10044.1670, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10044.1650390625
tensor(10044.1670, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10044.1669921875
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10044.166015625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10044.166015625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10044.166015625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -10044.166015625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10044.166015625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10044.1796875
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1797, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10044.1650390625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10044.1669921875
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1670, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10044.1640625
tensor(10044.1650, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10044.1650390625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10044.1650390625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10044.185546875
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1855, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10044.1689453125
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1689, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10044.171875
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1719, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10044.166015625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10044.1640625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10044.1650390625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10044.1650390625
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1650, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10044.1630859375
tensor(10044.1641, grad_fn=<NegBackward0>) tensor(10044.1631, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10044.1630859375
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1631, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10044.1630859375
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1631, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10044.166015625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1660, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -10044.1640625
tensor(10044.1631, grad_fn=<NegBackward0>) tensor(10044.1641, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[9.0402e-01, 9.5985e-02],
        [9.9972e-01, 2.8066e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9953, 0.0047], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1384, 0.0978],
         [0.5127, 0.1782]],

        [[0.5602, 0.1662],
         [0.5249, 0.5891]],

        [[0.5346, 0.1132],
         [0.7069, 0.5978]],

        [[0.5561, 0.1695],
         [0.6772, 0.5662]],

        [[0.5718, 0.1590],
         [0.6542, 0.5856]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23112.2109375
inf tensor(23112.2109, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10048.0078125
tensor(23112.2109, grad_fn=<NegBackward0>) tensor(10048.0078, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10046.7041015625
tensor(10048.0078, grad_fn=<NegBackward0>) tensor(10046.7041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10046.369140625
tensor(10046.7041, grad_fn=<NegBackward0>) tensor(10046.3691, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10046.150390625
tensor(10046.3691, grad_fn=<NegBackward0>) tensor(10046.1504, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10045.962890625
tensor(10046.1504, grad_fn=<NegBackward0>) tensor(10045.9629, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10045.7861328125
tensor(10045.9629, grad_fn=<NegBackward0>) tensor(10045.7861, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10045.6298828125
tensor(10045.7861, grad_fn=<NegBackward0>) tensor(10045.6299, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10045.482421875
tensor(10045.6299, grad_fn=<NegBackward0>) tensor(10045.4824, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10045.32421875
tensor(10045.4824, grad_fn=<NegBackward0>) tensor(10045.3242, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10045.1396484375
tensor(10045.3242, grad_fn=<NegBackward0>) tensor(10045.1396, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10044.9052734375
tensor(10045.1396, grad_fn=<NegBackward0>) tensor(10044.9053, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10044.595703125
tensor(10044.9053, grad_fn=<NegBackward0>) tensor(10044.5957, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10044.2177734375
tensor(10044.5957, grad_fn=<NegBackward0>) tensor(10044.2178, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10043.8759765625
tensor(10044.2178, grad_fn=<NegBackward0>) tensor(10043.8760, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10043.630859375
tensor(10043.8760, grad_fn=<NegBackward0>) tensor(10043.6309, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10043.484375
tensor(10043.6309, grad_fn=<NegBackward0>) tensor(10043.4844, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10043.4013671875
tensor(10043.4844, grad_fn=<NegBackward0>) tensor(10043.4014, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10043.3505859375
tensor(10043.4014, grad_fn=<NegBackward0>) tensor(10043.3506, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10043.314453125
tensor(10043.3506, grad_fn=<NegBackward0>) tensor(10043.3145, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10043.2822265625
tensor(10043.3145, grad_fn=<NegBackward0>) tensor(10043.2822, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10043.251953125
tensor(10043.2822, grad_fn=<NegBackward0>) tensor(10043.2520, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10043.2197265625
tensor(10043.2520, grad_fn=<NegBackward0>) tensor(10043.2197, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10043.19140625
tensor(10043.2197, grad_fn=<NegBackward0>) tensor(10043.1914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10043.169921875
tensor(10043.1914, grad_fn=<NegBackward0>) tensor(10043.1699, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10043.1572265625
tensor(10043.1699, grad_fn=<NegBackward0>) tensor(10043.1572, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10043.150390625
tensor(10043.1572, grad_fn=<NegBackward0>) tensor(10043.1504, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10043.1455078125
tensor(10043.1504, grad_fn=<NegBackward0>) tensor(10043.1455, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10043.1416015625
tensor(10043.1455, grad_fn=<NegBackward0>) tensor(10043.1416, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10043.138671875
tensor(10043.1416, grad_fn=<NegBackward0>) tensor(10043.1387, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10043.13671875
tensor(10043.1387, grad_fn=<NegBackward0>) tensor(10043.1367, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10043.1357421875
tensor(10043.1367, grad_fn=<NegBackward0>) tensor(10043.1357, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10043.1337890625
tensor(10043.1357, grad_fn=<NegBackward0>) tensor(10043.1338, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10043.1328125
tensor(10043.1338, grad_fn=<NegBackward0>) tensor(10043.1328, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10043.1318359375
tensor(10043.1328, grad_fn=<NegBackward0>) tensor(10043.1318, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10043.1298828125
tensor(10043.1318, grad_fn=<NegBackward0>) tensor(10043.1299, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10043.1298828125
tensor(10043.1299, grad_fn=<NegBackward0>) tensor(10043.1299, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10043.1279296875
tensor(10043.1299, grad_fn=<NegBackward0>) tensor(10043.1279, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10043.126953125
tensor(10043.1279, grad_fn=<NegBackward0>) tensor(10043.1270, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10043.1259765625
tensor(10043.1270, grad_fn=<NegBackward0>) tensor(10043.1260, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10043.125
tensor(10043.1260, grad_fn=<NegBackward0>) tensor(10043.1250, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10043.1259765625
tensor(10043.1250, grad_fn=<NegBackward0>) tensor(10043.1260, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10043.125
tensor(10043.1250, grad_fn=<NegBackward0>) tensor(10043.1250, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10043.123046875
tensor(10043.1250, grad_fn=<NegBackward0>) tensor(10043.1230, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10043.123046875
tensor(10043.1230, grad_fn=<NegBackward0>) tensor(10043.1230, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10043.12109375
tensor(10043.1230, grad_fn=<NegBackward0>) tensor(10043.1211, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10043.12109375
tensor(10043.1211, grad_fn=<NegBackward0>) tensor(10043.1211, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10043.12109375
tensor(10043.1211, grad_fn=<NegBackward0>) tensor(10043.1211, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10043.1201171875
tensor(10043.1211, grad_fn=<NegBackward0>) tensor(10043.1201, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10043.1201171875
tensor(10043.1201, grad_fn=<NegBackward0>) tensor(10043.1201, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10043.1201171875
tensor(10043.1201, grad_fn=<NegBackward0>) tensor(10043.1201, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10043.1201171875
tensor(10043.1201, grad_fn=<NegBackward0>) tensor(10043.1201, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10043.1181640625
tensor(10043.1201, grad_fn=<NegBackward0>) tensor(10043.1182, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10043.1181640625
tensor(10043.1182, grad_fn=<NegBackward0>) tensor(10043.1182, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10043.1181640625
tensor(10043.1182, grad_fn=<NegBackward0>) tensor(10043.1182, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10043.119140625
tensor(10043.1182, grad_fn=<NegBackward0>) tensor(10043.1191, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10043.1181640625
tensor(10043.1182, grad_fn=<NegBackward0>) tensor(10043.1182, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10043.1171875
tensor(10043.1182, grad_fn=<NegBackward0>) tensor(10043.1172, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10043.1162109375
tensor(10043.1172, grad_fn=<NegBackward0>) tensor(10043.1162, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10043.119140625
tensor(10043.1162, grad_fn=<NegBackward0>) tensor(10043.1191, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10043.1162109375
tensor(10043.1162, grad_fn=<NegBackward0>) tensor(10043.1162, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10043.1220703125
tensor(10043.1162, grad_fn=<NegBackward0>) tensor(10043.1221, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10043.1162109375
tensor(10043.1162, grad_fn=<NegBackward0>) tensor(10043.1162, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10043.115234375
tensor(10043.1162, grad_fn=<NegBackward0>) tensor(10043.1152, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10043.1162109375
tensor(10043.1152, grad_fn=<NegBackward0>) tensor(10043.1162, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10043.158203125
tensor(10043.1152, grad_fn=<NegBackward0>) tensor(10043.1582, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10043.1142578125
tensor(10043.1152, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10043.1142578125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10043.1142578125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10043.115234375
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1152, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10043.115234375
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1152, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10043.1142578125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10043.1142578125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10043.1142578125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10043.115234375
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1152, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10043.11328125
tensor(10043.1143, grad_fn=<NegBackward0>) tensor(10043.1133, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10043.1123046875
tensor(10043.1133, grad_fn=<NegBackward0>) tensor(10043.1123, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10043.1396484375
tensor(10043.1123, grad_fn=<NegBackward0>) tensor(10043.1396, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10043.11328125
tensor(10043.1123, grad_fn=<NegBackward0>) tensor(10043.1133, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10043.1162109375
tensor(10043.1123, grad_fn=<NegBackward0>) tensor(10043.1162, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10043.1142578125
tensor(10043.1123, grad_fn=<NegBackward0>) tensor(10043.1143, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10043.14453125
tensor(10043.1123, grad_fn=<NegBackward0>) tensor(10043.1445, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.0014, 0.9986],
        [0.2197, 0.7803]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1778, 0.2596],
         [0.5704, 0.1346]],

        [[0.5965, 0.1571],
         [0.7149, 0.6050]],

        [[0.7154, 0.1339],
         [0.6276, 0.5960]],

        [[0.5310, 0.1598],
         [0.7076, 0.5182]],

        [[0.6002, 0.1555],
         [0.6470, 0.6095]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010533137159789224
Average Adjusted Rand Index: 0.0012150384168710007
[0.0, 0.0010533137159789224] [0.0, 0.0012150384168710007] [10044.1640625, 10043.14453125]
-------------------------------------
This iteration is 27
True Objective function: Loss = -10126.267269019521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21139.158203125
inf tensor(21139.1582, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10004.3173828125
tensor(21139.1582, grad_fn=<NegBackward0>) tensor(10004.3174, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10002.5849609375
tensor(10004.3174, grad_fn=<NegBackward0>) tensor(10002.5850, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10002.1220703125
tensor(10002.5850, grad_fn=<NegBackward0>) tensor(10002.1221, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10001.845703125
tensor(10002.1221, grad_fn=<NegBackward0>) tensor(10001.8457, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10001.6552734375
tensor(10001.8457, grad_fn=<NegBackward0>) tensor(10001.6553, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10001.501953125
tensor(10001.6553, grad_fn=<NegBackward0>) tensor(10001.5020, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10001.361328125
tensor(10001.5020, grad_fn=<NegBackward0>) tensor(10001.3613, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10001.2353515625
tensor(10001.3613, grad_fn=<NegBackward0>) tensor(10001.2354, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10001.1376953125
tensor(10001.2354, grad_fn=<NegBackward0>) tensor(10001.1377, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10001.0703125
tensor(10001.1377, grad_fn=<NegBackward0>) tensor(10001.0703, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10001.0185546875
tensor(10001.0703, grad_fn=<NegBackward0>) tensor(10001.0186, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10000.970703125
tensor(10001.0186, grad_fn=<NegBackward0>) tensor(10000.9707, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10000.9267578125
tensor(10000.9707, grad_fn=<NegBackward0>) tensor(10000.9268, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10000.875
tensor(10000.9268, grad_fn=<NegBackward0>) tensor(10000.8750, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10000.8193359375
tensor(10000.8750, grad_fn=<NegBackward0>) tensor(10000.8193, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10000.7578125
tensor(10000.8193, grad_fn=<NegBackward0>) tensor(10000.7578, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10000.6865234375
tensor(10000.7578, grad_fn=<NegBackward0>) tensor(10000.6865, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10000.6083984375
tensor(10000.6865, grad_fn=<NegBackward0>) tensor(10000.6084, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10000.5322265625
tensor(10000.6084, grad_fn=<NegBackward0>) tensor(10000.5322, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10000.46875
tensor(10000.5322, grad_fn=<NegBackward0>) tensor(10000.4688, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10000.419921875
tensor(10000.4688, grad_fn=<NegBackward0>) tensor(10000.4199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10000.388671875
tensor(10000.4199, grad_fn=<NegBackward0>) tensor(10000.3887, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10000.3681640625
tensor(10000.3887, grad_fn=<NegBackward0>) tensor(10000.3682, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10000.3486328125
tensor(10000.3682, grad_fn=<NegBackward0>) tensor(10000.3486, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10000.326171875
tensor(10000.3486, grad_fn=<NegBackward0>) tensor(10000.3262, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10000.28515625
tensor(10000.3262, grad_fn=<NegBackward0>) tensor(10000.2852, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10000.1953125
tensor(10000.2852, grad_fn=<NegBackward0>) tensor(10000.1953, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10000.03515625
tensor(10000.1953, grad_fn=<NegBackward0>) tensor(10000.0352, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9999.974609375
tensor(10000.0352, grad_fn=<NegBackward0>) tensor(9999.9746, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9999.966796875
tensor(9999.9746, grad_fn=<NegBackward0>) tensor(9999.9668, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9999.962890625
tensor(9999.9668, grad_fn=<NegBackward0>) tensor(9999.9629, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9999.9609375
tensor(9999.9629, grad_fn=<NegBackward0>) tensor(9999.9609, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9999.958984375
tensor(9999.9609, grad_fn=<NegBackward0>) tensor(9999.9590, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9999.958984375
tensor(9999.9590, grad_fn=<NegBackward0>) tensor(9999.9590, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9999.95703125
tensor(9999.9590, grad_fn=<NegBackward0>) tensor(9999.9570, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9999.9580078125
tensor(9999.9570, grad_fn=<NegBackward0>) tensor(9999.9580, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9999.95703125
tensor(9999.9570, grad_fn=<NegBackward0>) tensor(9999.9570, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9999.9560546875
tensor(9999.9570, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9999.95703125
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9570, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9999.9560546875
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9999.9560546875
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9999.9560546875
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9999.9560546875
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9999.9560546875
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9999.955078125
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9999.955078125
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9999.955078125
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9999.955078125
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9999.9560546875
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9999.9541015625
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9541, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9999.9560546875
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9999.95703125
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9570, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9999.9560546875
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9999.955078125
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -9999.9560546875
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.8140, 0.1860],
        [0.6453, 0.3547]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9654, 0.0346], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1301, 0.2485],
         [0.5101, 0.1843]],

        [[0.5001, 0.1508],
         [0.5893, 0.6985]],

        [[0.6575, 0.1605],
         [0.5380, 0.6143]],

        [[0.6390, 0.1543],
         [0.6577, 0.7083]],

        [[0.5978, 0.1420],
         [0.6782, 0.7247]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.020424516829035635
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0033385575919811067
Average Adjusted Rand Index: 0.003418460637907747
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22820.76953125
inf tensor(22820.7695, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10008.275390625
tensor(22820.7695, grad_fn=<NegBackward0>) tensor(10008.2754, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10003.8671875
tensor(10008.2754, grad_fn=<NegBackward0>) tensor(10003.8672, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10002.7822265625
tensor(10003.8672, grad_fn=<NegBackward0>) tensor(10002.7822, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10002.2236328125
tensor(10002.7822, grad_fn=<NegBackward0>) tensor(10002.2236, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10001.9140625
tensor(10002.2236, grad_fn=<NegBackward0>) tensor(10001.9141, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10001.7275390625
tensor(10001.9141, grad_fn=<NegBackward0>) tensor(10001.7275, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10001.591796875
tensor(10001.7275, grad_fn=<NegBackward0>) tensor(10001.5918, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10001.4697265625
tensor(10001.5918, grad_fn=<NegBackward0>) tensor(10001.4697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10001.3623046875
tensor(10001.4697, grad_fn=<NegBackward0>) tensor(10001.3623, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10001.2802734375
tensor(10001.3623, grad_fn=<NegBackward0>) tensor(10001.2803, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10001.22265625
tensor(10001.2803, grad_fn=<NegBackward0>) tensor(10001.2227, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10001.1845703125
tensor(10001.2227, grad_fn=<NegBackward0>) tensor(10001.1846, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10001.15625
tensor(10001.1846, grad_fn=<NegBackward0>) tensor(10001.1562, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10001.1337890625
tensor(10001.1562, grad_fn=<NegBackward0>) tensor(10001.1338, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10001.1142578125
tensor(10001.1338, grad_fn=<NegBackward0>) tensor(10001.1143, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10001.09765625
tensor(10001.1143, grad_fn=<NegBackward0>) tensor(10001.0977, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10001.078125
tensor(10001.0977, grad_fn=<NegBackward0>) tensor(10001.0781, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10001.0595703125
tensor(10001.0781, grad_fn=<NegBackward0>) tensor(10001.0596, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10001.0390625
tensor(10001.0596, grad_fn=<NegBackward0>) tensor(10001.0391, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10001.017578125
tensor(10001.0391, grad_fn=<NegBackward0>) tensor(10001.0176, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10000.9921875
tensor(10001.0176, grad_fn=<NegBackward0>) tensor(10000.9922, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10000.9599609375
tensor(10000.9922, grad_fn=<NegBackward0>) tensor(10000.9600, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10000.9228515625
tensor(10000.9600, grad_fn=<NegBackward0>) tensor(10000.9229, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10000.876953125
tensor(10000.9229, grad_fn=<NegBackward0>) tensor(10000.8770, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10000.8203125
tensor(10000.8770, grad_fn=<NegBackward0>) tensor(10000.8203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10000.75390625
tensor(10000.8203, grad_fn=<NegBackward0>) tensor(10000.7539, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10000.693359375
tensor(10000.7539, grad_fn=<NegBackward0>) tensor(10000.6934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10000.6533203125
tensor(10000.6934, grad_fn=<NegBackward0>) tensor(10000.6533, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10000.6298828125
tensor(10000.6533, grad_fn=<NegBackward0>) tensor(10000.6299, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10000.6162109375
tensor(10000.6299, grad_fn=<NegBackward0>) tensor(10000.6162, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10000.611328125
tensor(10000.6162, grad_fn=<NegBackward0>) tensor(10000.6113, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10000.6064453125
tensor(10000.6113, grad_fn=<NegBackward0>) tensor(10000.6064, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10000.603515625
tensor(10000.6064, grad_fn=<NegBackward0>) tensor(10000.6035, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10000.6015625
tensor(10000.6035, grad_fn=<NegBackward0>) tensor(10000.6016, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10000.6015625
tensor(10000.6016, grad_fn=<NegBackward0>) tensor(10000.6016, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10000.5986328125
tensor(10000.6016, grad_fn=<NegBackward0>) tensor(10000.5986, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10000.59765625
tensor(10000.5986, grad_fn=<NegBackward0>) tensor(10000.5977, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10000.5966796875
tensor(10000.5977, grad_fn=<NegBackward0>) tensor(10000.5967, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10000.5947265625
tensor(10000.5967, grad_fn=<NegBackward0>) tensor(10000.5947, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10000.5947265625
tensor(10000.5947, grad_fn=<NegBackward0>) tensor(10000.5947, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10000.59375
tensor(10000.5947, grad_fn=<NegBackward0>) tensor(10000.5938, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10000.59375
tensor(10000.5938, grad_fn=<NegBackward0>) tensor(10000.5938, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10000.59375
tensor(10000.5938, grad_fn=<NegBackward0>) tensor(10000.5938, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10000.591796875
tensor(10000.5938, grad_fn=<NegBackward0>) tensor(10000.5918, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10000.5908203125
tensor(10000.5918, grad_fn=<NegBackward0>) tensor(10000.5908, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10000.591796875
tensor(10000.5908, grad_fn=<NegBackward0>) tensor(10000.5918, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10000.5908203125
tensor(10000.5908, grad_fn=<NegBackward0>) tensor(10000.5908, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10000.5908203125
tensor(10000.5908, grad_fn=<NegBackward0>) tensor(10000.5908, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10000.58984375
tensor(10000.5908, grad_fn=<NegBackward0>) tensor(10000.5898, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10000.587890625
tensor(10000.5898, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10000.587890625
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10000.5908203125
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5908, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10000.587890625
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10000.5888671875
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5889, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10000.587890625
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10000.587890625
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10000.5849609375
tensor(10000.5879, grad_fn=<NegBackward0>) tensor(10000.5850, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10000.587890625
tensor(10000.5850, grad_fn=<NegBackward0>) tensor(10000.5879, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10000.5859375
tensor(10000.5850, grad_fn=<NegBackward0>) tensor(10000.5859, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10000.5859375
tensor(10000.5850, grad_fn=<NegBackward0>) tensor(10000.5859, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10000.5859375
tensor(10000.5850, grad_fn=<NegBackward0>) tensor(10000.5859, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10000.5859375
tensor(10000.5850, grad_fn=<NegBackward0>) tensor(10000.5859, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.0017, 0.9983],
        [0.0308, 0.9692]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0298, 0.9702], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2697, 0.2559],
         [0.6013, 0.1371]],

        [[0.7011, 0.1711],
         [0.6545, 0.6897]],

        [[0.5086, 0.1858],
         [0.5279, 0.5551]],

        [[0.6486, 0.1854],
         [0.7275, 0.5794]],

        [[0.5816, 0.0754],
         [0.5357, 0.6118]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 6.489882528926777e-05
Average Adjusted Rand Index: -0.00029351952271763465
[0.0033385575919811067, 6.489882528926777e-05] [0.003418460637907747, -0.00029351952271763465] [9999.9560546875, 10000.5859375]
-------------------------------------
This iteration is 28
True Objective function: Loss = -10021.148046299195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23071.0
inf tensor(23071., grad_fn=<NegBackward0>)
Iteration 100: Loss = -9935.8359375
tensor(23071., grad_fn=<NegBackward0>) tensor(9935.8359, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9932.2158203125
tensor(9935.8359, grad_fn=<NegBackward0>) tensor(9932.2158, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9930.265625
tensor(9932.2158, grad_fn=<NegBackward0>) tensor(9930.2656, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9924.80859375
tensor(9930.2656, grad_fn=<NegBackward0>) tensor(9924.8086, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9922.7060546875
tensor(9924.8086, grad_fn=<NegBackward0>) tensor(9922.7061, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9921.9921875
tensor(9922.7061, grad_fn=<NegBackward0>) tensor(9921.9922, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9921.0087890625
tensor(9921.9922, grad_fn=<NegBackward0>) tensor(9921.0088, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9920.1435546875
tensor(9921.0088, grad_fn=<NegBackward0>) tensor(9920.1436, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9919.5234375
tensor(9920.1436, grad_fn=<NegBackward0>) tensor(9919.5234, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9919.1669921875
tensor(9919.5234, grad_fn=<NegBackward0>) tensor(9919.1670, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9918.9384765625
tensor(9919.1670, grad_fn=<NegBackward0>) tensor(9918.9385, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9918.8330078125
tensor(9918.9385, grad_fn=<NegBackward0>) tensor(9918.8330, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9918.70703125
tensor(9918.8330, grad_fn=<NegBackward0>) tensor(9918.7070, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9918.5751953125
tensor(9918.7070, grad_fn=<NegBackward0>) tensor(9918.5752, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9918.4443359375
tensor(9918.5752, grad_fn=<NegBackward0>) tensor(9918.4443, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9918.306640625
tensor(9918.4443, grad_fn=<NegBackward0>) tensor(9918.3066, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9918.21484375
tensor(9918.3066, grad_fn=<NegBackward0>) tensor(9918.2148, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9918.0654296875
tensor(9918.2148, grad_fn=<NegBackward0>) tensor(9918.0654, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9917.9443359375
tensor(9918.0654, grad_fn=<NegBackward0>) tensor(9917.9443, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9917.4765625
tensor(9917.9443, grad_fn=<NegBackward0>) tensor(9917.4766, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9913.869140625
tensor(9917.4766, grad_fn=<NegBackward0>) tensor(9913.8691, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9912.736328125
tensor(9913.8691, grad_fn=<NegBackward0>) tensor(9912.7363, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9912.3291015625
tensor(9912.7363, grad_fn=<NegBackward0>) tensor(9912.3291, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9912.08203125
tensor(9912.3291, grad_fn=<NegBackward0>) tensor(9912.0820, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9911.9111328125
tensor(9912.0820, grad_fn=<NegBackward0>) tensor(9911.9111, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9911.7548828125
tensor(9911.9111, grad_fn=<NegBackward0>) tensor(9911.7549, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9911.7490234375
tensor(9911.7549, grad_fn=<NegBackward0>) tensor(9911.7490, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9911.7451171875
tensor(9911.7490, grad_fn=<NegBackward0>) tensor(9911.7451, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9911.7431640625
tensor(9911.7451, grad_fn=<NegBackward0>) tensor(9911.7432, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9911.7392578125
tensor(9911.7432, grad_fn=<NegBackward0>) tensor(9911.7393, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9911.734375
tensor(9911.7393, grad_fn=<NegBackward0>) tensor(9911.7344, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9911.728515625
tensor(9911.7344, grad_fn=<NegBackward0>) tensor(9911.7285, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9911.72265625
tensor(9911.7285, grad_fn=<NegBackward0>) tensor(9911.7227, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9911.69140625
tensor(9911.7227, grad_fn=<NegBackward0>) tensor(9911.6914, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9911.685546875
tensor(9911.6914, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9911.6845703125
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9911.685546875
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9911.6845703125
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9911.68359375
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9911.68359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -9911.68359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9911.68359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9911.68359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9911.6884765625
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9911.68359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6836, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9911.685546875
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9911.6943359375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6943, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9911.685546875
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9911.6865234375
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9911.6845703125
tensor(9911.6836, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.7522, 0.2478],
        [0.1347, 0.8653]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9952, 0.0048], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1252, 0.0799],
         [0.7168, 0.2098]],

        [[0.7180, 0.1575],
         [0.5535, 0.5955]],

        [[0.7168, 0.1361],
         [0.5522, 0.7209]],

        [[0.6716, 0.1252],
         [0.6798, 0.5102]],

        [[0.5721, 0.0898],
         [0.6645, 0.7179]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012973723718715429
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18481577238362476
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 83
Adjusted Rand Index: 0.42992355884998895
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.18455376143112934
Average Adjusted Rand Index: 0.2799668534147083
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21720.423828125
inf tensor(21720.4238, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9937.1142578125
tensor(21720.4238, grad_fn=<NegBackward0>) tensor(9937.1143, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9933.822265625
tensor(9937.1143, grad_fn=<NegBackward0>) tensor(9933.8223, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9926.0517578125
tensor(9933.8223, grad_fn=<NegBackward0>) tensor(9926.0518, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9923.1923828125
tensor(9926.0518, grad_fn=<NegBackward0>) tensor(9923.1924, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9922.2353515625
tensor(9923.1924, grad_fn=<NegBackward0>) tensor(9922.2354, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9921.4736328125
tensor(9922.2354, grad_fn=<NegBackward0>) tensor(9921.4736, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9920.6015625
tensor(9921.4736, grad_fn=<NegBackward0>) tensor(9920.6016, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9919.693359375
tensor(9920.6016, grad_fn=<NegBackward0>) tensor(9919.6934, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9919.158203125
tensor(9919.6934, grad_fn=<NegBackward0>) tensor(9919.1582, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9918.828125
tensor(9919.1582, grad_fn=<NegBackward0>) tensor(9918.8281, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9918.65234375
tensor(9918.8281, grad_fn=<NegBackward0>) tensor(9918.6523, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9918.5166015625
tensor(9918.6523, grad_fn=<NegBackward0>) tensor(9918.5166, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9918.26171875
tensor(9918.5166, grad_fn=<NegBackward0>) tensor(9918.2617, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9918.15625
tensor(9918.2617, grad_fn=<NegBackward0>) tensor(9918.1562, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9918.0703125
tensor(9918.1562, grad_fn=<NegBackward0>) tensor(9918.0703, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9917.9794921875
tensor(9918.0703, grad_fn=<NegBackward0>) tensor(9917.9795, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9917.861328125
tensor(9917.9795, grad_fn=<NegBackward0>) tensor(9917.8613, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9915.734375
tensor(9917.8613, grad_fn=<NegBackward0>) tensor(9915.7344, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9913.3818359375
tensor(9915.7344, grad_fn=<NegBackward0>) tensor(9913.3818, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9912.4296875
tensor(9913.3818, grad_fn=<NegBackward0>) tensor(9912.4297, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9912.04296875
tensor(9912.4297, grad_fn=<NegBackward0>) tensor(9912.0430, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9911.99609375
tensor(9912.0430, grad_fn=<NegBackward0>) tensor(9911.9961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9911.9873046875
tensor(9911.9961, grad_fn=<NegBackward0>) tensor(9911.9873, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9911.9775390625
tensor(9911.9873, grad_fn=<NegBackward0>) tensor(9911.9775, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9911.9697265625
tensor(9911.9775, grad_fn=<NegBackward0>) tensor(9911.9697, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9911.95703125
tensor(9911.9697, grad_fn=<NegBackward0>) tensor(9911.9570, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9911.74609375
tensor(9911.9570, grad_fn=<NegBackward0>) tensor(9911.7461, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9911.73828125
tensor(9911.7461, grad_fn=<NegBackward0>) tensor(9911.7383, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9911.7353515625
tensor(9911.7383, grad_fn=<NegBackward0>) tensor(9911.7354, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9911.73046875
tensor(9911.7354, grad_fn=<NegBackward0>) tensor(9911.7305, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9911.69921875
tensor(9911.7305, grad_fn=<NegBackward0>) tensor(9911.6992, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9911.6953125
tensor(9911.6992, grad_fn=<NegBackward0>) tensor(9911.6953, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9911.69140625
tensor(9911.6953, grad_fn=<NegBackward0>) tensor(9911.6914, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9911.689453125
tensor(9911.6914, grad_fn=<NegBackward0>) tensor(9911.6895, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9911.6904296875
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6904, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9911.6904296875
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6904, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9911.689453125
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6895, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9911.6904296875
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6904, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9911.689453125
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6895, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9911.6884765625
tensor(9911.6895, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9911.689453125
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6895, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9911.6884765625
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9911.6884765625
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9911.6953125
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6953, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9911.6884765625
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9911.6884765625
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6885, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9911.6875
tensor(9911.6885, grad_fn=<NegBackward0>) tensor(9911.6875, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9911.685546875
tensor(9911.6875, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9911.6865234375
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9911.6865234375
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9911.685546875
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9911.6865234375
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9911.6845703125
tensor(9911.6855, grad_fn=<NegBackward0>) tensor(9911.6846, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9911.685546875
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9911.685546875
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9911.685546875
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6855, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9911.6865234375
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -9911.6865234375
tensor(9911.6846, grad_fn=<NegBackward0>) tensor(9911.6865, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.7522, 0.2478],
        [0.1347, 0.8653]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9952, 0.0048], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1252, 0.0799],
         [0.6213, 0.2098]],

        [[0.7265, 0.1575],
         [0.5010, 0.6213]],

        [[0.6853, 0.1361],
         [0.6265, 0.6966]],

        [[0.5740, 0.1252],
         [0.5732, 0.6316]],

        [[0.5992, 0.0898],
         [0.6830, 0.6287]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012973723718715429
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18481577238362476
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 83
Adjusted Rand Index: 0.42992355884998895
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.18455376143112934
Average Adjusted Rand Index: 0.2799668534147083
[0.18455376143112934, 0.18455376143112934] [0.2799668534147083, 0.2799668534147083] [9911.6845703125, 9911.6865234375]
-------------------------------------
This iteration is 29
True Objective function: Loss = -9978.63692274032
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23581.0
inf tensor(23581., grad_fn=<NegBackward0>)
Iteration 100: Loss = -9879.14453125
tensor(23581., grad_fn=<NegBackward0>) tensor(9879.1445, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9878.033203125
tensor(9879.1445, grad_fn=<NegBackward0>) tensor(9878.0332, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9877.546875
tensor(9878.0332, grad_fn=<NegBackward0>) tensor(9877.5469, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9877.2880859375
tensor(9877.5469, grad_fn=<NegBackward0>) tensor(9877.2881, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9877.1103515625
tensor(9877.2881, grad_fn=<NegBackward0>) tensor(9877.1104, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9877.0107421875
tensor(9877.1104, grad_fn=<NegBackward0>) tensor(9877.0107, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9876.94921875
tensor(9877.0107, grad_fn=<NegBackward0>) tensor(9876.9492, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9876.904296875
tensor(9876.9492, grad_fn=<NegBackward0>) tensor(9876.9043, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9876.8642578125
tensor(9876.9043, grad_fn=<NegBackward0>) tensor(9876.8643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9876.8251953125
tensor(9876.8643, grad_fn=<NegBackward0>) tensor(9876.8252, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9876.78125
tensor(9876.8252, grad_fn=<NegBackward0>) tensor(9876.7812, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9876.728515625
tensor(9876.7812, grad_fn=<NegBackward0>) tensor(9876.7285, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9876.6591796875
tensor(9876.7285, grad_fn=<NegBackward0>) tensor(9876.6592, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9876.55859375
tensor(9876.6592, grad_fn=<NegBackward0>) tensor(9876.5586, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9876.3916015625
tensor(9876.5586, grad_fn=<NegBackward0>) tensor(9876.3916, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9876.0947265625
tensor(9876.3916, grad_fn=<NegBackward0>) tensor(9876.0947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9875.6376953125
tensor(9876.0947, grad_fn=<NegBackward0>) tensor(9875.6377, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9875.3046875
tensor(9875.6377, grad_fn=<NegBackward0>) tensor(9875.3047, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9875.154296875
tensor(9875.3047, grad_fn=<NegBackward0>) tensor(9875.1543, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9875.064453125
tensor(9875.1543, grad_fn=<NegBackward0>) tensor(9875.0645, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9875.001953125
tensor(9875.0645, grad_fn=<NegBackward0>) tensor(9875.0020, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9874.962890625
tensor(9875.0020, grad_fn=<NegBackward0>) tensor(9874.9629, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9874.94140625
tensor(9874.9629, grad_fn=<NegBackward0>) tensor(9874.9414, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9874.9248046875
tensor(9874.9414, grad_fn=<NegBackward0>) tensor(9874.9248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9874.912109375
tensor(9874.9248, grad_fn=<NegBackward0>) tensor(9874.9121, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9874.9013671875
tensor(9874.9121, grad_fn=<NegBackward0>) tensor(9874.9014, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9874.8916015625
tensor(9874.9014, grad_fn=<NegBackward0>) tensor(9874.8916, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9874.884765625
tensor(9874.8916, grad_fn=<NegBackward0>) tensor(9874.8848, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9874.87890625
tensor(9874.8848, grad_fn=<NegBackward0>) tensor(9874.8789, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9874.8740234375
tensor(9874.8789, grad_fn=<NegBackward0>) tensor(9874.8740, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9874.8681640625
tensor(9874.8740, grad_fn=<NegBackward0>) tensor(9874.8682, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9874.8642578125
tensor(9874.8682, grad_fn=<NegBackward0>) tensor(9874.8643, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9874.859375
tensor(9874.8643, grad_fn=<NegBackward0>) tensor(9874.8594, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9874.8564453125
tensor(9874.8594, grad_fn=<NegBackward0>) tensor(9874.8564, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9874.853515625
tensor(9874.8564, grad_fn=<NegBackward0>) tensor(9874.8535, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9874.8515625
tensor(9874.8535, grad_fn=<NegBackward0>) tensor(9874.8516, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9874.8486328125
tensor(9874.8516, grad_fn=<NegBackward0>) tensor(9874.8486, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9874.84765625
tensor(9874.8486, grad_fn=<NegBackward0>) tensor(9874.8477, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9874.84375
tensor(9874.8477, grad_fn=<NegBackward0>) tensor(9874.8438, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9874.8427734375
tensor(9874.8438, grad_fn=<NegBackward0>) tensor(9874.8428, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9874.841796875
tensor(9874.8428, grad_fn=<NegBackward0>) tensor(9874.8418, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9874.83984375
tensor(9874.8418, grad_fn=<NegBackward0>) tensor(9874.8398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9874.837890625
tensor(9874.8398, grad_fn=<NegBackward0>) tensor(9874.8379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9874.8359375
tensor(9874.8379, grad_fn=<NegBackward0>) tensor(9874.8359, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9874.8359375
tensor(9874.8359, grad_fn=<NegBackward0>) tensor(9874.8359, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9874.8349609375
tensor(9874.8359, grad_fn=<NegBackward0>) tensor(9874.8350, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9874.8330078125
tensor(9874.8350, grad_fn=<NegBackward0>) tensor(9874.8330, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9874.8330078125
tensor(9874.8330, grad_fn=<NegBackward0>) tensor(9874.8330, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9874.8330078125
tensor(9874.8330, grad_fn=<NegBackward0>) tensor(9874.8330, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9874.8310546875
tensor(9874.8330, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9874.8310546875
tensor(9874.8311, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9874.830078125
tensor(9874.8311, grad_fn=<NegBackward0>) tensor(9874.8301, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9874.830078125
tensor(9874.8301, grad_fn=<NegBackward0>) tensor(9874.8301, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9874.828125
tensor(9874.8301, grad_fn=<NegBackward0>) tensor(9874.8281, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9874.8271484375
tensor(9874.8281, grad_fn=<NegBackward0>) tensor(9874.8271, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9874.8271484375
tensor(9874.8271, grad_fn=<NegBackward0>) tensor(9874.8271, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9874.8271484375
tensor(9874.8271, grad_fn=<NegBackward0>) tensor(9874.8271, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9874.826171875
tensor(9874.8271, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9874.8251953125
tensor(9874.8262, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9874.82421875
tensor(9874.8252, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9874.826171875
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9874.8251953125
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9874.82421875
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9874.8251953125
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9874.8232421875
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9874.8310546875
tensor(9874.8232, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9874.822265625
tensor(9874.8232, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9874.82421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9874.8232421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9874.8232421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9874.8212890625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8213, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9874.82421875
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9874.8212890625
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8213, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9874.8251953125
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9874.822265625
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9874.8232421875
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9874.8212890625
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8213, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9874.8203125
tensor(9874.8213, grad_fn=<NegBackward0>) tensor(9874.8203, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9874.8193359375
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8193, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9874.822265625
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9874.8203125
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8203, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9874.8193359375
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8193, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9874.841796875
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8418, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9874.8203125
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8203, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9874.8310546875
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -9874.8203125
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8203, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -9874.8310546875
tensor(9874.8193, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[5.4660e-01, 4.5340e-01],
        [2.4317e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1518, 0.8482], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.1573],
         [0.6847, 0.1331]],

        [[0.6154, 0.1524],
         [0.5070, 0.5500]],

        [[0.7123, 0.1805],
         [0.6109, 0.6797]],

        [[0.6060, 0.2353],
         [0.7283, 0.6447]],

        [[0.7206, 0.1303],
         [0.5148, 0.6537]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: 0.001296339264509808
Average Adjusted Rand Index: -0.002952115601253938
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23334.3984375
inf tensor(23334.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9879.6845703125
tensor(23334.3984, grad_fn=<NegBackward0>) tensor(9879.6846, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9878.546875
tensor(9879.6846, grad_fn=<NegBackward0>) tensor(9878.5469, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9878.09765625
tensor(9878.5469, grad_fn=<NegBackward0>) tensor(9878.0977, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9877.7294921875
tensor(9878.0977, grad_fn=<NegBackward0>) tensor(9877.7295, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9877.46484375
tensor(9877.7295, grad_fn=<NegBackward0>) tensor(9877.4648, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9877.3212890625
tensor(9877.4648, grad_fn=<NegBackward0>) tensor(9877.3213, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9877.2119140625
tensor(9877.3213, grad_fn=<NegBackward0>) tensor(9877.2119, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9877.1142578125
tensor(9877.2119, grad_fn=<NegBackward0>) tensor(9877.1143, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9877.03125
tensor(9877.1143, grad_fn=<NegBackward0>) tensor(9877.0312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9876.9619140625
tensor(9877.0312, grad_fn=<NegBackward0>) tensor(9876.9619, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9876.896484375
tensor(9876.9619, grad_fn=<NegBackward0>) tensor(9876.8965, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9876.826171875
tensor(9876.8965, grad_fn=<NegBackward0>) tensor(9876.8262, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9876.7353515625
tensor(9876.8262, grad_fn=<NegBackward0>) tensor(9876.7354, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9876.6123046875
tensor(9876.7354, grad_fn=<NegBackward0>) tensor(9876.6123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9876.4130859375
tensor(9876.6123, grad_fn=<NegBackward0>) tensor(9876.4131, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9876.1005859375
tensor(9876.4131, grad_fn=<NegBackward0>) tensor(9876.1006, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9875.8173828125
tensor(9876.1006, grad_fn=<NegBackward0>) tensor(9875.8174, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9875.6162109375
tensor(9875.8174, grad_fn=<NegBackward0>) tensor(9875.6162, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9875.458984375
tensor(9875.6162, grad_fn=<NegBackward0>) tensor(9875.4590, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9875.3525390625
tensor(9875.4590, grad_fn=<NegBackward0>) tensor(9875.3525, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9875.2724609375
tensor(9875.3525, grad_fn=<NegBackward0>) tensor(9875.2725, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9875.2001953125
tensor(9875.2725, grad_fn=<NegBackward0>) tensor(9875.2002, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9875.1416015625
tensor(9875.2002, grad_fn=<NegBackward0>) tensor(9875.1416, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9875.0771484375
tensor(9875.1416, grad_fn=<NegBackward0>) tensor(9875.0771, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9875.0205078125
tensor(9875.0771, grad_fn=<NegBackward0>) tensor(9875.0205, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9874.98046875
tensor(9875.0205, grad_fn=<NegBackward0>) tensor(9874.9805, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9874.94921875
tensor(9874.9805, grad_fn=<NegBackward0>) tensor(9874.9492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9874.923828125
tensor(9874.9492, grad_fn=<NegBackward0>) tensor(9874.9238, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9874.908203125
tensor(9874.9238, grad_fn=<NegBackward0>) tensor(9874.9082, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9874.900390625
tensor(9874.9082, grad_fn=<NegBackward0>) tensor(9874.9004, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9874.8916015625
tensor(9874.9004, grad_fn=<NegBackward0>) tensor(9874.8916, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9874.888671875
tensor(9874.8916, grad_fn=<NegBackward0>) tensor(9874.8887, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9874.8818359375
tensor(9874.8887, grad_fn=<NegBackward0>) tensor(9874.8818, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9874.8779296875
tensor(9874.8818, grad_fn=<NegBackward0>) tensor(9874.8779, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9874.8720703125
tensor(9874.8779, grad_fn=<NegBackward0>) tensor(9874.8721, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9874.8681640625
tensor(9874.8721, grad_fn=<NegBackward0>) tensor(9874.8682, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9874.865234375
tensor(9874.8682, grad_fn=<NegBackward0>) tensor(9874.8652, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9874.8623046875
tensor(9874.8652, grad_fn=<NegBackward0>) tensor(9874.8623, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9874.859375
tensor(9874.8623, grad_fn=<NegBackward0>) tensor(9874.8594, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9874.8564453125
tensor(9874.8594, grad_fn=<NegBackward0>) tensor(9874.8564, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9874.8525390625
tensor(9874.8564, grad_fn=<NegBackward0>) tensor(9874.8525, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9874.8505859375
tensor(9874.8525, grad_fn=<NegBackward0>) tensor(9874.8506, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9874.8486328125
tensor(9874.8506, grad_fn=<NegBackward0>) tensor(9874.8486, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9874.84765625
tensor(9874.8486, grad_fn=<NegBackward0>) tensor(9874.8477, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9874.8447265625
tensor(9874.8477, grad_fn=<NegBackward0>) tensor(9874.8447, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9874.8427734375
tensor(9874.8447, grad_fn=<NegBackward0>) tensor(9874.8428, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9874.841796875
tensor(9874.8428, grad_fn=<NegBackward0>) tensor(9874.8418, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9874.83984375
tensor(9874.8418, grad_fn=<NegBackward0>) tensor(9874.8398, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9874.83984375
tensor(9874.8398, grad_fn=<NegBackward0>) tensor(9874.8398, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9874.8369140625
tensor(9874.8398, grad_fn=<NegBackward0>) tensor(9874.8369, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9874.83984375
tensor(9874.8369, grad_fn=<NegBackward0>) tensor(9874.8398, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9874.8349609375
tensor(9874.8369, grad_fn=<NegBackward0>) tensor(9874.8350, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9874.833984375
tensor(9874.8350, grad_fn=<NegBackward0>) tensor(9874.8340, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9874.8359375
tensor(9874.8340, grad_fn=<NegBackward0>) tensor(9874.8359, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9874.8330078125
tensor(9874.8340, grad_fn=<NegBackward0>) tensor(9874.8330, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9874.83203125
tensor(9874.8330, grad_fn=<NegBackward0>) tensor(9874.8320, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9874.8310546875
tensor(9874.8320, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9874.8310546875
tensor(9874.8311, grad_fn=<NegBackward0>) tensor(9874.8311, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9874.830078125
tensor(9874.8311, grad_fn=<NegBackward0>) tensor(9874.8301, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9874.8291015625
tensor(9874.8301, grad_fn=<NegBackward0>) tensor(9874.8291, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9874.828125
tensor(9874.8291, grad_fn=<NegBackward0>) tensor(9874.8281, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9874.8291015625
tensor(9874.8281, grad_fn=<NegBackward0>) tensor(9874.8291, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9874.828125
tensor(9874.8281, grad_fn=<NegBackward0>) tensor(9874.8281, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9874.826171875
tensor(9874.8281, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9874.8271484375
tensor(9874.8262, grad_fn=<NegBackward0>) tensor(9874.8271, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9874.8251953125
tensor(9874.8262, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9874.826171875
tensor(9874.8252, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9874.826171875
tensor(9874.8252, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9874.82421875
tensor(9874.8252, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9874.8251953125
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8252, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9874.826171875
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8262, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9874.8291015625
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8291, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9874.8232421875
tensor(9874.8242, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9874.822265625
tensor(9874.8232, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9874.82421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8242, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9874.8232421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9874.8232421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9874.8232421875
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9874.822265625
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9874.8203125
tensor(9874.8223, grad_fn=<NegBackward0>) tensor(9874.8203, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9874.8212890625
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8213, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9874.8291015625
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8291, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9874.8212890625
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8213, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -9874.8232421875
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8232, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -9874.822265625
tensor(9874.8203, grad_fn=<NegBackward0>) tensor(9874.8223, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[5.4798e-01, 4.5202e-01],
        [4.0997e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1527, 0.8473], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.1573],
         [0.6701, 0.1327]],

        [[0.5543, 0.1525],
         [0.5452, 0.7148]],

        [[0.6292, 0.1804],
         [0.6376, 0.5683]],

        [[0.6865, 0.2352],
         [0.6211, 0.6423]],

        [[0.6790, 0.1303],
         [0.5630, 0.6429]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: 0.001296339264509808
Average Adjusted Rand Index: -0.002952115601253938
[0.001296339264509808, 0.001296339264509808] [-0.002952115601253938, -0.002952115601253938] [9874.8310546875, 9874.822265625]
-------------------------------------
This iteration is 30
True Objective function: Loss = -9926.852373623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21534.849609375
inf tensor(21534.8496, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9808.6279296875
tensor(21534.8496, grad_fn=<NegBackward0>) tensor(9808.6279, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9806.3701171875
tensor(9808.6279, grad_fn=<NegBackward0>) tensor(9806.3701, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9805.8544921875
tensor(9806.3701, grad_fn=<NegBackward0>) tensor(9805.8545, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9805.5078125
tensor(9805.8545, grad_fn=<NegBackward0>) tensor(9805.5078, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9804.861328125
tensor(9805.5078, grad_fn=<NegBackward0>) tensor(9804.8613, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9804.0302734375
tensor(9804.8613, grad_fn=<NegBackward0>) tensor(9804.0303, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9803.5556640625
tensor(9804.0303, grad_fn=<NegBackward0>) tensor(9803.5557, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9803.3916015625
tensor(9803.5557, grad_fn=<NegBackward0>) tensor(9803.3916, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9803.310546875
tensor(9803.3916, grad_fn=<NegBackward0>) tensor(9803.3105, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9803.259765625
tensor(9803.3105, grad_fn=<NegBackward0>) tensor(9803.2598, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9803.2236328125
tensor(9803.2598, grad_fn=<NegBackward0>) tensor(9803.2236, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9803.1923828125
tensor(9803.2236, grad_fn=<NegBackward0>) tensor(9803.1924, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9803.1787109375
tensor(9803.1924, grad_fn=<NegBackward0>) tensor(9803.1787, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9803.1708984375
tensor(9803.1787, grad_fn=<NegBackward0>) tensor(9803.1709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9803.16796875
tensor(9803.1709, grad_fn=<NegBackward0>) tensor(9803.1680, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9803.1650390625
tensor(9803.1680, grad_fn=<NegBackward0>) tensor(9803.1650, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9803.162109375
tensor(9803.1650, grad_fn=<NegBackward0>) tensor(9803.1621, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9803.1611328125
tensor(9803.1621, grad_fn=<NegBackward0>) tensor(9803.1611, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9803.1591796875
tensor(9803.1611, grad_fn=<NegBackward0>) tensor(9803.1592, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9803.1591796875
tensor(9803.1592, grad_fn=<NegBackward0>) tensor(9803.1592, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9803.1572265625
tensor(9803.1592, grad_fn=<NegBackward0>) tensor(9803.1572, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9803.15625
tensor(9803.1572, grad_fn=<NegBackward0>) tensor(9803.1562, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9803.15625
tensor(9803.1562, grad_fn=<NegBackward0>) tensor(9803.1562, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9803.1552734375
tensor(9803.1562, grad_fn=<NegBackward0>) tensor(9803.1553, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9803.154296875
tensor(9803.1553, grad_fn=<NegBackward0>) tensor(9803.1543, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9803.1552734375
tensor(9803.1543, grad_fn=<NegBackward0>) tensor(9803.1553, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -9803.1552734375
tensor(9803.1543, grad_fn=<NegBackward0>) tensor(9803.1553, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -9803.1552734375
tensor(9803.1543, grad_fn=<NegBackward0>) tensor(9803.1553, grad_fn=<NegBackward0>)
3
Iteration 2900: Loss = -9803.1533203125
tensor(9803.1543, grad_fn=<NegBackward0>) tensor(9803.1533, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9803.154296875
tensor(9803.1533, grad_fn=<NegBackward0>) tensor(9803.1543, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9803.154296875
tensor(9803.1533, grad_fn=<NegBackward0>) tensor(9803.1543, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -9803.15234375
tensor(9803.1533, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9803.1533203125
tensor(9803.1523, grad_fn=<NegBackward0>) tensor(9803.1533, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9803.15234375
tensor(9803.1523, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9803.15234375
tensor(9803.1523, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9803.1513671875
tensor(9803.1523, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9803.1533203125
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1533, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9803.15234375
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9803.1513671875
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9803.15234375
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9803.15234375
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1523, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9803.1513671875
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9803.1513671875
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9803.1494140625
tensor(9803.1514, grad_fn=<NegBackward0>) tensor(9803.1494, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9803.150390625
tensor(9803.1494, grad_fn=<NegBackward0>) tensor(9803.1504, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9803.150390625
tensor(9803.1494, grad_fn=<NegBackward0>) tensor(9803.1504, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9803.150390625
tensor(9803.1494, grad_fn=<NegBackward0>) tensor(9803.1504, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -9803.1513671875
tensor(9803.1494, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -9803.1513671875
tensor(9803.1494, grad_fn=<NegBackward0>) tensor(9803.1514, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.9894, 0.0106],
        [0.6091, 0.3909]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9381, 0.0619], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1328, 0.2013],
         [0.6798, 0.2995]],

        [[0.6680, 0.1805],
         [0.5399, 0.6480]],

        [[0.6502, 0.0490],
         [0.6714, 0.6655]],

        [[0.7221, 0.1932],
         [0.7273, 0.5992]],

        [[0.7016, 0.1647],
         [0.6749, 0.5878]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00038189214017276575
Average Adjusted Rand Index: -0.0002726049879675555
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20739.990234375
inf tensor(20739.9902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9809.2177734375
tensor(20739.9902, grad_fn=<NegBackward0>) tensor(9809.2178, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9806.990234375
tensor(9809.2178, grad_fn=<NegBackward0>) tensor(9806.9902, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9806.4296875
tensor(9806.9902, grad_fn=<NegBackward0>) tensor(9806.4297, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9805.8466796875
tensor(9806.4297, grad_fn=<NegBackward0>) tensor(9805.8467, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9805.3671875
tensor(9805.8467, grad_fn=<NegBackward0>) tensor(9805.3672, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9804.94921875
tensor(9805.3672, grad_fn=<NegBackward0>) tensor(9804.9492, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9804.212890625
tensor(9804.9492, grad_fn=<NegBackward0>) tensor(9804.2129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9803.2451171875
tensor(9804.2129, grad_fn=<NegBackward0>) tensor(9803.2451, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9802.8359375
tensor(9803.2451, grad_fn=<NegBackward0>) tensor(9802.8359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9802.6337890625
tensor(9802.8359, grad_fn=<NegBackward0>) tensor(9802.6338, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9802.5078125
tensor(9802.6338, grad_fn=<NegBackward0>) tensor(9802.5078, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9802.4111328125
tensor(9802.5078, grad_fn=<NegBackward0>) tensor(9802.4111, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9802.2978515625
tensor(9802.4111, grad_fn=<NegBackward0>) tensor(9802.2979, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9802.2412109375
tensor(9802.2979, grad_fn=<NegBackward0>) tensor(9802.2412, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9802.1826171875
tensor(9802.2412, grad_fn=<NegBackward0>) tensor(9802.1826, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9801.923828125
tensor(9802.1826, grad_fn=<NegBackward0>) tensor(9801.9238, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9801.8828125
tensor(9801.9238, grad_fn=<NegBackward0>) tensor(9801.8828, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9801.8642578125
tensor(9801.8828, grad_fn=<NegBackward0>) tensor(9801.8643, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9801.8486328125
tensor(9801.8643, grad_fn=<NegBackward0>) tensor(9801.8486, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9801.8349609375
tensor(9801.8486, grad_fn=<NegBackward0>) tensor(9801.8350, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9801.8173828125
tensor(9801.8350, grad_fn=<NegBackward0>) tensor(9801.8174, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9801.779296875
tensor(9801.8174, grad_fn=<NegBackward0>) tensor(9801.7793, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9801.7626953125
tensor(9801.7793, grad_fn=<NegBackward0>) tensor(9801.7627, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9801.7568359375
tensor(9801.7627, grad_fn=<NegBackward0>) tensor(9801.7568, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9801.751953125
tensor(9801.7568, grad_fn=<NegBackward0>) tensor(9801.7520, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9801.748046875
tensor(9801.7520, grad_fn=<NegBackward0>) tensor(9801.7480, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9801.740234375
tensor(9801.7480, grad_fn=<NegBackward0>) tensor(9801.7402, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9801.7177734375
tensor(9801.7402, grad_fn=<NegBackward0>) tensor(9801.7178, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9801.65234375
tensor(9801.7178, grad_fn=<NegBackward0>) tensor(9801.6523, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9801.623046875
tensor(9801.6523, grad_fn=<NegBackward0>) tensor(9801.6230, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9801.611328125
tensor(9801.6230, grad_fn=<NegBackward0>) tensor(9801.6113, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9801.599609375
tensor(9801.6113, grad_fn=<NegBackward0>) tensor(9801.5996, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9801.5849609375
tensor(9801.5996, grad_fn=<NegBackward0>) tensor(9801.5850, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9801.5693359375
tensor(9801.5850, grad_fn=<NegBackward0>) tensor(9801.5693, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9801.5517578125
tensor(9801.5693, grad_fn=<NegBackward0>) tensor(9801.5518, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9801.513671875
tensor(9801.5518, grad_fn=<NegBackward0>) tensor(9801.5137, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9801.4921875
tensor(9801.5137, grad_fn=<NegBackward0>) tensor(9801.4922, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9801.4013671875
tensor(9801.4922, grad_fn=<NegBackward0>) tensor(9801.4014, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9800.8251953125
tensor(9801.4014, grad_fn=<NegBackward0>) tensor(9800.8252, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9800.5009765625
tensor(9800.8252, grad_fn=<NegBackward0>) tensor(9800.5010, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9800.1162109375
tensor(9800.5010, grad_fn=<NegBackward0>) tensor(9800.1162, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9799.2978515625
tensor(9800.1162, grad_fn=<NegBackward0>) tensor(9799.2979, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9799.08984375
tensor(9799.2979, grad_fn=<NegBackward0>) tensor(9799.0898, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9798.5830078125
tensor(9799.0898, grad_fn=<NegBackward0>) tensor(9798.5830, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9798.501953125
tensor(9798.5830, grad_fn=<NegBackward0>) tensor(9798.5020, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9798.4482421875
tensor(9798.5020, grad_fn=<NegBackward0>) tensor(9798.4482, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9798.4208984375
tensor(9798.4482, grad_fn=<NegBackward0>) tensor(9798.4209, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9798.1435546875
tensor(9798.4209, grad_fn=<NegBackward0>) tensor(9798.1436, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9798.134765625
tensor(9798.1436, grad_fn=<NegBackward0>) tensor(9798.1348, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9798.1259765625
tensor(9798.1348, grad_fn=<NegBackward0>) tensor(9798.1260, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9798.1201171875
tensor(9798.1260, grad_fn=<NegBackward0>) tensor(9798.1201, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9798.119140625
tensor(9798.1201, grad_fn=<NegBackward0>) tensor(9798.1191, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9798.1181640625
tensor(9798.1191, grad_fn=<NegBackward0>) tensor(9798.1182, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9798.1181640625
tensor(9798.1182, grad_fn=<NegBackward0>) tensor(9798.1182, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9798.1171875
tensor(9798.1182, grad_fn=<NegBackward0>) tensor(9798.1172, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9798.1162109375
tensor(9798.1172, grad_fn=<NegBackward0>) tensor(9798.1162, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9798.1181640625
tensor(9798.1162, grad_fn=<NegBackward0>) tensor(9798.1182, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9798.1123046875
tensor(9798.1162, grad_fn=<NegBackward0>) tensor(9798.1123, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9798.1123046875
tensor(9798.1123, grad_fn=<NegBackward0>) tensor(9798.1123, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9798.1103515625
tensor(9798.1123, grad_fn=<NegBackward0>) tensor(9798.1104, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9798.1083984375
tensor(9798.1104, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9798.1083984375
tensor(9798.1084, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9798.1083984375
tensor(9798.1084, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9798.107421875
tensor(9798.1084, grad_fn=<NegBackward0>) tensor(9798.1074, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9798.1083984375
tensor(9798.1074, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9798.109375
tensor(9798.1074, grad_fn=<NegBackward0>) tensor(9798.1094, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9798.1083984375
tensor(9798.1074, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -9798.1083984375
tensor(9798.1074, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -9798.1083984375
tensor(9798.1074, grad_fn=<NegBackward0>) tensor(9798.1084, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.8422, 0.1578],
        [0.0572, 0.9428]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0721, 0.9279], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3223, 0.2009],
         [0.5249, 0.1281]],

        [[0.6041, 0.1426],
         [0.7042, 0.7303]],

        [[0.5508, 0.1149],
         [0.6370, 0.5731]],

        [[0.5237, 0.1335],
         [0.5899, 0.7043]],

        [[0.5413, 0.1402],
         [0.6531, 0.6597]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.03595357580886173
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.06284858553158974
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.08486325425186611
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.03878787878787879
Global Adjusted Rand Index: 0.04389554850436038
Average Adjusted Rand Index: 0.04532404282793139
[-0.00038189214017276575, 0.04389554850436038] [-0.0002726049879675555, 0.04532404282793139] [9803.1513671875, 9798.1083984375]
-------------------------------------
This iteration is 31
True Objective function: Loss = -10157.675595287514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22553.419921875
inf tensor(22553.4199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10021.21875
tensor(22553.4199, grad_fn=<NegBackward0>) tensor(10021.2188, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10020.48046875
tensor(10021.2188, grad_fn=<NegBackward0>) tensor(10020.4805, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10020.0166015625
tensor(10020.4805, grad_fn=<NegBackward0>) tensor(10020.0166, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10019.7021484375
tensor(10020.0166, grad_fn=<NegBackward0>) tensor(10019.7021, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10019.490234375
tensor(10019.7021, grad_fn=<NegBackward0>) tensor(10019.4902, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10019.3056640625
tensor(10019.4902, grad_fn=<NegBackward0>) tensor(10019.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10019.16796875
tensor(10019.3057, grad_fn=<NegBackward0>) tensor(10019.1680, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10019.083984375
tensor(10019.1680, grad_fn=<NegBackward0>) tensor(10019.0840, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10019.037109375
tensor(10019.0840, grad_fn=<NegBackward0>) tensor(10019.0371, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10019.0087890625
tensor(10019.0371, grad_fn=<NegBackward0>) tensor(10019.0088, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10018.9912109375
tensor(10019.0088, grad_fn=<NegBackward0>) tensor(10018.9912, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10018.978515625
tensor(10018.9912, grad_fn=<NegBackward0>) tensor(10018.9785, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10018.9677734375
tensor(10018.9785, grad_fn=<NegBackward0>) tensor(10018.9678, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10018.95703125
tensor(10018.9678, grad_fn=<NegBackward0>) tensor(10018.9570, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10018.9521484375
tensor(10018.9570, grad_fn=<NegBackward0>) tensor(10018.9521, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10018.9453125
tensor(10018.9521, grad_fn=<NegBackward0>) tensor(10018.9453, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10018.9404296875
tensor(10018.9453, grad_fn=<NegBackward0>) tensor(10018.9404, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10018.9345703125
tensor(10018.9404, grad_fn=<NegBackward0>) tensor(10018.9346, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10018.9306640625
tensor(10018.9346, grad_fn=<NegBackward0>) tensor(10018.9307, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10018.92578125
tensor(10018.9307, grad_fn=<NegBackward0>) tensor(10018.9258, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10018.919921875
tensor(10018.9258, grad_fn=<NegBackward0>) tensor(10018.9199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10018.9140625
tensor(10018.9199, grad_fn=<NegBackward0>) tensor(10018.9141, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10018.91015625
tensor(10018.9141, grad_fn=<NegBackward0>) tensor(10018.9102, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10018.90625
tensor(10018.9102, grad_fn=<NegBackward0>) tensor(10018.9062, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10018.900390625
tensor(10018.9062, grad_fn=<NegBackward0>) tensor(10018.9004, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10018.8935546875
tensor(10018.9004, grad_fn=<NegBackward0>) tensor(10018.8936, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10018.896484375
tensor(10018.8936, grad_fn=<NegBackward0>) tensor(10018.8965, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10018.8857421875
tensor(10018.8936, grad_fn=<NegBackward0>) tensor(10018.8857, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10018.8798828125
tensor(10018.8857, grad_fn=<NegBackward0>) tensor(10018.8799, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10018.8779296875
tensor(10018.8799, grad_fn=<NegBackward0>) tensor(10018.8779, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10018.876953125
tensor(10018.8779, grad_fn=<NegBackward0>) tensor(10018.8770, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10018.8720703125
tensor(10018.8770, grad_fn=<NegBackward0>) tensor(10018.8721, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10018.8720703125
tensor(10018.8721, grad_fn=<NegBackward0>) tensor(10018.8721, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10018.8662109375
tensor(10018.8721, grad_fn=<NegBackward0>) tensor(10018.8662, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10018.865234375
tensor(10018.8662, grad_fn=<NegBackward0>) tensor(10018.8652, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10018.86328125
tensor(10018.8652, grad_fn=<NegBackward0>) tensor(10018.8633, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10018.861328125
tensor(10018.8633, grad_fn=<NegBackward0>) tensor(10018.8613, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10018.8603515625
tensor(10018.8613, grad_fn=<NegBackward0>) tensor(10018.8604, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10018.8603515625
tensor(10018.8604, grad_fn=<NegBackward0>) tensor(10018.8604, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10018.8583984375
tensor(10018.8604, grad_fn=<NegBackward0>) tensor(10018.8584, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10018.8564453125
tensor(10018.8584, grad_fn=<NegBackward0>) tensor(10018.8564, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10018.857421875
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8574, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10018.8564453125
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8564, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10018.8583984375
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8584, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10018.8544921875
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8545, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10018.8544921875
tensor(10018.8545, grad_fn=<NegBackward0>) tensor(10018.8545, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10018.8525390625
tensor(10018.8545, grad_fn=<NegBackward0>) tensor(10018.8525, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10018.8525390625
tensor(10018.8525, grad_fn=<NegBackward0>) tensor(10018.8525, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10018.8515625
tensor(10018.8525, grad_fn=<NegBackward0>) tensor(10018.8516, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10018.8515625
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8516, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10018.861328125
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8613, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10018.8515625
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8516, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10018.8505859375
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10018.8505859375
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10018.849609375
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10018.849609375
tensor(10018.8496, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10018.849609375
tensor(10018.8496, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10018.8486328125
tensor(10018.8496, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10018.8486328125
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10018.8486328125
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10018.8486328125
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10018.84765625
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10018.84765625
tensor(10018.8477, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10018.84765625
tensor(10018.8477, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10018.8486328125
tensor(10018.8477, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10018.845703125
tensor(10018.8477, grad_fn=<NegBackward0>) tensor(10018.8457, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10018.8466796875
tensor(10018.8457, grad_fn=<NegBackward0>) tensor(10018.8467, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10018.8466796875
tensor(10018.8457, grad_fn=<NegBackward0>) tensor(10018.8467, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10018.84765625
tensor(10018.8457, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10018.8466796875
tensor(10018.8457, grad_fn=<NegBackward0>) tensor(10018.8467, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10018.8486328125
tensor(10018.8457, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.9427, 0.0573],
        [0.9973, 0.0027]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9201, 0.0799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1361, 0.1566],
         [0.6974, 0.2043]],

        [[0.6216, 0.1328],
         [0.6557, 0.6738]],

        [[0.6917, 0.1640],
         [0.7181, 0.7017]],

        [[0.6060, 0.1961],
         [0.5410, 0.7240]],

        [[0.6384, 0.1914],
         [0.5122, 0.6814]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
Global Adjusted Rand Index: 5.748821533720524e-07
Average Adjusted Rand Index: -0.001709043850739314
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22586.869140625
inf tensor(22586.8691, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10023.048828125
tensor(22586.8691, grad_fn=<NegBackward0>) tensor(10023.0488, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10021.404296875
tensor(10023.0488, grad_fn=<NegBackward0>) tensor(10021.4043, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10020.802734375
tensor(10021.4043, grad_fn=<NegBackward0>) tensor(10020.8027, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10020.302734375
tensor(10020.8027, grad_fn=<NegBackward0>) tensor(10020.3027, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10019.7255859375
tensor(10020.3027, grad_fn=<NegBackward0>) tensor(10019.7256, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10019.400390625
tensor(10019.7256, grad_fn=<NegBackward0>) tensor(10019.4004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10019.2548828125
tensor(10019.4004, grad_fn=<NegBackward0>) tensor(10019.2549, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10019.173828125
tensor(10019.2549, grad_fn=<NegBackward0>) tensor(10019.1738, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10019.1181640625
tensor(10019.1738, grad_fn=<NegBackward0>) tensor(10019.1182, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10019.0791015625
tensor(10019.1182, grad_fn=<NegBackward0>) tensor(10019.0791, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10019.0517578125
tensor(10019.0791, grad_fn=<NegBackward0>) tensor(10019.0518, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10019.0302734375
tensor(10019.0518, grad_fn=<NegBackward0>) tensor(10019.0303, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10019.0126953125
tensor(10019.0303, grad_fn=<NegBackward0>) tensor(10019.0127, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10019.0
tensor(10019.0127, grad_fn=<NegBackward0>) tensor(10019., grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10018.9873046875
tensor(10019., grad_fn=<NegBackward0>) tensor(10018.9873, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10018.9794921875
tensor(10018.9873, grad_fn=<NegBackward0>) tensor(10018.9795, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10018.97265625
tensor(10018.9795, grad_fn=<NegBackward0>) tensor(10018.9727, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10018.9658203125
tensor(10018.9727, grad_fn=<NegBackward0>) tensor(10018.9658, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10018.9609375
tensor(10018.9658, grad_fn=<NegBackward0>) tensor(10018.9609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10018.9560546875
tensor(10018.9609, grad_fn=<NegBackward0>) tensor(10018.9561, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10018.953125
tensor(10018.9561, grad_fn=<NegBackward0>) tensor(10018.9531, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10018.94921875
tensor(10018.9531, grad_fn=<NegBackward0>) tensor(10018.9492, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10018.947265625
tensor(10018.9492, grad_fn=<NegBackward0>) tensor(10018.9473, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10018.9443359375
tensor(10018.9473, grad_fn=<NegBackward0>) tensor(10018.9443, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10018.9423828125
tensor(10018.9443, grad_fn=<NegBackward0>) tensor(10018.9424, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10018.9404296875
tensor(10018.9424, grad_fn=<NegBackward0>) tensor(10018.9404, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10018.9365234375
tensor(10018.9404, grad_fn=<NegBackward0>) tensor(10018.9365, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10018.9345703125
tensor(10018.9365, grad_fn=<NegBackward0>) tensor(10018.9346, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10018.9306640625
tensor(10018.9346, grad_fn=<NegBackward0>) tensor(10018.9307, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10018.927734375
tensor(10018.9307, grad_fn=<NegBackward0>) tensor(10018.9277, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10018.9228515625
tensor(10018.9277, grad_fn=<NegBackward0>) tensor(10018.9229, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10018.9208984375
tensor(10018.9229, grad_fn=<NegBackward0>) tensor(10018.9209, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10018.9169921875
tensor(10018.9209, grad_fn=<NegBackward0>) tensor(10018.9170, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10018.912109375
tensor(10018.9170, grad_fn=<NegBackward0>) tensor(10018.9121, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10018.90625
tensor(10018.9121, grad_fn=<NegBackward0>) tensor(10018.9062, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10018.9013671875
tensor(10018.9062, grad_fn=<NegBackward0>) tensor(10018.9014, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10018.8974609375
tensor(10018.9014, grad_fn=<NegBackward0>) tensor(10018.8975, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10018.892578125
tensor(10018.8975, grad_fn=<NegBackward0>) tensor(10018.8926, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10018.888671875
tensor(10018.8926, grad_fn=<NegBackward0>) tensor(10018.8887, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10018.8837890625
tensor(10018.8887, grad_fn=<NegBackward0>) tensor(10018.8838, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10018.880859375
tensor(10018.8838, grad_fn=<NegBackward0>) tensor(10018.8809, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10018.876953125
tensor(10018.8809, grad_fn=<NegBackward0>) tensor(10018.8770, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10018.8740234375
tensor(10018.8770, grad_fn=<NegBackward0>) tensor(10018.8740, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10018.87109375
tensor(10018.8740, grad_fn=<NegBackward0>) tensor(10018.8711, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10018.869140625
tensor(10018.8711, grad_fn=<NegBackward0>) tensor(10018.8691, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10018.8662109375
tensor(10018.8691, grad_fn=<NegBackward0>) tensor(10018.8662, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10018.8642578125
tensor(10018.8662, grad_fn=<NegBackward0>) tensor(10018.8643, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10018.861328125
tensor(10018.8643, grad_fn=<NegBackward0>) tensor(10018.8613, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10018.861328125
tensor(10018.8613, grad_fn=<NegBackward0>) tensor(10018.8613, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10018.8583984375
tensor(10018.8613, grad_fn=<NegBackward0>) tensor(10018.8584, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10018.8583984375
tensor(10018.8584, grad_fn=<NegBackward0>) tensor(10018.8584, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10018.857421875
tensor(10018.8584, grad_fn=<NegBackward0>) tensor(10018.8574, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10018.8564453125
tensor(10018.8574, grad_fn=<NegBackward0>) tensor(10018.8564, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10018.8564453125
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8564, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10018.8544921875
tensor(10018.8564, grad_fn=<NegBackward0>) tensor(10018.8545, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10018.8544921875
tensor(10018.8545, grad_fn=<NegBackward0>) tensor(10018.8545, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10018.8544921875
tensor(10018.8545, grad_fn=<NegBackward0>) tensor(10018.8545, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10018.853515625
tensor(10018.8545, grad_fn=<NegBackward0>) tensor(10018.8535, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10018.8525390625
tensor(10018.8535, grad_fn=<NegBackward0>) tensor(10018.8525, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10018.8515625
tensor(10018.8525, grad_fn=<NegBackward0>) tensor(10018.8516, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10018.8525390625
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8525, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10018.8505859375
tensor(10018.8516, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10018.8505859375
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10018.8505859375
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10018.8515625
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8516, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10018.8505859375
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8506, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10018.8486328125
tensor(10018.8506, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10018.849609375
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10018.8486328125
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10018.849609375
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10018.849609375
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -10018.84765625
tensor(10018.8486, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10018.8466796875
tensor(10018.8477, grad_fn=<NegBackward0>) tensor(10018.8467, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10018.84765625
tensor(10018.8467, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10018.84765625
tensor(10018.8467, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10018.8486328125
tensor(10018.8467, grad_fn=<NegBackward0>) tensor(10018.8486, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10018.84765625
tensor(10018.8467, grad_fn=<NegBackward0>) tensor(10018.8477, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -10018.849609375
tensor(10018.8467, grad_fn=<NegBackward0>) tensor(10018.8496, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.0033, 0.9967],
        [0.0578, 0.9422]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0796, 0.9204], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2043, 0.1566],
         [0.6036, 0.1362]],

        [[0.5817, 0.1328],
         [0.6910, 0.6356]],

        [[0.7057, 0.1640],
         [0.5059, 0.6808]],

        [[0.6265, 0.1961],
         [0.5394, 0.6945]],

        [[0.7308, 0.1914],
         [0.5353, 0.6498]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
Global Adjusted Rand Index: 5.748821533720524e-07
Average Adjusted Rand Index: -0.001709043850739314
[5.748821533720524e-07, 5.748821533720524e-07] [-0.001709043850739314, -0.001709043850739314] [10018.8486328125, 10018.849609375]
-------------------------------------
This iteration is 32
True Objective function: Loss = -9818.52321048985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22188.177734375
inf tensor(22188.1777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9722.869140625
tensor(22188.1777, grad_fn=<NegBackward0>) tensor(9722.8691, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9721.349609375
tensor(9722.8691, grad_fn=<NegBackward0>) tensor(9721.3496, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9720.736328125
tensor(9721.3496, grad_fn=<NegBackward0>) tensor(9720.7363, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9720.0986328125
tensor(9720.7363, grad_fn=<NegBackward0>) tensor(9720.0986, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9719.64453125
tensor(9720.0986, grad_fn=<NegBackward0>) tensor(9719.6445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9719.3447265625
tensor(9719.6445, grad_fn=<NegBackward0>) tensor(9719.3447, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9719.134765625
tensor(9719.3447, grad_fn=<NegBackward0>) tensor(9719.1348, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9718.9453125
tensor(9719.1348, grad_fn=<NegBackward0>) tensor(9718.9453, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9718.77734375
tensor(9718.9453, grad_fn=<NegBackward0>) tensor(9718.7773, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9718.626953125
tensor(9718.7773, grad_fn=<NegBackward0>) tensor(9718.6270, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9718.478515625
tensor(9718.6270, grad_fn=<NegBackward0>) tensor(9718.4785, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9718.3349609375
tensor(9718.4785, grad_fn=<NegBackward0>) tensor(9718.3350, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9718.1796875
tensor(9718.3350, grad_fn=<NegBackward0>) tensor(9718.1797, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9717.9931640625
tensor(9718.1797, grad_fn=<NegBackward0>) tensor(9717.9932, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9717.763671875
tensor(9717.9932, grad_fn=<NegBackward0>) tensor(9717.7637, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9717.50390625
tensor(9717.7637, grad_fn=<NegBackward0>) tensor(9717.5039, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9717.2109375
tensor(9717.5039, grad_fn=<NegBackward0>) tensor(9717.2109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9716.876953125
tensor(9717.2109, grad_fn=<NegBackward0>) tensor(9716.8770, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9716.5712890625
tensor(9716.8770, grad_fn=<NegBackward0>) tensor(9716.5713, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9716.2041015625
tensor(9716.5713, grad_fn=<NegBackward0>) tensor(9716.2041, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9715.755859375
tensor(9716.2041, grad_fn=<NegBackward0>) tensor(9715.7559, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9714.537109375
tensor(9715.7559, grad_fn=<NegBackward0>) tensor(9714.5371, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9693.427734375
tensor(9714.5371, grad_fn=<NegBackward0>) tensor(9693.4277, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9689.3037109375
tensor(9693.4277, grad_fn=<NegBackward0>) tensor(9689.3037, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9688.916015625
tensor(9689.3037, grad_fn=<NegBackward0>) tensor(9688.9160, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9688.849609375
tensor(9688.9160, grad_fn=<NegBackward0>) tensor(9688.8496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9688.80078125
tensor(9688.8496, grad_fn=<NegBackward0>) tensor(9688.8008, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9688.7744140625
tensor(9688.8008, grad_fn=<NegBackward0>) tensor(9688.7744, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9688.744140625
tensor(9688.7744, grad_fn=<NegBackward0>) tensor(9688.7441, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9688.736328125
tensor(9688.7441, grad_fn=<NegBackward0>) tensor(9688.7363, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9688.7314453125
tensor(9688.7363, grad_fn=<NegBackward0>) tensor(9688.7314, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9688.7275390625
tensor(9688.7314, grad_fn=<NegBackward0>) tensor(9688.7275, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9688.7236328125
tensor(9688.7275, grad_fn=<NegBackward0>) tensor(9688.7236, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9688.71875
tensor(9688.7236, grad_fn=<NegBackward0>) tensor(9688.7188, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9688.716796875
tensor(9688.7188, grad_fn=<NegBackward0>) tensor(9688.7168, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9688.71484375
tensor(9688.7168, grad_fn=<NegBackward0>) tensor(9688.7148, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9688.7138671875
tensor(9688.7148, grad_fn=<NegBackward0>) tensor(9688.7139, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9688.712890625
tensor(9688.7139, grad_fn=<NegBackward0>) tensor(9688.7129, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9688.7109375
tensor(9688.7129, grad_fn=<NegBackward0>) tensor(9688.7109, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9688.705078125
tensor(9688.7109, grad_fn=<NegBackward0>) tensor(9688.7051, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9688.6845703125
tensor(9688.7051, grad_fn=<NegBackward0>) tensor(9688.6846, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9688.67578125
tensor(9688.6846, grad_fn=<NegBackward0>) tensor(9688.6758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9688.673828125
tensor(9688.6758, grad_fn=<NegBackward0>) tensor(9688.6738, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9688.685546875
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6855, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9688.6728515625
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6729, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9688.6728515625
tensor(9688.6729, grad_fn=<NegBackward0>) tensor(9688.6729, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9688.671875
tensor(9688.6729, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9688.671875
tensor(9688.6719, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9688.6708984375
tensor(9688.6719, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9688.669921875
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9688.6708984375
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9688.671875
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9688.6708984375
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9688.669921875
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9688.669921875
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9688.6708984375
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9688.669921875
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9688.6669921875
tensor(9688.6699, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9688.6669921875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9688.669921875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9688.6669921875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9688.66796875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9688.66796875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9688.66796875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9688.66796875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -9688.6708984375
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.9184, 0.0816],
        [0.1196, 0.8804]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4725, 0.5275], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1892, 0.1195],
         [0.5835, 0.1408]],

        [[0.7225, 0.1041],
         [0.6710, 0.6580]],

        [[0.5375, 0.0875],
         [0.5290, 0.6287]],

        [[0.6424, 0.0932],
         [0.7087, 0.5950]],

        [[0.6862, 0.1010],
         [0.5611, 0.5511]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 32
Adjusted Rand Index: 0.12077578607111372
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 17
Adjusted Rand Index: 0.4298677172619937
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 9
Adjusted Rand Index: 0.6690727572201083
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 16
Adjusted Rand Index: 0.4569625163087556
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 20
Adjusted Rand Index: 0.35353535353535354
Global Adjusted Rand Index: 0.38815349979372654
Average Adjusted Rand Index: 0.40604282607946496
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25090.57421875
inf tensor(25090.5742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9721.78515625
tensor(25090.5742, grad_fn=<NegBackward0>) tensor(9721.7852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9720.642578125
tensor(9721.7852, grad_fn=<NegBackward0>) tensor(9720.6426, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9720.2705078125
tensor(9720.6426, grad_fn=<NegBackward0>) tensor(9720.2705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9720.0537109375
tensor(9720.2705, grad_fn=<NegBackward0>) tensor(9720.0537, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9719.896484375
tensor(9720.0537, grad_fn=<NegBackward0>) tensor(9719.8965, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9719.76171875
tensor(9719.8965, grad_fn=<NegBackward0>) tensor(9719.7617, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9719.6572265625
tensor(9719.7617, grad_fn=<NegBackward0>) tensor(9719.6572, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9719.5458984375
tensor(9719.6572, grad_fn=<NegBackward0>) tensor(9719.5459, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9719.458984375
tensor(9719.5459, grad_fn=<NegBackward0>) tensor(9719.4590, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9719.396484375
tensor(9719.4590, grad_fn=<NegBackward0>) tensor(9719.3965, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9719.3544921875
tensor(9719.3965, grad_fn=<NegBackward0>) tensor(9719.3545, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9719.3115234375
tensor(9719.3545, grad_fn=<NegBackward0>) tensor(9719.3115, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9719.2529296875
tensor(9719.3115, grad_fn=<NegBackward0>) tensor(9719.2529, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9719.1455078125
tensor(9719.2529, grad_fn=<NegBackward0>) tensor(9719.1455, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9718.833984375
tensor(9719.1455, grad_fn=<NegBackward0>) tensor(9718.8340, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9718.267578125
tensor(9718.8340, grad_fn=<NegBackward0>) tensor(9718.2676, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9717.822265625
tensor(9718.2676, grad_fn=<NegBackward0>) tensor(9717.8223, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9716.8134765625
tensor(9717.8223, grad_fn=<NegBackward0>) tensor(9716.8135, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9702.48046875
tensor(9716.8135, grad_fn=<NegBackward0>) tensor(9702.4805, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9697.533203125
tensor(9702.4805, grad_fn=<NegBackward0>) tensor(9697.5332, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9693.1806640625
tensor(9697.5332, grad_fn=<NegBackward0>) tensor(9693.1807, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9692.509765625
tensor(9693.1807, grad_fn=<NegBackward0>) tensor(9692.5098, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9691.755859375
tensor(9692.5098, grad_fn=<NegBackward0>) tensor(9691.7559, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9689.9130859375
tensor(9691.7559, grad_fn=<NegBackward0>) tensor(9689.9131, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9688.708984375
tensor(9689.9131, grad_fn=<NegBackward0>) tensor(9688.7090, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9688.6845703125
tensor(9688.7090, grad_fn=<NegBackward0>) tensor(9688.6846, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9688.6806640625
tensor(9688.6846, grad_fn=<NegBackward0>) tensor(9688.6807, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9688.6787109375
tensor(9688.6807, grad_fn=<NegBackward0>) tensor(9688.6787, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9688.6796875
tensor(9688.6787, grad_fn=<NegBackward0>) tensor(9688.6797, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9688.6787109375
tensor(9688.6787, grad_fn=<NegBackward0>) tensor(9688.6787, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9688.677734375
tensor(9688.6787, grad_fn=<NegBackward0>) tensor(9688.6777, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9688.6787109375
tensor(9688.6777, grad_fn=<NegBackward0>) tensor(9688.6787, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9688.677734375
tensor(9688.6777, grad_fn=<NegBackward0>) tensor(9688.6777, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9688.677734375
tensor(9688.6777, grad_fn=<NegBackward0>) tensor(9688.6777, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9688.677734375
tensor(9688.6777, grad_fn=<NegBackward0>) tensor(9688.6777, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9688.67578125
tensor(9688.6777, grad_fn=<NegBackward0>) tensor(9688.6758, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9688.67578125
tensor(9688.6758, grad_fn=<NegBackward0>) tensor(9688.6758, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9688.6748046875
tensor(9688.6758, grad_fn=<NegBackward0>) tensor(9688.6748, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9688.67578125
tensor(9688.6748, grad_fn=<NegBackward0>) tensor(9688.6758, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9688.673828125
tensor(9688.6748, grad_fn=<NegBackward0>) tensor(9688.6738, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9688.673828125
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6738, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9688.6748046875
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6748, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9688.6767578125
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6768, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9688.673828125
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6738, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9688.671875
tensor(9688.6738, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9688.6708984375
tensor(9688.6719, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9688.6708984375
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9688.671875
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9688.6708984375
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6709, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9688.671875
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6719, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9688.6669921875
tensor(9688.6709, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9688.6669921875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9688.66796875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9688.669921875
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6699, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9688.666015625
tensor(9688.6670, grad_fn=<NegBackward0>) tensor(9688.6660, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9688.66796875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6680, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9688.666015625
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6660, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9688.666015625
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6660, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -9688.6669921875
tensor(9688.6660, grad_fn=<NegBackward0>) tensor(9688.6670, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[0.8812, 0.1188],
        [0.0824, 0.9176]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5242, 0.4758], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1408, 0.1195],
         [0.5906, 0.1892]],

        [[0.5151, 0.1041],
         [0.6105, 0.6746]],

        [[0.6232, 0.0874],
         [0.7181, 0.5638]],

        [[0.5288, 0.0932],
         [0.6867, 0.6173]],

        [[0.7028, 0.1010],
         [0.6462, 0.5782]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.12077578607111372
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 83
Adjusted Rand Index: 0.4298677172619937
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6690727572201083
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.4569625163087556
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.35353535353535354
Global Adjusted Rand Index: 0.38815349979372654
Average Adjusted Rand Index: 0.40604282607946496
[0.38815349979372654, 0.38815349979372654] [0.40604282607946496, 0.40604282607946496] [9688.6708984375, 9688.6669921875]
-------------------------------------
This iteration is 33
True Objective function: Loss = -9864.086769147678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20053.966796875
inf tensor(20053.9668, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9781.240234375
tensor(20053.9668, grad_fn=<NegBackward0>) tensor(9781.2402, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9776.419921875
tensor(9781.2402, grad_fn=<NegBackward0>) tensor(9776.4199, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9772.625
tensor(9776.4199, grad_fn=<NegBackward0>) tensor(9772.6250, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9771.5927734375
tensor(9772.6250, grad_fn=<NegBackward0>) tensor(9771.5928, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9771.2734375
tensor(9771.5928, grad_fn=<NegBackward0>) tensor(9771.2734, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9771.0576171875
tensor(9771.2734, grad_fn=<NegBackward0>) tensor(9771.0576, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9770.837890625
tensor(9771.0576, grad_fn=<NegBackward0>) tensor(9770.8379, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9770.599609375
tensor(9770.8379, grad_fn=<NegBackward0>) tensor(9770.5996, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9770.376953125
tensor(9770.5996, grad_fn=<NegBackward0>) tensor(9770.3770, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9770.181640625
tensor(9770.3770, grad_fn=<NegBackward0>) tensor(9770.1816, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9770.01953125
tensor(9770.1816, grad_fn=<NegBackward0>) tensor(9770.0195, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9769.8623046875
tensor(9770.0195, grad_fn=<NegBackward0>) tensor(9769.8623, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9769.67578125
tensor(9769.8623, grad_fn=<NegBackward0>) tensor(9769.6758, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9769.4013671875
tensor(9769.6758, grad_fn=<NegBackward0>) tensor(9769.4014, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9769.037109375
tensor(9769.4014, grad_fn=<NegBackward0>) tensor(9769.0371, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9768.6455078125
tensor(9769.0371, grad_fn=<NegBackward0>) tensor(9768.6455, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9768.3408203125
tensor(9768.6455, grad_fn=<NegBackward0>) tensor(9768.3408, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9768.041015625
tensor(9768.3408, grad_fn=<NegBackward0>) tensor(9768.0410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9767.826171875
tensor(9768.0410, grad_fn=<NegBackward0>) tensor(9767.8262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9767.69140625
tensor(9767.8262, grad_fn=<NegBackward0>) tensor(9767.6914, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9767.5595703125
tensor(9767.6914, grad_fn=<NegBackward0>) tensor(9767.5596, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9767.521484375
tensor(9767.5596, grad_fn=<NegBackward0>) tensor(9767.5215, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9767.5009765625
tensor(9767.5215, grad_fn=<NegBackward0>) tensor(9767.5010, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9766.5908203125
tensor(9767.5010, grad_fn=<NegBackward0>) tensor(9766.5908, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9741.966796875
tensor(9766.5908, grad_fn=<NegBackward0>) tensor(9741.9668, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9741.900390625
tensor(9741.9668, grad_fn=<NegBackward0>) tensor(9741.9004, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9741.88671875
tensor(9741.9004, grad_fn=<NegBackward0>) tensor(9741.8867, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9741.8828125
tensor(9741.8867, grad_fn=<NegBackward0>) tensor(9741.8828, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9741.876953125
tensor(9741.8828, grad_fn=<NegBackward0>) tensor(9741.8770, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9741.875
tensor(9741.8770, grad_fn=<NegBackward0>) tensor(9741.8750, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9741.8740234375
tensor(9741.8750, grad_fn=<NegBackward0>) tensor(9741.8740, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9741.8720703125
tensor(9741.8740, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9741.873046875
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8730, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9741.87109375
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9741.8740234375
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8740, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9741.87109375
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9741.8701171875
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9741.8701171875
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9741.8701171875
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9741.8701171875
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9741.8701171875
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9741.869140625
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8691, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9741.8720703125
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9741.8681640625
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8682, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9741.869140625
tensor(9741.8682, grad_fn=<NegBackward0>) tensor(9741.8691, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9741.8720703125
tensor(9741.8682, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9741.8701171875
tensor(9741.8682, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -9741.8701171875
tensor(9741.8682, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -9741.869140625
tensor(9741.8682, grad_fn=<NegBackward0>) tensor(9741.8691, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.8861, 0.1139],
        [0.1754, 0.8246]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6422, 0.3578], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1786, 0.0930],
         [0.7201, 0.1595]],

        [[0.5335, 0.0799],
         [0.5286, 0.6111]],

        [[0.6725, 0.0906],
         [0.5115, 0.6852]],

        [[0.5488, 0.0900],
         [0.5525, 0.5724]],

        [[0.6683, 0.0975],
         [0.7124, 0.5876]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 17
Adjusted Rand Index: 0.4286925115935066
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 11
Adjusted Rand Index: 0.6046210397853521
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.5136575420537136
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 13
Adjusted Rand Index: 0.5429121065649299
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 25
Adjusted Rand Index: 0.24267348560400323
Global Adjusted Rand Index: 0.4611512912237388
Average Adjusted Rand Index: 0.46651133712030113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23636.14453125
inf tensor(23636.1445, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9782.9482421875
tensor(23636.1445, grad_fn=<NegBackward0>) tensor(9782.9482, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9779.0087890625
tensor(9782.9482, grad_fn=<NegBackward0>) tensor(9779.0088, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9777.1240234375
tensor(9779.0088, grad_fn=<NegBackward0>) tensor(9777.1240, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9773.896484375
tensor(9777.1240, grad_fn=<NegBackward0>) tensor(9773.8965, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9772.6318359375
tensor(9773.8965, grad_fn=<NegBackward0>) tensor(9772.6318, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9772.2060546875
tensor(9772.6318, grad_fn=<NegBackward0>) tensor(9772.2061, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9771.9375
tensor(9772.2061, grad_fn=<NegBackward0>) tensor(9771.9375, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9771.73046875
tensor(9771.9375, grad_fn=<NegBackward0>) tensor(9771.7305, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9771.5546875
tensor(9771.7305, grad_fn=<NegBackward0>) tensor(9771.5547, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9771.3994140625
tensor(9771.5547, grad_fn=<NegBackward0>) tensor(9771.3994, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9771.2578125
tensor(9771.3994, grad_fn=<NegBackward0>) tensor(9771.2578, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9771.1181640625
tensor(9771.2578, grad_fn=<NegBackward0>) tensor(9771.1182, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9770.9765625
tensor(9771.1182, grad_fn=<NegBackward0>) tensor(9770.9766, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9770.8251953125
tensor(9770.9766, grad_fn=<NegBackward0>) tensor(9770.8252, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9770.662109375
tensor(9770.8252, grad_fn=<NegBackward0>) tensor(9770.6621, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9770.484375
tensor(9770.6621, grad_fn=<NegBackward0>) tensor(9770.4844, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9770.283203125
tensor(9770.4844, grad_fn=<NegBackward0>) tensor(9770.2832, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9770.0380859375
tensor(9770.2832, grad_fn=<NegBackward0>) tensor(9770.0381, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9769.70703125
tensor(9770.0381, grad_fn=<NegBackward0>) tensor(9769.7070, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9769.28125
tensor(9769.7070, grad_fn=<NegBackward0>) tensor(9769.2812, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9768.8193359375
tensor(9769.2812, grad_fn=<NegBackward0>) tensor(9768.8193, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9768.3291015625
tensor(9768.8193, grad_fn=<NegBackward0>) tensor(9768.3291, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9768.0712890625
tensor(9768.3291, grad_fn=<NegBackward0>) tensor(9768.0713, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9767.814453125
tensor(9768.0713, grad_fn=<NegBackward0>) tensor(9767.8145, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9767.6435546875
tensor(9767.8145, grad_fn=<NegBackward0>) tensor(9767.6436, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9767.5546875
tensor(9767.6436, grad_fn=<NegBackward0>) tensor(9767.5547, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9767.5166015625
tensor(9767.5547, grad_fn=<NegBackward0>) tensor(9767.5166, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9767.49609375
tensor(9767.5166, grad_fn=<NegBackward0>) tensor(9767.4961, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9763.2431640625
tensor(9767.4961, grad_fn=<NegBackward0>) tensor(9763.2432, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9742.0673828125
tensor(9763.2432, grad_fn=<NegBackward0>) tensor(9742.0674, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9741.9326171875
tensor(9742.0674, grad_fn=<NegBackward0>) tensor(9741.9326, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9741.904296875
tensor(9741.9326, grad_fn=<NegBackward0>) tensor(9741.9043, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9741.892578125
tensor(9741.9043, grad_fn=<NegBackward0>) tensor(9741.8926, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9741.88671875
tensor(9741.8926, grad_fn=<NegBackward0>) tensor(9741.8867, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9741.8837890625
tensor(9741.8867, grad_fn=<NegBackward0>) tensor(9741.8838, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9741.87890625
tensor(9741.8838, grad_fn=<NegBackward0>) tensor(9741.8789, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9741.8779296875
tensor(9741.8789, grad_fn=<NegBackward0>) tensor(9741.8779, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9741.8759765625
tensor(9741.8779, grad_fn=<NegBackward0>) tensor(9741.8760, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9741.8759765625
tensor(9741.8760, grad_fn=<NegBackward0>) tensor(9741.8760, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9741.8759765625
tensor(9741.8760, grad_fn=<NegBackward0>) tensor(9741.8760, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9741.8740234375
tensor(9741.8760, grad_fn=<NegBackward0>) tensor(9741.8740, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9741.875
tensor(9741.8740, grad_fn=<NegBackward0>) tensor(9741.8750, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9741.8720703125
tensor(9741.8740, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9741.8740234375
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8740, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9741.873046875
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8730, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9741.873046875
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8730, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9741.8720703125
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9741.8720703125
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9741.873046875
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8730, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9741.8720703125
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9741.87109375
tensor(9741.8721, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9741.87109375
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9741.87109375
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9741.8720703125
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9741.8720703125
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -9741.8701171875
tensor(9741.8711, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9741.869140625
tensor(9741.8701, grad_fn=<NegBackward0>) tensor(9741.8691, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9741.8701171875
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9741.87109375
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8711, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9741.8720703125
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9741.8701171875
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8701, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -9741.8720703125
tensor(9741.8691, grad_fn=<NegBackward0>) tensor(9741.8721, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.8237, 0.1763],
        [0.1154, 0.8846]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3567, 0.6433], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1595, 0.0930],
         [0.7200, 0.1787]],

        [[0.5920, 0.0799],
         [0.6834, 0.5549]],

        [[0.7288, 0.0906],
         [0.6863, 0.6900]],

        [[0.5799, 0.0900],
         [0.6001, 0.6039]],

        [[0.5861, 0.0975],
         [0.6738, 0.7199]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 83
Adjusted Rand Index: 0.4286925115935066
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 89
Adjusted Rand Index: 0.6046210397853521
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 86
Adjusted Rand Index: 0.5136575420537136
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 87
Adjusted Rand Index: 0.5429121065649299
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24267348560400323
Global Adjusted Rand Index: 0.4611512912237388
Average Adjusted Rand Index: 0.46651133712030113
[0.4611512912237388, 0.4611512912237388] [0.46651133712030113, 0.46651133712030113] [9741.869140625, 9741.8720703125]
-------------------------------------
This iteration is 34
True Objective function: Loss = -9996.26995242104
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22129.572265625
inf tensor(22129.5723, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9847.224609375
tensor(22129.5723, grad_fn=<NegBackward0>) tensor(9847.2246, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9846.296875
tensor(9847.2246, grad_fn=<NegBackward0>) tensor(9846.2969, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9845.763671875
tensor(9846.2969, grad_fn=<NegBackward0>) tensor(9845.7637, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9845.3427734375
tensor(9845.7637, grad_fn=<NegBackward0>) tensor(9845.3428, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9845.111328125
tensor(9845.3428, grad_fn=<NegBackward0>) tensor(9845.1113, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9844.921875
tensor(9845.1113, grad_fn=<NegBackward0>) tensor(9844.9219, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9844.765625
tensor(9844.9219, grad_fn=<NegBackward0>) tensor(9844.7656, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9844.6591796875
tensor(9844.7656, grad_fn=<NegBackward0>) tensor(9844.6592, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9844.5830078125
tensor(9844.6592, grad_fn=<NegBackward0>) tensor(9844.5830, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9844.5185546875
tensor(9844.5830, grad_fn=<NegBackward0>) tensor(9844.5186, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9844.4609375
tensor(9844.5186, grad_fn=<NegBackward0>) tensor(9844.4609, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9844.408203125
tensor(9844.4609, grad_fn=<NegBackward0>) tensor(9844.4082, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9844.3623046875
tensor(9844.4082, grad_fn=<NegBackward0>) tensor(9844.3623, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9844.3212890625
tensor(9844.3623, grad_fn=<NegBackward0>) tensor(9844.3213, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9844.291015625
tensor(9844.3213, grad_fn=<NegBackward0>) tensor(9844.2910, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9844.26953125
tensor(9844.2910, grad_fn=<NegBackward0>) tensor(9844.2695, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9844.2578125
tensor(9844.2695, grad_fn=<NegBackward0>) tensor(9844.2578, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9844.2509765625
tensor(9844.2578, grad_fn=<NegBackward0>) tensor(9844.2510, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9844.244140625
tensor(9844.2510, grad_fn=<NegBackward0>) tensor(9844.2441, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9844.2412109375
tensor(9844.2441, grad_fn=<NegBackward0>) tensor(9844.2412, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9844.23828125
tensor(9844.2412, grad_fn=<NegBackward0>) tensor(9844.2383, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9844.2333984375
tensor(9844.2383, grad_fn=<NegBackward0>) tensor(9844.2334, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9844.2275390625
tensor(9844.2334, grad_fn=<NegBackward0>) tensor(9844.2275, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9844.21875
tensor(9844.2275, grad_fn=<NegBackward0>) tensor(9844.2188, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9844.20703125
tensor(9844.2188, grad_fn=<NegBackward0>) tensor(9844.2070, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9844.1845703125
tensor(9844.2070, grad_fn=<NegBackward0>) tensor(9844.1846, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9844.138671875
tensor(9844.1846, grad_fn=<NegBackward0>) tensor(9844.1387, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9844.0283203125
tensor(9844.1387, grad_fn=<NegBackward0>) tensor(9844.0283, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9843.84765625
tensor(9844.0283, grad_fn=<NegBackward0>) tensor(9843.8477, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9843.767578125
tensor(9843.8477, grad_fn=<NegBackward0>) tensor(9843.7676, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9843.7353515625
tensor(9843.7676, grad_fn=<NegBackward0>) tensor(9843.7354, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9843.7197265625
tensor(9843.7354, grad_fn=<NegBackward0>) tensor(9843.7197, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9843.7119140625
tensor(9843.7197, grad_fn=<NegBackward0>) tensor(9843.7119, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9843.7041015625
tensor(9843.7119, grad_fn=<NegBackward0>) tensor(9843.7041, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9843.697265625
tensor(9843.7041, grad_fn=<NegBackward0>) tensor(9843.6973, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9843.693359375
tensor(9843.6973, grad_fn=<NegBackward0>) tensor(9843.6934, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9843.689453125
tensor(9843.6934, grad_fn=<NegBackward0>) tensor(9843.6895, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9843.685546875
tensor(9843.6895, grad_fn=<NegBackward0>) tensor(9843.6855, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9843.6796875
tensor(9843.6855, grad_fn=<NegBackward0>) tensor(9843.6797, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9843.6689453125
tensor(9843.6797, grad_fn=<NegBackward0>) tensor(9843.6689, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9843.6494140625
tensor(9843.6689, grad_fn=<NegBackward0>) tensor(9843.6494, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9843.603515625
tensor(9843.6494, grad_fn=<NegBackward0>) tensor(9843.6035, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9843.5068359375
tensor(9843.6035, grad_fn=<NegBackward0>) tensor(9843.5068, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9843.419921875
tensor(9843.5068, grad_fn=<NegBackward0>) tensor(9843.4199, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9843.37109375
tensor(9843.4199, grad_fn=<NegBackward0>) tensor(9843.3711, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9843.3447265625
tensor(9843.3711, grad_fn=<NegBackward0>) tensor(9843.3447, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9843.3291015625
tensor(9843.3447, grad_fn=<NegBackward0>) tensor(9843.3291, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9843.3203125
tensor(9843.3291, grad_fn=<NegBackward0>) tensor(9843.3203, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9843.3134765625
tensor(9843.3203, grad_fn=<NegBackward0>) tensor(9843.3135, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9843.30859375
tensor(9843.3135, grad_fn=<NegBackward0>) tensor(9843.3086, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9843.3037109375
tensor(9843.3086, grad_fn=<NegBackward0>) tensor(9843.3037, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9843.298828125
tensor(9843.3037, grad_fn=<NegBackward0>) tensor(9843.2988, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9843.296875
tensor(9843.2988, grad_fn=<NegBackward0>) tensor(9843.2969, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9843.2939453125
tensor(9843.2969, grad_fn=<NegBackward0>) tensor(9843.2939, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9843.29296875
tensor(9843.2939, grad_fn=<NegBackward0>) tensor(9843.2930, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9843.291015625
tensor(9843.2930, grad_fn=<NegBackward0>) tensor(9843.2910, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9843.2900390625
tensor(9843.2910, grad_fn=<NegBackward0>) tensor(9843.2900, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9843.2900390625
tensor(9843.2900, grad_fn=<NegBackward0>) tensor(9843.2900, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9843.2880859375
tensor(9843.2900, grad_fn=<NegBackward0>) tensor(9843.2881, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9843.2880859375
tensor(9843.2881, grad_fn=<NegBackward0>) tensor(9843.2881, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9843.28515625
tensor(9843.2881, grad_fn=<NegBackward0>) tensor(9843.2852, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9843.2841796875
tensor(9843.2852, grad_fn=<NegBackward0>) tensor(9843.2842, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9843.2861328125
tensor(9843.2842, grad_fn=<NegBackward0>) tensor(9843.2861, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9843.283203125
tensor(9843.2842, grad_fn=<NegBackward0>) tensor(9843.2832, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9843.2841796875
tensor(9843.2832, grad_fn=<NegBackward0>) tensor(9843.2842, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9843.283203125
tensor(9843.2832, grad_fn=<NegBackward0>) tensor(9843.2832, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9843.2822265625
tensor(9843.2832, grad_fn=<NegBackward0>) tensor(9843.2822, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9843.28125
tensor(9843.2822, grad_fn=<NegBackward0>) tensor(9843.2812, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9843.28125
tensor(9843.2812, grad_fn=<NegBackward0>) tensor(9843.2812, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9843.2802734375
tensor(9843.2812, grad_fn=<NegBackward0>) tensor(9843.2803, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9843.2822265625
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2822, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9843.28125
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2812, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9843.2802734375
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2803, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9843.28125
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2812, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9843.2802734375
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2803, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9843.283203125
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2832, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9843.279296875
tensor(9843.2803, grad_fn=<NegBackward0>) tensor(9843.2793, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9843.28125
tensor(9843.2793, grad_fn=<NegBackward0>) tensor(9843.2812, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9843.2802734375
tensor(9843.2793, grad_fn=<NegBackward0>) tensor(9843.2803, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9843.279296875
tensor(9843.2793, grad_fn=<NegBackward0>) tensor(9843.2793, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9843.2802734375
tensor(9843.2793, grad_fn=<NegBackward0>) tensor(9843.2803, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9843.2783203125
tensor(9843.2793, grad_fn=<NegBackward0>) tensor(9843.2783, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9843.2783203125
tensor(9843.2783, grad_fn=<NegBackward0>) tensor(9843.2783, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9843.27734375
tensor(9843.2783, grad_fn=<NegBackward0>) tensor(9843.2773, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9843.2783203125
tensor(9843.2773, grad_fn=<NegBackward0>) tensor(9843.2783, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9843.279296875
tensor(9843.2773, grad_fn=<NegBackward0>) tensor(9843.2793, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -9843.27734375
tensor(9843.2773, grad_fn=<NegBackward0>) tensor(9843.2773, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9843.27734375
tensor(9843.2773, grad_fn=<NegBackward0>) tensor(9843.2773, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9843.2763671875
tensor(9843.2773, grad_fn=<NegBackward0>) tensor(9843.2764, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9843.27734375
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2773, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9843.2763671875
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2764, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9843.2861328125
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2861, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9843.2763671875
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2764, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9843.3212890625
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.3213, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9843.2763671875
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2764, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9843.2763671875
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2764, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9843.2783203125
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2783, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9843.275390625
tensor(9843.2764, grad_fn=<NegBackward0>) tensor(9843.2754, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9843.4736328125
tensor(9843.2754, grad_fn=<NegBackward0>) tensor(9843.4736, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9987e-01, 1.3140e-04],
        [1.1055e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0651, 0.9349], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1804, 0.1682],
         [0.5666, 0.1319]],

        [[0.5336, 0.1873],
         [0.6033, 0.5413]],

        [[0.6549, 0.1413],
         [0.7221, 0.6140]],

        [[0.6463, 0.1884],
         [0.5240, 0.7029]],

        [[0.5400, 0.1378],
         [0.5573, 0.7069]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.009618156350865796
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.04056271981242673
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.02548983702618568
Global Adjusted Rand Index: 0.014975387167688713
Average Adjusted Rand Index: 0.014563179278250815
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21802.369140625
inf tensor(21802.3691, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9847.5517578125
tensor(21802.3691, grad_fn=<NegBackward0>) tensor(9847.5518, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9846.841796875
tensor(9847.5518, grad_fn=<NegBackward0>) tensor(9846.8418, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9846.7646484375
tensor(9846.8418, grad_fn=<NegBackward0>) tensor(9846.7646, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9846.673828125
tensor(9846.7646, grad_fn=<NegBackward0>) tensor(9846.6738, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9846.4853515625
tensor(9846.6738, grad_fn=<NegBackward0>) tensor(9846.4854, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9845.4541015625
tensor(9846.4854, grad_fn=<NegBackward0>) tensor(9845.4541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9843.982421875
tensor(9845.4541, grad_fn=<NegBackward0>) tensor(9843.9824, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9843.61328125
tensor(9843.9824, grad_fn=<NegBackward0>) tensor(9843.6133, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9843.4638671875
tensor(9843.6133, grad_fn=<NegBackward0>) tensor(9843.4639, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9843.392578125
tensor(9843.4639, grad_fn=<NegBackward0>) tensor(9843.3926, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9843.3603515625
tensor(9843.3926, grad_fn=<NegBackward0>) tensor(9843.3604, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9843.345703125
tensor(9843.3604, grad_fn=<NegBackward0>) tensor(9843.3457, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9843.3369140625
tensor(9843.3457, grad_fn=<NegBackward0>) tensor(9843.3369, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9843.3330078125
tensor(9843.3369, grad_fn=<NegBackward0>) tensor(9843.3330, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9843.3310546875
tensor(9843.3330, grad_fn=<NegBackward0>) tensor(9843.3311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9843.328125
tensor(9843.3311, grad_fn=<NegBackward0>) tensor(9843.3281, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9843.326171875
tensor(9843.3281, grad_fn=<NegBackward0>) tensor(9843.3262, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9843.32421875
tensor(9843.3262, grad_fn=<NegBackward0>) tensor(9843.3242, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9843.32421875
tensor(9843.3242, grad_fn=<NegBackward0>) tensor(9843.3242, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9843.32421875
tensor(9843.3242, grad_fn=<NegBackward0>) tensor(9843.3242, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9843.3232421875
tensor(9843.3242, grad_fn=<NegBackward0>) tensor(9843.3232, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9843.32421875
tensor(9843.3232, grad_fn=<NegBackward0>) tensor(9843.3242, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9843.322265625
tensor(9843.3232, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9843.3232421875
tensor(9843.3223, grad_fn=<NegBackward0>) tensor(9843.3232, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -9843.322265625
tensor(9843.3223, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9843.3212890625
tensor(9843.3223, grad_fn=<NegBackward0>) tensor(9843.3213, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9843.322265625
tensor(9843.3213, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9843.322265625
tensor(9843.3213, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -9843.322265625
tensor(9843.3213, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
3
Iteration 3000: Loss = -9843.3232421875
tensor(9843.3213, grad_fn=<NegBackward0>) tensor(9843.3232, grad_fn=<NegBackward0>)
4
Iteration 3100: Loss = -9843.322265625
tensor(9843.3213, grad_fn=<NegBackward0>) tensor(9843.3223, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3100 due to no improvement.
pi: tensor([[0.7268, 0.2732],
        [0.0030, 0.9970]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0646, 0.9354], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1739, 0.1577],
         [0.6342, 0.1345]],

        [[0.6357, 0.1776],
         [0.6807, 0.7113]],

        [[0.7201, 0.1106],
         [0.6001, 0.7116]],

        [[0.7096, 0.2170],
         [0.6391, 0.5471]],

        [[0.6973, 0.0873],
         [0.6823, 0.5881]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: 0.003277124183283542
Average Adjusted Rand Index: 0.0016933351461071438
[0.014975387167688713, 0.003277124183283542] [0.014563179278250815, 0.0016933351461071438] [9843.27734375, 9843.322265625]
-------------------------------------
This iteration is 35
True Objective function: Loss = -10097.946103100014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19957.876953125
inf tensor(19957.8770, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9985.1494140625
tensor(19957.8770, grad_fn=<NegBackward0>) tensor(9985.1494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9984.779296875
tensor(9985.1494, grad_fn=<NegBackward0>) tensor(9984.7793, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9984.708984375
tensor(9984.7793, grad_fn=<NegBackward0>) tensor(9984.7090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9984.65625
tensor(9984.7090, grad_fn=<NegBackward0>) tensor(9984.6562, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9984.6064453125
tensor(9984.6562, grad_fn=<NegBackward0>) tensor(9984.6064, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9984.55078125
tensor(9984.6064, grad_fn=<NegBackward0>) tensor(9984.5508, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9984.486328125
tensor(9984.5508, grad_fn=<NegBackward0>) tensor(9984.4863, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9984.4130859375
tensor(9984.4863, grad_fn=<NegBackward0>) tensor(9984.4131, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9984.349609375
tensor(9984.4131, grad_fn=<NegBackward0>) tensor(9984.3496, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9984.3056640625
tensor(9984.3496, grad_fn=<NegBackward0>) tensor(9984.3057, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9984.2763671875
tensor(9984.3057, grad_fn=<NegBackward0>) tensor(9984.2764, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9984.2548828125
tensor(9984.2764, grad_fn=<NegBackward0>) tensor(9984.2549, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9984.2353515625
tensor(9984.2549, grad_fn=<NegBackward0>) tensor(9984.2354, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9984.2158203125
tensor(9984.2354, grad_fn=<NegBackward0>) tensor(9984.2158, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9984.193359375
tensor(9984.2158, grad_fn=<NegBackward0>) tensor(9984.1934, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9984.166015625
tensor(9984.1934, grad_fn=<NegBackward0>) tensor(9984.1660, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9984.1298828125
tensor(9984.1660, grad_fn=<NegBackward0>) tensor(9984.1299, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9984.0654296875
tensor(9984.1299, grad_fn=<NegBackward0>) tensor(9984.0654, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9983.8974609375
tensor(9984.0654, grad_fn=<NegBackward0>) tensor(9983.8975, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9983.125
tensor(9983.8975, grad_fn=<NegBackward0>) tensor(9983.1250, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9982.255859375
tensor(9983.1250, grad_fn=<NegBackward0>) tensor(9982.2559, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9968.751953125
tensor(9982.2559, grad_fn=<NegBackward0>) tensor(9968.7520, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9964.8408203125
tensor(9968.7520, grad_fn=<NegBackward0>) tensor(9964.8408, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9964.7314453125
tensor(9964.8408, grad_fn=<NegBackward0>) tensor(9964.7314, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9964.7138671875
tensor(9964.7314, grad_fn=<NegBackward0>) tensor(9964.7139, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9964.69921875
tensor(9964.7139, grad_fn=<NegBackward0>) tensor(9964.6992, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9964.6865234375
tensor(9964.6992, grad_fn=<NegBackward0>) tensor(9964.6865, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9964.6748046875
tensor(9964.6865, grad_fn=<NegBackward0>) tensor(9964.6748, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9964.65234375
tensor(9964.6748, grad_fn=<NegBackward0>) tensor(9964.6523, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9964.634765625
tensor(9964.6523, grad_fn=<NegBackward0>) tensor(9964.6348, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9964.6259765625
tensor(9964.6348, grad_fn=<NegBackward0>) tensor(9964.6260, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9964.6220703125
tensor(9964.6260, grad_fn=<NegBackward0>) tensor(9964.6221, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9964.6181640625
tensor(9964.6221, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9964.619140625
tensor(9964.6182, grad_fn=<NegBackward0>) tensor(9964.6191, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9964.6171875
tensor(9964.6182, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9964.6162109375
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6162, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9964.6171875
tensor(9964.6162, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9964.6171875
tensor(9964.6162, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9964.6171875
tensor(9964.6162, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -9964.6171875
tensor(9964.6162, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -9964.6171875
tensor(9964.6162, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.9224, 0.0776],
        [0.2181, 0.7819]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1285, 0.8715], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2036, 0.1520],
         [0.5969, 0.1383]],

        [[0.6936, 0.1242],
         [0.5556, 0.6747]],

        [[0.7173, 0.1025],
         [0.7003, 0.7213]],

        [[0.6635, 0.1130],
         [0.5907, 0.6527]],

        [[0.5886, 0.0895],
         [0.6378, 0.5744]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 20
Adjusted Rand Index: 0.35353535353535354
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 13
Adjusted Rand Index: 0.5430499964984674
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 22
Adjusted Rand Index: 0.30660723906548465
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 16
Adjusted Rand Index: 0.45696969696969697
Global Adjusted Rand Index: 0.2525389042313254
Average Adjusted Rand Index: 0.3314397787257749
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22629.56640625
inf tensor(22629.5664, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9985.8037109375
tensor(22629.5664, grad_fn=<NegBackward0>) tensor(9985.8037, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9984.9677734375
tensor(9985.8037, grad_fn=<NegBackward0>) tensor(9984.9678, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9984.7587890625
tensor(9984.9678, grad_fn=<NegBackward0>) tensor(9984.7588, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9984.6826171875
tensor(9984.7588, grad_fn=<NegBackward0>) tensor(9984.6826, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9984.6396484375
tensor(9984.6826, grad_fn=<NegBackward0>) tensor(9984.6396, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9984.6064453125
tensor(9984.6396, grad_fn=<NegBackward0>) tensor(9984.6064, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9984.57421875
tensor(9984.6064, grad_fn=<NegBackward0>) tensor(9984.5742, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9984.5439453125
tensor(9984.5742, grad_fn=<NegBackward0>) tensor(9984.5439, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9984.51171875
tensor(9984.5439, grad_fn=<NegBackward0>) tensor(9984.5117, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9984.478515625
tensor(9984.5117, grad_fn=<NegBackward0>) tensor(9984.4785, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9984.447265625
tensor(9984.4785, grad_fn=<NegBackward0>) tensor(9984.4473, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9984.4189453125
tensor(9984.4473, grad_fn=<NegBackward0>) tensor(9984.4189, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9984.390625
tensor(9984.4189, grad_fn=<NegBackward0>) tensor(9984.3906, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9984.3681640625
tensor(9984.3906, grad_fn=<NegBackward0>) tensor(9984.3682, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9984.345703125
tensor(9984.3682, grad_fn=<NegBackward0>) tensor(9984.3457, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9984.3271484375
tensor(9984.3457, grad_fn=<NegBackward0>) tensor(9984.3271, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9984.3095703125
tensor(9984.3271, grad_fn=<NegBackward0>) tensor(9984.3096, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9984.2919921875
tensor(9984.3096, grad_fn=<NegBackward0>) tensor(9984.2920, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9984.27734375
tensor(9984.2920, grad_fn=<NegBackward0>) tensor(9984.2773, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9984.2587890625
tensor(9984.2773, grad_fn=<NegBackward0>) tensor(9984.2588, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9984.240234375
tensor(9984.2588, grad_fn=<NegBackward0>) tensor(9984.2402, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9984.21875
tensor(9984.2402, grad_fn=<NegBackward0>) tensor(9984.2188, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9984.19140625
tensor(9984.2188, grad_fn=<NegBackward0>) tensor(9984.1914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9984.1533203125
tensor(9984.1914, grad_fn=<NegBackward0>) tensor(9984.1533, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9984.0908203125
tensor(9984.1533, grad_fn=<NegBackward0>) tensor(9984.0908, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9983.9150390625
tensor(9984.0908, grad_fn=<NegBackward0>) tensor(9983.9150, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9983.10546875
tensor(9983.9150, grad_fn=<NegBackward0>) tensor(9983.1055, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9982.400390625
tensor(9983.1055, grad_fn=<NegBackward0>) tensor(9982.4004, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9976.3115234375
tensor(9982.4004, grad_fn=<NegBackward0>) tensor(9976.3115, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9965.30859375
tensor(9976.3115, grad_fn=<NegBackward0>) tensor(9965.3086, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9964.7900390625
tensor(9965.3086, grad_fn=<NegBackward0>) tensor(9964.7900, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9964.7578125
tensor(9964.7900, grad_fn=<NegBackward0>) tensor(9964.7578, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9964.7431640625
tensor(9964.7578, grad_fn=<NegBackward0>) tensor(9964.7432, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9964.732421875
tensor(9964.7432, grad_fn=<NegBackward0>) tensor(9964.7324, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9964.7197265625
tensor(9964.7324, grad_fn=<NegBackward0>) tensor(9964.7197, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9964.7138671875
tensor(9964.7197, grad_fn=<NegBackward0>) tensor(9964.7139, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9964.7099609375
tensor(9964.7139, grad_fn=<NegBackward0>) tensor(9964.7100, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9964.70703125
tensor(9964.7100, grad_fn=<NegBackward0>) tensor(9964.7070, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9964.7060546875
tensor(9964.7070, grad_fn=<NegBackward0>) tensor(9964.7061, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9964.703125
tensor(9964.7061, grad_fn=<NegBackward0>) tensor(9964.7031, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9964.705078125
tensor(9964.7031, grad_fn=<NegBackward0>) tensor(9964.7051, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9964.69921875
tensor(9964.7031, grad_fn=<NegBackward0>) tensor(9964.6992, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9964.6953125
tensor(9964.6992, grad_fn=<NegBackward0>) tensor(9964.6953, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9964.689453125
tensor(9964.6953, grad_fn=<NegBackward0>) tensor(9964.6895, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9964.662109375
tensor(9964.6895, grad_fn=<NegBackward0>) tensor(9964.6621, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9964.6435546875
tensor(9964.6621, grad_fn=<NegBackward0>) tensor(9964.6436, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9964.6279296875
tensor(9964.6436, grad_fn=<NegBackward0>) tensor(9964.6279, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9964.6220703125
tensor(9964.6279, grad_fn=<NegBackward0>) tensor(9964.6221, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9964.6201171875
tensor(9964.6221, grad_fn=<NegBackward0>) tensor(9964.6201, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9964.619140625
tensor(9964.6201, grad_fn=<NegBackward0>) tensor(9964.6191, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9964.6181640625
tensor(9964.6191, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9964.6171875
tensor(9964.6182, grad_fn=<NegBackward0>) tensor(9964.6172, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9964.6181640625
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9964.6181640625
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9964.6181640625
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9964.6181640625
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9964.6181640625
tensor(9964.6172, grad_fn=<NegBackward0>) tensor(9964.6182, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.7822, 0.2178],
        [0.0775, 0.9225]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8707, 0.1293], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1383, 0.1519],
         [0.6801, 0.2036]],

        [[0.5138, 0.1241],
         [0.5753, 0.5865]],

        [[0.5159, 0.1024],
         [0.5367, 0.6581]],

        [[0.5368, 0.1129],
         [0.6350, 0.6787]],

        [[0.6393, 0.0895],
         [0.6486, 0.6926]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 80
Adjusted Rand Index: 0.35353535353535354
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 87
Adjusted Rand Index: 0.5430499964984674
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.30660723906548465
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.45696969696969697
Global Adjusted Rand Index: 0.2525389042313254
Average Adjusted Rand Index: 0.3314397787257749
[0.2525389042313254, 0.2525389042313254] [0.3314397787257749, 0.3314397787257749] [9964.6171875, 9964.6181640625]
-------------------------------------
This iteration is 36
True Objective function: Loss = -9999.405066951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21890.146484375
inf tensor(21890.1465, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9901.505859375
tensor(21890.1465, grad_fn=<NegBackward0>) tensor(9901.5059, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9900.693359375
tensor(9901.5059, grad_fn=<NegBackward0>) tensor(9900.6934, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9900.470703125
tensor(9900.6934, grad_fn=<NegBackward0>) tensor(9900.4707, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9900.37109375
tensor(9900.4707, grad_fn=<NegBackward0>) tensor(9900.3711, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9900.2978515625
tensor(9900.3711, grad_fn=<NegBackward0>) tensor(9900.2979, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9900.21875
tensor(9900.2979, grad_fn=<NegBackward0>) tensor(9900.2188, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9900.1171875
tensor(9900.2188, grad_fn=<NegBackward0>) tensor(9900.1172, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9899.9697265625
tensor(9900.1172, grad_fn=<NegBackward0>) tensor(9899.9697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9899.7783203125
tensor(9899.9697, grad_fn=<NegBackward0>) tensor(9899.7783, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9899.626953125
tensor(9899.7783, grad_fn=<NegBackward0>) tensor(9899.6270, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9899.5380859375
tensor(9899.6270, grad_fn=<NegBackward0>) tensor(9899.5381, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9899.48046875
tensor(9899.5381, grad_fn=<NegBackward0>) tensor(9899.4805, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9899.4345703125
tensor(9899.4805, grad_fn=<NegBackward0>) tensor(9899.4346, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9899.3955078125
tensor(9899.4346, grad_fn=<NegBackward0>) tensor(9899.3955, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9899.365234375
tensor(9899.3955, grad_fn=<NegBackward0>) tensor(9899.3652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9899.341796875
tensor(9899.3652, grad_fn=<NegBackward0>) tensor(9899.3418, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9899.32421875
tensor(9899.3418, grad_fn=<NegBackward0>) tensor(9899.3242, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9899.3134765625
tensor(9899.3242, grad_fn=<NegBackward0>) tensor(9899.3135, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9899.3046875
tensor(9899.3135, grad_fn=<NegBackward0>) tensor(9899.3047, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9899.294921875
tensor(9899.3047, grad_fn=<NegBackward0>) tensor(9899.2949, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9899.2919921875
tensor(9899.2949, grad_fn=<NegBackward0>) tensor(9899.2920, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9899.2861328125
tensor(9899.2920, grad_fn=<NegBackward0>) tensor(9899.2861, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9899.28515625
tensor(9899.2861, grad_fn=<NegBackward0>) tensor(9899.2852, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9899.2822265625
tensor(9899.2852, grad_fn=<NegBackward0>) tensor(9899.2822, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9899.28125
tensor(9899.2822, grad_fn=<NegBackward0>) tensor(9899.2812, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9899.2802734375
tensor(9899.2812, grad_fn=<NegBackward0>) tensor(9899.2803, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9899.27734375
tensor(9899.2803, grad_fn=<NegBackward0>) tensor(9899.2773, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9899.2763671875
tensor(9899.2773, grad_fn=<NegBackward0>) tensor(9899.2764, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9899.27734375
tensor(9899.2764, grad_fn=<NegBackward0>) tensor(9899.2773, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9899.27734375
tensor(9899.2764, grad_fn=<NegBackward0>) tensor(9899.2773, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -9899.275390625
tensor(9899.2764, grad_fn=<NegBackward0>) tensor(9899.2754, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9899.2744140625
tensor(9899.2754, grad_fn=<NegBackward0>) tensor(9899.2744, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9899.2734375
tensor(9899.2744, grad_fn=<NegBackward0>) tensor(9899.2734, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9899.2724609375
tensor(9899.2734, grad_fn=<NegBackward0>) tensor(9899.2725, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9899.271484375
tensor(9899.2725, grad_fn=<NegBackward0>) tensor(9899.2715, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9899.271484375
tensor(9899.2715, grad_fn=<NegBackward0>) tensor(9899.2715, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9899.26953125
tensor(9899.2715, grad_fn=<NegBackward0>) tensor(9899.2695, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9899.26171875
tensor(9899.2695, grad_fn=<NegBackward0>) tensor(9899.2617, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9899.1708984375
tensor(9899.2617, grad_fn=<NegBackward0>) tensor(9899.1709, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9897.48828125
tensor(9899.1709, grad_fn=<NegBackward0>) tensor(9897.4883, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9896.5791015625
tensor(9897.4883, grad_fn=<NegBackward0>) tensor(9896.5791, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9896.5029296875
tensor(9896.5791, grad_fn=<NegBackward0>) tensor(9896.5029, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9896.486328125
tensor(9896.5029, grad_fn=<NegBackward0>) tensor(9896.4863, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9896.478515625
tensor(9896.4863, grad_fn=<NegBackward0>) tensor(9896.4785, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9896.474609375
tensor(9896.4785, grad_fn=<NegBackward0>) tensor(9896.4746, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9896.4736328125
tensor(9896.4746, grad_fn=<NegBackward0>) tensor(9896.4736, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9896.470703125
tensor(9896.4736, grad_fn=<NegBackward0>) tensor(9896.4707, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9896.4677734375
tensor(9896.4707, grad_fn=<NegBackward0>) tensor(9896.4678, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9896.4677734375
tensor(9896.4678, grad_fn=<NegBackward0>) tensor(9896.4678, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9896.4677734375
tensor(9896.4678, grad_fn=<NegBackward0>) tensor(9896.4678, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9896.4658203125
tensor(9896.4678, grad_fn=<NegBackward0>) tensor(9896.4658, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9896.4658203125
tensor(9896.4658, grad_fn=<NegBackward0>) tensor(9896.4658, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9896.46484375
tensor(9896.4658, grad_fn=<NegBackward0>) tensor(9896.4648, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9896.46484375
tensor(9896.4648, grad_fn=<NegBackward0>) tensor(9896.4648, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9896.4638671875
tensor(9896.4648, grad_fn=<NegBackward0>) tensor(9896.4639, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9896.4658203125
tensor(9896.4639, grad_fn=<NegBackward0>) tensor(9896.4658, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9896.4638671875
tensor(9896.4639, grad_fn=<NegBackward0>) tensor(9896.4639, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9896.4638671875
tensor(9896.4639, grad_fn=<NegBackward0>) tensor(9896.4639, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9896.4638671875
tensor(9896.4639, grad_fn=<NegBackward0>) tensor(9896.4639, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9896.462890625
tensor(9896.4639, grad_fn=<NegBackward0>) tensor(9896.4629, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9896.462890625
tensor(9896.4629, grad_fn=<NegBackward0>) tensor(9896.4629, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9896.462890625
tensor(9896.4629, grad_fn=<NegBackward0>) tensor(9896.4629, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9896.462890625
tensor(9896.4629, grad_fn=<NegBackward0>) tensor(9896.4629, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9896.4619140625
tensor(9896.4629, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9896.462890625
tensor(9896.4619, grad_fn=<NegBackward0>) tensor(9896.4629, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9896.4609375
tensor(9896.4619, grad_fn=<NegBackward0>) tensor(9896.4609, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9896.4619140625
tensor(9896.4609, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9896.4619140625
tensor(9896.4609, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9896.4619140625
tensor(9896.4609, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -9896.4619140625
tensor(9896.4609, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -9896.4619140625
tensor(9896.4609, grad_fn=<NegBackward0>) tensor(9896.4619, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[1.9166e-04, 9.9981e-01],
        [1.2396e-02, 9.8760e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0739, 0.9261], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2126, 0.1648],
         [0.5909, 0.1368]],

        [[0.5056, 0.1704],
         [0.6478, 0.6706]],

        [[0.7191, 0.0593],
         [0.5369, 0.6996]],

        [[0.5545, 0.1976],
         [0.6885, 0.5971]],

        [[0.6401, 0.0203],
         [0.5247, 0.6419]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.00042551643232125797
Average Adjusted Rand Index: 0.0004967735997764998
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22067.578125
inf tensor(22067.5781, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9901.763671875
tensor(22067.5781, grad_fn=<NegBackward0>) tensor(9901.7637, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9900.77734375
tensor(9901.7637, grad_fn=<NegBackward0>) tensor(9900.7773, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9900.4912109375
tensor(9900.7773, grad_fn=<NegBackward0>) tensor(9900.4912, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9900.240234375
tensor(9900.4912, grad_fn=<NegBackward0>) tensor(9900.2402, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9900.0673828125
tensor(9900.2402, grad_fn=<NegBackward0>) tensor(9900.0674, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9899.9775390625
tensor(9900.0674, grad_fn=<NegBackward0>) tensor(9899.9775, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9899.8603515625
tensor(9899.9775, grad_fn=<NegBackward0>) tensor(9899.8604, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9899.6572265625
tensor(9899.8604, grad_fn=<NegBackward0>) tensor(9899.6572, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9899.384765625
tensor(9899.6572, grad_fn=<NegBackward0>) tensor(9899.3848, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9899.205078125
tensor(9899.3848, grad_fn=<NegBackward0>) tensor(9899.2051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9899.1005859375
tensor(9899.2051, grad_fn=<NegBackward0>) tensor(9899.1006, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9899.03125
tensor(9899.1006, grad_fn=<NegBackward0>) tensor(9899.0312, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9898.982421875
tensor(9899.0312, grad_fn=<NegBackward0>) tensor(9898.9824, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9898.947265625
tensor(9898.9824, grad_fn=<NegBackward0>) tensor(9898.9473, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9898.921875
tensor(9898.9473, grad_fn=<NegBackward0>) tensor(9898.9219, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9898.9072265625
tensor(9898.9219, grad_fn=<NegBackward0>) tensor(9898.9072, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9898.8984375
tensor(9898.9072, grad_fn=<NegBackward0>) tensor(9898.8984, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9898.892578125
tensor(9898.8984, grad_fn=<NegBackward0>) tensor(9898.8926, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9898.8876953125
tensor(9898.8926, grad_fn=<NegBackward0>) tensor(9898.8877, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9898.8857421875
tensor(9898.8877, grad_fn=<NegBackward0>) tensor(9898.8857, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9898.8837890625
tensor(9898.8857, grad_fn=<NegBackward0>) tensor(9898.8838, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9898.8837890625
tensor(9898.8838, grad_fn=<NegBackward0>) tensor(9898.8838, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9898.8828125
tensor(9898.8838, grad_fn=<NegBackward0>) tensor(9898.8828, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9898.8837890625
tensor(9898.8828, grad_fn=<NegBackward0>) tensor(9898.8838, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -9898.8828125
tensor(9898.8828, grad_fn=<NegBackward0>) tensor(9898.8828, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9898.8828125
tensor(9898.8828, grad_fn=<NegBackward0>) tensor(9898.8828, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9898.8818359375
tensor(9898.8828, grad_fn=<NegBackward0>) tensor(9898.8818, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9898.8818359375
tensor(9898.8818, grad_fn=<NegBackward0>) tensor(9898.8818, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9898.8818359375
tensor(9898.8818, grad_fn=<NegBackward0>) tensor(9898.8818, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9898.8818359375
tensor(9898.8818, grad_fn=<NegBackward0>) tensor(9898.8818, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9898.880859375
tensor(9898.8818, grad_fn=<NegBackward0>) tensor(9898.8809, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9898.880859375
tensor(9898.8809, grad_fn=<NegBackward0>) tensor(9898.8809, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9898.8798828125
tensor(9898.8809, grad_fn=<NegBackward0>) tensor(9898.8799, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9898.8779296875
tensor(9898.8799, grad_fn=<NegBackward0>) tensor(9898.8779, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9898.875
tensor(9898.8779, grad_fn=<NegBackward0>) tensor(9898.8750, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9898.8544921875
tensor(9898.8750, grad_fn=<NegBackward0>) tensor(9898.8545, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9898.599609375
tensor(9898.8545, grad_fn=<NegBackward0>) tensor(9898.5996, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9897.966796875
tensor(9898.5996, grad_fn=<NegBackward0>) tensor(9897.9668, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9897.6982421875
tensor(9897.9668, grad_fn=<NegBackward0>) tensor(9897.6982, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9897.44140625
tensor(9897.6982, grad_fn=<NegBackward0>) tensor(9897.4414, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9895.5869140625
tensor(9897.4414, grad_fn=<NegBackward0>) tensor(9895.5869, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9895.416015625
tensor(9895.5869, grad_fn=<NegBackward0>) tensor(9895.4160, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9895.3876953125
tensor(9895.4160, grad_fn=<NegBackward0>) tensor(9895.3877, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9895.3740234375
tensor(9895.3877, grad_fn=<NegBackward0>) tensor(9895.3740, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9895.365234375
tensor(9895.3740, grad_fn=<NegBackward0>) tensor(9895.3652, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9895.359375
tensor(9895.3652, grad_fn=<NegBackward0>) tensor(9895.3594, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9895.3544921875
tensor(9895.3594, grad_fn=<NegBackward0>) tensor(9895.3545, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9895.3515625
tensor(9895.3545, grad_fn=<NegBackward0>) tensor(9895.3516, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9895.349609375
tensor(9895.3516, grad_fn=<NegBackward0>) tensor(9895.3496, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9895.3486328125
tensor(9895.3496, grad_fn=<NegBackward0>) tensor(9895.3486, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9895.345703125
tensor(9895.3486, grad_fn=<NegBackward0>) tensor(9895.3457, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9895.34375
tensor(9895.3457, grad_fn=<NegBackward0>) tensor(9895.3438, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9895.3427734375
tensor(9895.3438, grad_fn=<NegBackward0>) tensor(9895.3428, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9895.3427734375
tensor(9895.3428, grad_fn=<NegBackward0>) tensor(9895.3428, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9895.33984375
tensor(9895.3428, grad_fn=<NegBackward0>) tensor(9895.3398, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9895.341796875
tensor(9895.3398, grad_fn=<NegBackward0>) tensor(9895.3418, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9895.3388671875
tensor(9895.3398, grad_fn=<NegBackward0>) tensor(9895.3389, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9895.3388671875
tensor(9895.3389, grad_fn=<NegBackward0>) tensor(9895.3389, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9895.3388671875
tensor(9895.3389, grad_fn=<NegBackward0>) tensor(9895.3389, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9895.337890625
tensor(9895.3389, grad_fn=<NegBackward0>) tensor(9895.3379, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9895.3388671875
tensor(9895.3379, grad_fn=<NegBackward0>) tensor(9895.3389, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9895.3359375
tensor(9895.3379, grad_fn=<NegBackward0>) tensor(9895.3359, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9895.3359375
tensor(9895.3359, grad_fn=<NegBackward0>) tensor(9895.3359, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9895.3359375
tensor(9895.3359, grad_fn=<NegBackward0>) tensor(9895.3359, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9895.33203125
tensor(9895.3359, grad_fn=<NegBackward0>) tensor(9895.3320, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9895.3310546875
tensor(9895.3320, grad_fn=<NegBackward0>) tensor(9895.3311, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9895.330078125
tensor(9895.3311, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9895.330078125
tensor(9895.3301, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9895.3291015625
tensor(9895.3301, grad_fn=<NegBackward0>) tensor(9895.3291, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9895.3291015625
tensor(9895.3291, grad_fn=<NegBackward0>) tensor(9895.3291, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9895.330078125
tensor(9895.3291, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9895.330078125
tensor(9895.3291, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9895.328125
tensor(9895.3291, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9895.330078125
tensor(9895.3281, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9895.330078125
tensor(9895.3281, grad_fn=<NegBackward0>) tensor(9895.3301, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -9895.3291015625
tensor(9895.3281, grad_fn=<NegBackward0>) tensor(9895.3291, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -9895.3291015625
tensor(9895.3281, grad_fn=<NegBackward0>) tensor(9895.3291, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -9895.3271484375
tensor(9895.3281, grad_fn=<NegBackward0>) tensor(9895.3271, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9895.328125
tensor(9895.3271, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9895.328125
tensor(9895.3271, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9895.328125
tensor(9895.3271, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -9895.328125
tensor(9895.3271, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -9895.328125
tensor(9895.3271, grad_fn=<NegBackward0>) tensor(9895.3281, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[9.9983e-01, 1.6666e-04],
        [2.5096e-02, 9.7490e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0163, 0.9837], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3754, 0.1878],
         [0.6309, 0.1345]],

        [[0.5127, 0.0714],
         [0.5093, 0.6740]],

        [[0.5657, 0.1561],
         [0.6401, 0.6688]],

        [[0.5784, 0.1665],
         [0.5042, 0.6832]],

        [[0.6335, 0.1494],
         [0.5372, 0.6335]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006214218236452756
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.022102326722050213
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.022102326722050213
Global Adjusted Rand Index: 0.0048725034285101694
Average Adjusted Rand Index: 0.009040896636715447
[0.00042551643232125797, 0.0048725034285101694] [0.0004967735997764998, 0.009040896636715447] [9896.4619140625, 9895.328125]
-------------------------------------
This iteration is 37
True Objective function: Loss = -9994.816384478168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24143.521484375
inf tensor(24143.5215, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9896.4111328125
tensor(24143.5215, grad_fn=<NegBackward0>) tensor(9896.4111, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9893.73828125
tensor(9896.4111, grad_fn=<NegBackward0>) tensor(9893.7383, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9893.2265625
tensor(9893.7383, grad_fn=<NegBackward0>) tensor(9893.2266, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9893.08203125
tensor(9893.2266, grad_fn=<NegBackward0>) tensor(9893.0820, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9893.01171875
tensor(9893.0820, grad_fn=<NegBackward0>) tensor(9893.0117, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9892.9638671875
tensor(9893.0117, grad_fn=<NegBackward0>) tensor(9892.9639, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9892.9169921875
tensor(9892.9639, grad_fn=<NegBackward0>) tensor(9892.9170, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9892.8671875
tensor(9892.9170, grad_fn=<NegBackward0>) tensor(9892.8672, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9892.8115234375
tensor(9892.8672, grad_fn=<NegBackward0>) tensor(9892.8115, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9892.751953125
tensor(9892.8115, grad_fn=<NegBackward0>) tensor(9892.7520, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9892.6845703125
tensor(9892.7520, grad_fn=<NegBackward0>) tensor(9892.6846, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9892.6123046875
tensor(9892.6846, grad_fn=<NegBackward0>) tensor(9892.6123, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9892.5302734375
tensor(9892.6123, grad_fn=<NegBackward0>) tensor(9892.5303, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9892.4365234375
tensor(9892.5303, grad_fn=<NegBackward0>) tensor(9892.4365, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9892.3173828125
tensor(9892.4365, grad_fn=<NegBackward0>) tensor(9892.3174, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9892.169921875
tensor(9892.3174, grad_fn=<NegBackward0>) tensor(9892.1699, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9891.99609375
tensor(9892.1699, grad_fn=<NegBackward0>) tensor(9891.9961, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9891.7900390625
tensor(9891.9961, grad_fn=<NegBackward0>) tensor(9891.7900, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9891.5712890625
tensor(9891.7900, grad_fn=<NegBackward0>) tensor(9891.5713, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9891.25
tensor(9891.5713, grad_fn=<NegBackward0>) tensor(9891.2500, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9890.3583984375
tensor(9891.2500, grad_fn=<NegBackward0>) tensor(9890.3584, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9889.0732421875
tensor(9890.3584, grad_fn=<NegBackward0>) tensor(9889.0732, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9888.5048828125
tensor(9889.0732, grad_fn=<NegBackward0>) tensor(9888.5049, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9888.1865234375
tensor(9888.5049, grad_fn=<NegBackward0>) tensor(9888.1865, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9888.0126953125
tensor(9888.1865, grad_fn=<NegBackward0>) tensor(9888.0127, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9887.90234375
tensor(9888.0127, grad_fn=<NegBackward0>) tensor(9887.9023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9887.8193359375
tensor(9887.9023, grad_fn=<NegBackward0>) tensor(9887.8193, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9887.7529296875
tensor(9887.8193, grad_fn=<NegBackward0>) tensor(9887.7529, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9887.7138671875
tensor(9887.7529, grad_fn=<NegBackward0>) tensor(9887.7139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9887.685546875
tensor(9887.7139, grad_fn=<NegBackward0>) tensor(9887.6855, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9887.6611328125
tensor(9887.6855, grad_fn=<NegBackward0>) tensor(9887.6611, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9887.640625
tensor(9887.6611, grad_fn=<NegBackward0>) tensor(9887.6406, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9887.6220703125
tensor(9887.6406, grad_fn=<NegBackward0>) tensor(9887.6221, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9887.6044921875
tensor(9887.6221, grad_fn=<NegBackward0>) tensor(9887.6045, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9887.591796875
tensor(9887.6045, grad_fn=<NegBackward0>) tensor(9887.5918, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9887.58203125
tensor(9887.5918, grad_fn=<NegBackward0>) tensor(9887.5820, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9887.572265625
tensor(9887.5820, grad_fn=<NegBackward0>) tensor(9887.5723, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9887.5625
tensor(9887.5723, grad_fn=<NegBackward0>) tensor(9887.5625, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9887.5546875
tensor(9887.5625, grad_fn=<NegBackward0>) tensor(9887.5547, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9887.5498046875
tensor(9887.5547, grad_fn=<NegBackward0>) tensor(9887.5498, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9887.54296875
tensor(9887.5498, grad_fn=<NegBackward0>) tensor(9887.5430, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9887.537109375
tensor(9887.5430, grad_fn=<NegBackward0>) tensor(9887.5371, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9887.533203125
tensor(9887.5371, grad_fn=<NegBackward0>) tensor(9887.5332, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9887.529296875
tensor(9887.5332, grad_fn=<NegBackward0>) tensor(9887.5293, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9887.525390625
tensor(9887.5293, grad_fn=<NegBackward0>) tensor(9887.5254, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9887.521484375
tensor(9887.5254, grad_fn=<NegBackward0>) tensor(9887.5215, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9887.51953125
tensor(9887.5215, grad_fn=<NegBackward0>) tensor(9887.5195, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9887.515625
tensor(9887.5195, grad_fn=<NegBackward0>) tensor(9887.5156, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9887.513671875
tensor(9887.5156, grad_fn=<NegBackward0>) tensor(9887.5137, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9887.51171875
tensor(9887.5137, grad_fn=<NegBackward0>) tensor(9887.5117, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9887.509765625
tensor(9887.5117, grad_fn=<NegBackward0>) tensor(9887.5098, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9887.5068359375
tensor(9887.5098, grad_fn=<NegBackward0>) tensor(9887.5068, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9887.505859375
tensor(9887.5068, grad_fn=<NegBackward0>) tensor(9887.5059, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9887.5048828125
tensor(9887.5059, grad_fn=<NegBackward0>) tensor(9887.5049, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9887.50390625
tensor(9887.5049, grad_fn=<NegBackward0>) tensor(9887.5039, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9887.501953125
tensor(9887.5039, grad_fn=<NegBackward0>) tensor(9887.5020, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9887.5
tensor(9887.5020, grad_fn=<NegBackward0>) tensor(9887.5000, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9887.5
tensor(9887.5000, grad_fn=<NegBackward0>) tensor(9887.5000, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9887.4990234375
tensor(9887.5000, grad_fn=<NegBackward0>) tensor(9887.4990, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9887.4951171875
tensor(9887.4990, grad_fn=<NegBackward0>) tensor(9887.4951, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9887.4951171875
tensor(9887.4951, grad_fn=<NegBackward0>) tensor(9887.4951, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9887.4951171875
tensor(9887.4951, grad_fn=<NegBackward0>) tensor(9887.4951, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9887.4951171875
tensor(9887.4951, grad_fn=<NegBackward0>) tensor(9887.4951, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9887.4931640625
tensor(9887.4951, grad_fn=<NegBackward0>) tensor(9887.4932, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9887.4931640625
tensor(9887.4932, grad_fn=<NegBackward0>) tensor(9887.4932, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9887.4912109375
tensor(9887.4932, grad_fn=<NegBackward0>) tensor(9887.4912, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9887.4912109375
tensor(9887.4912, grad_fn=<NegBackward0>) tensor(9887.4912, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9887.4892578125
tensor(9887.4912, grad_fn=<NegBackward0>) tensor(9887.4893, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9887.490234375
tensor(9887.4893, grad_fn=<NegBackward0>) tensor(9887.4902, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9887.490234375
tensor(9887.4893, grad_fn=<NegBackward0>) tensor(9887.4902, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9887.4892578125
tensor(9887.4893, grad_fn=<NegBackward0>) tensor(9887.4893, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9887.48828125
tensor(9887.4893, grad_fn=<NegBackward0>) tensor(9887.4883, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9887.4873046875
tensor(9887.4883, grad_fn=<NegBackward0>) tensor(9887.4873, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9887.48828125
tensor(9887.4873, grad_fn=<NegBackward0>) tensor(9887.4883, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9887.48828125
tensor(9887.4873, grad_fn=<NegBackward0>) tensor(9887.4883, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -9887.48828125
tensor(9887.4873, grad_fn=<NegBackward0>) tensor(9887.4883, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -9887.486328125
tensor(9887.4873, grad_fn=<NegBackward0>) tensor(9887.4863, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9887.486328125
tensor(9887.4863, grad_fn=<NegBackward0>) tensor(9887.4863, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9887.4853515625
tensor(9887.4863, grad_fn=<NegBackward0>) tensor(9887.4854, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9887.486328125
tensor(9887.4854, grad_fn=<NegBackward0>) tensor(9887.4863, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9887.5009765625
tensor(9887.4854, grad_fn=<NegBackward0>) tensor(9887.5010, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9887.486328125
tensor(9887.4854, grad_fn=<NegBackward0>) tensor(9887.4863, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9887.4853515625
tensor(9887.4854, grad_fn=<NegBackward0>) tensor(9887.4854, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9887.484375
tensor(9887.4854, grad_fn=<NegBackward0>) tensor(9887.4844, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9887.4921875
tensor(9887.4844, grad_fn=<NegBackward0>) tensor(9887.4922, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9887.4833984375
tensor(9887.4844, grad_fn=<NegBackward0>) tensor(9887.4834, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9887.484375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4844, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9887.484375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4844, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -9887.4853515625
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4854, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -9887.560546875
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.5605, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -9887.4833984375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4834, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9887.4833984375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4834, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9887.484375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4844, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9887.484375
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4844, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9887.6669921875
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.6670, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -9887.482421875
tensor(9887.4834, grad_fn=<NegBackward0>) tensor(9887.4824, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9887.4833984375
tensor(9887.4824, grad_fn=<NegBackward0>) tensor(9887.4834, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9887.482421875
tensor(9887.4824, grad_fn=<NegBackward0>) tensor(9887.4824, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9887.4814453125
tensor(9887.4824, grad_fn=<NegBackward0>) tensor(9887.4814, grad_fn=<NegBackward0>)
pi: tensor([[9.9999e-01, 9.8921e-06],
        [1.7561e-01, 8.2439e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8856, 0.1144], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1330, 0.1547],
         [0.6204, 0.2425]],

        [[0.6518, 0.1684],
         [0.6141, 0.6579]],

        [[0.5032, 0.1116],
         [0.5251, 0.6810]],

        [[0.6502, 0.1535],
         [0.6256, 0.6620]],

        [[0.5160, 0.2149],
         [0.5944, 0.6615]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.01150857271232653
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0028959952356207414
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.015253231171422693
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006438680677883739
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0015221131464426677
Global Adjusted Rand Index: 0.0067007556153294715
Average Adjusted Rand Index: 0.005498363581027784
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24804.158203125
inf tensor(24804.1582, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9895.7734375
tensor(24804.1582, grad_fn=<NegBackward0>) tensor(9895.7734, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9893.76171875
tensor(9895.7734, grad_fn=<NegBackward0>) tensor(9893.7617, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9893.306640625
tensor(9893.7617, grad_fn=<NegBackward0>) tensor(9893.3066, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9893.162109375
tensor(9893.3066, grad_fn=<NegBackward0>) tensor(9893.1621, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9893.099609375
tensor(9893.1621, grad_fn=<NegBackward0>) tensor(9893.0996, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9893.0625
tensor(9893.0996, grad_fn=<NegBackward0>) tensor(9893.0625, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9893.029296875
tensor(9893.0625, grad_fn=<NegBackward0>) tensor(9893.0293, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9892.9951171875
tensor(9893.0293, grad_fn=<NegBackward0>) tensor(9892.9951, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9892.951171875
tensor(9892.9951, grad_fn=<NegBackward0>) tensor(9892.9512, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9892.8857421875
tensor(9892.9512, grad_fn=<NegBackward0>) tensor(9892.8857, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9892.7529296875
tensor(9892.8857, grad_fn=<NegBackward0>) tensor(9892.7529, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9892.48046875
tensor(9892.7529, grad_fn=<NegBackward0>) tensor(9892.4805, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9892.1796875
tensor(9892.4805, grad_fn=<NegBackward0>) tensor(9892.1797, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9891.9814453125
tensor(9892.1797, grad_fn=<NegBackward0>) tensor(9891.9814, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9891.8125
tensor(9891.9814, grad_fn=<NegBackward0>) tensor(9891.8125, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9891.6650390625
tensor(9891.8125, grad_fn=<NegBackward0>) tensor(9891.6650, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9891.53125
tensor(9891.6650, grad_fn=<NegBackward0>) tensor(9891.5312, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9891.3994140625
tensor(9891.5312, grad_fn=<NegBackward0>) tensor(9891.3994, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9891.2607421875
tensor(9891.3994, grad_fn=<NegBackward0>) tensor(9891.2607, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9891.1474609375
tensor(9891.2607, grad_fn=<NegBackward0>) tensor(9891.1475, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9891.0419921875
tensor(9891.1475, grad_fn=<NegBackward0>) tensor(9891.0420, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9890.9453125
tensor(9891.0420, grad_fn=<NegBackward0>) tensor(9890.9453, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9890.8701171875
tensor(9890.9453, grad_fn=<NegBackward0>) tensor(9890.8701, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9890.794921875
tensor(9890.8701, grad_fn=<NegBackward0>) tensor(9890.7949, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9890.6875
tensor(9890.7949, grad_fn=<NegBackward0>) tensor(9890.6875, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9890.5576171875
tensor(9890.6875, grad_fn=<NegBackward0>) tensor(9890.5576, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9890.3505859375
tensor(9890.5576, grad_fn=<NegBackward0>) tensor(9890.3506, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9890.0478515625
tensor(9890.3506, grad_fn=<NegBackward0>) tensor(9890.0479, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9889.58984375
tensor(9890.0479, grad_fn=<NegBackward0>) tensor(9889.5898, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9889.3837890625
tensor(9889.5898, grad_fn=<NegBackward0>) tensor(9889.3838, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9889.2978515625
tensor(9889.3838, grad_fn=<NegBackward0>) tensor(9889.2979, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9889.2265625
tensor(9889.2979, grad_fn=<NegBackward0>) tensor(9889.2266, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9884.38671875
tensor(9889.2266, grad_fn=<NegBackward0>) tensor(9884.3867, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9881.1044921875
tensor(9884.3867, grad_fn=<NegBackward0>) tensor(9881.1045, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9878.4013671875
tensor(9881.1045, grad_fn=<NegBackward0>) tensor(9878.4014, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9877.318359375
tensor(9878.4014, grad_fn=<NegBackward0>) tensor(9877.3184, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9877.1923828125
tensor(9877.3184, grad_fn=<NegBackward0>) tensor(9877.1924, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9877.1337890625
tensor(9877.1924, grad_fn=<NegBackward0>) tensor(9877.1338, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9877.103515625
tensor(9877.1338, grad_fn=<NegBackward0>) tensor(9877.1035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9877.087890625
tensor(9877.1035, grad_fn=<NegBackward0>) tensor(9877.0879, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9877.0703125
tensor(9877.0879, grad_fn=<NegBackward0>) tensor(9877.0703, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9877.060546875
tensor(9877.0703, grad_fn=<NegBackward0>) tensor(9877.0605, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9877.052734375
tensor(9877.0605, grad_fn=<NegBackward0>) tensor(9877.0527, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9877.0458984375
tensor(9877.0527, grad_fn=<NegBackward0>) tensor(9877.0459, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9877.041015625
tensor(9877.0459, grad_fn=<NegBackward0>) tensor(9877.0410, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9877.0361328125
tensor(9877.0410, grad_fn=<NegBackward0>) tensor(9877.0361, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9877.033203125
tensor(9877.0361, grad_fn=<NegBackward0>) tensor(9877.0332, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9877.0302734375
tensor(9877.0332, grad_fn=<NegBackward0>) tensor(9877.0303, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9877.0283203125
tensor(9877.0303, grad_fn=<NegBackward0>) tensor(9877.0283, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9877.025390625
tensor(9877.0283, grad_fn=<NegBackward0>) tensor(9877.0254, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9877.0244140625
tensor(9877.0254, grad_fn=<NegBackward0>) tensor(9877.0244, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9877.021484375
tensor(9877.0244, grad_fn=<NegBackward0>) tensor(9877.0215, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9877.0205078125
tensor(9877.0215, grad_fn=<NegBackward0>) tensor(9877.0205, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9877.0185546875
tensor(9877.0205, grad_fn=<NegBackward0>) tensor(9877.0186, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9877.0166015625
tensor(9877.0186, grad_fn=<NegBackward0>) tensor(9877.0166, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9877.015625
tensor(9877.0166, grad_fn=<NegBackward0>) tensor(9877.0156, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9877.015625
tensor(9877.0156, grad_fn=<NegBackward0>) tensor(9877.0156, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9877.0146484375
tensor(9877.0156, grad_fn=<NegBackward0>) tensor(9877.0146, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9877.0126953125
tensor(9877.0146, grad_fn=<NegBackward0>) tensor(9877.0127, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9877.0126953125
tensor(9877.0127, grad_fn=<NegBackward0>) tensor(9877.0127, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9877.01171875
tensor(9877.0127, grad_fn=<NegBackward0>) tensor(9877.0117, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9877.01171875
tensor(9877.0117, grad_fn=<NegBackward0>) tensor(9877.0117, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9877.0107421875
tensor(9877.0117, grad_fn=<NegBackward0>) tensor(9877.0107, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9877.009765625
tensor(9877.0107, grad_fn=<NegBackward0>) tensor(9877.0098, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9877.009765625
tensor(9877.0098, grad_fn=<NegBackward0>) tensor(9877.0098, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9877.009765625
tensor(9877.0098, grad_fn=<NegBackward0>) tensor(9877.0098, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9877.0126953125
tensor(9877.0098, grad_fn=<NegBackward0>) tensor(9877.0127, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9877.0068359375
tensor(9877.0098, grad_fn=<NegBackward0>) tensor(9877.0068, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9877.0087890625
tensor(9877.0068, grad_fn=<NegBackward0>) tensor(9877.0088, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9877.0068359375
tensor(9877.0068, grad_fn=<NegBackward0>) tensor(9877.0068, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9877.005859375
tensor(9877.0068, grad_fn=<NegBackward0>) tensor(9877.0059, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9877.005859375
tensor(9877.0059, grad_fn=<NegBackward0>) tensor(9877.0059, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9877.0068359375
tensor(9877.0059, grad_fn=<NegBackward0>) tensor(9877.0068, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9877.0048828125
tensor(9877.0059, grad_fn=<NegBackward0>) tensor(9877.0049, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9877.005859375
tensor(9877.0049, grad_fn=<NegBackward0>) tensor(9877.0059, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9877.0048828125
tensor(9877.0049, grad_fn=<NegBackward0>) tensor(9877.0049, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9877.0048828125
tensor(9877.0049, grad_fn=<NegBackward0>) tensor(9877.0049, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9877.0048828125
tensor(9877.0049, grad_fn=<NegBackward0>) tensor(9877.0049, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9877.0029296875
tensor(9877.0049, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9877.0029296875
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9877.0029296875
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9877.00390625
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0039, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9877.0029296875
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9877.0029296875
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9877.0029296875
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9877.001953125
tensor(9877.0029, grad_fn=<NegBackward0>) tensor(9877.0020, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9877.0029296875
tensor(9877.0020, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9877.0029296875
tensor(9877.0020, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -9877.0009765625
tensor(9877.0020, grad_fn=<NegBackward0>) tensor(9877.0010, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9877.001953125
tensor(9877.0010, grad_fn=<NegBackward0>) tensor(9877.0020, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9877.0029296875
tensor(9877.0010, grad_fn=<NegBackward0>) tensor(9877.0029, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -9877.001953125
tensor(9877.0010, grad_fn=<NegBackward0>) tensor(9877.0020, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -9877.001953125
tensor(9877.0010, grad_fn=<NegBackward0>) tensor(9877.0020, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -9877.0048828125
tensor(9877.0010, grad_fn=<NegBackward0>) tensor(9877.0049, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[8.9128e-01, 1.0872e-01],
        [1.0317e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4612, 0.5388], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2075, 0.1053],
         [0.5564, 0.1426]],

        [[0.6010, 0.1191],
         [0.5041, 0.6000]],

        [[0.6681, 0.1062],
         [0.5328, 0.6828]],

        [[0.7032, 0.1217],
         [0.6021, 0.5864]],

        [[0.6827, 0.1065],
         [0.6758, 0.6981]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 24
Adjusted Rand Index: 0.263030303030303
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 26
Adjusted Rand Index: 0.22282608695652173
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 26
Adjusted Rand Index: 0.22182135865784408
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 29
Adjusted Rand Index: 0.16905423802272404
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 25
Adjusted Rand Index: 0.24346167483829947
Global Adjusted Rand Index: 0.22892754367025955
Average Adjusted Rand Index: 0.22403873230113844
[0.0067007556153294715, 0.22892754367025955] [0.005498363581027784, 0.22403873230113844] [9887.4853515625, 9877.0048828125]
-------------------------------------
This iteration is 38
True Objective function: Loss = -10069.408119781672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22309.119140625
inf tensor(22309.1191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9977.923828125
tensor(22309.1191, grad_fn=<NegBackward0>) tensor(9977.9238, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9977.337890625
tensor(9977.9238, grad_fn=<NegBackward0>) tensor(9977.3379, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9977.08203125
tensor(9977.3379, grad_fn=<NegBackward0>) tensor(9977.0820, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9976.7529296875
tensor(9977.0820, grad_fn=<NegBackward0>) tensor(9976.7529, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9976.3505859375
tensor(9976.7529, grad_fn=<NegBackward0>) tensor(9976.3506, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9975.884765625
tensor(9976.3506, grad_fn=<NegBackward0>) tensor(9975.8848, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9975.44140625
tensor(9975.8848, grad_fn=<NegBackward0>) tensor(9975.4414, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9974.9765625
tensor(9975.4414, grad_fn=<NegBackward0>) tensor(9974.9766, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9974.40234375
tensor(9974.9766, grad_fn=<NegBackward0>) tensor(9974.4023, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9973.5576171875
tensor(9974.4023, grad_fn=<NegBackward0>) tensor(9973.5576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9972.76171875
tensor(9973.5576, grad_fn=<NegBackward0>) tensor(9972.7617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9972.314453125
tensor(9972.7617, grad_fn=<NegBackward0>) tensor(9972.3145, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9972.017578125
tensor(9972.3145, grad_fn=<NegBackward0>) tensor(9972.0176, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9971.83984375
tensor(9972.0176, grad_fn=<NegBackward0>) tensor(9971.8398, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9971.7373046875
tensor(9971.8398, grad_fn=<NegBackward0>) tensor(9971.7373, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9971.68359375
tensor(9971.7373, grad_fn=<NegBackward0>) tensor(9971.6836, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9971.654296875
tensor(9971.6836, grad_fn=<NegBackward0>) tensor(9971.6543, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9971.634765625
tensor(9971.6543, grad_fn=<NegBackward0>) tensor(9971.6348, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9971.62109375
tensor(9971.6348, grad_fn=<NegBackward0>) tensor(9971.6211, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9971.6123046875
tensor(9971.6211, grad_fn=<NegBackward0>) tensor(9971.6123, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9971.603515625
tensor(9971.6123, grad_fn=<NegBackward0>) tensor(9971.6035, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9971.599609375
tensor(9971.6035, grad_fn=<NegBackward0>) tensor(9971.5996, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9971.59765625
tensor(9971.5996, grad_fn=<NegBackward0>) tensor(9971.5977, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9971.5966796875
tensor(9971.5977, grad_fn=<NegBackward0>) tensor(9971.5967, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9971.5947265625
tensor(9971.5967, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9971.59375
tensor(9971.5947, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9971.5927734375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
2
Iteration 3000: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
3
Iteration 3100: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
4
Iteration 3200: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9971.5947265625
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -9971.5947265625
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4000 due to no improvement.
pi: tensor([[0.8860, 0.1140],
        [0.8345, 0.1655]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6327, 0.3673], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1269, 0.1526],
         [0.5794, 0.1831]],

        [[0.6645, 0.1516],
         [0.5604, 0.6235]],

        [[0.5682, 0.1675],
         [0.6868, 0.5224]],

        [[0.6375, 0.1590],
         [0.5287, 0.5934]],

        [[0.6763, 0.2059],
         [0.6607, 0.5300]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.005259442857121893
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0007830659075602383
Global Adjusted Rand Index: 0.0009016332689831364
Average Adjusted Rand Index: -0.0021051497292841324
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20705.162109375
inf tensor(20705.1621, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9977.515625
tensor(20705.1621, grad_fn=<NegBackward0>) tensor(9977.5156, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9976.37890625
tensor(9977.5156, grad_fn=<NegBackward0>) tensor(9976.3789, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9975.2900390625
tensor(9976.3789, grad_fn=<NegBackward0>) tensor(9975.2900, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9974.3818359375
tensor(9975.2900, grad_fn=<NegBackward0>) tensor(9974.3818, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9973.5595703125
tensor(9974.3818, grad_fn=<NegBackward0>) tensor(9973.5596, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9973.0556640625
tensor(9973.5596, grad_fn=<NegBackward0>) tensor(9973.0557, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9972.7021484375
tensor(9973.0557, grad_fn=<NegBackward0>) tensor(9972.7021, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9972.427734375
tensor(9972.7021, grad_fn=<NegBackward0>) tensor(9972.4277, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9972.2373046875
tensor(9972.4277, grad_fn=<NegBackward0>) tensor(9972.2373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9972.1650390625
tensor(9972.2373, grad_fn=<NegBackward0>) tensor(9972.1650, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9972.1279296875
tensor(9972.1650, grad_fn=<NegBackward0>) tensor(9972.1279, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9972.0986328125
tensor(9972.1279, grad_fn=<NegBackward0>) tensor(9972.0986, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9972.0673828125
tensor(9972.0986, grad_fn=<NegBackward0>) tensor(9972.0674, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9972.037109375
tensor(9972.0674, grad_fn=<NegBackward0>) tensor(9972.0371, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9972.0009765625
tensor(9972.0371, grad_fn=<NegBackward0>) tensor(9972.0010, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9971.9580078125
tensor(9972.0010, grad_fn=<NegBackward0>) tensor(9971.9580, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9971.9052734375
tensor(9971.9580, grad_fn=<NegBackward0>) tensor(9971.9053, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9971.8388671875
tensor(9971.9053, grad_fn=<NegBackward0>) tensor(9971.8389, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9971.76953125
tensor(9971.8389, grad_fn=<NegBackward0>) tensor(9971.7695, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9971.70703125
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 40%|████      | 40/100 [8:52:14<11:51:21, 711.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 41%|████      | 41/100 [9:09:08<13:08:37, 801.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 42%|████▏     | 42/100 [9:25:36<13:49:19, 857.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 43%|████▎     | 43/100 [9:38:38<13:13:29, 835.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 44%|████▍     | 44/100 [9:57:03<14:14:59, 916.06s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 45%|████▌     | 45/100 [10:06:24<12:22:04, 809.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 46%|████▌     | 46/100 [10:24:33<13:23:57, 893.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 47%|████▋     | 47/100 [10:32:17<11:15:17, 764.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 48%|████▊     | 48/100 [10:49:38<12:14:31, 847.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 49%|████▉     | 49/100 [11:01:59<11:33:14, 815.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 50%|█████     | 50/100 [11:17:15<11:44:43, 845.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 51%|█████     | 51/100 [11:30:34<11:19:09, 831.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 52%|█████▏    | 52/100 [11:42:15<10:33:54, 792.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 53%|█████▎    | 53/100 [12:00:32<11:32:23, 883.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 54%|█████▍    | 54/100 [12:15:56<11:26:52, 895.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 55%|█████▌    | 55/100 [12:25:12<9:55:33, 794.08s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 56%|█████▌    | 56/100 [12:35:23<9:01:57, 739.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 57%|█████▋    | 57/100 [12:52:32<9:51:59, 826.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 58%|█████▊    | 58/100 [13:04:32<9:16:01, 794.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 59%|█████▉    | 59/100 [13:15:38<8:36:29, 755.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 60%|██████    | 60/100 [13:27:12<8:11:23, 737.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 61%|██████    | 61/100 [13:36:28<7:23:55, 682.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 62%|██████▏   | 62/100 [13:45:19<6:43:29, 637.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 63%|██████▎   | 63/100 [14:00:16<7:20:57, 715.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 64%|██████▍   | 64/100 [14:14:35<7:35:06, 758.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 65%|██████▌   | 65/100 [14:29:47<7:49:20, 804.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 66%|██████▌   | 66/100 [14:44:45<7:51:43, 832.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 67%|██████▋   | 67/100 [14:57:18<7:24:46, 808.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 68%|██████▊   | 68/100 [15:09:37<7:00:09, 787.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 69%|██████▉   | 69/100 [15:19:30<6:16:49, 729.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 70%|███████   | 70/100 [15:31:59<6:07:35, 735.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 71%|███████   | 71/100 [15:50:00<6:45:27, 838.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 72%|███████▏  | 72/100 [16:07:51<7:03:56, 908.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 73%|███████▎  | 73/100 [16:24:28<7:00:51, 935.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 74%|███████▍  | 74/100 [16:35:32<6:09:56, 853.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 75%|███████▌  | 75/100 [16:50:30<6:01:15, 867.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 76%|███████▌  | 76/100 [17:06:41<5:59:14, 898.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
tensor(9971.7695, grad_fn=<NegBackward0>) tensor(9971.7070, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9971.6611328125
tensor(9971.7070, grad_fn=<NegBackward0>) tensor(9971.6611, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9971.6357421875
tensor(9971.6611, grad_fn=<NegBackward0>) tensor(9971.6357, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9971.6162109375
tensor(9971.6357, grad_fn=<NegBackward0>) tensor(9971.6162, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9971.60546875
tensor(9971.6162, grad_fn=<NegBackward0>) tensor(9971.6055, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9971.607421875
tensor(9971.6055, grad_fn=<NegBackward0>) tensor(9971.6074, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -9971.595703125
tensor(9971.6055, grad_fn=<NegBackward0>) tensor(9971.5957, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9971.5947265625
tensor(9971.5957, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9971.59375
tensor(9971.5947, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9971.59375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9971.5947265625
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9971.59375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9971.59375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9971.59375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9971.5927734375
tensor(9971.5938, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9971.59375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9971.5927734375
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9971.591796875
tensor(9971.5928, grad_fn=<NegBackward0>) tensor(9971.5918, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9971.5947265625
tensor(9971.5918, grad_fn=<NegBackward0>) tensor(9971.5947, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9971.5927734375
tensor(9971.5918, grad_fn=<NegBackward0>) tensor(9971.5928, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9971.59375
tensor(9971.5918, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -9971.59375
tensor(9971.5918, grad_fn=<NegBackward0>) tensor(9971.5938, grad_fn=<NegBackward0>)
4
Iteration 4400: Loss = -9971.5966796875
tensor(9971.5918, grad_fn=<NegBackward0>) tensor(9971.5967, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4400 due to no improvement.
pi: tensor([[0.1652, 0.8348],
        [0.1140, 0.8860]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3643, 0.6357], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1831, 0.1526],
         [0.5681, 0.1269]],

        [[0.6901, 0.1516],
         [0.6102, 0.7017]],

        [[0.5240, 0.1675],
         [0.7007, 0.5294]],

        [[0.6030, 0.1590],
         [0.6716, 0.5732]],

        [[0.5575, 0.2059],
         [0.6631, 0.7112]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.005259442857121893
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0007830659075602383
Global Adjusted Rand Index: 0.0009016332689831364
Average Adjusted Rand Index: -0.0021051497292841324
[0.0009016332689831364, 0.0009016332689831364] [-0.0021051497292841324, -0.0021051497292841324] [9971.5947265625, 9971.5966796875]
-------------------------------------
This iteration is 39
True Objective function: Loss = -10038.200132493004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23182.326171875
inf tensor(23182.3262, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9955.6005859375
tensor(23182.3262, grad_fn=<NegBackward0>) tensor(9955.6006, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9953.9853515625
tensor(9955.6006, grad_fn=<NegBackward0>) tensor(9953.9854, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9953.3349609375
tensor(9953.9854, grad_fn=<NegBackward0>) tensor(9953.3350, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9952.9296875
tensor(9953.3350, grad_fn=<NegBackward0>) tensor(9952.9297, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9952.400390625
tensor(9952.9297, grad_fn=<NegBackward0>) tensor(9952.4004, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9952.0078125
tensor(9952.4004, grad_fn=<NegBackward0>) tensor(9952.0078, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9951.8232421875
tensor(9952.0078, grad_fn=<NegBackward0>) tensor(9951.8232, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9951.6787109375
tensor(9951.8232, grad_fn=<NegBackward0>) tensor(9951.6787, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9951.560546875
tensor(9951.6787, grad_fn=<NegBackward0>) tensor(9951.5605, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9951.46875
tensor(9951.5605, grad_fn=<NegBackward0>) tensor(9951.4688, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9951.400390625
tensor(9951.4688, grad_fn=<NegBackward0>) tensor(9951.4004, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9951.3544921875
tensor(9951.4004, grad_fn=<NegBackward0>) tensor(9951.3545, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9951.3251953125
tensor(9951.3545, grad_fn=<NegBackward0>) tensor(9951.3252, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9951.306640625
tensor(9951.3252, grad_fn=<NegBackward0>) tensor(9951.3066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9951.2978515625
tensor(9951.3066, grad_fn=<NegBackward0>) tensor(9951.2979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9951.29296875
tensor(9951.2979, grad_fn=<NegBackward0>) tensor(9951.2930, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9951.291015625
tensor(9951.2930, grad_fn=<NegBackward0>) tensor(9951.2910, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9951.2890625
tensor(9951.2910, grad_fn=<NegBackward0>) tensor(9951.2891, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9951.2890625
tensor(9951.2891, grad_fn=<NegBackward0>) tensor(9951.2891, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9951.2880859375
tensor(9951.2891, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9951.2880859375
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9951.2890625
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2891, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9951.2880859375
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9951.287109375
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2871, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -9951.287109375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2871, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
3
Iteration 3000: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
4
Iteration 3100: Loss = -9951.2880859375
tensor(9951.2871, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3100 due to no improvement.
pi: tensor([[0.8776, 0.1224],
        [0.7162, 0.2838]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9972, 0.0028], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1325, 0.1403],
         [0.5049, 0.1921]],

        [[0.5901, 0.1636],
         [0.6499, 0.5786]],

        [[0.5570, 0.1682],
         [0.5228, 0.6108]],

        [[0.5823, 0.1575],
         [0.7123, 0.6120]],

        [[0.5804, 0.1442],
         [0.6852, 0.6710]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.489882528926777e-05
Average Adjusted Rand Index: 1.427637013092014e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23100.5234375
inf tensor(23100.5234, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9954.1826171875
tensor(23100.5234, grad_fn=<NegBackward0>) tensor(9954.1826, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9953.43359375
tensor(9954.1826, grad_fn=<NegBackward0>) tensor(9953.4336, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9953.0673828125
tensor(9953.4336, grad_fn=<NegBackward0>) tensor(9953.0674, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9952.763671875
tensor(9953.0674, grad_fn=<NegBackward0>) tensor(9952.7637, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9952.560546875
tensor(9952.7637, grad_fn=<NegBackward0>) tensor(9952.5605, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9952.4150390625
tensor(9952.5605, grad_fn=<NegBackward0>) tensor(9952.4150, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9952.2822265625
tensor(9952.4150, grad_fn=<NegBackward0>) tensor(9952.2822, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9952.1533203125
tensor(9952.2822, grad_fn=<NegBackward0>) tensor(9952.1533, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9952.025390625
tensor(9952.1533, grad_fn=<NegBackward0>) tensor(9952.0254, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9951.9052734375
tensor(9952.0254, grad_fn=<NegBackward0>) tensor(9951.9053, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9951.794921875
tensor(9951.9053, grad_fn=<NegBackward0>) tensor(9951.7949, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9951.6953125
tensor(9951.7949, grad_fn=<NegBackward0>) tensor(9951.6953, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9951.60546875
tensor(9951.6953, grad_fn=<NegBackward0>) tensor(9951.6055, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9951.5322265625
tensor(9951.6055, grad_fn=<NegBackward0>) tensor(9951.5322, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9951.4677734375
tensor(9951.5322, grad_fn=<NegBackward0>) tensor(9951.4678, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9951.41796875
tensor(9951.4678, grad_fn=<NegBackward0>) tensor(9951.4180, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9951.3798828125
tensor(9951.4180, grad_fn=<NegBackward0>) tensor(9951.3799, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9951.3505859375
tensor(9951.3799, grad_fn=<NegBackward0>) tensor(9951.3506, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9951.330078125
tensor(9951.3506, grad_fn=<NegBackward0>) tensor(9951.3301, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9951.3154296875
tensor(9951.3301, grad_fn=<NegBackward0>) tensor(9951.3154, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9951.3056640625
tensor(9951.3154, grad_fn=<NegBackward0>) tensor(9951.3057, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9951.296875
tensor(9951.3057, grad_fn=<NegBackward0>) tensor(9951.2969, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9951.29296875
tensor(9951.2969, grad_fn=<NegBackward0>) tensor(9951.2930, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9951.291015625
tensor(9951.2930, grad_fn=<NegBackward0>) tensor(9951.2910, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9951.2890625
tensor(9951.2910, grad_fn=<NegBackward0>) tensor(9951.2891, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9951.2880859375
tensor(9951.2891, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9951.2880859375
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2881, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9951.2861328125
tensor(9951.2881, grad_fn=<NegBackward0>) tensor(9951.2861, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9951.2861328125
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2861, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9951.2861328125
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2861, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9951.287109375
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2871, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9951.287109375
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2871, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -9951.287109375
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2871, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -9951.28515625
tensor(9951.2861, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9951.28515625
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9951.28515625
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9951.28515625
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9951.28515625
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9951.2861328125
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2861, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9951.28515625
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9951.2841796875
tensor(9951.2852, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9951.2841796875
tensor(9951.2842, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9951.28515625
tensor(9951.2842, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9951.2841796875
tensor(9951.2842, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9951.28515625
tensor(9951.2842, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9951.283203125
tensor(9951.2842, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9951.283203125
tensor(9951.2832, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9951.283203125
tensor(9951.2832, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9951.2841796875
tensor(9951.2832, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9951.283203125
tensor(9951.2832, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9951.2822265625
tensor(9951.2832, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9951.3017578125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.3018, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9951.2861328125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2861, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9951.2841796875
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9951.2841796875
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9951.2841796875
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9951.28515625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2852, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9951.283203125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9951.2822265625
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9951.2841796875
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2842, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9951.28125
tensor(9951.2822, grad_fn=<NegBackward0>) tensor(9951.2812, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9951.28125
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2812, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9951.2822265625
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9951.283203125
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9951.2822265625
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2822, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -9951.283203125
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -9951.283203125
tensor(9951.2812, grad_fn=<NegBackward0>) tensor(9951.2832, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.8771, 0.1229],
        [0.6958, 0.3042]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9832, 0.0168], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1324, 0.1408],
         [0.6224, 0.1915]],

        [[0.5124, 0.1634],
         [0.6447, 0.6883]],

        [[0.5956, 0.1678],
         [0.6823, 0.6113]],

        [[0.6221, 0.1571],
         [0.6978, 0.6385]],

        [[0.5987, 0.1441],
         [0.6242, 0.5570]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.489882528926777e-05
Average Adjusted Rand Index: 1.427637013092014e-05
[6.489882528926777e-05, 6.489882528926777e-05] [1.427637013092014e-05, 1.427637013092014e-05] [9951.2880859375, 9951.283203125]
-------------------------------------
This iteration is 40
True Objective function: Loss = -10248.829604828867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20736.20703125
inf tensor(20736.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10136.197265625
tensor(20736.2070, grad_fn=<NegBackward0>) tensor(10136.1973, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10133.763671875
tensor(10136.1973, grad_fn=<NegBackward0>) tensor(10133.7637, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10132.9404296875
tensor(10133.7637, grad_fn=<NegBackward0>) tensor(10132.9404, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10132.5439453125
tensor(10132.9404, grad_fn=<NegBackward0>) tensor(10132.5439, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10132.2841796875
tensor(10132.5439, grad_fn=<NegBackward0>) tensor(10132.2842, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10132.060546875
tensor(10132.2842, grad_fn=<NegBackward0>) tensor(10132.0605, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10131.640625
tensor(10132.0605, grad_fn=<NegBackward0>) tensor(10131.6406, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10131.26953125
tensor(10131.6406, grad_fn=<NegBackward0>) tensor(10131.2695, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10131.1953125
tensor(10131.2695, grad_fn=<NegBackward0>) tensor(10131.1953, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10131.1591796875
tensor(10131.1953, grad_fn=<NegBackward0>) tensor(10131.1592, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10131.1396484375
tensor(10131.1592, grad_fn=<NegBackward0>) tensor(10131.1396, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10131.123046875
tensor(10131.1396, grad_fn=<NegBackward0>) tensor(10131.1230, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10131.11328125
tensor(10131.1230, grad_fn=<NegBackward0>) tensor(10131.1133, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10131.10546875
tensor(10131.1133, grad_fn=<NegBackward0>) tensor(10131.1055, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10131.0966796875
tensor(10131.1055, grad_fn=<NegBackward0>) tensor(10131.0967, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10131.091796875
tensor(10131.0967, grad_fn=<NegBackward0>) tensor(10131.0918, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10131.0869140625
tensor(10131.0918, grad_fn=<NegBackward0>) tensor(10131.0869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10131.0830078125
tensor(10131.0869, grad_fn=<NegBackward0>) tensor(10131.0830, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10131.08203125
tensor(10131.0830, grad_fn=<NegBackward0>) tensor(10131.0820, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10131.080078125
tensor(10131.0820, grad_fn=<NegBackward0>) tensor(10131.0801, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10131.0771484375
tensor(10131.0801, grad_fn=<NegBackward0>) tensor(10131.0771, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10131.0751953125
tensor(10131.0771, grad_fn=<NegBackward0>) tensor(10131.0752, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10131.072265625
tensor(10131.0752, grad_fn=<NegBackward0>) tensor(10131.0723, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10131.072265625
tensor(10131.0723, grad_fn=<NegBackward0>) tensor(10131.0723, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10131.0703125
tensor(10131.0723, grad_fn=<NegBackward0>) tensor(10131.0703, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10131.068359375
tensor(10131.0703, grad_fn=<NegBackward0>) tensor(10131.0684, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10131.0673828125
tensor(10131.0684, grad_fn=<NegBackward0>) tensor(10131.0674, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10131.0654296875
tensor(10131.0674, grad_fn=<NegBackward0>) tensor(10131.0654, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10131.06640625
tensor(10131.0654, grad_fn=<NegBackward0>) tensor(10131.0664, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -10131.0654296875
tensor(10131.0654, grad_fn=<NegBackward0>) tensor(10131.0654, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10131.0634765625
tensor(10131.0654, grad_fn=<NegBackward0>) tensor(10131.0635, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10131.0625
tensor(10131.0635, grad_fn=<NegBackward0>) tensor(10131.0625, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10131.0615234375
tensor(10131.0625, grad_fn=<NegBackward0>) tensor(10131.0615, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10131.0615234375
tensor(10131.0615, grad_fn=<NegBackward0>) tensor(10131.0615, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10131.0615234375
tensor(10131.0615, grad_fn=<NegBackward0>) tensor(10131.0615, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10131.0595703125
tensor(10131.0615, grad_fn=<NegBackward0>) tensor(10131.0596, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10131.0595703125
tensor(10131.0596, grad_fn=<NegBackward0>) tensor(10131.0596, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10131.05859375
tensor(10131.0596, grad_fn=<NegBackward0>) tensor(10131.0586, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10131.05859375
tensor(10131.0586, grad_fn=<NegBackward0>) tensor(10131.0586, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10131.056640625
tensor(10131.0586, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10131.0576171875
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10131.0576171875
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -10131.0576171875
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -10131.0556640625
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10131.056640625
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10131.056640625
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -10131.056640625
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -10131.0556640625
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10131.0556640625
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10131.0546875
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10131.0546875
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10131.0537109375
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10131.0546875
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10131.0546875
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -10131.0537109375
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10131.052734375
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10131.0625
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0625, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10131.0517578125
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10131.1064453125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.1064, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10131.0869140625
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0869, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10131.087890625
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0879, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10131.052734375
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10131.052734375
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10131.0517578125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10131.171875
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.1719, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10131.05078125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0508, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10131.05078125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0508, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10131.052734375
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10131.0517578125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -10131.0517578125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -10131.05078125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0508, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10131.0498046875
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0498, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10131.0888671875
tensor(10131.0498, grad_fn=<NegBackward0>) tensor(10131.0889, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10131.0517578125
tensor(10131.0498, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -10131.0517578125
tensor(10131.0498, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -10131.052734375
tensor(10131.0498, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
4
pi: tensor([[9.6322e-01, 3.6778e-02],
        [9.9990e-01, 9.7118e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9602, 0.0398], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1377, 0.1889],
         [0.5629, 0.3242]],

        [[0.6771, 0.2832],
         [0.6663, 0.6059]],

        [[0.6099, 0.2013],
         [0.6149, 0.7120]],

        [[0.7230, 0.1915],
         [0.5711, 0.6891]],

        [[0.5731, 0.1611],
         [0.5764, 0.7168]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: -0.022176585651428493
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0036366570395561493
Average Adjusted Rand Index: -0.005162589857558426
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26462.111328125
inf tensor(26462.1113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10136.298828125
tensor(26462.1113, grad_fn=<NegBackward0>) tensor(10136.2988, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10133.2158203125
tensor(10136.2988, grad_fn=<NegBackward0>) tensor(10133.2158, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10132.42578125
tensor(10133.2158, grad_fn=<NegBackward0>) tensor(10132.4258, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10132.0478515625
tensor(10132.4258, grad_fn=<NegBackward0>) tensor(10132.0479, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10131.8310546875
tensor(10132.0479, grad_fn=<NegBackward0>) tensor(10131.8311, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10131.6884765625
tensor(10131.8311, grad_fn=<NegBackward0>) tensor(10131.6885, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10131.6015625
tensor(10131.6885, grad_fn=<NegBackward0>) tensor(10131.6016, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10131.5478515625
tensor(10131.6016, grad_fn=<NegBackward0>) tensor(10131.5479, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10131.5126953125
tensor(10131.5479, grad_fn=<NegBackward0>) tensor(10131.5127, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10131.4892578125
tensor(10131.5127, grad_fn=<NegBackward0>) tensor(10131.4893, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10131.4755859375
tensor(10131.4893, grad_fn=<NegBackward0>) tensor(10131.4756, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10131.46484375
tensor(10131.4756, grad_fn=<NegBackward0>) tensor(10131.4648, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10131.4609375
tensor(10131.4648, grad_fn=<NegBackward0>) tensor(10131.4609, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10131.4560546875
tensor(10131.4609, grad_fn=<NegBackward0>) tensor(10131.4561, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10131.453125
tensor(10131.4561, grad_fn=<NegBackward0>) tensor(10131.4531, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10131.4501953125
tensor(10131.4531, grad_fn=<NegBackward0>) tensor(10131.4502, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10131.4482421875
tensor(10131.4502, grad_fn=<NegBackward0>) tensor(10131.4482, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10131.447265625
tensor(10131.4482, grad_fn=<NegBackward0>) tensor(10131.4473, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10131.4453125
tensor(10131.4473, grad_fn=<NegBackward0>) tensor(10131.4453, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10131.4443359375
tensor(10131.4453, grad_fn=<NegBackward0>) tensor(10131.4443, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10131.4423828125
tensor(10131.4443, grad_fn=<NegBackward0>) tensor(10131.4424, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10131.44140625
tensor(10131.4424, grad_fn=<NegBackward0>) tensor(10131.4414, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10131.439453125
tensor(10131.4414, grad_fn=<NegBackward0>) tensor(10131.4395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10131.4384765625
tensor(10131.4395, grad_fn=<NegBackward0>) tensor(10131.4385, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10131.4375
tensor(10131.4385, grad_fn=<NegBackward0>) tensor(10131.4375, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10131.435546875
tensor(10131.4375, grad_fn=<NegBackward0>) tensor(10131.4355, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10131.43359375
tensor(10131.4355, grad_fn=<NegBackward0>) tensor(10131.4336, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10131.4296875
tensor(10131.4336, grad_fn=<NegBackward0>) tensor(10131.4297, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10131.421875
tensor(10131.4297, grad_fn=<NegBackward0>) tensor(10131.4219, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10131.3720703125
tensor(10131.4219, grad_fn=<NegBackward0>) tensor(10131.3721, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10131.1064453125
tensor(10131.3721, grad_fn=<NegBackward0>) tensor(10131.1064, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10131.0791015625
tensor(10131.1064, grad_fn=<NegBackward0>) tensor(10131.0791, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10131.0703125
tensor(10131.0791, grad_fn=<NegBackward0>) tensor(10131.0703, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10131.0654296875
tensor(10131.0703, grad_fn=<NegBackward0>) tensor(10131.0654, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10131.0634765625
tensor(10131.0654, grad_fn=<NegBackward0>) tensor(10131.0635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10131.0615234375
tensor(10131.0635, grad_fn=<NegBackward0>) tensor(10131.0615, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10131.060546875
tensor(10131.0615, grad_fn=<NegBackward0>) tensor(10131.0605, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10131.0595703125
tensor(10131.0605, grad_fn=<NegBackward0>) tensor(10131.0596, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10131.0595703125
tensor(10131.0596, grad_fn=<NegBackward0>) tensor(10131.0596, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10131.05859375
tensor(10131.0596, grad_fn=<NegBackward0>) tensor(10131.0586, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10131.05859375
tensor(10131.0586, grad_fn=<NegBackward0>) tensor(10131.0586, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10131.0576171875
tensor(10131.0586, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10131.0576171875
tensor(10131.0576, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10131.056640625
tensor(10131.0576, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10131.0576171875
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0576, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -10131.056640625
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10131.0556640625
tensor(10131.0566, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10131.0546875
tensor(10131.0557, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10131.0556640625
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10131.0546875
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10131.0546875
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10131.056640625
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10131.0537109375
tensor(10131.0547, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10131.056640625
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0566, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10131.0537109375
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10131.0537109375
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10131.052734375
tensor(10131.0537, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10131.0546875
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0547, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10131.0556640625
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0557, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10131.0537109375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10131.052734375
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10131.0517578125
tensor(10131.0527, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10131.05078125
tensor(10131.0518, grad_fn=<NegBackward0>) tensor(10131.0508, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10131.0537109375
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0537, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10131.052734375
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0527, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -10131.0517578125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -10131.0517578125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -10131.0517578125
tensor(10131.0508, grad_fn=<NegBackward0>) tensor(10131.0518, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[9.6324e-01, 3.6756e-02],
        [9.9973e-01, 2.6646e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9602, 0.0398], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1377, 0.1889],
         [0.5522, 0.3242]],

        [[0.7135, 0.2832],
         [0.6765, 0.6898]],

        [[0.6309, 0.2013],
         [0.5452, 0.5008]],

        [[0.6865, 0.1915],
         [0.7128, 0.5378]],

        [[0.6003, 0.1610],
         [0.5457, 0.5679]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: -0.022176585651428493
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0036366570395561493
Average Adjusted Rand Index: -0.005162589857558426
[-0.0036366570395561493, -0.0036366570395561493] [-0.005162589857558426, -0.005162589857558426] [10131.0517578125, 10131.0517578125]
-------------------------------------
This iteration is 41
True Objective function: Loss = -10062.13258548985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20851.689453125
inf tensor(20851.6895, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9928.2373046875
tensor(20851.6895, grad_fn=<NegBackward0>) tensor(9928.2373, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9927.29296875
tensor(9928.2373, grad_fn=<NegBackward0>) tensor(9927.2930, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9926.890625
tensor(9927.2930, grad_fn=<NegBackward0>) tensor(9926.8906, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9926.4951171875
tensor(9926.8906, grad_fn=<NegBackward0>) tensor(9926.4951, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9925.9619140625
tensor(9926.4951, grad_fn=<NegBackward0>) tensor(9925.9619, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9925.1005859375
tensor(9925.9619, grad_fn=<NegBackward0>) tensor(9925.1006, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9924.7900390625
tensor(9925.1006, grad_fn=<NegBackward0>) tensor(9924.7900, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9924.68359375
tensor(9924.7900, grad_fn=<NegBackward0>) tensor(9924.6836, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9924.6220703125
tensor(9924.6836, grad_fn=<NegBackward0>) tensor(9924.6221, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9924.5849609375
tensor(9924.6221, grad_fn=<NegBackward0>) tensor(9924.5850, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9924.556640625
tensor(9924.5850, grad_fn=<NegBackward0>) tensor(9924.5566, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9924.5380859375
tensor(9924.5566, grad_fn=<NegBackward0>) tensor(9924.5381, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9924.5224609375
tensor(9924.5381, grad_fn=<NegBackward0>) tensor(9924.5225, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9924.5087890625
tensor(9924.5225, grad_fn=<NegBackward0>) tensor(9924.5088, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9924.4970703125
tensor(9924.5088, grad_fn=<NegBackward0>) tensor(9924.4971, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9924.490234375
tensor(9924.4971, grad_fn=<NegBackward0>) tensor(9924.4902, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9924.48046875
tensor(9924.4902, grad_fn=<NegBackward0>) tensor(9924.4805, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9924.4736328125
tensor(9924.4805, grad_fn=<NegBackward0>) tensor(9924.4736, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9924.46875
tensor(9924.4736, grad_fn=<NegBackward0>) tensor(9924.4688, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9924.4599609375
tensor(9924.4688, grad_fn=<NegBackward0>) tensor(9924.4600, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9924.455078125
tensor(9924.4600, grad_fn=<NegBackward0>) tensor(9924.4551, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9924.4501953125
tensor(9924.4551, grad_fn=<NegBackward0>) tensor(9924.4502, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9924.4462890625
tensor(9924.4502, grad_fn=<NegBackward0>) tensor(9924.4463, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9924.44140625
tensor(9924.4463, grad_fn=<NegBackward0>) tensor(9924.4414, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9924.4375
tensor(9924.4414, grad_fn=<NegBackward0>) tensor(9924.4375, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9924.4345703125
tensor(9924.4375, grad_fn=<NegBackward0>) tensor(9924.4346, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9924.4306640625
tensor(9924.4346, grad_fn=<NegBackward0>) tensor(9924.4307, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9924.427734375
tensor(9924.4307, grad_fn=<NegBackward0>) tensor(9924.4277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9924.4228515625
tensor(9924.4277, grad_fn=<NegBackward0>) tensor(9924.4229, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9924.421875
tensor(9924.4229, grad_fn=<NegBackward0>) tensor(9924.4219, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9924.41796875
tensor(9924.4219, grad_fn=<NegBackward0>) tensor(9924.4180, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9924.416015625
tensor(9924.4180, grad_fn=<NegBackward0>) tensor(9924.4160, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9924.4150390625
tensor(9924.4160, grad_fn=<NegBackward0>) tensor(9924.4150, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9924.4111328125
tensor(9924.4150, grad_fn=<NegBackward0>) tensor(9924.4111, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9924.41015625
tensor(9924.4111, grad_fn=<NegBackward0>) tensor(9924.4102, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9924.41015625
tensor(9924.4102, grad_fn=<NegBackward0>) tensor(9924.4102, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9924.4072265625
tensor(9924.4102, grad_fn=<NegBackward0>) tensor(9924.4072, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9924.404296875
tensor(9924.4072, grad_fn=<NegBackward0>) tensor(9924.4043, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9924.404296875
tensor(9924.4043, grad_fn=<NegBackward0>) tensor(9924.4043, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9924.4033203125
tensor(9924.4043, grad_fn=<NegBackward0>) tensor(9924.4033, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9924.40234375
tensor(9924.4033, grad_fn=<NegBackward0>) tensor(9924.4023, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9924.400390625
tensor(9924.4023, grad_fn=<NegBackward0>) tensor(9924.4004, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9924.400390625
tensor(9924.4004, grad_fn=<NegBackward0>) tensor(9924.4004, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9924.3984375
tensor(9924.4004, grad_fn=<NegBackward0>) tensor(9924.3984, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9924.3984375
tensor(9924.3984, grad_fn=<NegBackward0>) tensor(9924.3984, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9924.3984375
tensor(9924.3984, grad_fn=<NegBackward0>) tensor(9924.3984, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9924.396484375
tensor(9924.3984, grad_fn=<NegBackward0>) tensor(9924.3965, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9924.396484375
tensor(9924.3965, grad_fn=<NegBackward0>) tensor(9924.3965, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9924.396484375
tensor(9924.3965, grad_fn=<NegBackward0>) tensor(9924.3965, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9924.3955078125
tensor(9924.3965, grad_fn=<NegBackward0>) tensor(9924.3955, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9924.3935546875
tensor(9924.3955, grad_fn=<NegBackward0>) tensor(9924.3936, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9924.39453125
tensor(9924.3936, grad_fn=<NegBackward0>) tensor(9924.3945, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9924.392578125
tensor(9924.3936, grad_fn=<NegBackward0>) tensor(9924.3926, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9924.3935546875
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3936, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9924.392578125
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3926, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9924.392578125
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3926, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9924.390625
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9924.39453125
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3945, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9924.3896484375
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9924.3916015625
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3916, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9924.3896484375
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9924.392578125
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3926, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9924.390625
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9924.390625
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9924.3896484375
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9924.388671875
tensor(9924.3896, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9924.390625
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9924.388671875
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9924.388671875
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9924.3876953125
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9924.404296875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.4043, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9924.3896484375
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9924.388671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -9924.388671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9924.3896484375
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9924.388671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9924.3896484375
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9924.6171875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.6172, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9924.388671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9924.38671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9924.38671875
tensor(9924.3867, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9924.3857421875
tensor(9924.3867, grad_fn=<NegBackward0>) tensor(9924.3857, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9924.4228515625
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.4229, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9924.38671875
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -9924.38671875
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -9924.3876953125
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -9924.38671875
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[2.2745e-04, 9.9977e-01],
        [7.2976e-02, 9.2702e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0331, 0.9669], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2347, 0.1610],
         [0.5335, 0.1320]],

        [[0.5976, 0.1843],
         [0.7139, 0.5851]],

        [[0.6806, 0.1564],
         [0.6415, 0.5872]],

        [[0.6770, 0.2006],
         [0.6729, 0.6657]],

        [[0.5611, 0.1842],
         [0.6517, 0.5962]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.00015602119653182684
Average Adjusted Rand Index: -0.0013094323783398728
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20658.396484375
inf tensor(20658.3965, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9928.947265625
tensor(20658.3965, grad_fn=<NegBackward0>) tensor(9928.9473, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9927.984375
tensor(9928.9473, grad_fn=<NegBackward0>) tensor(9927.9844, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9927.69921875
tensor(9927.9844, grad_fn=<NegBackward0>) tensor(9927.6992, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9926.53125
tensor(9927.6992, grad_fn=<NegBackward0>) tensor(9926.5312, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9925.2109375
tensor(9926.5312, grad_fn=<NegBackward0>) tensor(9925.2109, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9924.8837890625
tensor(9925.2109, grad_fn=<NegBackward0>) tensor(9924.8838, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9924.7216796875
tensor(9924.8838, grad_fn=<NegBackward0>) tensor(9924.7217, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9924.634765625
tensor(9924.7217, grad_fn=<NegBackward0>) tensor(9924.6348, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9924.5859375
tensor(9924.6348, grad_fn=<NegBackward0>) tensor(9924.5859, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9924.5537109375
tensor(9924.5859, grad_fn=<NegBackward0>) tensor(9924.5537, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9924.5302734375
tensor(9924.5537, grad_fn=<NegBackward0>) tensor(9924.5303, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9924.5126953125
tensor(9924.5303, grad_fn=<NegBackward0>) tensor(9924.5127, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9924.4990234375
tensor(9924.5127, grad_fn=<NegBackward0>) tensor(9924.4990, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9924.4873046875
tensor(9924.4990, grad_fn=<NegBackward0>) tensor(9924.4873, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9924.4775390625
tensor(9924.4873, grad_fn=<NegBackward0>) tensor(9924.4775, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9924.4697265625
tensor(9924.4775, grad_fn=<NegBackward0>) tensor(9924.4697, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9924.4609375
tensor(9924.4697, grad_fn=<NegBackward0>) tensor(9924.4609, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9924.4541015625
tensor(9924.4609, grad_fn=<NegBackward0>) tensor(9924.4541, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9924.4482421875
tensor(9924.4541, grad_fn=<NegBackward0>) tensor(9924.4482, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9924.4423828125
tensor(9924.4482, grad_fn=<NegBackward0>) tensor(9924.4424, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9924.4375
tensor(9924.4424, grad_fn=<NegBackward0>) tensor(9924.4375, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9924.43359375
tensor(9924.4375, grad_fn=<NegBackward0>) tensor(9924.4336, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9924.4306640625
tensor(9924.4336, grad_fn=<NegBackward0>) tensor(9924.4307, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9924.42578125
tensor(9924.4307, grad_fn=<NegBackward0>) tensor(9924.4258, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9924.421875
tensor(9924.4258, grad_fn=<NegBackward0>) tensor(9924.4219, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9924.41796875
tensor(9924.4219, grad_fn=<NegBackward0>) tensor(9924.4180, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9924.4150390625
tensor(9924.4180, grad_fn=<NegBackward0>) tensor(9924.4150, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9924.4140625
tensor(9924.4150, grad_fn=<NegBackward0>) tensor(9924.4141, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9924.4111328125
tensor(9924.4141, grad_fn=<NegBackward0>) tensor(9924.4111, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9924.408203125
tensor(9924.4111, grad_fn=<NegBackward0>) tensor(9924.4082, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9924.4091796875
tensor(9924.4082, grad_fn=<NegBackward0>) tensor(9924.4092, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9924.4052734375
tensor(9924.4082, grad_fn=<NegBackward0>) tensor(9924.4053, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9924.404296875
tensor(9924.4053, grad_fn=<NegBackward0>) tensor(9924.4043, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9924.4033203125
tensor(9924.4043, grad_fn=<NegBackward0>) tensor(9924.4033, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9924.4013671875
tensor(9924.4033, grad_fn=<NegBackward0>) tensor(9924.4014, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9924.400390625
tensor(9924.4014, grad_fn=<NegBackward0>) tensor(9924.4004, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9924.3994140625
tensor(9924.4004, grad_fn=<NegBackward0>) tensor(9924.3994, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9924.3984375
tensor(9924.3994, grad_fn=<NegBackward0>) tensor(9924.3984, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9924.396484375
tensor(9924.3984, grad_fn=<NegBackward0>) tensor(9924.3965, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9924.3974609375
tensor(9924.3965, grad_fn=<NegBackward0>) tensor(9924.3975, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9924.3955078125
tensor(9924.3965, grad_fn=<NegBackward0>) tensor(9924.3955, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9924.3955078125
tensor(9924.3955, grad_fn=<NegBackward0>) tensor(9924.3955, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9924.3955078125
tensor(9924.3955, grad_fn=<NegBackward0>) tensor(9924.3955, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9924.3955078125
tensor(9924.3955, grad_fn=<NegBackward0>) tensor(9924.3955, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9924.392578125
tensor(9924.3955, grad_fn=<NegBackward0>) tensor(9924.3926, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9924.3935546875
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3936, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9924.39453125
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3945, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9924.3935546875
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3936, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9924.3935546875
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3936, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -9924.3916015625
tensor(9924.3926, grad_fn=<NegBackward0>) tensor(9924.3916, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9924.390625
tensor(9924.3916, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9924.390625
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9924.3916015625
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3916, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9924.390625
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9924.390625
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9924.390625
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9924.388671875
tensor(9924.3906, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9924.3916015625
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3916, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9924.3896484375
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9924.390625
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3906, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9924.3896484375
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -9924.388671875
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9924.3896484375
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9924.3896484375
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9924.3994140625
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3994, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -9924.388671875
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9924.39453125
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3945, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9924.3876953125
tensor(9924.3887, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9924.3974609375
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3975, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9924.3896484375
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9924.388671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3887, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9924.3876953125
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9924.38671875
tensor(9924.3877, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9924.3876953125
tensor(9924.3867, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9924.3857421875
tensor(9924.3867, grad_fn=<NegBackward0>) tensor(9924.3857, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9924.3896484375
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9924.3876953125
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9924.3896484375
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3896, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9924.3876953125
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3877, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -9924.38671875
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3867, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[2.9037e-04, 9.9971e-01],
        [7.2956e-02, 9.2704e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0331, 0.9669], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2347, 0.1610],
         [0.6461, 0.1320]],

        [[0.6577, 0.1843],
         [0.5517, 0.6012]],

        [[0.5438, 0.1564],
         [0.5354, 0.6198]],

        [[0.6439, 0.2006],
         [0.5251, 0.6879]],

        [[0.6813, 0.1842],
         [0.5593, 0.6386]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.00015602119653182684
Average Adjusted Rand Index: -0.0013094323783398728
[-0.00015602119653182684, -0.00015602119653182684] [-0.0013094323783398728, -0.0013094323783398728] [9924.38671875, 9924.38671875]
-------------------------------------
This iteration is 42
True Objective function: Loss = -9930.703854028518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25043.775390625
inf tensor(25043.7754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9841.138671875
tensor(25043.7754, grad_fn=<NegBackward0>) tensor(9841.1387, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9839.646484375
tensor(9841.1387, grad_fn=<NegBackward0>) tensor(9839.6465, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9839.2998046875
tensor(9839.6465, grad_fn=<NegBackward0>) tensor(9839.2998, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9839.11328125
tensor(9839.2998, grad_fn=<NegBackward0>) tensor(9839.1133, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9839.0
tensor(9839.1133, grad_fn=<NegBackward0>) tensor(9839., grad_fn=<NegBackward0>)
Iteration 600: Loss = -9838.8994140625
tensor(9839., grad_fn=<NegBackward0>) tensor(9838.8994, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9838.8349609375
tensor(9838.8994, grad_fn=<NegBackward0>) tensor(9838.8350, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9838.783203125
tensor(9838.8350, grad_fn=<NegBackward0>) tensor(9838.7832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9838.73046875
tensor(9838.7832, grad_fn=<NegBackward0>) tensor(9838.7305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9838.6865234375
tensor(9838.7305, grad_fn=<NegBackward0>) tensor(9838.6865, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9838.6455078125
tensor(9838.6865, grad_fn=<NegBackward0>) tensor(9838.6455, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9838.5869140625
tensor(9838.6455, grad_fn=<NegBackward0>) tensor(9838.5869, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9838.4033203125
tensor(9838.5869, grad_fn=<NegBackward0>) tensor(9838.4033, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9837.5966796875
tensor(9838.4033, grad_fn=<NegBackward0>) tensor(9837.5967, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9837.3466796875
tensor(9837.5967, grad_fn=<NegBackward0>) tensor(9837.3467, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9837.2900390625
tensor(9837.3467, grad_fn=<NegBackward0>) tensor(9837.2900, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9837.2666015625
tensor(9837.2900, grad_fn=<NegBackward0>) tensor(9837.2666, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9837.255859375
tensor(9837.2666, grad_fn=<NegBackward0>) tensor(9837.2559, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9837.2490234375
tensor(9837.2559, grad_fn=<NegBackward0>) tensor(9837.2490, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9837.2451171875
tensor(9837.2490, grad_fn=<NegBackward0>) tensor(9837.2451, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9837.240234375
tensor(9837.2451, grad_fn=<NegBackward0>) tensor(9837.2402, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9837.240234375
tensor(9837.2402, grad_fn=<NegBackward0>) tensor(9837.2402, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9837.2392578125
tensor(9837.2402, grad_fn=<NegBackward0>) tensor(9837.2393, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9837.2373046875
tensor(9837.2393, grad_fn=<NegBackward0>) tensor(9837.2373, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9837.236328125
tensor(9837.2373, grad_fn=<NegBackward0>) tensor(9837.2363, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9837.234375
tensor(9837.2363, grad_fn=<NegBackward0>) tensor(9837.2344, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9837.234375
tensor(9837.2344, grad_fn=<NegBackward0>) tensor(9837.2344, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9837.232421875
tensor(9837.2344, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9837.234375
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2344, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9837.232421875
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9837.2333984375
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2334, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9837.232421875
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9837.2333984375
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2334, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9837.232421875
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9837.2314453125
tensor(9837.2324, grad_fn=<NegBackward0>) tensor(9837.2314, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9837.232421875
tensor(9837.2314, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9837.232421875
tensor(9837.2314, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -9837.232421875
tensor(9837.2314, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -9837.232421875
tensor(9837.2314, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -9837.232421875
tensor(9837.2314, grad_fn=<NegBackward0>) tensor(9837.2324, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4000 due to no improvement.
pi: tensor([[0.9748, 0.0252],
        [0.9696, 0.0304]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9742, 0.0258], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1347, 0.1780],
         [0.7242, 0.3011]],

        [[0.6621, 0.0493],
         [0.6803, 0.6729]],

        [[0.5339, 0.1942],
         [0.5258, 0.7246]],

        [[0.5848, 0.1964],
         [0.5980, 0.6186]],

        [[0.5591, 0.1720],
         [0.5453, 0.6448]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.764727547676425e-05
Average Adjusted Rand Index: -0.0019636328650514006
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24902.119140625
inf tensor(24902.1191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9840.0263671875
tensor(24902.1191, grad_fn=<NegBackward0>) tensor(9840.0264, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9838.912109375
tensor(9840.0264, grad_fn=<NegBackward0>) tensor(9838.9121, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9838.537109375
tensor(9838.9121, grad_fn=<NegBackward0>) tensor(9838.5371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9837.97265625
tensor(9838.5371, grad_fn=<NegBackward0>) tensor(9837.9727, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9837.546875
tensor(9837.9727, grad_fn=<NegBackward0>) tensor(9837.5469, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9837.3427734375
tensor(9837.5469, grad_fn=<NegBackward0>) tensor(9837.3428, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9837.2470703125
tensor(9837.3428, grad_fn=<NegBackward0>) tensor(9837.2471, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9837.19921875
tensor(9837.2471, grad_fn=<NegBackward0>) tensor(9837.1992, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9837.1728515625
tensor(9837.1992, grad_fn=<NegBackward0>) tensor(9837.1729, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9837.15625
tensor(9837.1729, grad_fn=<NegBackward0>) tensor(9837.1562, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9837.14453125
tensor(9837.1562, grad_fn=<NegBackward0>) tensor(9837.1445, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9837.13671875
tensor(9837.1445, grad_fn=<NegBackward0>) tensor(9837.1367, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9837.12890625
tensor(9837.1367, grad_fn=<NegBackward0>) tensor(9837.1289, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9837.1240234375
tensor(9837.1289, grad_fn=<NegBackward0>) tensor(9837.1240, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9837.12109375
tensor(9837.1240, grad_fn=<NegBackward0>) tensor(9837.1211, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9837.1181640625
tensor(9837.1211, grad_fn=<NegBackward0>) tensor(9837.1182, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9837.1162109375
tensor(9837.1182, grad_fn=<NegBackward0>) tensor(9837.1162, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9837.111328125
tensor(9837.1162, grad_fn=<NegBackward0>) tensor(9837.1113, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9837.111328125
tensor(9837.1113, grad_fn=<NegBackward0>) tensor(9837.1113, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9837.1083984375
tensor(9837.1113, grad_fn=<NegBackward0>) tensor(9837.1084, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9837.1083984375
tensor(9837.1084, grad_fn=<NegBackward0>) tensor(9837.1084, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9837.1064453125
tensor(9837.1084, grad_fn=<NegBackward0>) tensor(9837.1064, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9837.10546875
tensor(9837.1064, grad_fn=<NegBackward0>) tensor(9837.1055, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9837.10546875
tensor(9837.1055, grad_fn=<NegBackward0>) tensor(9837.1055, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9837.1025390625
tensor(9837.1055, grad_fn=<NegBackward0>) tensor(9837.1025, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9837.1025390625
tensor(9837.1025, grad_fn=<NegBackward0>) tensor(9837.1025, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9837.1015625
tensor(9837.1025, grad_fn=<NegBackward0>) tensor(9837.1016, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9837.1005859375
tensor(9837.1016, grad_fn=<NegBackward0>) tensor(9837.1006, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9837.1005859375
tensor(9837.1006, grad_fn=<NegBackward0>) tensor(9837.1006, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9837.099609375
tensor(9837.1006, grad_fn=<NegBackward0>) tensor(9837.0996, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9837.09765625
tensor(9837.0996, grad_fn=<NegBackward0>) tensor(9837.0977, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9837.0986328125
tensor(9837.0977, grad_fn=<NegBackward0>) tensor(9837.0986, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9837.0986328125
tensor(9837.0977, grad_fn=<NegBackward0>) tensor(9837.0986, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9837.0986328125
tensor(9837.0977, grad_fn=<NegBackward0>) tensor(9837.0986, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -9837.0966796875
tensor(9837.0977, grad_fn=<NegBackward0>) tensor(9837.0967, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9837.09765625
tensor(9837.0967, grad_fn=<NegBackward0>) tensor(9837.0977, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9837.0966796875
tensor(9837.0967, grad_fn=<NegBackward0>) tensor(9837.0967, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9837.095703125
tensor(9837.0967, grad_fn=<NegBackward0>) tensor(9837.0957, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9837.0966796875
tensor(9837.0957, grad_fn=<NegBackward0>) tensor(9837.0967, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9837.095703125
tensor(9837.0957, grad_fn=<NegBackward0>) tensor(9837.0957, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9837.0947265625
tensor(9837.0957, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9837.0947265625
tensor(9837.0947, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9837.0947265625
tensor(9837.0947, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9837.09375
tensor(9837.0947, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9837.0947265625
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9837.09375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9837.09375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9837.09375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9837.09375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9837.09375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9837.0947265625
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9837.0927734375
tensor(9837.0938, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9837.0927734375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9837.0927734375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9837.0927734375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9837.0947265625
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9837.09375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -9837.0927734375
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9837.091796875
tensor(9837.0928, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9837.09375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9837.0947265625
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0947, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9837.099609375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0996, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9837.09375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9837.09375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9837.09375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9837.1064453125
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.1064, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9837.1201171875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.1201, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9837.1005859375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.1006, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9837.09375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0938, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9837.2919921875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.2920, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9837.091796875
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0918, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9837.2255859375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.2256, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9837.0927734375
tensor(9837.0918, grad_fn=<NegBackward0>) tensor(9837.0928, grad_fn=<NegBackward0>)
3
pi: tensor([[6.5016e-05, 9.9994e-01],
        [2.6600e-02, 9.7340e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0393, 0.9607], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1629],
         [0.5562, 0.1363]],

        [[0.6462, 0.0528],
         [0.6154, 0.6595]],

        [[0.7126, 0.1880],
         [0.5744, 0.5778]],

        [[0.5007, 0.0702],
         [0.7029, 0.5580]],

        [[0.5142, 0.1611],
         [0.5936, 0.6946]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: -0.0009013459011305159
[6.764727547676425e-05, 0.00038770756378241793] [-0.0019636328650514006, -0.0009013459011305159] [9837.232421875, 9837.0927734375]
-------------------------------------
This iteration is 43
True Objective function: Loss = -9929.83941150153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22679.505859375
inf tensor(22679.5059, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9788.2744140625
tensor(22679.5059, grad_fn=<NegBackward0>) tensor(9788.2744, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9787.396484375
tensor(9788.2744, grad_fn=<NegBackward0>) tensor(9787.3965, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9787.123046875
tensor(9787.3965, grad_fn=<NegBackward0>) tensor(9787.1230, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9786.8974609375
tensor(9787.1230, grad_fn=<NegBackward0>) tensor(9786.8975, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9786.736328125
tensor(9786.8975, grad_fn=<NegBackward0>) tensor(9786.7363, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9786.65234375
tensor(9786.7363, grad_fn=<NegBackward0>) tensor(9786.6523, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9786.59765625
tensor(9786.6523, grad_fn=<NegBackward0>) tensor(9786.5977, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9786.560546875
tensor(9786.5977, grad_fn=<NegBackward0>) tensor(9786.5605, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9786.5322265625
tensor(9786.5605, grad_fn=<NegBackward0>) tensor(9786.5322, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9786.509765625
tensor(9786.5322, grad_fn=<NegBackward0>) tensor(9786.5098, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9786.4892578125
tensor(9786.5098, grad_fn=<NegBackward0>) tensor(9786.4893, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9786.474609375
tensor(9786.4893, grad_fn=<NegBackward0>) tensor(9786.4746, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9786.4619140625
tensor(9786.4746, grad_fn=<NegBackward0>) tensor(9786.4619, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9786.447265625
tensor(9786.4619, grad_fn=<NegBackward0>) tensor(9786.4473, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9786.42578125
tensor(9786.4473, grad_fn=<NegBackward0>) tensor(9786.4258, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9786.3203125
tensor(9786.4258, grad_fn=<NegBackward0>) tensor(9786.3203, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9785.6328125
tensor(9786.3203, grad_fn=<NegBackward0>) tensor(9785.6328, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9785.484375
tensor(9785.6328, grad_fn=<NegBackward0>) tensor(9785.4844, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9785.4326171875
tensor(9785.4844, grad_fn=<NegBackward0>) tensor(9785.4326, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9785.404296875
tensor(9785.4326, grad_fn=<NegBackward0>) tensor(9785.4043, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9785.3916015625
tensor(9785.4043, grad_fn=<NegBackward0>) tensor(9785.3916, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9785.3857421875
tensor(9785.3916, grad_fn=<NegBackward0>) tensor(9785.3857, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9785.37890625
tensor(9785.3857, grad_fn=<NegBackward0>) tensor(9785.3789, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9785.376953125
tensor(9785.3789, grad_fn=<NegBackward0>) tensor(9785.3770, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9785.3740234375
tensor(9785.3770, grad_fn=<NegBackward0>) tensor(9785.3740, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9785.3720703125
tensor(9785.3740, grad_fn=<NegBackward0>) tensor(9785.3721, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9785.369140625
tensor(9785.3721, grad_fn=<NegBackward0>) tensor(9785.3691, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9785.369140625
tensor(9785.3691, grad_fn=<NegBackward0>) tensor(9785.3691, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9785.3671875
tensor(9785.3691, grad_fn=<NegBackward0>) tensor(9785.3672, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9785.3671875
tensor(9785.3672, grad_fn=<NegBackward0>) tensor(9785.3672, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9785.365234375
tensor(9785.3672, grad_fn=<NegBackward0>) tensor(9785.3652, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9785.3642578125
tensor(9785.3652, grad_fn=<NegBackward0>) tensor(9785.3643, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9785.3623046875
tensor(9785.3643, grad_fn=<NegBackward0>) tensor(9785.3623, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9785.36328125
tensor(9785.3623, grad_fn=<NegBackward0>) tensor(9785.3633, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9785.3623046875
tensor(9785.3623, grad_fn=<NegBackward0>) tensor(9785.3623, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9785.361328125
tensor(9785.3623, grad_fn=<NegBackward0>) tensor(9785.3613, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9785.361328125
tensor(9785.3613, grad_fn=<NegBackward0>) tensor(9785.3613, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9785.361328125
tensor(9785.3613, grad_fn=<NegBackward0>) tensor(9785.3613, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9785.3603515625
tensor(9785.3613, grad_fn=<NegBackward0>) tensor(9785.3604, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9785.3583984375
tensor(9785.3604, grad_fn=<NegBackward0>) tensor(9785.3584, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9785.3583984375
tensor(9785.3584, grad_fn=<NegBackward0>) tensor(9785.3584, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9785.3583984375
tensor(9785.3584, grad_fn=<NegBackward0>) tensor(9785.3584, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9785.357421875
tensor(9785.3584, grad_fn=<NegBackward0>) tensor(9785.3574, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9785.3583984375
tensor(9785.3574, grad_fn=<NegBackward0>) tensor(9785.3584, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9785.357421875
tensor(9785.3574, grad_fn=<NegBackward0>) tensor(9785.3574, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9785.3564453125
tensor(9785.3574, grad_fn=<NegBackward0>) tensor(9785.3564, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9785.35546875
tensor(9785.3564, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9785.35546875
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9785.3564453125
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3564, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9785.35546875
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9785.35546875
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9785.35546875
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9785.35546875
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3555, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9785.3564453125
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3564, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9785.353515625
tensor(9785.3555, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9785.353515625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3535, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9785.3525390625
tensor(9785.3535, grad_fn=<NegBackward0>) tensor(9785.3525, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9785.3515625
tensor(9785.3525, grad_fn=<NegBackward0>) tensor(9785.3516, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9785.3486328125
tensor(9785.3516, grad_fn=<NegBackward0>) tensor(9785.3486, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9785.3369140625
tensor(9785.3486, grad_fn=<NegBackward0>) tensor(9785.3369, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9784.884765625
tensor(9785.3369, grad_fn=<NegBackward0>) tensor(9784.8848, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9784.880859375
tensor(9784.8848, grad_fn=<NegBackward0>) tensor(9784.8809, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9784.8779296875
tensor(9784.8809, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9784.87890625
tensor(9784.8779, grad_fn=<NegBackward0>) tensor(9784.8789, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9784.880859375
tensor(9784.8779, grad_fn=<NegBackward0>) tensor(9784.8809, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9784.876953125
tensor(9784.8779, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -9784.87890625
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8789, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9784.8955078125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8955, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9784.8798828125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8799, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9784.87890625
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8789, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9784.8779296875
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -9784.884765625
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8848, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -9784.876953125
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9784.8759765625
tensor(9784.8770, grad_fn=<NegBackward0>) tensor(9784.8760, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9784.87890625
tensor(9784.8760, grad_fn=<NegBackward0>) tensor(9784.8789, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9784.876953125
tensor(9784.8760, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9784.8779296875
tensor(9784.8760, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -9784.876953125
tensor(9784.8760, grad_fn=<NegBackward0>) tensor(9784.8770, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -9784.8779296875
tensor(9784.8760, grad_fn=<NegBackward0>) tensor(9784.8779, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[7.7259e-05, 9.9992e-01],
        [3.9558e-02, 9.6044e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0161, 0.9839], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2134, 0.0617],
         [0.5968, 0.1333]],

        [[0.6936, 0.1985],
         [0.6957, 0.7020]],

        [[0.7274, 0.1666],
         [0.6987, 0.6309]],

        [[0.5361, 0.1935],
         [0.5628, 0.5308]],

        [[0.5813, 0.0925],
         [0.5441, 0.5381]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003140670332584823
Average Adjusted Rand Index: -0.0006936617740552836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22833.92578125
inf tensor(22833.9258, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9788.4599609375
tensor(22833.9258, grad_fn=<NegBackward0>) tensor(9788.4600, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9787.298828125
tensor(9788.4600, grad_fn=<NegBackward0>) tensor(9787.2988, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9786.6142578125
tensor(9787.2988, grad_fn=<NegBackward0>) tensor(9786.6143, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9786.078125
tensor(9786.6143, grad_fn=<NegBackward0>) tensor(9786.0781, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9785.7421875
tensor(9786.0781, grad_fn=<NegBackward0>) tensor(9785.7422, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9785.4775390625
tensor(9785.7422, grad_fn=<NegBackward0>) tensor(9785.4775, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9785.2392578125
tensor(9785.4775, grad_fn=<NegBackward0>) tensor(9785.2393, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9785.0361328125
tensor(9785.2393, grad_fn=<NegBackward0>) tensor(9785.0361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9784.8681640625
tensor(9785.0361, grad_fn=<NegBackward0>) tensor(9784.8682, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9784.724609375
tensor(9784.8682, grad_fn=<NegBackward0>) tensor(9784.7246, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9784.6005859375
tensor(9784.7246, grad_fn=<NegBackward0>) tensor(9784.6006, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9784.498046875
tensor(9784.6006, grad_fn=<NegBackward0>) tensor(9784.4980, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9784.416015625
tensor(9784.4980, grad_fn=<NegBackward0>) tensor(9784.4160, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9784.3544921875
tensor(9784.4160, grad_fn=<NegBackward0>) tensor(9784.3545, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9784.3125
tensor(9784.3545, grad_fn=<NegBackward0>) tensor(9784.3125, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9784.283203125
tensor(9784.3125, grad_fn=<NegBackward0>) tensor(9784.2832, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9784.2666015625
tensor(9784.2832, grad_fn=<NegBackward0>) tensor(9784.2666, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9784.2529296875
tensor(9784.2666, grad_fn=<NegBackward0>) tensor(9784.2529, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9784.2451171875
tensor(9784.2529, grad_fn=<NegBackward0>) tensor(9784.2451, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9784.244140625
tensor(9784.2451, grad_fn=<NegBackward0>) tensor(9784.2441, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9784.2412109375
tensor(9784.2441, grad_fn=<NegBackward0>) tensor(9784.2412, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9784.2412109375
tensor(9784.2412, grad_fn=<NegBackward0>) tensor(9784.2412, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9784.240234375
tensor(9784.2412, grad_fn=<NegBackward0>) tensor(9784.2402, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9784.240234375
tensor(9784.2402, grad_fn=<NegBackward0>) tensor(9784.2402, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9784.240234375
tensor(9784.2402, grad_fn=<NegBackward0>) tensor(9784.2402, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9784.23828125
tensor(9784.2402, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9784.2392578125
tensor(9784.2383, grad_fn=<NegBackward0>) tensor(9784.2393, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9784.23828125
tensor(9784.2383, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9784.23828125
tensor(9784.2383, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9784.2373046875
tensor(9784.2383, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9784.23828125
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9784.23828125
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -9784.23828125
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -9784.2373046875
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9784.23828125
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2383, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9784.2373046875
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9784.2373046875
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9784.2373046875
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9784.2373046875
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9784.236328125
tensor(9784.2373, grad_fn=<NegBackward0>) tensor(9784.2363, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9784.236328125
tensor(9784.2363, grad_fn=<NegBackward0>) tensor(9784.2363, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9784.2373046875
tensor(9784.2363, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9784.236328125
tensor(9784.2363, grad_fn=<NegBackward0>) tensor(9784.2363, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9784.2373046875
tensor(9784.2363, grad_fn=<NegBackward0>) tensor(9784.2373, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9784.2353515625
tensor(9784.2363, grad_fn=<NegBackward0>) tensor(9784.2354, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9784.236328125
tensor(9784.2354, grad_fn=<NegBackward0>) tensor(9784.2363, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9784.2353515625
tensor(9784.2354, grad_fn=<NegBackward0>) tensor(9784.2354, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9784.234375
tensor(9784.2354, grad_fn=<NegBackward0>) tensor(9784.2344, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9784.2294921875
tensor(9784.2344, grad_fn=<NegBackward0>) tensor(9784.2295, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9784.177734375
tensor(9784.2295, grad_fn=<NegBackward0>) tensor(9784.1777, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9784.08203125
tensor(9784.1777, grad_fn=<NegBackward0>) tensor(9784.0820, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9784.078125
tensor(9784.0820, grad_fn=<NegBackward0>) tensor(9784.0781, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9784.0849609375
tensor(9784.0781, grad_fn=<NegBackward0>) tensor(9784.0850, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9784.0751953125
tensor(9784.0781, grad_fn=<NegBackward0>) tensor(9784.0752, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9784.1025390625
tensor(9784.0752, grad_fn=<NegBackward0>) tensor(9784.1025, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9784.0751953125
tensor(9784.0752, grad_fn=<NegBackward0>) tensor(9784.0752, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9784.07421875
tensor(9784.0752, grad_fn=<NegBackward0>) tensor(9784.0742, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9784.0751953125
tensor(9784.0742, grad_fn=<NegBackward0>) tensor(9784.0752, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9784.0732421875
tensor(9784.0742, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9784.0732421875
tensor(9784.0732, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9784.0732421875
tensor(9784.0732, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9784.0732421875
tensor(9784.0732, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9784.0732421875
tensor(9784.0732, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9784.072265625
tensor(9784.0732, grad_fn=<NegBackward0>) tensor(9784.0723, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9784.0712890625
tensor(9784.0723, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9784.0712890625
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9784.072265625
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0723, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9784.0712890625
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9784.0712890625
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9784.072265625
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0723, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9784.0703125
tensor(9784.0713, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9784.072265625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0723, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9784.072265625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0723, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9784.1474609375
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.1475, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9784.07421875
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0742, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9784.0712890625
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9784.0703125
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9784.0693359375
tensor(9784.0703, grad_fn=<NegBackward0>) tensor(9784.0693, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9784.0703125
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9784.0703125
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0703, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9784.0869140625
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0869, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -9784.0712890625
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0713, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -9784.0693359375
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0693, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9784.0732421875
tensor(9784.0693, grad_fn=<NegBackward0>) tensor(9784.0732, grad_fn=<NegBackward0>)
1
pi: tensor([[5.4748e-04, 9.9945e-01],
        [1.1101e-01, 8.8899e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0090, 0.9910], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2258, 0.0580],
         [0.5165, 0.1284]],

        [[0.5387, 0.1760],
         [0.6741, 0.7294]],

        [[0.5745, 0.1647],
         [0.6619, 0.7310]],

        [[0.6255, 0.1716],
         [0.6129, 0.5277]],

        [[0.5267, 0.1605],
         [0.6313, 0.5426]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.012285862605987194
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0013652179134287512
Average Adjusted Rand Index: -0.0021237269706115046
[-0.0003140670332584823, -0.0013652179134287512] [-0.0006936617740552836, -0.0021237269706115046] [9784.8779296875, 9784.0703125]
-------------------------------------
This iteration is 44
True Objective function: Loss = -10142.033603772208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21311.056640625
inf tensor(21311.0566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10035.3330078125
tensor(21311.0566, grad_fn=<NegBackward0>) tensor(10035.3330, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10033.2177734375
tensor(10035.3330, grad_fn=<NegBackward0>) tensor(10033.2178, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10031.87890625
tensor(10033.2178, grad_fn=<NegBackward0>) tensor(10031.8789, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10031.205078125
tensor(10031.8789, grad_fn=<NegBackward0>) tensor(10031.2051, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10030.6064453125
tensor(10031.2051, grad_fn=<NegBackward0>) tensor(10030.6064, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10029.1279296875
tensor(10030.6064, grad_fn=<NegBackward0>) tensor(10029.1279, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10027.94140625
tensor(10029.1279, grad_fn=<NegBackward0>) tensor(10027.9414, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10027.0693359375
tensor(10027.9414, grad_fn=<NegBackward0>) tensor(10027.0693, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10026.2109375
tensor(10027.0693, grad_fn=<NegBackward0>) tensor(10026.2109, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10025.2763671875
tensor(10026.2109, grad_fn=<NegBackward0>) tensor(10025.2764, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10024.173828125
tensor(10025.2764, grad_fn=<NegBackward0>) tensor(10024.1738, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10022.4541015625
tensor(10024.1738, grad_fn=<NegBackward0>) tensor(10022.4541, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10014.724609375
tensor(10022.4541, grad_fn=<NegBackward0>) tensor(10014.7246, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10012.44921875
tensor(10014.7246, grad_fn=<NegBackward0>) tensor(10012.4492, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10012.33203125
tensor(10012.4492, grad_fn=<NegBackward0>) tensor(10012.3320, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10012.2724609375
tensor(10012.3320, grad_fn=<NegBackward0>) tensor(10012.2725, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10012.2646484375
tensor(10012.2725, grad_fn=<NegBackward0>) tensor(10012.2646, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10012.26171875
tensor(10012.2646, grad_fn=<NegBackward0>) tensor(10012.2617, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10012.259765625
tensor(10012.2617, grad_fn=<NegBackward0>) tensor(10012.2598, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10012.2578125
tensor(10012.2598, grad_fn=<NegBackward0>) tensor(10012.2578, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10012.2568359375
tensor(10012.2578, grad_fn=<NegBackward0>) tensor(10012.2568, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10012.2578125
tensor(10012.2568, grad_fn=<NegBackward0>) tensor(10012.2578, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -10012.255859375
tensor(10012.2568, grad_fn=<NegBackward0>) tensor(10012.2559, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10012.265625
tensor(10012.2559, grad_fn=<NegBackward0>) tensor(10012.2656, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -10012.2548828125
tensor(10012.2559, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10012.2548828125
tensor(10012.2549, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10012.2548828125
tensor(10012.2549, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10012.2548828125
tensor(10012.2549, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10012.2548828125
tensor(10012.2549, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10012.25390625
tensor(10012.2549, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10012.2578125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2578, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -10012.2548828125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10012.2548828125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10012.2548828125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -10012.2548828125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -10012.2548828125
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10012.25390625
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10012.255859375
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2559, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10012.2529296875
tensor(10012.2539, grad_fn=<NegBackward0>) tensor(10012.2529, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10012.2529296875
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2529, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10012.25390625
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -10012.25390625
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -10012.2548828125
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -10012.2548828125
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2549, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -10012.25390625
tensor(10012.2529, grad_fn=<NegBackward0>) tensor(10012.2539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.7665, 0.2335],
        [0.2450, 0.7550]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3079, 0.6921], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2184, 0.1579],
         [0.5255, 0.1312]],

        [[0.7272, 0.1225],
         [0.7109, 0.6971]],

        [[0.6348, 0.0890],
         [0.6038, 0.7029]],

        [[0.5162, 0.1151],
         [0.6924, 0.5581]],

        [[0.5992, 0.0966],
         [0.5280, 0.6490]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.121519254935582
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 25
Adjusted Rand Index: 0.24248656859821266
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 8
Adjusted Rand Index: 0.7026542224128787
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 13
Adjusted Rand Index: 0.5430195604230916
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 17
Adjusted Rand Index: 0.4298989898989899
Global Adjusted Rand Index: 0.38314542014039565
Average Adjusted Rand Index: 0.407915719253751
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22151.052734375
inf tensor(22151.0527, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10035.5009765625
tensor(22151.0527, grad_fn=<NegBackward0>) tensor(10035.5010, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10032.4716796875
tensor(10035.5010, grad_fn=<NegBackward0>) tensor(10032.4717, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10031.3466796875
tensor(10032.4717, grad_fn=<NegBackward0>) tensor(10031.3467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10030.841796875
tensor(10031.3467, grad_fn=<NegBackward0>) tensor(10030.8418, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10030.224609375
tensor(10030.8418, grad_fn=<NegBackward0>) tensor(10030.2246, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10029.744140625
tensor(10030.2246, grad_fn=<NegBackward0>) tensor(10029.7441, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10029.4970703125
tensor(10029.7441, grad_fn=<NegBackward0>) tensor(10029.4971, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10029.34765625
tensor(10029.4971, grad_fn=<NegBackward0>) tensor(10029.3477, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10029.2568359375
tensor(10029.3477, grad_fn=<NegBackward0>) tensor(10029.2568, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10029.1982421875
tensor(10029.2568, grad_fn=<NegBackward0>) tensor(10029.1982, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10029.1630859375
tensor(10029.1982, grad_fn=<NegBackward0>) tensor(10029.1631, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10029.1416015625
tensor(10029.1631, grad_fn=<NegBackward0>) tensor(10029.1416, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10029.130859375
tensor(10029.1416, grad_fn=<NegBackward0>) tensor(10029.1309, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10029.1240234375
tensor(10029.1309, grad_fn=<NegBackward0>) tensor(10029.1240, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10029.1181640625
tensor(10029.1240, grad_fn=<NegBackward0>) tensor(10029.1182, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10029.115234375
tensor(10029.1182, grad_fn=<NegBackward0>) tensor(10029.1152, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10029.1142578125
tensor(10029.1152, grad_fn=<NegBackward0>) tensor(10029.1143, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10029.11328125
tensor(10029.1143, grad_fn=<NegBackward0>) tensor(10029.1133, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10029.1123046875
tensor(10029.1133, grad_fn=<NegBackward0>) tensor(10029.1123, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10029.111328125
tensor(10029.1123, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10029.1123046875
tensor(10029.1113, grad_fn=<NegBackward0>) tensor(10029.1123, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -10029.111328125
tensor(10029.1113, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10029.111328125
tensor(10029.1113, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10029.111328125
tensor(10029.1113, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10029.1103515625
tensor(10029.1113, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10029.1103515625
tensor(10029.1104, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10029.1123046875
tensor(10029.1104, grad_fn=<NegBackward0>) tensor(10029.1123, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10029.1103515625
tensor(10029.1104, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10029.1103515625
tensor(10029.1104, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10029.109375
tensor(10029.1104, grad_fn=<NegBackward0>) tensor(10029.1094, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10029.111328125
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -10029.1123046875
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1123, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -10029.111328125
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
4
Iteration 3500: Loss = -10029.109375
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1094, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10029.111328125
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -10029.109375
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1094, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10029.109375
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1094, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10029.109375
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1094, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10029.111328125
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10029.111328125
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1113, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -10029.1103515625
tensor(10029.1094, grad_fn=<NegBackward0>) tensor(10029.1104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.1064, 0.8936],
        [0.0494, 0.9506]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2306, 0.7694], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2331, 0.1678],
         [0.5831, 0.1344]],

        [[0.7282, 0.1716],
         [0.6330, 0.5799]],

        [[0.5122, 0.1679],
         [0.7103, 0.7106]],

        [[0.7156, 0.1900],
         [0.6328, 0.6820]],

        [[0.5218, 0.0892],
         [0.5115, 0.5115]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0067428906479038405
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.003317229704443994
Average Adjusted Rand Index: 0.0004481037487065735
[0.38314542014039565, -0.003317229704443994] [0.407915719253751, 0.0004481037487065735] [10012.25390625, 10029.1103515625]
-------------------------------------
This iteration is 45
True Objective function: Loss = -9864.111810867535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24407.296875
inf tensor(24407.2969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9709.1103515625
tensor(24407.2969, grad_fn=<NegBackward0>) tensor(9709.1104, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9704.8095703125
tensor(9709.1104, grad_fn=<NegBackward0>) tensor(9704.8096, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9702.744140625
tensor(9704.8096, grad_fn=<NegBackward0>) tensor(9702.7441, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9701.875
tensor(9702.7441, grad_fn=<NegBackward0>) tensor(9701.8750, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9701.15234375
tensor(9701.8750, grad_fn=<NegBackward0>) tensor(9701.1523, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9700.48828125
tensor(9701.1523, grad_fn=<NegBackward0>) tensor(9700.4883, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9699.955078125
tensor(9700.4883, grad_fn=<NegBackward0>) tensor(9699.9551, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9699.623046875
tensor(9699.9551, grad_fn=<NegBackward0>) tensor(9699.6230, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9699.3525390625
tensor(9699.6230, grad_fn=<NegBackward0>) tensor(9699.3525, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9699.150390625
tensor(9699.3525, grad_fn=<NegBackward0>) tensor(9699.1504, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9698.998046875
tensor(9699.1504, grad_fn=<NegBackward0>) tensor(9698.9980, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9698.87109375
tensor(9698.9980, grad_fn=<NegBackward0>) tensor(9698.8711, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9698.755859375
tensor(9698.8711, grad_fn=<NegBackward0>) tensor(9698.7559, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9698.6689453125
tensor(9698.7559, grad_fn=<NegBackward0>) tensor(9698.6689, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9698.5830078125
tensor(9698.6689, grad_fn=<NegBackward0>) tensor(9698.5830, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9698.5029296875
tensor(9698.5830, grad_fn=<NegBackward0>) tensor(9698.5029, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9698.4306640625
tensor(9698.5029, grad_fn=<NegBackward0>) tensor(9698.4307, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9698.3798828125
tensor(9698.4307, grad_fn=<NegBackward0>) tensor(9698.3799, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9698.3505859375
tensor(9698.3799, grad_fn=<NegBackward0>) tensor(9698.3506, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9698.3203125
tensor(9698.3506, grad_fn=<NegBackward0>) tensor(9698.3203, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9698.28515625
tensor(9698.3203, grad_fn=<NegBackward0>) tensor(9698.2852, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9698.267578125
tensor(9698.2852, grad_fn=<NegBackward0>) tensor(9698.2676, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9698.2509765625
tensor(9698.2676, grad_fn=<NegBackward0>) tensor(9698.2510, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9698.2373046875
tensor(9698.2510, grad_fn=<NegBackward0>) tensor(9698.2373, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9698.22265625
tensor(9698.2373, grad_fn=<NegBackward0>) tensor(9698.2227, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9698.2099609375
tensor(9698.2227, grad_fn=<NegBackward0>) tensor(9698.2100, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9698.197265625
tensor(9698.2100, grad_fn=<NegBackward0>) tensor(9698.1973, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9698.1650390625
tensor(9698.1973, grad_fn=<NegBackward0>) tensor(9698.1650, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9698.158203125
tensor(9698.1650, grad_fn=<NegBackward0>) tensor(9698.1582, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9698.15234375
tensor(9698.1582, grad_fn=<NegBackward0>) tensor(9698.1523, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9698.1484375
tensor(9698.1523, grad_fn=<NegBackward0>) tensor(9698.1484, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9698.142578125
tensor(9698.1484, grad_fn=<NegBackward0>) tensor(9698.1426, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9698.1396484375
tensor(9698.1426, grad_fn=<NegBackward0>) tensor(9698.1396, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9698.1328125
tensor(9698.1396, grad_fn=<NegBackward0>) tensor(9698.1328, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9698.12890625
tensor(9698.1328, grad_fn=<NegBackward0>) tensor(9698.1289, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9698.123046875
tensor(9698.1289, grad_fn=<NegBackward0>) tensor(9698.1230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9698.1201171875
tensor(9698.1230, grad_fn=<NegBackward0>) tensor(9698.1201, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9698.115234375
tensor(9698.1201, grad_fn=<NegBackward0>) tensor(9698.1152, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9698.111328125
tensor(9698.1152, grad_fn=<NegBackward0>) tensor(9698.1113, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9698.107421875
tensor(9698.1113, grad_fn=<NegBackward0>) tensor(9698.1074, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9698.103515625
tensor(9698.1074, grad_fn=<NegBackward0>) tensor(9698.1035, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9698.1005859375
tensor(9698.1035, grad_fn=<NegBackward0>) tensor(9698.1006, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9698.0947265625
tensor(9698.1006, grad_fn=<NegBackward0>) tensor(9698.0947, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9698.0810546875
tensor(9698.0947, grad_fn=<NegBackward0>) tensor(9698.0811, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9698.0791015625
tensor(9698.0811, grad_fn=<NegBackward0>) tensor(9698.0791, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9698.076171875
tensor(9698.0791, grad_fn=<NegBackward0>) tensor(9698.0762, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9698.07421875
tensor(9698.0762, grad_fn=<NegBackward0>) tensor(9698.0742, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9698.07421875
tensor(9698.0742, grad_fn=<NegBackward0>) tensor(9698.0742, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9698.0712890625
tensor(9698.0742, grad_fn=<NegBackward0>) tensor(9698.0713, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9698.0712890625
tensor(9698.0713, grad_fn=<NegBackward0>) tensor(9698.0713, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9698.068359375
tensor(9698.0713, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9698.06640625
tensor(9698.0684, grad_fn=<NegBackward0>) tensor(9698.0664, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9698.06640625
tensor(9698.0664, grad_fn=<NegBackward0>) tensor(9698.0664, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9698.0625
tensor(9698.0664, grad_fn=<NegBackward0>) tensor(9698.0625, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9698.0615234375
tensor(9698.0625, grad_fn=<NegBackward0>) tensor(9698.0615, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9698.0615234375
tensor(9698.0615, grad_fn=<NegBackward0>) tensor(9698.0615, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9698.060546875
tensor(9698.0615, grad_fn=<NegBackward0>) tensor(9698.0605, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9698.0576171875
tensor(9698.0605, grad_fn=<NegBackward0>) tensor(9698.0576, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9698.056640625
tensor(9698.0576, grad_fn=<NegBackward0>) tensor(9698.0566, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9698.0546875
tensor(9698.0566, grad_fn=<NegBackward0>) tensor(9698.0547, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9698.0537109375
tensor(9698.0547, grad_fn=<NegBackward0>) tensor(9698.0537, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9698.052734375
tensor(9698.0537, grad_fn=<NegBackward0>) tensor(9698.0527, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9698.0517578125
tensor(9698.0527, grad_fn=<NegBackward0>) tensor(9698.0518, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9698.05078125
tensor(9698.0518, grad_fn=<NegBackward0>) tensor(9698.0508, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9698.0498046875
tensor(9698.0508, grad_fn=<NegBackward0>) tensor(9698.0498, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9698.048828125
tensor(9698.0498, grad_fn=<NegBackward0>) tensor(9698.0488, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9698.046875
tensor(9698.0488, grad_fn=<NegBackward0>) tensor(9698.0469, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9698.0458984375
tensor(9698.0469, grad_fn=<NegBackward0>) tensor(9698.0459, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9698.0283203125
tensor(9698.0459, grad_fn=<NegBackward0>) tensor(9698.0283, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9698.0234375
tensor(9698.0283, grad_fn=<NegBackward0>) tensor(9698.0234, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9698.0224609375
tensor(9698.0234, grad_fn=<NegBackward0>) tensor(9698.0225, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9698.0244140625
tensor(9698.0225, grad_fn=<NegBackward0>) tensor(9698.0244, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9698.0224609375
tensor(9698.0225, grad_fn=<NegBackward0>) tensor(9698.0225, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9698.0224609375
tensor(9698.0225, grad_fn=<NegBackward0>) tensor(9698.0225, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9698.0224609375
tensor(9698.0225, grad_fn=<NegBackward0>) tensor(9698.0225, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9698.021484375
tensor(9698.0225, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9698.021484375
tensor(9698.0215, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9698.0205078125
tensor(9698.0215, grad_fn=<NegBackward0>) tensor(9698.0205, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9698.021484375
tensor(9698.0205, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9698.021484375
tensor(9698.0205, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9698.0205078125
tensor(9698.0205, grad_fn=<NegBackward0>) tensor(9698.0205, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9698.021484375
tensor(9698.0205, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9698.017578125
tensor(9698.0205, grad_fn=<NegBackward0>) tensor(9698.0176, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9698.017578125
tensor(9698.0176, grad_fn=<NegBackward0>) tensor(9698.0176, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9698.0185546875
tensor(9698.0176, grad_fn=<NegBackward0>) tensor(9698.0186, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9698.017578125
tensor(9698.0176, grad_fn=<NegBackward0>) tensor(9698.0176, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9698.015625
tensor(9698.0176, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9698.0166015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0166, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9698.0400390625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0400, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9698.021484375
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0215, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9698.0732421875
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0732, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9698.0517578125
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0518, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9698.015625
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0156, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9698.013671875
tensor(9698.0156, grad_fn=<NegBackward0>) tensor(9698.0137, grad_fn=<NegBackward0>)
pi: tensor([[9.9971e-01, 2.8888e-04],
        [2.1786e-02, 9.7821e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0179, 0.9821], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2374, 0.1206],
         [0.6074, 0.1286]],

        [[0.5633, 0.2223],
         [0.5403, 0.5058]],

        [[0.6379, 0.2137],
         [0.6133, 0.6688]],

        [[0.6746, 0.1655],
         [0.5749, 0.6290]],

        [[0.5433, 0.1299],
         [0.6988, 0.5227]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.01357925919519104
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.012368369840205852
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0022264033076300205
Global Adjusted Rand Index: 0.006223057865800915
Average Adjusted Rand Index: 0.0006885220107029893
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21308.296875
inf tensor(21308.2969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9709.8935546875
tensor(21308.2969, grad_fn=<NegBackward0>) tensor(9709.8936, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9706.0947265625
tensor(9709.8936, grad_fn=<NegBackward0>) tensor(9706.0947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9702.814453125
tensor(9706.0947, grad_fn=<NegBackward0>) tensor(9702.8145, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9701.611328125
tensor(9702.8145, grad_fn=<NegBackward0>) tensor(9701.6113, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9701.0537109375
tensor(9701.6113, grad_fn=<NegBackward0>) tensor(9701.0537, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9700.5048828125
tensor(9701.0537, grad_fn=<NegBackward0>) tensor(9700.5049, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9700.0029296875
tensor(9700.5049, grad_fn=<NegBackward0>) tensor(9700.0029, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9699.611328125
tensor(9700.0029, grad_fn=<NegBackward0>) tensor(9699.6113, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9699.3251953125
tensor(9699.6113, grad_fn=<NegBackward0>) tensor(9699.3252, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9699.1181640625
tensor(9699.3252, grad_fn=<NegBackward0>) tensor(9699.1182, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9698.9638671875
tensor(9699.1182, grad_fn=<NegBackward0>) tensor(9698.9639, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9698.8515625
tensor(9698.9639, grad_fn=<NegBackward0>) tensor(9698.8516, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9698.75
tensor(9698.8516, grad_fn=<NegBackward0>) tensor(9698.7500, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9698.6640625
tensor(9698.7500, grad_fn=<NegBackward0>) tensor(9698.6641, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9698.587890625
tensor(9698.6641, grad_fn=<NegBackward0>) tensor(9698.5879, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9698.525390625
tensor(9698.5879, grad_fn=<NegBackward0>) tensor(9698.5254, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9698.4755859375
tensor(9698.5254, grad_fn=<NegBackward0>) tensor(9698.4756, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9698.435546875
tensor(9698.4756, grad_fn=<NegBackward0>) tensor(9698.4355, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9698.3984375
tensor(9698.4355, grad_fn=<NegBackward0>) tensor(9698.3984, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9698.3662109375
tensor(9698.3984, grad_fn=<NegBackward0>) tensor(9698.3662, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9698.33984375
tensor(9698.3662, grad_fn=<NegBackward0>) tensor(9698.3398, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9698.3173828125
tensor(9698.3398, grad_fn=<NegBackward0>) tensor(9698.3174, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9698.30078125
tensor(9698.3174, grad_fn=<NegBackward0>) tensor(9698.3008, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9698.2880859375
tensor(9698.3008, grad_fn=<NegBackward0>) tensor(9698.2881, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9698.2763671875
tensor(9698.2881, grad_fn=<NegBackward0>) tensor(9698.2764, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9698.2666015625
tensor(9698.2764, grad_fn=<NegBackward0>) tensor(9698.2666, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9698.2568359375
tensor(9698.2666, grad_fn=<NegBackward0>) tensor(9698.2568, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9698.244140625
tensor(9698.2568, grad_fn=<NegBackward0>) tensor(9698.2441, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9698.2294921875
tensor(9698.2441, grad_fn=<NegBackward0>) tensor(9698.2295, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9698.21484375
tensor(9698.2295, grad_fn=<NegBackward0>) tensor(9698.2148, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9698.203125
tensor(9698.2148, grad_fn=<NegBackward0>) tensor(9698.2031, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9698.197265625
tensor(9698.2031, grad_fn=<NegBackward0>) tensor(9698.1973, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9698.19140625
tensor(9698.1973, grad_fn=<NegBackward0>) tensor(9698.1914, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9698.1845703125
tensor(9698.1914, grad_fn=<NegBackward0>) tensor(9698.1846, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9698.1787109375
tensor(9698.1846, grad_fn=<NegBackward0>) tensor(9698.1787, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9698.173828125
tensor(9698.1787, grad_fn=<NegBackward0>) tensor(9698.1738, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9698.1669921875
tensor(9698.1738, grad_fn=<NegBackward0>) tensor(9698.1670, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9698.1650390625
tensor(9698.1670, grad_fn=<NegBackward0>) tensor(9698.1650, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9698.16015625
tensor(9698.1650, grad_fn=<NegBackward0>) tensor(9698.1602, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9698.15625
tensor(9698.1602, grad_fn=<NegBackward0>) tensor(9698.1562, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9698.1513671875
tensor(9698.1562, grad_fn=<NegBackward0>) tensor(9698.1514, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9698.1416015625
tensor(9698.1514, grad_fn=<NegBackward0>) tensor(9698.1416, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9698.138671875
tensor(9698.1416, grad_fn=<NegBackward0>) tensor(9698.1387, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9698.1357421875
tensor(9698.1387, grad_fn=<NegBackward0>) tensor(9698.1357, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9698.1337890625
tensor(9698.1357, grad_fn=<NegBackward0>) tensor(9698.1338, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9698.1318359375
tensor(9698.1338, grad_fn=<NegBackward0>) tensor(9698.1318, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9698.1279296875
tensor(9698.1318, grad_fn=<NegBackward0>) tensor(9698.1279, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9698.1220703125
tensor(9698.1279, grad_fn=<NegBackward0>) tensor(9698.1221, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9698.119140625
tensor(9698.1221, grad_fn=<NegBackward0>) tensor(9698.1191, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9698.1171875
tensor(9698.1191, grad_fn=<NegBackward0>) tensor(9698.1172, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9698.115234375
tensor(9698.1172, grad_fn=<NegBackward0>) tensor(9698.1152, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9698.1142578125
tensor(9698.1152, grad_fn=<NegBackward0>) tensor(9698.1143, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9698.109375
tensor(9698.1143, grad_fn=<NegBackward0>) tensor(9698.1094, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9698.109375
tensor(9698.1094, grad_fn=<NegBackward0>) tensor(9698.1094, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9698.1083984375
tensor(9698.1094, grad_fn=<NegBackward0>) tensor(9698.1084, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9698.1064453125
tensor(9698.1084, grad_fn=<NegBackward0>) tensor(9698.1064, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9698.1064453125
tensor(9698.1064, grad_fn=<NegBackward0>) tensor(9698.1064, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9698.10546875
tensor(9698.1064, grad_fn=<NegBackward0>) tensor(9698.1055, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9698.10546875
tensor(9698.1055, grad_fn=<NegBackward0>) tensor(9698.1055, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9698.103515625
tensor(9698.1055, grad_fn=<NegBackward0>) tensor(9698.1035, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9698.103515625
tensor(9698.1035, grad_fn=<NegBackward0>) tensor(9698.1035, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9698.1015625
tensor(9698.1035, grad_fn=<NegBackward0>) tensor(9698.1016, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9698.1015625
tensor(9698.1016, grad_fn=<NegBackward0>) tensor(9698.1016, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9698.099609375
tensor(9698.1016, grad_fn=<NegBackward0>) tensor(9698.0996, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9698.0986328125
tensor(9698.0996, grad_fn=<NegBackward0>) tensor(9698.0986, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9698.09765625
tensor(9698.0986, grad_fn=<NegBackward0>) tensor(9698.0977, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9698.09765625
tensor(9698.0977, grad_fn=<NegBackward0>) tensor(9698.0977, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9698.095703125
tensor(9698.0977, grad_fn=<NegBackward0>) tensor(9698.0957, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9698.0966796875
tensor(9698.0957, grad_fn=<NegBackward0>) tensor(9698.0967, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9698.0947265625
tensor(9698.0957, grad_fn=<NegBackward0>) tensor(9698.0947, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9698.09375
tensor(9698.0947, grad_fn=<NegBackward0>) tensor(9698.0938, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9698.09375
tensor(9698.0938, grad_fn=<NegBackward0>) tensor(9698.0938, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9698.0947265625
tensor(9698.0938, grad_fn=<NegBackward0>) tensor(9698.0947, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9698.0927734375
tensor(9698.0938, grad_fn=<NegBackward0>) tensor(9698.0928, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9698.0927734375
tensor(9698.0928, grad_fn=<NegBackward0>) tensor(9698.0928, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9698.0927734375
tensor(9698.0928, grad_fn=<NegBackward0>) tensor(9698.0928, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9698.0927734375
tensor(9698.0928, grad_fn=<NegBackward0>) tensor(9698.0928, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9698.0908203125
tensor(9698.0928, grad_fn=<NegBackward0>) tensor(9698.0908, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9698.0830078125
tensor(9698.0908, grad_fn=<NegBackward0>) tensor(9698.0830, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9698.0751953125
tensor(9698.0830, grad_fn=<NegBackward0>) tensor(9698.0752, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9698.072265625
tensor(9698.0752, grad_fn=<NegBackward0>) tensor(9698.0723, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9698.07421875
tensor(9698.0723, grad_fn=<NegBackward0>) tensor(9698.0742, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9698.0673828125
tensor(9698.0723, grad_fn=<NegBackward0>) tensor(9698.0674, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9698.068359375
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9698.068359375
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9698.068359375
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9698.068359375
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9698.0673828125
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0674, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9698.0673828125
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0674, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9698.068359375
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0684, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9698.0634765625
tensor(9698.0674, grad_fn=<NegBackward0>) tensor(9698.0635, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9698.0654296875
tensor(9698.0635, grad_fn=<NegBackward0>) tensor(9698.0654, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9698.0634765625
tensor(9698.0635, grad_fn=<NegBackward0>) tensor(9698.0635, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9698.23046875
tensor(9698.0635, grad_fn=<NegBackward0>) tensor(9698.2305, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9698.064453125
tensor(9698.0635, grad_fn=<NegBackward0>) tensor(9698.0645, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9698.0625
tensor(9698.0635, grad_fn=<NegBackward0>) tensor(9698.0625, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9698.0625
tensor(9698.0625, grad_fn=<NegBackward0>) tensor(9698.0625, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9698.0625
tensor(9698.0625, grad_fn=<NegBackward0>) tensor(9698.0625, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9698.0625
tensor(9698.0625, grad_fn=<NegBackward0>) tensor(9698.0625, grad_fn=<NegBackward0>)
pi: tensor([[9.7399e-01, 2.6013e-02],
        [1.2987e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9924, 0.0076], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1282, 0.1243],
         [0.5565, 0.2363]],

        [[0.5302, 0.2276],
         [0.5941, 0.6527]],

        [[0.6683, 0.2144],
         [0.5402, 0.6575]],

        [[0.7164, 0.1656],
         [0.6501, 0.6805]],

        [[0.5649, 0.1303],
         [0.5304, 0.6916]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.01333633701284073
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.006127506195088984
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.007984204356142407
Global Adjusted Rand Index: 0.005494712108077148
Average Adjusted Rand Index: 0.0006404939278521549
[0.006223057865800915, 0.005494712108077148] [0.0006885220107029893, 0.0006404939278521549] [9698.0224609375, 9698.0625]
-------------------------------------
This iteration is 46
True Objective function: Loss = -9961.666096472329
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23578.27734375
inf tensor(23578.2773, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9865.52734375
tensor(23578.2773, grad_fn=<NegBackward0>) tensor(9865.5273, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9864.703125
tensor(9865.5273, grad_fn=<NegBackward0>) tensor(9864.7031, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9864.353515625
tensor(9864.7031, grad_fn=<NegBackward0>) tensor(9864.3535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9863.5810546875
tensor(9864.3535, grad_fn=<NegBackward0>) tensor(9863.5811, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9862.7998046875
tensor(9863.5811, grad_fn=<NegBackward0>) tensor(9862.7998, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9862.62890625
tensor(9862.7998, grad_fn=<NegBackward0>) tensor(9862.6289, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9862.513671875
tensor(9862.6289, grad_fn=<NegBackward0>) tensor(9862.5137, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9862.423828125
tensor(9862.5137, grad_fn=<NegBackward0>) tensor(9862.4238, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9862.3388671875
tensor(9862.4238, grad_fn=<NegBackward0>) tensor(9862.3389, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9862.20703125
tensor(9862.3389, grad_fn=<NegBackward0>) tensor(9862.2070, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9861.810546875
tensor(9862.2070, grad_fn=<NegBackward0>) tensor(9861.8105, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9861.6552734375
tensor(9861.8105, grad_fn=<NegBackward0>) tensor(9861.6553, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9861.5927734375
tensor(9861.6553, grad_fn=<NegBackward0>) tensor(9861.5928, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9861.5009765625
tensor(9861.5928, grad_fn=<NegBackward0>) tensor(9861.5010, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9861.3076171875
tensor(9861.5010, grad_fn=<NegBackward0>) tensor(9861.3076, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9860.9765625
tensor(9861.3076, grad_fn=<NegBackward0>) tensor(9860.9766, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9860.75390625
tensor(9860.9766, grad_fn=<NegBackward0>) tensor(9860.7539, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9860.609375
tensor(9860.7539, grad_fn=<NegBackward0>) tensor(9860.6094, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9860.521484375
tensor(9860.6094, grad_fn=<NegBackward0>) tensor(9860.5215, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9860.4755859375
tensor(9860.5215, grad_fn=<NegBackward0>) tensor(9860.4756, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9860.453125
tensor(9860.4756, grad_fn=<NegBackward0>) tensor(9860.4531, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9860.4443359375
tensor(9860.4531, grad_fn=<NegBackward0>) tensor(9860.4443, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9860.439453125
tensor(9860.4443, grad_fn=<NegBackward0>) tensor(9860.4395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9860.4375
tensor(9860.4395, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9860.4375
tensor(9860.4375, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9860.4375
tensor(9860.4375, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9860.4365234375
tensor(9860.4375, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9860.4365234375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9860.435546875
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4355, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9860.4365234375
tensor(9860.4355, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9860.4384765625
tensor(9860.4355, grad_fn=<NegBackward0>) tensor(9860.4385, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9860.4365234375
tensor(9860.4355, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -9860.4365234375
tensor(9860.4355, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -9860.4375
tensor(9860.4355, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.6050, 0.3950],
        [0.5582, 0.4418]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5974, 0.4026], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1127, 0.1330],
         [0.5469, 0.1772]],

        [[0.6173, 0.1395],
         [0.5750, 0.6760]],

        [[0.6909, 0.1340],
         [0.5625, 0.7188]],

        [[0.5744, 0.1479],
         [0.5619, 0.7252]],

        [[0.6995, 0.1421],
         [0.6581, 0.5934]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18650036683785767
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.09427016903727861
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.007945558212249402
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12130383005217259
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 67
Adjusted Rand Index: 0.10727875591880681
Global Adjusted Rand Index: 0.07672159457489133
Average Adjusted Rand Index: 0.10028151272677326
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21079.888671875
inf tensor(21079.8887, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9866.2529296875
tensor(21079.8887, grad_fn=<NegBackward0>) tensor(9866.2529, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9865.515625
tensor(9866.2529, grad_fn=<NegBackward0>) tensor(9865.5156, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9864.9521484375
tensor(9865.5156, grad_fn=<NegBackward0>) tensor(9864.9521, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9864.3388671875
tensor(9864.9521, grad_fn=<NegBackward0>) tensor(9864.3389, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9863.72265625
tensor(9864.3389, grad_fn=<NegBackward0>) tensor(9863.7227, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9863.1728515625
tensor(9863.7227, grad_fn=<NegBackward0>) tensor(9863.1729, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9862.708984375
tensor(9863.1729, grad_fn=<NegBackward0>) tensor(9862.7090, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9862.4072265625
tensor(9862.7090, grad_fn=<NegBackward0>) tensor(9862.4072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9862.2021484375
tensor(9862.4072, grad_fn=<NegBackward0>) tensor(9862.2021, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9862.046875
tensor(9862.2021, grad_fn=<NegBackward0>) tensor(9862.0469, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9861.92578125
tensor(9862.0469, grad_fn=<NegBackward0>) tensor(9861.9258, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9861.822265625
tensor(9861.9258, grad_fn=<NegBackward0>) tensor(9861.8223, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9861.740234375
tensor(9861.8223, grad_fn=<NegBackward0>) tensor(9861.7402, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9861.66796875
tensor(9861.7402, grad_fn=<NegBackward0>) tensor(9861.6680, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9861.60546875
tensor(9861.6680, grad_fn=<NegBackward0>) tensor(9861.6055, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9861.548828125
tensor(9861.6055, grad_fn=<NegBackward0>) tensor(9861.5488, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9861.4931640625
tensor(9861.5488, grad_fn=<NegBackward0>) tensor(9861.4932, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9861.4365234375
tensor(9861.4932, grad_fn=<NegBackward0>) tensor(9861.4365, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9861.36328125
tensor(9861.4365, grad_fn=<NegBackward0>) tensor(9861.3633, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9861.267578125
tensor(9861.3633, grad_fn=<NegBackward0>) tensor(9861.2676, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9861.1416015625
tensor(9861.2676, grad_fn=<NegBackward0>) tensor(9861.1416, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9861.0048828125
tensor(9861.1416, grad_fn=<NegBackward0>) tensor(9861.0049, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9860.880859375
tensor(9861.0049, grad_fn=<NegBackward0>) tensor(9860.8809, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9860.7763671875
tensor(9860.8809, grad_fn=<NegBackward0>) tensor(9860.7764, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9860.6962890625
tensor(9860.7764, grad_fn=<NegBackward0>) tensor(9860.6963, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9860.611328125
tensor(9860.6963, grad_fn=<NegBackward0>) tensor(9860.6113, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9860.552734375
tensor(9860.6113, grad_fn=<NegBackward0>) tensor(9860.5527, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9860.525390625
tensor(9860.5527, grad_fn=<NegBackward0>) tensor(9860.5254, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9860.4833984375
tensor(9860.5254, grad_fn=<NegBackward0>) tensor(9860.4834, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9860.4677734375
tensor(9860.4834, grad_fn=<NegBackward0>) tensor(9860.4678, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9860.4521484375
tensor(9860.4678, grad_fn=<NegBackward0>) tensor(9860.4521, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9860.4482421875
tensor(9860.4521, grad_fn=<NegBackward0>) tensor(9860.4482, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9860.4423828125
tensor(9860.4482, grad_fn=<NegBackward0>) tensor(9860.4424, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9860.4404296875
tensor(9860.4424, grad_fn=<NegBackward0>) tensor(9860.4404, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9860.4375
tensor(9860.4404, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9860.4375
tensor(9860.4375, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9860.4365234375
tensor(9860.4375, grad_fn=<NegBackward0>) tensor(9860.4365, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9860.4375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9860.4375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -9860.4375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -9860.4375
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4375, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -9860.4501953125
tensor(9860.4365, grad_fn=<NegBackward0>) tensor(9860.4502, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.6057, 0.3943],
        [0.5654, 0.4346]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5984, 0.4016], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1127, 0.1331],
         [0.5603, 0.1772]],

        [[0.6402, 0.1395],
         [0.6128, 0.5068]],

        [[0.7181, 0.1340],
         [0.5246, 0.5435]],

        [[0.5405, 0.1479],
         [0.6017, 0.6351]],

        [[0.6321, 0.1421],
         [0.6128, 0.5020]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18650036683785767
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.09452533377548161
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.010131235630083585
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12130383005217259
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 67
Adjusted Rand Index: 0.10727875591880681
Global Adjusted Rand Index: 0.07899398687880535
Average Adjusted Rand Index: 0.09989541019084704
[0.07672159457489133, 0.07899398687880535] [0.10028151272677326, 0.09989541019084704] [9860.4375, 9860.4501953125]
-------------------------------------
This iteration is 47
True Objective function: Loss = -9998.636738025833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23106.1953125
inf tensor(23106.1953, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9871.2919921875
tensor(23106.1953, grad_fn=<NegBackward0>) tensor(9871.2920, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9869.87890625
tensor(9871.2920, grad_fn=<NegBackward0>) tensor(9869.8789, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9869.474609375
tensor(9869.8789, grad_fn=<NegBackward0>) tensor(9869.4746, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9868.95703125
tensor(9869.4746, grad_fn=<NegBackward0>) tensor(9868.9570, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9868.5107421875
tensor(9868.9570, grad_fn=<NegBackward0>) tensor(9868.5107, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9868.0751953125
tensor(9868.5107, grad_fn=<NegBackward0>) tensor(9868.0752, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9867.7490234375
tensor(9868.0752, grad_fn=<NegBackward0>) tensor(9867.7490, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9867.498046875
tensor(9867.7490, grad_fn=<NegBackward0>) tensor(9867.4980, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9867.3466796875
tensor(9867.4980, grad_fn=<NegBackward0>) tensor(9867.3467, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9867.2470703125
tensor(9867.3467, grad_fn=<NegBackward0>) tensor(9867.2471, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9867.1494140625
tensor(9867.2471, grad_fn=<NegBackward0>) tensor(9867.1494, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9867.0673828125
tensor(9867.1494, grad_fn=<NegBackward0>) tensor(9867.0674, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9867.0078125
tensor(9867.0674, grad_fn=<NegBackward0>) tensor(9867.0078, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9866.9560546875
tensor(9867.0078, grad_fn=<NegBackward0>) tensor(9866.9561, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9866.888671875
tensor(9866.9561, grad_fn=<NegBackward0>) tensor(9866.8887, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9866.748046875
tensor(9866.8887, grad_fn=<NegBackward0>) tensor(9866.7480, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9866.646484375
tensor(9866.7480, grad_fn=<NegBackward0>) tensor(9866.6465, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9866.6123046875
tensor(9866.6465, grad_fn=<NegBackward0>) tensor(9866.6123, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9866.5888671875
tensor(9866.6123, grad_fn=<NegBackward0>) tensor(9866.5889, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9866.57421875
tensor(9866.5889, grad_fn=<NegBackward0>) tensor(9866.5742, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9866.5654296875
tensor(9866.5742, grad_fn=<NegBackward0>) tensor(9866.5654, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9866.5576171875
tensor(9866.5654, grad_fn=<NegBackward0>) tensor(9866.5576, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9866.5283203125
tensor(9866.5576, grad_fn=<NegBackward0>) tensor(9866.5283, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9866.517578125
tensor(9866.5283, grad_fn=<NegBackward0>) tensor(9866.5176, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9866.5107421875
tensor(9866.5176, grad_fn=<NegBackward0>) tensor(9866.5107, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9866.4931640625
tensor(9866.5107, grad_fn=<NegBackward0>) tensor(9866.4932, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9866.4775390625
tensor(9866.4932, grad_fn=<NegBackward0>) tensor(9866.4775, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9866.4736328125
tensor(9866.4775, grad_fn=<NegBackward0>) tensor(9866.4736, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9866.4716796875
tensor(9866.4736, grad_fn=<NegBackward0>) tensor(9866.4717, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9866.46875
tensor(9866.4717, grad_fn=<NegBackward0>) tensor(9866.4688, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9866.466796875
tensor(9866.4688, grad_fn=<NegBackward0>) tensor(9866.4668, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9866.4609375
tensor(9866.4668, grad_fn=<NegBackward0>) tensor(9866.4609, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9866.458984375
tensor(9866.4609, grad_fn=<NegBackward0>) tensor(9866.4590, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9866.45703125
tensor(9866.4590, grad_fn=<NegBackward0>) tensor(9866.4570, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9866.4521484375
tensor(9866.4570, grad_fn=<NegBackward0>) tensor(9866.4521, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9866.451171875
tensor(9866.4521, grad_fn=<NegBackward0>) tensor(9866.4512, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9866.44921875
tensor(9866.4512, grad_fn=<NegBackward0>) tensor(9866.4492, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9866.447265625
tensor(9866.4492, grad_fn=<NegBackward0>) tensor(9866.4473, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9866.447265625
tensor(9866.4473, grad_fn=<NegBackward0>) tensor(9866.4473, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9866.4453125
tensor(9866.4473, grad_fn=<NegBackward0>) tensor(9866.4453, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9866.4443359375
tensor(9866.4453, grad_fn=<NegBackward0>) tensor(9866.4443, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9866.4423828125
tensor(9866.4443, grad_fn=<NegBackward0>) tensor(9866.4424, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9866.443359375
tensor(9866.4424, grad_fn=<NegBackward0>) tensor(9866.4434, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9866.4423828125
tensor(9866.4424, grad_fn=<NegBackward0>) tensor(9866.4424, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9866.439453125
tensor(9866.4424, grad_fn=<NegBackward0>) tensor(9866.4395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9866.4404296875
tensor(9866.4395, grad_fn=<NegBackward0>) tensor(9866.4404, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9866.4375
tensor(9866.4395, grad_fn=<NegBackward0>) tensor(9866.4375, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9866.4384765625
tensor(9866.4375, grad_fn=<NegBackward0>) tensor(9866.4385, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9866.4365234375
tensor(9866.4375, grad_fn=<NegBackward0>) tensor(9866.4365, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9866.4375
tensor(9866.4365, grad_fn=<NegBackward0>) tensor(9866.4375, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9866.4365234375
tensor(9866.4365, grad_fn=<NegBackward0>) tensor(9866.4365, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9866.4365234375
tensor(9866.4365, grad_fn=<NegBackward0>) tensor(9866.4365, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9866.4345703125
tensor(9866.4365, grad_fn=<NegBackward0>) tensor(9866.4346, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9866.4345703125
tensor(9866.4346, grad_fn=<NegBackward0>) tensor(9866.4346, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9866.4365234375
tensor(9866.4346, grad_fn=<NegBackward0>) tensor(9866.4365, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9866.43359375
tensor(9866.4346, grad_fn=<NegBackward0>) tensor(9866.4336, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9866.43359375
tensor(9866.4336, grad_fn=<NegBackward0>) tensor(9866.4336, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9866.4326171875
tensor(9866.4336, grad_fn=<NegBackward0>) tensor(9866.4326, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9866.4326171875
tensor(9866.4326, grad_fn=<NegBackward0>) tensor(9866.4326, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9866.431640625
tensor(9866.4326, grad_fn=<NegBackward0>) tensor(9866.4316, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9866.431640625
tensor(9866.4316, grad_fn=<NegBackward0>) tensor(9866.4316, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9866.431640625
tensor(9866.4316, grad_fn=<NegBackward0>) tensor(9866.4316, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9866.431640625
tensor(9866.4316, grad_fn=<NegBackward0>) tensor(9866.4316, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9866.4306640625
tensor(9866.4316, grad_fn=<NegBackward0>) tensor(9866.4307, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9866.4306640625
tensor(9866.4307, grad_fn=<NegBackward0>) tensor(9866.4307, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9866.4306640625
tensor(9866.4307, grad_fn=<NegBackward0>) tensor(9866.4307, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9866.4306640625
tensor(9866.4307, grad_fn=<NegBackward0>) tensor(9866.4307, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9866.4306640625
tensor(9866.4307, grad_fn=<NegBackward0>) tensor(9866.4307, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9866.4296875
tensor(9866.4307, grad_fn=<NegBackward0>) tensor(9866.4297, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9866.4296875
tensor(9866.4297, grad_fn=<NegBackward0>) tensor(9866.4297, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9866.427734375
tensor(9866.4297, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9866.4296875
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4297, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9866.427734375
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9866.4287109375
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4287, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9866.427734375
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9866.427734375
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9866.4267578125
tensor(9866.4277, grad_fn=<NegBackward0>) tensor(9866.4268, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9866.427734375
tensor(9866.4268, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9866.427734375
tensor(9866.4268, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9866.4267578125
tensor(9866.4268, grad_fn=<NegBackward0>) tensor(9866.4268, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9866.427734375
tensor(9866.4268, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9866.42578125
tensor(9866.4268, grad_fn=<NegBackward0>) tensor(9866.4258, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9866.4580078125
tensor(9866.4258, grad_fn=<NegBackward0>) tensor(9866.4580, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9866.583984375
tensor(9866.4258, grad_fn=<NegBackward0>) tensor(9866.5840, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9866.427734375
tensor(9866.4258, grad_fn=<NegBackward0>) tensor(9866.4277, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -9866.4873046875
tensor(9866.4258, grad_fn=<NegBackward0>) tensor(9866.4873, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -9866.4267578125
tensor(9866.4258, grad_fn=<NegBackward0>) tensor(9866.4268, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[9.9560e-01, 4.3963e-03],
        [5.9564e-04, 9.9940e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9771, 0.0229], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1356, 0.1221],
         [0.6382, 0.3335]],

        [[0.6803, 0.0999],
         [0.6845, 0.5665]],

        [[0.5956, 0.0850],
         [0.7220, 0.5454]],

        [[0.6209, 0.2045],
         [0.5547, 0.6007]],

        [[0.6450, 0.1966],
         [0.6173, 0.6359]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.004267232452421997
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: 0.00024743418602756585
Average Adjusted Rand Index: 0.0009226555415321767
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22713.6328125
inf tensor(22713.6328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9872.8505859375
tensor(22713.6328, grad_fn=<NegBackward0>) tensor(9872.8506, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9870.9951171875
tensor(9872.8506, grad_fn=<NegBackward0>) tensor(9870.9951, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9870.4892578125
tensor(9870.9951, grad_fn=<NegBackward0>) tensor(9870.4893, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9870.1630859375
tensor(9870.4893, grad_fn=<NegBackward0>) tensor(9870.1631, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9869.716796875
tensor(9870.1631, grad_fn=<NegBackward0>) tensor(9869.7168, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9869.37890625
tensor(9869.7168, grad_fn=<NegBackward0>) tensor(9869.3789, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9869.14453125
tensor(9869.3789, grad_fn=<NegBackward0>) tensor(9869.1445, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9868.9140625
tensor(9869.1445, grad_fn=<NegBackward0>) tensor(9868.9141, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9868.68359375
tensor(9868.9141, grad_fn=<NegBackward0>) tensor(9868.6836, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9868.4794921875
tensor(9868.6836, grad_fn=<NegBackward0>) tensor(9868.4795, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9868.2744140625
tensor(9868.4795, grad_fn=<NegBackward0>) tensor(9868.2744, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9868.1171875
tensor(9868.2744, grad_fn=<NegBackward0>) tensor(9868.1172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9867.974609375
tensor(9868.1172, grad_fn=<NegBackward0>) tensor(9867.9746, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9867.84765625
tensor(9867.9746, grad_fn=<NegBackward0>) tensor(9867.8477, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9867.7509765625
tensor(9867.8477, grad_fn=<NegBackward0>) tensor(9867.7510, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9867.650390625
tensor(9867.7510, grad_fn=<NegBackward0>) tensor(9867.6504, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9867.4423828125
tensor(9867.6504, grad_fn=<NegBackward0>) tensor(9867.4424, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9867.31640625
tensor(9867.4424, grad_fn=<NegBackward0>) tensor(9867.3164, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9867.193359375
tensor(9867.3164, grad_fn=<NegBackward0>) tensor(9867.1934, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9867.06640625
tensor(9867.1934, grad_fn=<NegBackward0>) tensor(9867.0664, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9866.927734375
tensor(9867.0664, grad_fn=<NegBackward0>) tensor(9866.9277, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9866.759765625
tensor(9866.9277, grad_fn=<NegBackward0>) tensor(9866.7598, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9866.5712890625
tensor(9866.7598, grad_fn=<NegBackward0>) tensor(9866.5713, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9866.3916015625
tensor(9866.5713, grad_fn=<NegBackward0>) tensor(9866.3916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9866.232421875
tensor(9866.3916, grad_fn=<NegBackward0>) tensor(9866.2324, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9866.0869140625
tensor(9866.2324, grad_fn=<NegBackward0>) tensor(9866.0869, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9865.931640625
tensor(9866.0869, grad_fn=<NegBackward0>) tensor(9865.9316, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9865.8310546875
tensor(9865.9316, grad_fn=<NegBackward0>) tensor(9865.8311, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9865.7529296875
tensor(9865.8311, grad_fn=<NegBackward0>) tensor(9865.7529, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9865.681640625
tensor(9865.7529, grad_fn=<NegBackward0>) tensor(9865.6816, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9865.6337890625
tensor(9865.6816, grad_fn=<NegBackward0>) tensor(9865.6338, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9865.5986328125
tensor(9865.6338, grad_fn=<NegBackward0>) tensor(9865.5986, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9865.568359375
tensor(9865.5986, grad_fn=<NegBackward0>) tensor(9865.5684, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9865.54296875
tensor(9865.5684, grad_fn=<NegBackward0>) tensor(9865.5430, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9865.5185546875
tensor(9865.5430, grad_fn=<NegBackward0>) tensor(9865.5186, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9865.4951171875
tensor(9865.5186, grad_fn=<NegBackward0>) tensor(9865.4951, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9865.4794921875
tensor(9865.4951, grad_fn=<NegBackward0>) tensor(9865.4795, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9865.4677734375
tensor(9865.4795, grad_fn=<NegBackward0>) tensor(9865.4678, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9865.45703125
tensor(9865.4678, grad_fn=<NegBackward0>) tensor(9865.4570, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9865.44921875
tensor(9865.4570, grad_fn=<NegBackward0>) tensor(9865.4492, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9865.439453125
tensor(9865.4492, grad_fn=<NegBackward0>) tensor(9865.4395, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9865.4326171875
tensor(9865.4395, grad_fn=<NegBackward0>) tensor(9865.4326, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9865.4248046875
tensor(9865.4326, grad_fn=<NegBackward0>) tensor(9865.4248, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9865.4208984375
tensor(9865.4248, grad_fn=<NegBackward0>) tensor(9865.4209, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9865.416015625
tensor(9865.4209, grad_fn=<NegBackward0>) tensor(9865.4160, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9865.4111328125
tensor(9865.4160, grad_fn=<NegBackward0>) tensor(9865.4111, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9865.4052734375
tensor(9865.4111, grad_fn=<NegBackward0>) tensor(9865.4053, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9865.4033203125
tensor(9865.4053, grad_fn=<NegBackward0>) tensor(9865.4033, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9865.3984375
tensor(9865.4033, grad_fn=<NegBackward0>) tensor(9865.3984, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9865.3935546875
tensor(9865.3984, grad_fn=<NegBackward0>) tensor(9865.3936, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9865.3916015625
tensor(9865.3936, grad_fn=<NegBackward0>) tensor(9865.3916, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9865.388671875
tensor(9865.3916, grad_fn=<NegBackward0>) tensor(9865.3887, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9865.3857421875
tensor(9865.3887, grad_fn=<NegBackward0>) tensor(9865.3857, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9865.384765625
tensor(9865.3857, grad_fn=<NegBackward0>) tensor(9865.3848, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9865.380859375
tensor(9865.3848, grad_fn=<NegBackward0>) tensor(9865.3809, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9865.37890625
tensor(9865.3809, grad_fn=<NegBackward0>) tensor(9865.3789, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9865.3779296875
tensor(9865.3789, grad_fn=<NegBackward0>) tensor(9865.3779, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9865.375
tensor(9865.3779, grad_fn=<NegBackward0>) tensor(9865.3750, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9865.375
tensor(9865.3750, grad_fn=<NegBackward0>) tensor(9865.3750, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9865.373046875
tensor(9865.3750, grad_fn=<NegBackward0>) tensor(9865.3730, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9865.37109375
tensor(9865.3730, grad_fn=<NegBackward0>) tensor(9865.3711, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9865.3701171875
tensor(9865.3711, grad_fn=<NegBackward0>) tensor(9865.3701, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9865.369140625
tensor(9865.3701, grad_fn=<NegBackward0>) tensor(9865.3691, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9865.3671875
tensor(9865.3691, grad_fn=<NegBackward0>) tensor(9865.3672, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9865.3671875
tensor(9865.3672, grad_fn=<NegBackward0>) tensor(9865.3672, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9865.365234375
tensor(9865.3672, grad_fn=<NegBackward0>) tensor(9865.3652, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9865.3662109375
tensor(9865.3652, grad_fn=<NegBackward0>) tensor(9865.3662, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9865.365234375
tensor(9865.3652, grad_fn=<NegBackward0>) tensor(9865.3652, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9865.36328125
tensor(9865.3652, grad_fn=<NegBackward0>) tensor(9865.3633, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9865.361328125
tensor(9865.3633, grad_fn=<NegBackward0>) tensor(9865.3613, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9865.361328125
tensor(9865.3613, grad_fn=<NegBackward0>) tensor(9865.3613, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9865.3603515625
tensor(9865.3613, grad_fn=<NegBackward0>) tensor(9865.3604, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9865.361328125
tensor(9865.3604, grad_fn=<NegBackward0>) tensor(9865.3613, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9865.3583984375
tensor(9865.3604, grad_fn=<NegBackward0>) tensor(9865.3584, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9865.3583984375
tensor(9865.3584, grad_fn=<NegBackward0>) tensor(9865.3584, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9865.357421875
tensor(9865.3584, grad_fn=<NegBackward0>) tensor(9865.3574, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9865.3583984375
tensor(9865.3574, grad_fn=<NegBackward0>) tensor(9865.3584, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9865.357421875
tensor(9865.3574, grad_fn=<NegBackward0>) tensor(9865.3574, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9865.359375
tensor(9865.3574, grad_fn=<NegBackward0>) tensor(9865.3594, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9865.41015625
tensor(9865.3574, grad_fn=<NegBackward0>) tensor(9865.4102, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9865.35546875
tensor(9865.3574, grad_fn=<NegBackward0>) tensor(9865.3555, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9865.35546875
tensor(9865.3555, grad_fn=<NegBackward0>) tensor(9865.3555, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9865.35546875
tensor(9865.3555, grad_fn=<NegBackward0>) tensor(9865.3555, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9865.3544921875
tensor(9865.3555, grad_fn=<NegBackward0>) tensor(9865.3545, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9865.3544921875
tensor(9865.3545, grad_fn=<NegBackward0>) tensor(9865.3545, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9865.3525390625
tensor(9865.3545, grad_fn=<NegBackward0>) tensor(9865.3525, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9865.353515625
tensor(9865.3525, grad_fn=<NegBackward0>) tensor(9865.3535, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9865.3525390625
tensor(9865.3525, grad_fn=<NegBackward0>) tensor(9865.3525, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9865.3544921875
tensor(9865.3525, grad_fn=<NegBackward0>) tensor(9865.3545, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9865.3525390625
tensor(9865.3525, grad_fn=<NegBackward0>) tensor(9865.3525, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9865.3505859375
tensor(9865.3525, grad_fn=<NegBackward0>) tensor(9865.3506, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9865.3525390625
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3525, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9865.3525390625
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3525, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -9865.3515625
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3516, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -9865.3515625
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3516, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -9865.3505859375
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3506, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9865.3544921875
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3545, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9865.3505859375
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3506, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9865.3505859375
tensor(9865.3506, grad_fn=<NegBackward0>) tensor(9865.3506, grad_fn=<NegBackward0>)
pi: tensor([[9.9974e-01, 2.5800e-04],
        [1.2059e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0391, 0.9609], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2822, 0.1612],
         [0.7204, 0.1344]],

        [[0.7163, 0.1037],
         [0.5873, 0.5762]],

        [[0.6350, 0.1278],
         [0.6966, 0.5667]],

        [[0.6252, 0.2016],
         [0.5056, 0.7246]],

        [[0.6228, 0.2027],
         [0.6664, 0.6783]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.014100938522947548
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: -0.0037581819666326235
Average Adjusted Rand Index: -0.004350380653267287
[0.00024743418602756585, -0.0037581819666326235] [0.0009226555415321767, -0.004350380653267287] [9866.4267578125, 9865.3623046875]
-------------------------------------
This iteration is 48
True Objective function: Loss = -10180.858881180035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22321.412109375
inf tensor(22321.4121, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10072.6220703125
tensor(22321.4121, grad_fn=<NegBackward0>) tensor(10072.6221, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10071.560546875
tensor(10072.6221, grad_fn=<NegBackward0>) tensor(10071.5605, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10071.0302734375
tensor(10071.5605, grad_fn=<NegBackward0>) tensor(10071.0303, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10070.7568359375
tensor(10071.0303, grad_fn=<NegBackward0>) tensor(10070.7568, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10070.6015625
tensor(10070.7568, grad_fn=<NegBackward0>) tensor(10070.6016, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10070.4814453125
tensor(10070.6016, grad_fn=<NegBackward0>) tensor(10070.4814, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10070.3564453125
tensor(10070.4814, grad_fn=<NegBackward0>) tensor(10070.3564, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10070.234375
tensor(10070.3564, grad_fn=<NegBackward0>) tensor(10070.2344, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10070.171875
tensor(10070.2344, grad_fn=<NegBackward0>) tensor(10070.1719, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10070.1416015625
tensor(10070.1719, grad_fn=<NegBackward0>) tensor(10070.1416, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10070.12109375
tensor(10070.1416, grad_fn=<NegBackward0>) tensor(10070.1211, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10070.1064453125
tensor(10070.1211, grad_fn=<NegBackward0>) tensor(10070.1064, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10070.091796875
tensor(10070.1064, grad_fn=<NegBackward0>) tensor(10070.0918, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10070.0810546875
tensor(10070.0918, grad_fn=<NegBackward0>) tensor(10070.0811, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10070.0693359375
tensor(10070.0811, grad_fn=<NegBackward0>) tensor(10070.0693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10070.0595703125
tensor(10070.0693, grad_fn=<NegBackward0>) tensor(10070.0596, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10070.0498046875
tensor(10070.0596, grad_fn=<NegBackward0>) tensor(10070.0498, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10070.041015625
tensor(10070.0498, grad_fn=<NegBackward0>) tensor(10070.0410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10070.033203125
tensor(10070.0410, grad_fn=<NegBackward0>) tensor(10070.0332, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10070.025390625
tensor(10070.0332, grad_fn=<NegBackward0>) tensor(10070.0254, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10070.017578125
tensor(10070.0254, grad_fn=<NegBackward0>) tensor(10070.0176, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10070.0107421875
tensor(10070.0176, grad_fn=<NegBackward0>) tensor(10070.0107, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10070.0009765625
tensor(10070.0107, grad_fn=<NegBackward0>) tensor(10070.0010, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10069.9921875
tensor(10070.0010, grad_fn=<NegBackward0>) tensor(10069.9922, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10069.9814453125
tensor(10069.9922, grad_fn=<NegBackward0>) tensor(10069.9814, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10069.96875
tensor(10069.9814, grad_fn=<NegBackward0>) tensor(10069.9688, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10069.9521484375
tensor(10069.9688, grad_fn=<NegBackward0>) tensor(10069.9521, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10069.923828125
tensor(10069.9521, grad_fn=<NegBackward0>) tensor(10069.9238, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10069.8828125
tensor(10069.9238, grad_fn=<NegBackward0>) tensor(10069.8828, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10069.796875
tensor(10069.8828, grad_fn=<NegBackward0>) tensor(10069.7969, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10069.6533203125
tensor(10069.7969, grad_fn=<NegBackward0>) tensor(10069.6533, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10069.51953125
tensor(10069.6533, grad_fn=<NegBackward0>) tensor(10069.5195, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10069.4130859375
tensor(10069.5195, grad_fn=<NegBackward0>) tensor(10069.4131, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10069.23828125
tensor(10069.4131, grad_fn=<NegBackward0>) tensor(10069.2383, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10053.1689453125
tensor(10069.2383, grad_fn=<NegBackward0>) tensor(10053.1689, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10044.3046875
tensor(10053.1689, grad_fn=<NegBackward0>) tensor(10044.3047, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10044.248046875
tensor(10044.3047, grad_fn=<NegBackward0>) tensor(10044.2480, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10044.2392578125
tensor(10044.2480, grad_fn=<NegBackward0>) tensor(10044.2393, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10044.2333984375
tensor(10044.2393, grad_fn=<NegBackward0>) tensor(10044.2334, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10044.2294921875
tensor(10044.2334, grad_fn=<NegBackward0>) tensor(10044.2295, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10044.228515625
tensor(10044.2295, grad_fn=<NegBackward0>) tensor(10044.2285, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10044.2275390625
tensor(10044.2285, grad_fn=<NegBackward0>) tensor(10044.2275, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10044.2255859375
tensor(10044.2275, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10044.2255859375
tensor(10044.2256, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10044.224609375
tensor(10044.2256, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10044.228515625
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2285, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10044.2265625
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2266, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10044.232421875
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2324, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10044.228515625
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2285, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -10044.2392578125
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2393, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10044.2294921875
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2295, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10044.2236328125
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2236, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10044.2255859375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10044.228515625
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2285, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.8274, 0.1726],
        [0.0917, 0.9083]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6261, 0.3739], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2023, 0.0960],
         [0.7068, 0.1356]],

        [[0.5036, 0.1171],
         [0.5391, 0.5612]],

        [[0.6863, 0.0944],
         [0.5958, 0.6053]],

        [[0.7283, 0.1179],
         [0.6741, 0.5823]],

        [[0.7291, 0.1286],
         [0.6095, 0.6363]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 14
Adjusted Rand Index: 0.5135353535353535
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 22
Adjusted Rand Index: 0.30666666666666664
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6363408394869687
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.2843977160022693
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.2226749911728827
Global Adjusted Rand Index: 0.3831665616012749
Average Adjusted Rand Index: 0.39272311337282817
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25099.705078125
inf tensor(25099.7051, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10072.169921875
tensor(25099.7051, grad_fn=<NegBackward0>) tensor(10072.1699, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10071.07421875
tensor(10072.1699, grad_fn=<NegBackward0>) tensor(10071.0742, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10070.857421875
tensor(10071.0742, grad_fn=<NegBackward0>) tensor(10070.8574, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10070.7158203125
tensor(10070.8574, grad_fn=<NegBackward0>) tensor(10070.7158, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10070.607421875
tensor(10070.7158, grad_fn=<NegBackward0>) tensor(10070.6074, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10070.525390625
tensor(10070.6074, grad_fn=<NegBackward0>) tensor(10070.5254, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10070.462890625
tensor(10070.5254, grad_fn=<NegBackward0>) tensor(10070.4629, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10070.408203125
tensor(10070.4629, grad_fn=<NegBackward0>) tensor(10070.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10070.359375
tensor(10070.4082, grad_fn=<NegBackward0>) tensor(10070.3594, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10070.310546875
tensor(10070.3594, grad_fn=<NegBackward0>) tensor(10070.3105, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10070.26171875
tensor(10070.3105, grad_fn=<NegBackward0>) tensor(10070.2617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10070.2119140625
tensor(10070.2617, grad_fn=<NegBackward0>) tensor(10070.2119, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10070.16796875
tensor(10070.2119, grad_fn=<NegBackward0>) tensor(10070.1680, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10070.140625
tensor(10070.1680, grad_fn=<NegBackward0>) tensor(10070.1406, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10070.1220703125
tensor(10070.1406, grad_fn=<NegBackward0>) tensor(10070.1221, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10070.109375
tensor(10070.1221, grad_fn=<NegBackward0>) tensor(10070.1094, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10070.0986328125
tensor(10070.1094, grad_fn=<NegBackward0>) tensor(10070.0986, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10070.0888671875
tensor(10070.0986, grad_fn=<NegBackward0>) tensor(10070.0889, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10070.078125
tensor(10070.0889, grad_fn=<NegBackward0>) tensor(10070.0781, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10070.0693359375
tensor(10070.0781, grad_fn=<NegBackward0>) tensor(10070.0693, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10070.0625
tensor(10070.0693, grad_fn=<NegBackward0>) tensor(10070.0625, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10070.0537109375
tensor(10070.0625, grad_fn=<NegBackward0>) tensor(10070.0537, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10070.046875
tensor(10070.0537, grad_fn=<NegBackward0>) tensor(10070.0469, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10070.0419921875
tensor(10070.0469, grad_fn=<NegBackward0>) tensor(10070.0420, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10070.03515625
tensor(10070.0420, grad_fn=<NegBackward0>) tensor(10070.0352, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10070.03125
tensor(10070.0352, grad_fn=<NegBackward0>) tensor(10070.0312, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10070.025390625
tensor(10070.0312, grad_fn=<NegBackward0>) tensor(10070.0254, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10070.01953125
tensor(10070.0254, grad_fn=<NegBackward0>) tensor(10070.0195, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10070.0146484375
tensor(10070.0195, grad_fn=<NegBackward0>) tensor(10070.0146, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10070.0068359375
tensor(10070.0146, grad_fn=<NegBackward0>) tensor(10070.0068, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10069.998046875
tensor(10070.0068, grad_fn=<NegBackward0>) tensor(10069.9980, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10069.9873046875
tensor(10069.9980, grad_fn=<NegBackward0>) tensor(10069.9873, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10069.9736328125
tensor(10069.9873, grad_fn=<NegBackward0>) tensor(10069.9736, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10069.951171875
tensor(10069.9736, grad_fn=<NegBackward0>) tensor(10069.9512, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10069.919921875
tensor(10069.9512, grad_fn=<NegBackward0>) tensor(10069.9199, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10069.8583984375
tensor(10069.9199, grad_fn=<NegBackward0>) tensor(10069.8584, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10069.728515625
tensor(10069.8584, grad_fn=<NegBackward0>) tensor(10069.7285, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10069.5849609375
tensor(10069.7285, grad_fn=<NegBackward0>) tensor(10069.5850, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10069.4677734375
tensor(10069.5850, grad_fn=<NegBackward0>) tensor(10069.4678, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10069.341796875
tensor(10069.4678, grad_fn=<NegBackward0>) tensor(10069.3418, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10054.23046875
tensor(10069.3418, grad_fn=<NegBackward0>) tensor(10054.2305, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10044.3359375
tensor(10054.2305, grad_fn=<NegBackward0>) tensor(10044.3359, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10044.2421875
tensor(10044.3359, grad_fn=<NegBackward0>) tensor(10044.2422, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10044.236328125
tensor(10044.2422, grad_fn=<NegBackward0>) tensor(10044.2363, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10044.234375
tensor(10044.2363, grad_fn=<NegBackward0>) tensor(10044.2344, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10044.236328125
tensor(10044.2344, grad_fn=<NegBackward0>) tensor(10044.2363, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10044.2275390625
tensor(10044.2344, grad_fn=<NegBackward0>) tensor(10044.2275, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10044.2275390625
tensor(10044.2275, grad_fn=<NegBackward0>) tensor(10044.2275, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10044.2265625
tensor(10044.2275, grad_fn=<NegBackward0>) tensor(10044.2266, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10044.224609375
tensor(10044.2266, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10044.224609375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10044.2255859375
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2256, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10044.2265625
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2266, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10044.2236328125
tensor(10044.2246, grad_fn=<NegBackward0>) tensor(10044.2236, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10044.2265625
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2266, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10044.23046875
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2305, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -10044.224609375
tensor(10044.2236, grad_fn=<NegBackward0>) tensor(10044.2246, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.8276, 0.1724],
        [0.0917, 0.9083]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6264, 0.3736], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2023, 0.0960],
         [0.5420, 0.1356]],

        [[0.6817, 0.1171],
         [0.7202, 0.5228]],

        [[0.5114, 0.0944],
         [0.7113, 0.7013]],

        [[0.5060, 0.1179],
         [0.6535, 0.6131]],

        [[0.6323, 0.1286],
         [0.6061, 0.7128]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 14
Adjusted Rand Index: 0.5135353535353535
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 22
Adjusted Rand Index: 0.30666666666666664
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6363408394869687
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.2843977160022693
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.2226749911728827
Global Adjusted Rand Index: 0.3831665616012749
Average Adjusted Rand Index: 0.39272311337282817
[0.3831665616012749, 0.3831665616012749] [0.39272311337282817, 0.39272311337282817] [10044.224609375, 10044.224609375]
-------------------------------------
This iteration is 49
True Objective function: Loss = -10407.096432153518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22900.126953125
inf tensor(22900.1270, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10313.419921875
tensor(22900.1270, grad_fn=<NegBackward0>) tensor(10313.4199, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10310.388671875
tensor(10313.4199, grad_fn=<NegBackward0>) tensor(10310.3887, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10308.6982421875
tensor(10310.3887, grad_fn=<NegBackward0>) tensor(10308.6982, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10308.0810546875
tensor(10308.6982, grad_fn=<NegBackward0>) tensor(10308.0811, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10307.7021484375
tensor(10308.0811, grad_fn=<NegBackward0>) tensor(10307.7021, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10307.4501953125
tensor(10307.7021, grad_fn=<NegBackward0>) tensor(10307.4502, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10307.27734375
tensor(10307.4502, grad_fn=<NegBackward0>) tensor(10307.2773, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10307.154296875
tensor(10307.2773, grad_fn=<NegBackward0>) tensor(10307.1543, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10307.0595703125
tensor(10307.1543, grad_fn=<NegBackward0>) tensor(10307.0596, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10306.9833984375
tensor(10307.0596, grad_fn=<NegBackward0>) tensor(10306.9834, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10306.9189453125
tensor(10306.9834, grad_fn=<NegBackward0>) tensor(10306.9189, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10306.85546875
tensor(10306.9189, grad_fn=<NegBackward0>) tensor(10306.8555, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10306.7958984375
tensor(10306.8555, grad_fn=<NegBackward0>) tensor(10306.7959, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10306.73046875
tensor(10306.7959, grad_fn=<NegBackward0>) tensor(10306.7305, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10306.66015625
tensor(10306.7305, grad_fn=<NegBackward0>) tensor(10306.6602, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10306.5859375
tensor(10306.6602, grad_fn=<NegBackward0>) tensor(10306.5859, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10306.51953125
tensor(10306.5859, grad_fn=<NegBackward0>) tensor(10306.5195, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10306.4638671875
tensor(10306.5195, grad_fn=<NegBackward0>) tensor(10306.4639, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10306.421875
tensor(10306.4639, grad_fn=<NegBackward0>) tensor(10306.4219, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10306.3876953125
tensor(10306.4219, grad_fn=<NegBackward0>) tensor(10306.3877, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10306.3583984375
tensor(10306.3877, grad_fn=<NegBackward0>) tensor(10306.3584, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10306.33203125
tensor(10306.3584, grad_fn=<NegBackward0>) tensor(10306.3320, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10306.3095703125
tensor(10306.3320, grad_fn=<NegBackward0>) tensor(10306.3096, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10306.2861328125
tensor(10306.3096, grad_fn=<NegBackward0>) tensor(10306.2861, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10306.2646484375
tensor(10306.2861, grad_fn=<NegBackward0>) tensor(10306.2646, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10306.24609375
tensor(10306.2646, grad_fn=<NegBackward0>) tensor(10306.2461, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10306.2275390625
tensor(10306.2461, grad_fn=<NegBackward0>) tensor(10306.2275, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10306.2099609375
tensor(10306.2275, grad_fn=<NegBackward0>) tensor(10306.2100, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10306.1923828125
tensor(10306.2100, grad_fn=<NegBackward0>) tensor(10306.1924, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10306.173828125
tensor(10306.1924, grad_fn=<NegBackward0>) tensor(10306.1738, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10306.1572265625
tensor(10306.1738, grad_fn=<NegBackward0>) tensor(10306.1572, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10306.13671875
tensor(10306.1572, grad_fn=<NegBackward0>) tensor(10306.1367, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10306.1220703125
tensor(10306.1367, grad_fn=<NegBackward0>) tensor(10306.1221, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10306.0830078125
tensor(10306.1221, grad_fn=<NegBackward0>) tensor(10306.0830, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10306.0419921875
tensor(10306.0830, grad_fn=<NegBackward0>) tensor(10306.0420, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10305.984375
tensor(10306.0420, grad_fn=<NegBackward0>) tensor(10305.9844, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10305.916015625
tensor(10305.9844, grad_fn=<NegBackward0>) tensor(10305.9160, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10305.849609375
tensor(10305.9160, grad_fn=<NegBackward0>) tensor(10305.8496, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10305.7890625
tensor(10305.8496, grad_fn=<NegBackward0>) tensor(10305.7891, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10305.740234375
tensor(10305.7891, grad_fn=<NegBackward0>) tensor(10305.7402, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10305.703125
tensor(10305.7402, grad_fn=<NegBackward0>) tensor(10305.7031, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10305.6708984375
tensor(10305.7031, grad_fn=<NegBackward0>) tensor(10305.6709, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10305.626953125
tensor(10305.6709, grad_fn=<NegBackward0>) tensor(10305.6270, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10305.525390625
tensor(10305.6270, grad_fn=<NegBackward0>) tensor(10305.5254, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10305.3876953125
tensor(10305.5254, grad_fn=<NegBackward0>) tensor(10305.3877, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10305.3251953125
tensor(10305.3877, grad_fn=<NegBackward0>) tensor(10305.3252, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10305.310546875
tensor(10305.3252, grad_fn=<NegBackward0>) tensor(10305.3105, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10305.306640625
tensor(10305.3105, grad_fn=<NegBackward0>) tensor(10305.3066, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10305.3046875
tensor(10305.3066, grad_fn=<NegBackward0>) tensor(10305.3047, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10305.3037109375
tensor(10305.3047, grad_fn=<NegBackward0>) tensor(10305.3037, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10305.302734375
tensor(10305.3037, grad_fn=<NegBackward0>) tensor(10305.3027, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10305.30078125
tensor(10305.3027, grad_fn=<NegBackward0>) tensor(10305.3008, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10305.298828125
tensor(10305.3008, grad_fn=<NegBackward0>) tensor(10305.2988, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10305.2978515625
tensor(10305.2988, grad_fn=<NegBackward0>) tensor(10305.2979, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10305.3369140625
tensor(10305.2979, grad_fn=<NegBackward0>) tensor(10305.3369, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10305.296875
tensor(10305.2979, grad_fn=<NegBackward0>) tensor(10305.2969, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10305.296875
tensor(10305.2969, grad_fn=<NegBackward0>) tensor(10305.2969, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10305.2958984375
tensor(10305.2969, grad_fn=<NegBackward0>) tensor(10305.2959, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10305.294921875
tensor(10305.2959, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10305.3115234375
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.3115, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10305.294921875
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10305.29296875
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10305.2939453125
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2939, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10305.29296875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10305.2939453125
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2939, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10305.29296875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10305.29296875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10305.2919921875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10305.2939453125
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2939, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10305.291015625
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10305.29296875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10305.2919921875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10305.2919921875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -10305.29296875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -10305.291015625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10305.333984375
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.3340, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10305.2919921875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10305.291015625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10305.291015625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10305.29296875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10305.291015625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10305.2998046875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2998, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10305.291015625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10305.2900390625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10305.2900390625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10305.2900390625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10305.291015625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10305.4375
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.4375, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -10305.291015625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -10305.2890625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2891, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10305.2900390625
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10305.2890625
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2891, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10305.296875
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2969, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10305.2919921875
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10305.330078125
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.3301, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10305.2900390625
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -10305.291015625
tensor(10305.2891, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[1.2806e-05, 9.9999e-01],
        [3.3991e-01, 6.6009e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9685, 0.0315], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1594, 0.1230],
         [0.5783, 0.1389]],

        [[0.7061, 0.2960],
         [0.5526, 0.5984]],

        [[0.5006, 0.1521],
         [0.5720, 0.7083]],

        [[0.7118, 0.1553],
         [0.5351, 0.5781]],

        [[0.6841, 0.1394],
         [0.5447, 0.6572]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005790243843013313
Average Adjusted Rand Index: -0.0007582173912768069
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24107.640625
inf tensor(24107.6406, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10312.927734375
tensor(24107.6406, grad_fn=<NegBackward0>) tensor(10312.9277, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10309.3984375
tensor(10312.9277, grad_fn=<NegBackward0>) tensor(10309.3984, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10308.447265625
tensor(10309.3984, grad_fn=<NegBackward0>) tensor(10308.4473, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10307.7724609375
tensor(10308.4473, grad_fn=<NegBackward0>) tensor(10307.7725, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10307.3203125
tensor(10307.7725, grad_fn=<NegBackward0>) tensor(10307.3203, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10307.021484375
tensor(10307.3203, grad_fn=<NegBackward0>) tensor(10307.0215, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10306.8251953125
tensor(10307.0215, grad_fn=<NegBackward0>) tensor(10306.8252, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10306.6923828125
tensor(10306.8252, grad_fn=<NegBackward0>) tensor(10306.6924, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10306.5966796875
tensor(10306.6924, grad_fn=<NegBackward0>) tensor(10306.5967, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10306.5224609375
tensor(10306.5967, grad_fn=<NegBackward0>) tensor(10306.5225, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10306.4677734375
tensor(10306.5225, grad_fn=<NegBackward0>) tensor(10306.4678, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10306.4228515625
tensor(10306.4678, grad_fn=<NegBackward0>) tensor(10306.4229, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10306.38671875
tensor(10306.4229, grad_fn=<NegBackward0>) tensor(10306.3867, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10306.357421875
tensor(10306.3867, grad_fn=<NegBackward0>) tensor(10306.3574, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10306.328125
tensor(10306.3574, grad_fn=<NegBackward0>) tensor(10306.3281, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10306.3046875
tensor(10306.3281, grad_fn=<NegBackward0>) tensor(10306.3047, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10306.28125
tensor(10306.3047, grad_fn=<NegBackward0>) tensor(10306.2812, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10306.2578125
tensor(10306.2812, grad_fn=<NegBackward0>) tensor(10306.2578, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10306.251953125
tensor(10306.2578, grad_fn=<NegBackward0>) tensor(10306.2520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10306.216796875
tensor(10306.2520, grad_fn=<NegBackward0>) tensor(10306.2168, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10306.203125
tensor(10306.2168, grad_fn=<NegBackward0>) tensor(10306.2031, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10306.173828125
tensor(10306.2031, grad_fn=<NegBackward0>) tensor(10306.1738, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10306.1572265625
tensor(10306.1738, grad_fn=<NegBackward0>) tensor(10306.1572, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10306.1259765625
tensor(10306.1572, grad_fn=<NegBackward0>) tensor(10306.1260, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10306.0927734375
tensor(10306.1260, grad_fn=<NegBackward0>) tensor(10306.0928, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10306.033203125
tensor(10306.0928, grad_fn=<NegBackward0>) tensor(10306.0332, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10305.9326171875
tensor(10306.0332, grad_fn=<NegBackward0>) tensor(10305.9326, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10305.826171875
tensor(10305.9326, grad_fn=<NegBackward0>) tensor(10305.8262, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10305.7490234375
tensor(10305.8262, grad_fn=<NegBackward0>) tensor(10305.7490, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10305.6923828125
tensor(10305.7490, grad_fn=<NegBackward0>) tensor(10305.6924, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10305.6435546875
tensor(10305.6924, grad_fn=<NegBackward0>) tensor(10305.6436, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10305.517578125
tensor(10305.6436, grad_fn=<NegBackward0>) tensor(10305.5176, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10305.34375
tensor(10305.5176, grad_fn=<NegBackward0>) tensor(10305.3438, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10305.3115234375
tensor(10305.3438, grad_fn=<NegBackward0>) tensor(10305.3115, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10305.30859375
tensor(10305.3115, grad_fn=<NegBackward0>) tensor(10305.3086, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10305.3046875
tensor(10305.3086, grad_fn=<NegBackward0>) tensor(10305.3047, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10305.3037109375
tensor(10305.3047, grad_fn=<NegBackward0>) tensor(10305.3037, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10305.3017578125
tensor(10305.3037, grad_fn=<NegBackward0>) tensor(10305.3018, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10305.3310546875
tensor(10305.3018, grad_fn=<NegBackward0>) tensor(10305.3311, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10305.2998046875
tensor(10305.3018, grad_fn=<NegBackward0>) tensor(10305.2998, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10305.2998046875
tensor(10305.2998, grad_fn=<NegBackward0>) tensor(10305.2998, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10305.298828125
tensor(10305.2998, grad_fn=<NegBackward0>) tensor(10305.2988, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10305.296875
tensor(10305.2988, grad_fn=<NegBackward0>) tensor(10305.2969, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10305.2998046875
tensor(10305.2969, grad_fn=<NegBackward0>) tensor(10305.2998, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10305.294921875
tensor(10305.2969, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10305.294921875
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10305.294921875
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10305.2958984375
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2959, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10305.2958984375
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2959, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10305.2939453125
tensor(10305.2949, grad_fn=<NegBackward0>) tensor(10305.2939, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10305.2939453125
tensor(10305.2939, grad_fn=<NegBackward0>) tensor(10305.2939, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10305.29296875
tensor(10305.2939, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10305.29296875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10305.29296875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2930, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10305.2919921875
tensor(10305.2930, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10305.2919921875
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10305.2919921875
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10305.2919921875
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10305.2919921875
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2920, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10305.291015625
tensor(10305.2920, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10305.294921875
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2949, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10305.2900390625
tensor(10305.2910, grad_fn=<NegBackward0>) tensor(10305.2900, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10305.30859375
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.3086, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10305.291015625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -10305.2958984375
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2959, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -10305.291015625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -10305.291015625
tensor(10305.2900, grad_fn=<NegBackward0>) tensor(10305.2910, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[6.5956e-01, 3.4044e-01],
        [9.9997e-01, 2.9624e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0314, 0.9686], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1388, 0.1230],
         [0.6760, 0.1594]],

        [[0.6330, 0.2960],
         [0.6398, 0.5138]],

        [[0.7177, 0.1521],
         [0.6278, 0.6091]],

        [[0.5333, 0.1553],
         [0.5951, 0.5374]],

        [[0.6946, 0.1394],
         [0.5561, 0.7171]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.005790243843013313
Average Adjusted Rand Index: -0.0007582173912768069
[-0.005790243843013313, -0.005790243843013313] [-0.0007582173912768069, -0.0007582173912768069] [10305.291015625, 10305.291015625]
-------------------------------------
This iteration is 50
True Objective function: Loss = -9942.944006308191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22857.59765625
inf tensor(22857.5977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9807.5029296875
tensor(22857.5977, grad_fn=<NegBackward0>) tensor(9807.5029, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9806.5849609375
tensor(9807.5029, grad_fn=<NegBackward0>) tensor(9806.5850, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9806.4130859375
tensor(9806.5850, grad_fn=<NegBackward0>) tensor(9806.4131, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9806.3359375
tensor(9806.4131, grad_fn=<NegBackward0>) tensor(9806.3359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9806.287109375
tensor(9806.3359, grad_fn=<NegBackward0>) tensor(9806.2871, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9806.2431640625
tensor(9806.2871, grad_fn=<NegBackward0>) tensor(9806.2432, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9806.203125
tensor(9806.2432, grad_fn=<NegBackward0>) tensor(9806.2031, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9806.1533203125
tensor(9806.2031, grad_fn=<NegBackward0>) tensor(9806.1533, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9806.087890625
tensor(9806.1533, grad_fn=<NegBackward0>) tensor(9806.0879, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9806.01171875
tensor(9806.0879, grad_fn=<NegBackward0>) tensor(9806.0117, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9805.939453125
tensor(9806.0117, grad_fn=<NegBackward0>) tensor(9805.9395, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9805.8798828125
tensor(9805.9395, grad_fn=<NegBackward0>) tensor(9805.8799, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9805.8251953125
tensor(9805.8799, grad_fn=<NegBackward0>) tensor(9805.8252, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9805.75
tensor(9805.8252, grad_fn=<NegBackward0>) tensor(9805.7500, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9805.4814453125
tensor(9805.7500, grad_fn=<NegBackward0>) tensor(9805.4814, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9805.021484375
tensor(9805.4814, grad_fn=<NegBackward0>) tensor(9805.0215, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9804.5947265625
tensor(9805.0215, grad_fn=<NegBackward0>) tensor(9804.5947, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9804.419921875
tensor(9804.5947, grad_fn=<NegBackward0>) tensor(9804.4199, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9804.3359375
tensor(9804.4199, grad_fn=<NegBackward0>) tensor(9804.3359, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9804.2822265625
tensor(9804.3359, grad_fn=<NegBackward0>) tensor(9804.2822, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9804.244140625
tensor(9804.2822, grad_fn=<NegBackward0>) tensor(9804.2441, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9804.21484375
tensor(9804.2441, grad_fn=<NegBackward0>) tensor(9804.2148, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9804.19140625
tensor(9804.2148, grad_fn=<NegBackward0>) tensor(9804.1914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9804.173828125
tensor(9804.1914, grad_fn=<NegBackward0>) tensor(9804.1738, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9804.162109375
tensor(9804.1738, grad_fn=<NegBackward0>) tensor(9804.1621, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9804.1494140625
tensor(9804.1621, grad_fn=<NegBackward0>) tensor(9804.1494, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9804.1416015625
tensor(9804.1494, grad_fn=<NegBackward0>) tensor(9804.1416, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9804.1328125
tensor(9804.1416, grad_fn=<NegBackward0>) tensor(9804.1328, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9804.1259765625
tensor(9804.1328, grad_fn=<NegBackward0>) tensor(9804.1260, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9804.12109375
tensor(9804.1260, grad_fn=<NegBackward0>) tensor(9804.1211, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9804.1142578125
tensor(9804.1211, grad_fn=<NegBackward0>) tensor(9804.1143, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9804.1103515625
tensor(9804.1143, grad_fn=<NegBackward0>) tensor(9804.1104, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9804.10546875
tensor(9804.1104, grad_fn=<NegBackward0>) tensor(9804.1055, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9804.099609375
tensor(9804.1055, grad_fn=<NegBackward0>) tensor(9804.0996, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9804.0966796875
tensor(9804.0996, grad_fn=<NegBackward0>) tensor(9804.0967, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9804.0927734375
tensor(9804.0967, grad_fn=<NegBackward0>) tensor(9804.0928, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9804.0888671875
tensor(9804.0928, grad_fn=<NegBackward0>) tensor(9804.0889, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9804.0869140625
tensor(9804.0889, grad_fn=<NegBackward0>) tensor(9804.0869, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9804.0810546875
tensor(9804.0869, grad_fn=<NegBackward0>) tensor(9804.0811, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9804.078125
tensor(9804.0811, grad_fn=<NegBackward0>) tensor(9804.0781, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9804.0751953125
tensor(9804.0781, grad_fn=<NegBackward0>) tensor(9804.0752, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9804.072265625
tensor(9804.0752, grad_fn=<NegBackward0>) tensor(9804.0723, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9804.06640625
tensor(9804.0723, grad_fn=<NegBackward0>) tensor(9804.0664, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9804.0634765625
tensor(9804.0664, grad_fn=<NegBackward0>) tensor(9804.0635, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9804.060546875
tensor(9804.0635, grad_fn=<NegBackward0>) tensor(9804.0605, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9804.0546875
tensor(9804.0605, grad_fn=<NegBackward0>) tensor(9804.0547, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9804.05078125
tensor(9804.0547, grad_fn=<NegBackward0>) tensor(9804.0508, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9804.046875
tensor(9804.0508, grad_fn=<NegBackward0>) tensor(9804.0469, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9804.0419921875
tensor(9804.0469, grad_fn=<NegBackward0>) tensor(9804.0420, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9804.0380859375
tensor(9804.0420, grad_fn=<NegBackward0>) tensor(9804.0381, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9804.0361328125
tensor(9804.0381, grad_fn=<NegBackward0>) tensor(9804.0361, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9804.0322265625
tensor(9804.0361, grad_fn=<NegBackward0>) tensor(9804.0322, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9804.0283203125
tensor(9804.0322, grad_fn=<NegBackward0>) tensor(9804.0283, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9804.0244140625
tensor(9804.0283, grad_fn=<NegBackward0>) tensor(9804.0244, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9804.0185546875
tensor(9804.0244, grad_fn=<NegBackward0>) tensor(9804.0186, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9804.0166015625
tensor(9804.0186, grad_fn=<NegBackward0>) tensor(9804.0166, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9804.01171875
tensor(9804.0166, grad_fn=<NegBackward0>) tensor(9804.0117, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9804.0068359375
tensor(9804.0117, grad_fn=<NegBackward0>) tensor(9804.0068, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9804.0029296875
tensor(9804.0068, grad_fn=<NegBackward0>) tensor(9804.0029, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9803.9990234375
tensor(9804.0029, grad_fn=<NegBackward0>) tensor(9803.9990, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9803.9951171875
tensor(9803.9990, grad_fn=<NegBackward0>) tensor(9803.9951, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9803.9912109375
tensor(9803.9951, grad_fn=<NegBackward0>) tensor(9803.9912, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9803.9873046875
tensor(9803.9912, grad_fn=<NegBackward0>) tensor(9803.9873, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9803.9833984375
tensor(9803.9873, grad_fn=<NegBackward0>) tensor(9803.9834, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9803.9794921875
tensor(9803.9834, grad_fn=<NegBackward0>) tensor(9803.9795, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9803.9755859375
tensor(9803.9795, grad_fn=<NegBackward0>) tensor(9803.9756, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9803.9716796875
tensor(9803.9756, grad_fn=<NegBackward0>) tensor(9803.9717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9803.9677734375
tensor(9803.9717, grad_fn=<NegBackward0>) tensor(9803.9678, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9803.966796875
tensor(9803.9678, grad_fn=<NegBackward0>) tensor(9803.9668, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9803.9638671875
tensor(9803.9668, grad_fn=<NegBackward0>) tensor(9803.9639, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9803.9619140625
tensor(9803.9639, grad_fn=<NegBackward0>) tensor(9803.9619, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9803.95703125
tensor(9803.9619, grad_fn=<NegBackward0>) tensor(9803.9570, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9803.955078125
tensor(9803.9570, grad_fn=<NegBackward0>) tensor(9803.9551, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9803.953125
tensor(9803.9551, grad_fn=<NegBackward0>) tensor(9803.9531, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9803.9501953125
tensor(9803.9531, grad_fn=<NegBackward0>) tensor(9803.9502, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9803.9482421875
tensor(9803.9502, grad_fn=<NegBackward0>) tensor(9803.9482, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9803.9443359375
tensor(9803.9482, grad_fn=<NegBackward0>) tensor(9803.9443, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9803.9443359375
tensor(9803.9443, grad_fn=<NegBackward0>) tensor(9803.9443, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9803.9423828125
tensor(9803.9443, grad_fn=<NegBackward0>) tensor(9803.9424, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9803.9384765625
tensor(9803.9424, grad_fn=<NegBackward0>) tensor(9803.9385, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9803.9384765625
tensor(9803.9385, grad_fn=<NegBackward0>) tensor(9803.9385, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9803.9375
tensor(9803.9385, grad_fn=<NegBackward0>) tensor(9803.9375, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9803.958984375
tensor(9803.9375, grad_fn=<NegBackward0>) tensor(9803.9590, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9803.93359375
tensor(9803.9375, grad_fn=<NegBackward0>) tensor(9803.9336, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9803.931640625
tensor(9803.9336, grad_fn=<NegBackward0>) tensor(9803.9316, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9803.9326171875
tensor(9803.9316, grad_fn=<NegBackward0>) tensor(9803.9326, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9803.9296875
tensor(9803.9316, grad_fn=<NegBackward0>) tensor(9803.9297, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9803.927734375
tensor(9803.9297, grad_fn=<NegBackward0>) tensor(9803.9277, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9803.9287109375
tensor(9803.9277, grad_fn=<NegBackward0>) tensor(9803.9287, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9803.927734375
tensor(9803.9277, grad_fn=<NegBackward0>) tensor(9803.9277, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9803.9248046875
tensor(9803.9277, grad_fn=<NegBackward0>) tensor(9803.9248, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9803.92578125
tensor(9803.9248, grad_fn=<NegBackward0>) tensor(9803.9258, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9803.92578125
tensor(9803.9248, grad_fn=<NegBackward0>) tensor(9803.9258, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -9803.923828125
tensor(9803.9248, grad_fn=<NegBackward0>) tensor(9803.9238, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9803.9228515625
tensor(9803.9238, grad_fn=<NegBackward0>) tensor(9803.9229, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9803.921875
tensor(9803.9229, grad_fn=<NegBackward0>) tensor(9803.9219, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9803.947265625
tensor(9803.9219, grad_fn=<NegBackward0>) tensor(9803.9473, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9803.9228515625
tensor(9803.9219, grad_fn=<NegBackward0>) tensor(9803.9229, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9803.9208984375
tensor(9803.9219, grad_fn=<NegBackward0>) tensor(9803.9209, grad_fn=<NegBackward0>)
pi: tensor([[9.9987e-01, 1.3052e-04],
        [1.0968e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0151, 0.9849], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0043, 0.1527],
         [0.5820, 0.1351]],

        [[0.6875, 0.0738],
         [0.6539, 0.6491]],

        [[0.5724, 0.1704],
         [0.6626, 0.7053]],

        [[0.6364, 0.2099],
         [0.5206, 0.5136]],

        [[0.5184, 0.0860],
         [0.6338, 0.5007]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0004446725451460072
Average Adjusted Rand Index: 0.0006054628603728482
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22088.388671875
inf tensor(22088.3887, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9807.794921875
tensor(22088.3887, grad_fn=<NegBackward0>) tensor(9807.7949, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9806.54296875
tensor(9807.7949, grad_fn=<NegBackward0>) tensor(9806.5430, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9806.1337890625
tensor(9806.5430, grad_fn=<NegBackward0>) tensor(9806.1338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9805.935546875
tensor(9806.1338, grad_fn=<NegBackward0>) tensor(9805.9355, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9805.7900390625
tensor(9805.9355, grad_fn=<NegBackward0>) tensor(9805.7900, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9805.6611328125
tensor(9805.7900, grad_fn=<NegBackward0>) tensor(9805.6611, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9805.546875
tensor(9805.6611, grad_fn=<NegBackward0>) tensor(9805.5469, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9805.439453125
tensor(9805.5469, grad_fn=<NegBackward0>) tensor(9805.4395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9805.3359375
tensor(9805.4395, grad_fn=<NegBackward0>) tensor(9805.3359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9805.23046875
tensor(9805.3359, grad_fn=<NegBackward0>) tensor(9805.2305, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9805.1279296875
tensor(9805.2305, grad_fn=<NegBackward0>) tensor(9805.1279, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9805.0283203125
tensor(9805.1279, grad_fn=<NegBackward0>) tensor(9805.0283, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9804.9326171875
tensor(9805.0283, grad_fn=<NegBackward0>) tensor(9804.9326, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9804.8388671875
tensor(9804.9326, grad_fn=<NegBackward0>) tensor(9804.8389, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9804.7431640625
tensor(9804.8389, grad_fn=<NegBackward0>) tensor(9804.7432, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9804.640625
tensor(9804.7432, grad_fn=<NegBackward0>) tensor(9804.6406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9804.53515625
tensor(9804.6406, grad_fn=<NegBackward0>) tensor(9804.5352, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9804.4296875
tensor(9804.5352, grad_fn=<NegBackward0>) tensor(9804.4297, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9804.3291015625
tensor(9804.4297, grad_fn=<NegBackward0>) tensor(9804.3291, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9804.234375
tensor(9804.3291, grad_fn=<NegBackward0>) tensor(9804.2344, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9804.15234375
tensor(9804.2344, grad_fn=<NegBackward0>) tensor(9804.1523, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9804.0751953125
tensor(9804.1523, grad_fn=<NegBackward0>) tensor(9804.0752, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9803.9912109375
tensor(9804.0752, grad_fn=<NegBackward0>) tensor(9803.9912, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9803.857421875
tensor(9803.9912, grad_fn=<NegBackward0>) tensor(9803.8574, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9803.5869140625
tensor(9803.8574, grad_fn=<NegBackward0>) tensor(9803.5869, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9803.1416015625
tensor(9803.5869, grad_fn=<NegBackward0>) tensor(9803.1416, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9802.75390625
tensor(9803.1416, grad_fn=<NegBackward0>) tensor(9802.7539, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9802.5478515625
tensor(9802.7539, grad_fn=<NegBackward0>) tensor(9802.5479, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9802.4560546875
tensor(9802.5479, grad_fn=<NegBackward0>) tensor(9802.4561, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9802.416015625
tensor(9802.4561, grad_fn=<NegBackward0>) tensor(9802.4160, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9802.40234375
tensor(9802.4160, grad_fn=<NegBackward0>) tensor(9802.4023, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9802.3974609375
tensor(9802.4023, grad_fn=<NegBackward0>) tensor(9802.3975, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9802.3984375
tensor(9802.3975, grad_fn=<NegBackward0>) tensor(9802.3984, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9802.3955078125
tensor(9802.3975, grad_fn=<NegBackward0>) tensor(9802.3955, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9802.3955078125
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3955, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9802.3955078125
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3955, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9802.3955078125
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3955, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9802.3994140625
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3994, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9802.40625
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.4062, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -9802.396484375
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.3965, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -9802.400390625
tensor(9802.3955, grad_fn=<NegBackward0>) tensor(9802.4004, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4600 due to no improvement.
pi: tensor([[0.5913, 0.4087],
        [0.3044, 0.6956]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4162, 0.5838], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1051, 0.1273],
         [0.6529, 0.1624]],

        [[0.7282, 0.1283],
         [0.6590, 0.5972]],

        [[0.6787, 0.1315],
         [0.5554, 0.5106]],

        [[0.5591, 0.1230],
         [0.5204, 0.6328]],

        [[0.5644, 0.1301],
         [0.6654, 0.7280]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03017241379310345
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.03878787878787879
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.04626523806813387
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.05864653041804156
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12103407755581669
Global Adjusted Rand Index: 0.06339994319682299
Average Adjusted Rand Index: 0.058981227724594874
[-0.0004446725451460072, 0.06339994319682299] [0.0006054628603728482, 0.058981227724594874] [9803.919921875, 9802.400390625]
-------------------------------------
This iteration is 51
True Objective function: Loss = -9918.461666528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23075.87890625
inf tensor(23075.8789, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9782.25390625
tensor(23075.8789, grad_fn=<NegBackward0>) tensor(9782.2539, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9781.39453125
tensor(9782.2539, grad_fn=<NegBackward0>) tensor(9781.3945, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9781.025390625
tensor(9781.3945, grad_fn=<NegBackward0>) tensor(9781.0254, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9780.4814453125
tensor(9781.0254, grad_fn=<NegBackward0>) tensor(9780.4814, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9778.947265625
tensor(9780.4814, grad_fn=<NegBackward0>) tensor(9778.9473, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9777.8017578125
tensor(9778.9473, grad_fn=<NegBackward0>) tensor(9777.8018, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9776.73046875
tensor(9777.8018, grad_fn=<NegBackward0>) tensor(9776.7305, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9776.28515625
tensor(9776.7305, grad_fn=<NegBackward0>) tensor(9776.2852, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9775.98046875
tensor(9776.2852, grad_fn=<NegBackward0>) tensor(9775.9805, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9775.8642578125
tensor(9775.9805, grad_fn=<NegBackward0>) tensor(9775.8643, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9775.7900390625
tensor(9775.8643, grad_fn=<NegBackward0>) tensor(9775.7900, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9775.728515625
tensor(9775.7900, grad_fn=<NegBackward0>) tensor(9775.7285, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9775.67578125
tensor(9775.7285, grad_fn=<NegBackward0>) tensor(9775.6758, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9775.57421875
tensor(9775.6758, grad_fn=<NegBackward0>) tensor(9775.5742, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9775.416015625
tensor(9775.5742, grad_fn=<NegBackward0>) tensor(9775.4160, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9775.16015625
tensor(9775.4160, grad_fn=<NegBackward0>) tensor(9775.1602, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9774.982421875
tensor(9775.1602, grad_fn=<NegBackward0>) tensor(9774.9824, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9774.822265625
tensor(9774.9824, grad_fn=<NegBackward0>) tensor(9774.8223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9774.0205078125
tensor(9774.8223, grad_fn=<NegBackward0>) tensor(9774.0205, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9773.62109375
tensor(9774.0205, grad_fn=<NegBackward0>) tensor(9773.6211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9773.3271484375
tensor(9773.6211, grad_fn=<NegBackward0>) tensor(9773.3271, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9773.3046875
tensor(9773.3271, grad_fn=<NegBackward0>) tensor(9773.3047, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9773.294921875
tensor(9773.3047, grad_fn=<NegBackward0>) tensor(9773.2949, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9773.287109375
tensor(9773.2949, grad_fn=<NegBackward0>) tensor(9773.2871, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9773.2822265625
tensor(9773.2871, grad_fn=<NegBackward0>) tensor(9773.2822, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9773.279296875
tensor(9773.2822, grad_fn=<NegBackward0>) tensor(9773.2793, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9773.2763671875
tensor(9773.2793, grad_fn=<NegBackward0>) tensor(9773.2764, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9773.2744140625
tensor(9773.2764, grad_fn=<NegBackward0>) tensor(9773.2744, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9773.2724609375
tensor(9773.2744, grad_fn=<NegBackward0>) tensor(9773.2725, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9773.2724609375
tensor(9773.2725, grad_fn=<NegBackward0>) tensor(9773.2725, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9773.2705078125
tensor(9773.2725, grad_fn=<NegBackward0>) tensor(9773.2705, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9773.26953125
tensor(9773.2705, grad_fn=<NegBackward0>) tensor(9773.2695, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9773.2685546875
tensor(9773.2695, grad_fn=<NegBackward0>) tensor(9773.2686, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9773.267578125
tensor(9773.2686, grad_fn=<NegBackward0>) tensor(9773.2676, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9773.2685546875
tensor(9773.2676, grad_fn=<NegBackward0>) tensor(9773.2686, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9773.267578125
tensor(9773.2676, grad_fn=<NegBackward0>) tensor(9773.2676, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9773.2666015625
tensor(9773.2676, grad_fn=<NegBackward0>) tensor(9773.2666, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9773.265625
tensor(9773.2666, grad_fn=<NegBackward0>) tensor(9773.2656, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9773.2666015625
tensor(9773.2656, grad_fn=<NegBackward0>) tensor(9773.2666, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9773.265625
tensor(9773.2656, grad_fn=<NegBackward0>) tensor(9773.2656, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9773.2724609375
tensor(9773.2656, grad_fn=<NegBackward0>) tensor(9773.2725, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9773.265625
tensor(9773.2656, grad_fn=<NegBackward0>) tensor(9773.2656, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9773.263671875
tensor(9773.2656, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9773.265625
tensor(9773.2637, grad_fn=<NegBackward0>) tensor(9773.2656, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9773.2646484375
tensor(9773.2637, grad_fn=<NegBackward0>) tensor(9773.2646, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9773.263671875
tensor(9773.2637, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9773.2626953125
tensor(9773.2637, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9773.263671875
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9773.2646484375
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2646, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9773.2646484375
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2646, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -9773.2626953125
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9773.2666015625
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2666, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9773.263671875
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9773.263671875
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9773.263671875
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2637, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -9773.2626953125
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9773.26171875
tensor(9773.2627, grad_fn=<NegBackward0>) tensor(9773.2617, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9773.2705078125
tensor(9773.2617, grad_fn=<NegBackward0>) tensor(9773.2705, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9773.26171875
tensor(9773.2617, grad_fn=<NegBackward0>) tensor(9773.2617, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9773.271484375
tensor(9773.2617, grad_fn=<NegBackward0>) tensor(9773.2715, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9773.2607421875
tensor(9773.2617, grad_fn=<NegBackward0>) tensor(9773.2607, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9773.2626953125
tensor(9773.2607, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9773.2626953125
tensor(9773.2607, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9773.2626953125
tensor(9773.2607, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9773.26171875
tensor(9773.2607, grad_fn=<NegBackward0>) tensor(9773.2617, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -9773.2626953125
tensor(9773.2607, grad_fn=<NegBackward0>) tensor(9773.2627, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[9.9997e-01, 2.7260e-05],
        [3.6430e-03, 9.9636e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2273, 0.7727], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2300, 0.1188],
         [0.7279, 0.1286]],

        [[0.5482, 0.1249],
         [0.6553, 0.7159]],

        [[0.6772, 0.1492],
         [0.6893, 0.7183]],

        [[0.6142, 0.1332],
         [0.5996, 0.6773]],

        [[0.5914, 0.1273],
         [0.7166, 0.6876]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.012633139431831
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.007028177187439551
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.005553403073744027
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 29
Adjusted Rand Index: 0.17046421057343658
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 26
Adjusted Rand Index: 0.22262626262626262
Global Adjusted Rand Index: 0.06421393020209758
Average Adjusted Rand Index: 0.08366103857854276
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21820.20703125
inf tensor(21820.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9783.259765625
tensor(21820.2070, grad_fn=<NegBackward0>) tensor(9783.2598, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9782.181640625
tensor(9783.2598, grad_fn=<NegBackward0>) tensor(9782.1816, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9781.732421875
tensor(9782.1816, grad_fn=<NegBackward0>) tensor(9781.7324, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9781.423828125
tensor(9781.7324, grad_fn=<NegBackward0>) tensor(9781.4238, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9781.130859375
tensor(9781.4238, grad_fn=<NegBackward0>) tensor(9781.1309, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9780.6748046875
tensor(9781.1309, grad_fn=<NegBackward0>) tensor(9780.6748, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9780.2431640625
tensor(9780.6748, grad_fn=<NegBackward0>) tensor(9780.2432, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9779.876953125
tensor(9780.2432, grad_fn=<NegBackward0>) tensor(9779.8770, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9779.439453125
tensor(9779.8770, grad_fn=<NegBackward0>) tensor(9779.4395, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9778.826171875
tensor(9779.4395, grad_fn=<NegBackward0>) tensor(9778.8262, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9778.0087890625
tensor(9778.8262, grad_fn=<NegBackward0>) tensor(9778.0088, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9777.19140625
tensor(9778.0088, grad_fn=<NegBackward0>) tensor(9777.1914, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9776.779296875
tensor(9777.1914, grad_fn=<NegBackward0>) tensor(9776.7793, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9776.4765625
tensor(9776.7793, grad_fn=<NegBackward0>) tensor(9776.4766, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9776.3515625
tensor(9776.4766, grad_fn=<NegBackward0>) tensor(9776.3516, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9776.26171875
tensor(9776.3516, grad_fn=<NegBackward0>) tensor(9776.2617, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9776.0732421875
tensor(9776.2617, grad_fn=<NegBackward0>) tensor(9776.0732, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9775.966796875
tensor(9776.0732, grad_fn=<NegBackward0>) tensor(9775.9668, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9775.9091796875
tensor(9775.9668, grad_fn=<NegBackward0>) tensor(9775.9092, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9775.87890625
tensor(9775.9092, grad_fn=<NegBackward0>) tensor(9775.8789, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9775.8525390625
tensor(9775.8789, grad_fn=<NegBackward0>) tensor(9775.8525, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9775.8349609375
tensor(9775.8525, grad_fn=<NegBackward0>) tensor(9775.8350, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9775.8232421875
tensor(9775.8350, grad_fn=<NegBackward0>) tensor(9775.8232, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9775.802734375
tensor(9775.8232, grad_fn=<NegBackward0>) tensor(9775.8027, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9775.7724609375
tensor(9775.8027, grad_fn=<NegBackward0>) tensor(9775.7725, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9775.7490234375
tensor(9775.7725, grad_fn=<NegBackward0>) tensor(9775.7490, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9775.73046875
tensor(9775.7490, grad_fn=<NegBackward0>) tensor(9775.7305, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9775.716796875
tensor(9775.7305, grad_fn=<NegBackward0>) tensor(9775.7168, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9775.693359375
tensor(9775.7168, grad_fn=<NegBackward0>) tensor(9775.6934, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9775.669921875
tensor(9775.6934, grad_fn=<NegBackward0>) tensor(9775.6699, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9775.6650390625
tensor(9775.6699, grad_fn=<NegBackward0>) tensor(9775.6650, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9775.65625
tensor(9775.6650, grad_fn=<NegBackward0>) tensor(9775.6562, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9775.6455078125
tensor(9775.6562, grad_fn=<NegBackward0>) tensor(9775.6455, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9775.6396484375
tensor(9775.6455, grad_fn=<NegBackward0>) tensor(9775.6396, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9775.6337890625
tensor(9775.6396, grad_fn=<NegBackward0>) tensor(9775.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9775.6318359375
tensor(9775.6338, grad_fn=<NegBackward0>) tensor(9775.6318, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9775.6298828125
tensor(9775.6318, grad_fn=<NegBackward0>) tensor(9775.6299, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9775.626953125
tensor(9775.6299, grad_fn=<NegBackward0>) tensor(9775.6270, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9775.6279296875
tensor(9775.6270, grad_fn=<NegBackward0>) tensor(9775.6279, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9775.626953125
tensor(9775.6270, grad_fn=<NegBackward0>) tensor(9775.6270, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9775.6259765625
tensor(9775.6270, grad_fn=<NegBackward0>) tensor(9775.6260, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9775.625
tensor(9775.6260, grad_fn=<NegBackward0>) tensor(9775.6250, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9775.625
tensor(9775.6250, grad_fn=<NegBackward0>) tensor(9775.6250, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9775.6240234375
tensor(9775.6250, grad_fn=<NegBackward0>) tensor(9775.6240, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9775.6240234375
tensor(9775.6240, grad_fn=<NegBackward0>) tensor(9775.6240, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9775.6240234375
tensor(9775.6240, grad_fn=<NegBackward0>) tensor(9775.6240, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9775.623046875
tensor(9775.6240, grad_fn=<NegBackward0>) tensor(9775.6230, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9775.6240234375
tensor(9775.6230, grad_fn=<NegBackward0>) tensor(9775.6240, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9775.6220703125
tensor(9775.6230, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9775.6220703125
tensor(9775.6221, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9775.6220703125
tensor(9775.6221, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9775.6220703125
tensor(9775.6221, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9775.6201171875
tensor(9775.6221, grad_fn=<NegBackward0>) tensor(9775.6201, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9775.6201171875
tensor(9775.6201, grad_fn=<NegBackward0>) tensor(9775.6201, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9775.6201171875
tensor(9775.6201, grad_fn=<NegBackward0>) tensor(9775.6201, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9775.619140625
tensor(9775.6201, grad_fn=<NegBackward0>) tensor(9775.6191, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9775.6201171875
tensor(9775.6191, grad_fn=<NegBackward0>) tensor(9775.6201, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9775.62109375
tensor(9775.6191, grad_fn=<NegBackward0>) tensor(9775.6211, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9775.62109375
tensor(9775.6191, grad_fn=<NegBackward0>) tensor(9775.6211, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9775.6220703125
tensor(9775.6191, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -9775.6220703125
tensor(9775.6191, grad_fn=<NegBackward0>) tensor(9775.6221, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.9976, 0.0024],
        [0.0687, 0.9313]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.0425e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.1172],
         [0.7019, 0.1262]],

        [[0.5083, 0.1539],
         [0.6990, 0.5030]],

        [[0.5188, 0.1635],
         [0.6167, 0.5181]],

        [[0.6298, 0.1548],
         [0.6837, 0.6924]],

        [[0.6168, 0.1523],
         [0.6786, 0.6804]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.004021803333230736
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.052993146457920554
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.05818181818181818
Global Adjusted Rand Index: 0.011865400579721916
Average Adjusted Rand Index: 0.020991969889870656
[0.06421393020209758, 0.011865400579721916] [0.08366103857854276, 0.020991969889870656] [9773.2626953125, 9775.6220703125]
-------------------------------------
This iteration is 52
True Objective function: Loss = -10317.94225842151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22061.43359375
inf tensor(22061.4336, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10191.8779296875
tensor(22061.4336, grad_fn=<NegBackward0>) tensor(10191.8779, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10191.283203125
tensor(10191.8779, grad_fn=<NegBackward0>) tensor(10191.2832, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10191.0791015625
tensor(10191.2832, grad_fn=<NegBackward0>) tensor(10191.0791, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10190.8359375
tensor(10191.0791, grad_fn=<NegBackward0>) tensor(10190.8359, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10190.5224609375
tensor(10190.8359, grad_fn=<NegBackward0>) tensor(10190.5225, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10190.345703125
tensor(10190.5225, grad_fn=<NegBackward0>) tensor(10190.3457, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10190.2880859375
tensor(10190.3457, grad_fn=<NegBackward0>) tensor(10190.2881, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10190.265625
tensor(10190.2881, grad_fn=<NegBackward0>) tensor(10190.2656, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10190.2490234375
tensor(10190.2656, grad_fn=<NegBackward0>) tensor(10190.2490, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10190.232421875
tensor(10190.2490, grad_fn=<NegBackward0>) tensor(10190.2324, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10190.1962890625
tensor(10190.2324, grad_fn=<NegBackward0>) tensor(10190.1963, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10190.0556640625
tensor(10190.1963, grad_fn=<NegBackward0>) tensor(10190.0557, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10188.9931640625
tensor(10190.0557, grad_fn=<NegBackward0>) tensor(10188.9932, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10188.2724609375
tensor(10188.9932, grad_fn=<NegBackward0>) tensor(10188.2725, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10188.123046875
tensor(10188.2725, grad_fn=<NegBackward0>) tensor(10188.1230, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10188.064453125
tensor(10188.1230, grad_fn=<NegBackward0>) tensor(10188.0645, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10188.0390625
tensor(10188.0645, grad_fn=<NegBackward0>) tensor(10188.0391, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10188.0224609375
tensor(10188.0391, grad_fn=<NegBackward0>) tensor(10188.0225, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10188.0126953125
tensor(10188.0225, grad_fn=<NegBackward0>) tensor(10188.0127, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10188.0029296875
tensor(10188.0127, grad_fn=<NegBackward0>) tensor(10188.0029, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10187.9970703125
tensor(10188.0029, grad_fn=<NegBackward0>) tensor(10187.9971, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10187.9912109375
tensor(10187.9971, grad_fn=<NegBackward0>) tensor(10187.9912, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10187.986328125
tensor(10187.9912, grad_fn=<NegBackward0>) tensor(10187.9863, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10187.9814453125
tensor(10187.9863, grad_fn=<NegBackward0>) tensor(10187.9814, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10187.978515625
tensor(10187.9814, grad_fn=<NegBackward0>) tensor(10187.9785, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10187.974609375
tensor(10187.9785, grad_fn=<NegBackward0>) tensor(10187.9746, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10187.9716796875
tensor(10187.9746, grad_fn=<NegBackward0>) tensor(10187.9717, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10187.9677734375
tensor(10187.9717, grad_fn=<NegBackward0>) tensor(10187.9678, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10187.9658203125
tensor(10187.9678, grad_fn=<NegBackward0>) tensor(10187.9658, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10187.962890625
tensor(10187.9658, grad_fn=<NegBackward0>) tensor(10187.9629, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10187.95703125
tensor(10187.9629, grad_fn=<NegBackward0>) tensor(10187.9570, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10187.95703125
tensor(10187.9570, grad_fn=<NegBackward0>) tensor(10187.9570, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10187.9560546875
tensor(10187.9570, grad_fn=<NegBackward0>) tensor(10187.9561, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10187.9521484375
tensor(10187.9561, grad_fn=<NegBackward0>) tensor(10187.9521, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10187.9482421875
tensor(10187.9521, grad_fn=<NegBackward0>) tensor(10187.9482, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10187.947265625
tensor(10187.9482, grad_fn=<NegBackward0>) tensor(10187.9473, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10187.9462890625
tensor(10187.9473, grad_fn=<NegBackward0>) tensor(10187.9463, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10187.943359375
tensor(10187.9463, grad_fn=<NegBackward0>) tensor(10187.9434, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10187.94140625
tensor(10187.9434, grad_fn=<NegBackward0>) tensor(10187.9414, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10187.9375
tensor(10187.9414, grad_fn=<NegBackward0>) tensor(10187.9375, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10187.9375
tensor(10187.9375, grad_fn=<NegBackward0>) tensor(10187.9375, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10187.935546875
tensor(10187.9375, grad_fn=<NegBackward0>) tensor(10187.9355, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10187.93359375
tensor(10187.9355, grad_fn=<NegBackward0>) tensor(10187.9336, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10187.9296875
tensor(10187.9336, grad_fn=<NegBackward0>) tensor(10187.9297, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10187.927734375
tensor(10187.9297, grad_fn=<NegBackward0>) tensor(10187.9277, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10187.9267578125
tensor(10187.9277, grad_fn=<NegBackward0>) tensor(10187.9268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10187.923828125
tensor(10187.9268, grad_fn=<NegBackward0>) tensor(10187.9238, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10187.9228515625
tensor(10187.9238, grad_fn=<NegBackward0>) tensor(10187.9229, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10187.9189453125
tensor(10187.9229, grad_fn=<NegBackward0>) tensor(10187.9189, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10187.916015625
tensor(10187.9189, grad_fn=<NegBackward0>) tensor(10187.9160, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10187.9130859375
tensor(10187.9160, grad_fn=<NegBackward0>) tensor(10187.9131, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10187.91015625
tensor(10187.9131, grad_fn=<NegBackward0>) tensor(10187.9102, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10187.9072265625
tensor(10187.9102, grad_fn=<NegBackward0>) tensor(10187.9072, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10187.9033203125
tensor(10187.9072, grad_fn=<NegBackward0>) tensor(10187.9033, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10187.8994140625
tensor(10187.9033, grad_fn=<NegBackward0>) tensor(10187.8994, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10187.89453125
tensor(10187.8994, grad_fn=<NegBackward0>) tensor(10187.8945, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10187.8896484375
tensor(10187.8945, grad_fn=<NegBackward0>) tensor(10187.8896, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10187.884765625
tensor(10187.8896, grad_fn=<NegBackward0>) tensor(10187.8848, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10187.8798828125
tensor(10187.8848, grad_fn=<NegBackward0>) tensor(10187.8799, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10187.8720703125
tensor(10187.8799, grad_fn=<NegBackward0>) tensor(10187.8721, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10187.8642578125
tensor(10187.8721, grad_fn=<NegBackward0>) tensor(10187.8643, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10187.859375
tensor(10187.8643, grad_fn=<NegBackward0>) tensor(10187.8594, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10187.8505859375
tensor(10187.8594, grad_fn=<NegBackward0>) tensor(10187.8506, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10187.8427734375
tensor(10187.8506, grad_fn=<NegBackward0>) tensor(10187.8428, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10187.8359375
tensor(10187.8428, grad_fn=<NegBackward0>) tensor(10187.8359, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10187.828125
tensor(10187.8359, grad_fn=<NegBackward0>) tensor(10187.8281, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10187.822265625
tensor(10187.8281, grad_fn=<NegBackward0>) tensor(10187.8223, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10187.81640625
tensor(10187.8223, grad_fn=<NegBackward0>) tensor(10187.8164, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10187.8095703125
tensor(10187.8164, grad_fn=<NegBackward0>) tensor(10187.8096, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10187.806640625
tensor(10187.8096, grad_fn=<NegBackward0>) tensor(10187.8066, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10187.8017578125
tensor(10187.8066, grad_fn=<NegBackward0>) tensor(10187.8018, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10187.796875
tensor(10187.8018, grad_fn=<NegBackward0>) tensor(10187.7969, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10187.7919921875
tensor(10187.7969, grad_fn=<NegBackward0>) tensor(10187.7920, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10187.7890625
tensor(10187.7920, grad_fn=<NegBackward0>) tensor(10187.7891, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10187.78515625
tensor(10187.7891, grad_fn=<NegBackward0>) tensor(10187.7852, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10187.7841796875
tensor(10187.7852, grad_fn=<NegBackward0>) tensor(10187.7842, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10187.7802734375
tensor(10187.7842, grad_fn=<NegBackward0>) tensor(10187.7803, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10187.779296875
tensor(10187.7803, grad_fn=<NegBackward0>) tensor(10187.7793, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10187.779296875
tensor(10187.7793, grad_fn=<NegBackward0>) tensor(10187.7793, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10187.7890625
tensor(10187.7793, grad_fn=<NegBackward0>) tensor(10187.7891, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10187.771484375
tensor(10187.7793, grad_fn=<NegBackward0>) tensor(10187.7715, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10187.7734375
tensor(10187.7715, grad_fn=<NegBackward0>) tensor(10187.7734, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10187.7685546875
tensor(10187.7715, grad_fn=<NegBackward0>) tensor(10187.7686, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10187.767578125
tensor(10187.7686, grad_fn=<NegBackward0>) tensor(10187.7676, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10187.7666015625
tensor(10187.7676, grad_fn=<NegBackward0>) tensor(10187.7666, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10187.765625
tensor(10187.7666, grad_fn=<NegBackward0>) tensor(10187.7656, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10187.7626953125
tensor(10187.7656, grad_fn=<NegBackward0>) tensor(10187.7627, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10187.7685546875
tensor(10187.7627, grad_fn=<NegBackward0>) tensor(10187.7686, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -10187.76171875
tensor(10187.7627, grad_fn=<NegBackward0>) tensor(10187.7617, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10187.7626953125
tensor(10187.7617, grad_fn=<NegBackward0>) tensor(10187.7627, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10187.7607421875
tensor(10187.7617, grad_fn=<NegBackward0>) tensor(10187.7607, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10187.759765625
tensor(10187.7607, grad_fn=<NegBackward0>) tensor(10187.7598, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10187.7578125
tensor(10187.7598, grad_fn=<NegBackward0>) tensor(10187.7578, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10187.7587890625
tensor(10187.7578, grad_fn=<NegBackward0>) tensor(10187.7588, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10187.7587890625
tensor(10187.7578, grad_fn=<NegBackward0>) tensor(10187.7588, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -10187.7568359375
tensor(10187.7578, grad_fn=<NegBackward0>) tensor(10187.7568, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10187.8427734375
tensor(10187.7568, grad_fn=<NegBackward0>) tensor(10187.8428, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10187.755859375
tensor(10187.7568, grad_fn=<NegBackward0>) tensor(10187.7559, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10187.806640625
tensor(10187.7559, grad_fn=<NegBackward0>) tensor(10187.8066, grad_fn=<NegBackward0>)
1
pi: tensor([[1.0000e+00, 1.9892e-06],
        [2.1405e-04, 9.9979e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9796, 0.0204], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1447, 0.1676],
         [0.7102, 0.0019]],

        [[0.5180, 0.1093],
         [0.5678, 0.7254]],

        [[0.6302, 0.0719],
         [0.6023, 0.7150]],

        [[0.6917, 0.0750],
         [0.6790, 0.6595]],

        [[0.6434, 0.1888],
         [0.6761, 0.6275]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: 0.002143772439992498
Average Adjusted Rand Index: 0.0031328364720643585
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25652.111328125
inf tensor(25652.1113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10193.4892578125
tensor(25652.1113, grad_fn=<NegBackward0>) tensor(10193.4893, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10192.4794921875
tensor(10193.4893, grad_fn=<NegBackward0>) tensor(10192.4795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10192.248046875
tensor(10192.4795, grad_fn=<NegBackward0>) tensor(10192.2480, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10192.1455078125
tensor(10192.2480, grad_fn=<NegBackward0>) tensor(10192.1455, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10192.0908203125
tensor(10192.1455, grad_fn=<NegBackward0>) tensor(10192.0908, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10192.05859375
tensor(10192.0908, grad_fn=<NegBackward0>) tensor(10192.0586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10192.037109375
tensor(10192.0586, grad_fn=<NegBackward0>) tensor(10192.0371, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10192.021484375
tensor(10192.0371, grad_fn=<NegBackward0>) tensor(10192.0215, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10192.0107421875
tensor(10192.0215, grad_fn=<NegBackward0>) tensor(10192.0107, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10192.0009765625
tensor(10192.0107, grad_fn=<NegBackward0>) tensor(10192.0010, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10191.9951171875
tensor(10192.0010, grad_fn=<NegBackward0>) tensor(10191.9951, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10191.98828125
tensor(10191.9951, grad_fn=<NegBackward0>) tensor(10191.9883, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10191.982421875
tensor(10191.9883, grad_fn=<NegBackward0>) tensor(10191.9824, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10191.9794921875
tensor(10191.9824, grad_fn=<NegBackward0>) tensor(10191.9795, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10191.9765625
tensor(10191.9795, grad_fn=<NegBackward0>) tensor(10191.9766, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10191.970703125
tensor(10191.9766, grad_fn=<NegBackward0>) tensor(10191.9707, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10191.9677734375
tensor(10191.9707, grad_fn=<NegBackward0>) tensor(10191.9678, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10191.96484375
tensor(10191.9678, grad_fn=<NegBackward0>) tensor(10191.9648, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10191.9599609375
tensor(10191.9648, grad_fn=<NegBackward0>) tensor(10191.9600, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10191.9560546875
tensor(10191.9600, grad_fn=<NegBackward0>) tensor(10191.9561, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10191.951171875
tensor(10191.9561, grad_fn=<NegBackward0>) tensor(10191.9512, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10191.947265625
tensor(10191.9512, grad_fn=<NegBackward0>) tensor(10191.9473, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10191.939453125
tensor(10191.9473, grad_fn=<NegBackward0>) tensor(10191.9395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10191.931640625
tensor(10191.9395, grad_fn=<NegBackward0>) tensor(10191.9316, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10191.919921875
tensor(10191.9316, grad_fn=<NegBackward0>) tensor(10191.9199, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10191.904296875
tensor(10191.9199, grad_fn=<NegBackward0>) tensor(10191.9043, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10191.87890625
tensor(10191.9043, grad_fn=<NegBackward0>) tensor(10191.8789, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10191.8369140625
tensor(10191.8789, grad_fn=<NegBackward0>) tensor(10191.8369, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10191.75
tensor(10191.8369, grad_fn=<NegBackward0>) tensor(10191.7500, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10191.6142578125
tensor(10191.7500, grad_fn=<NegBackward0>) tensor(10191.6143, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10191.4501953125
tensor(10191.6143, grad_fn=<NegBackward0>) tensor(10191.4502, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10189.486328125
tensor(10191.4502, grad_fn=<NegBackward0>) tensor(10189.4863, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10188.376953125
tensor(10189.4863, grad_fn=<NegBackward0>) tensor(10188.3770, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10188.2080078125
tensor(10188.3770, grad_fn=<NegBackward0>) tensor(10188.2080, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10188.1318359375
tensor(10188.2080, grad_fn=<NegBackward0>) tensor(10188.1318, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10188.0869140625
tensor(10188.1318, grad_fn=<NegBackward0>) tensor(10188.0869, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10188.056640625
tensor(10188.0869, grad_fn=<NegBackward0>) tensor(10188.0566, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10188.0361328125
tensor(10188.0566, grad_fn=<NegBackward0>) tensor(10188.0361, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10188.01953125
tensor(10188.0361, grad_fn=<NegBackward0>) tensor(10188.0195, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10188.0068359375
tensor(10188.0195, grad_fn=<NegBackward0>) tensor(10188.0068, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10187.9970703125
tensor(10188.0068, grad_fn=<NegBackward0>) tensor(10187.9971, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10187.9873046875
tensor(10187.9971, grad_fn=<NegBackward0>) tensor(10187.9873, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10187.978515625
tensor(10187.9873, grad_fn=<NegBackward0>) tensor(10187.9785, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10187.97265625
tensor(10187.9785, grad_fn=<NegBackward0>) tensor(10187.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10187.966796875
tensor(10187.9727, grad_fn=<NegBackward0>) tensor(10187.9668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10187.9619140625
tensor(10187.9668, grad_fn=<NegBackward0>) tensor(10187.9619, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10187.9560546875
tensor(10187.9619, grad_fn=<NegBackward0>) tensor(10187.9561, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10187.953125
tensor(10187.9561, grad_fn=<NegBackward0>) tensor(10187.9531, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10187.9482421875
tensor(10187.9531, grad_fn=<NegBackward0>) tensor(10187.9482, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10187.9443359375
tensor(10187.9482, grad_fn=<NegBackward0>) tensor(10187.9443, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10187.9423828125
tensor(10187.9443, grad_fn=<NegBackward0>) tensor(10187.9424, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10187.9384765625
tensor(10187.9424, grad_fn=<NegBackward0>) tensor(10187.9385, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10187.9345703125
tensor(10187.9385, grad_fn=<NegBackward0>) tensor(10187.9346, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10187.9296875
tensor(10187.9346, grad_fn=<NegBackward0>) tensor(10187.9297, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10187.92578125
tensor(10187.9297, grad_fn=<NegBackward0>) tensor(10187.9258, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10187.921875
tensor(10187.9258, grad_fn=<NegBackward0>) tensor(10187.9219, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10187.9169921875
tensor(10187.9219, grad_fn=<NegBackward0>) tensor(10187.9170, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10187.9130859375
tensor(10187.9170, grad_fn=<NegBackward0>) tensor(10187.9131, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10187.90625
tensor(10187.9131, grad_fn=<NegBackward0>) tensor(10187.9062, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10187.90234375
tensor(10187.9062, grad_fn=<NegBackward0>) tensor(10187.9023, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10187.8935546875
tensor(10187.9023, grad_fn=<NegBackward0>) tensor(10187.8936, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10187.8876953125
tensor(10187.8936, grad_fn=<NegBackward0>) tensor(10187.8877, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10187.8798828125
tensor(10187.8877, grad_fn=<NegBackward0>) tensor(10187.8799, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10187.87109375
tensor(10187.8799, grad_fn=<NegBackward0>) tensor(10187.8711, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10187.8623046875
tensor(10187.8711, grad_fn=<NegBackward0>) tensor(10187.8623, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10187.853515625
tensor(10187.8623, grad_fn=<NegBackward0>) tensor(10187.8535, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10187.8427734375
tensor(10187.8535, grad_fn=<NegBackward0>) tensor(10187.8428, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10187.8359375
tensor(10187.8428, grad_fn=<NegBackward0>) tensor(10187.8359, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10187.8291015625
tensor(10187.8359, grad_fn=<NegBackward0>) tensor(10187.8291, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10187.8212890625
tensor(10187.8291, grad_fn=<NegBackward0>) tensor(10187.8213, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10187.8134765625
tensor(10187.8213, grad_fn=<NegBackward0>) tensor(10187.8135, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10187.8076171875
tensor(10187.8135, grad_fn=<NegBackward0>) tensor(10187.8076, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10187.8017578125
tensor(10187.8076, grad_fn=<NegBackward0>) tensor(10187.8018, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10187.7978515625
tensor(10187.8018, grad_fn=<NegBackward0>) tensor(10187.7979, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10187.79296875
tensor(10187.7979, grad_fn=<NegBackward0>) tensor(10187.7930, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10187.791015625
tensor(10187.7930, grad_fn=<NegBackward0>) tensor(10187.7910, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10187.787109375
tensor(10187.7910, grad_fn=<NegBackward0>) tensor(10187.7871, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10187.7822265625
tensor(10187.7871, grad_fn=<NegBackward0>) tensor(10187.7822, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10187.7802734375
tensor(10187.7822, grad_fn=<NegBackward0>) tensor(10187.7803, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10187.77734375
tensor(10187.7803, grad_fn=<NegBackward0>) tensor(10187.7773, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10187.775390625
tensor(10187.7773, grad_fn=<NegBackward0>) tensor(10187.7754, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10187.7724609375
tensor(10187.7754, grad_fn=<NegBackward0>) tensor(10187.7725, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10187.771484375
tensor(10187.7725, grad_fn=<NegBackward0>) tensor(10187.7715, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10187.76953125
tensor(10187.7715, grad_fn=<NegBackward0>) tensor(10187.7695, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10187.767578125
tensor(10187.7695, grad_fn=<NegBackward0>) tensor(10187.7676, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10187.7666015625
tensor(10187.7676, grad_fn=<NegBackward0>) tensor(10187.7666, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10187.7646484375
tensor(10187.7666, grad_fn=<NegBackward0>) tensor(10187.7646, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10187.763671875
tensor(10187.7646, grad_fn=<NegBackward0>) tensor(10187.7637, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10187.7626953125
tensor(10187.7637, grad_fn=<NegBackward0>) tensor(10187.7627, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10187.763671875
tensor(10187.7627, grad_fn=<NegBackward0>) tensor(10187.7637, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10187.7607421875
tensor(10187.7627, grad_fn=<NegBackward0>) tensor(10187.7607, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10187.775390625
tensor(10187.7607, grad_fn=<NegBackward0>) tensor(10187.7754, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10187.7587890625
tensor(10187.7607, grad_fn=<NegBackward0>) tensor(10187.7588, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10187.7802734375
tensor(10187.7588, grad_fn=<NegBackward0>) tensor(10187.7803, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -10187.7587890625
tensor(10187.7588, grad_fn=<NegBackward0>) tensor(10187.7588, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10187.759765625
tensor(10187.7588, grad_fn=<NegBackward0>) tensor(10187.7598, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -10187.7568359375
tensor(10187.7588, grad_fn=<NegBackward0>) tensor(10187.7568, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10187.7568359375
tensor(10187.7568, grad_fn=<NegBackward0>) tensor(10187.7568, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10187.75390625
tensor(10187.7568, grad_fn=<NegBackward0>) tensor(10187.7539, grad_fn=<NegBackward0>)
pi: tensor([[9.9960e-01, 3.9566e-04],
        [2.2005e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0204, 0.9796], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0018, 0.1677],
         [0.6634, 0.1447]],

        [[0.7043, 0.1093],
         [0.5656, 0.5239]],

        [[0.6965, 0.0719],
         [0.5455, 0.5465]],

        [[0.5905, 0.0750],
         [0.5202, 0.6463]],

        [[0.7227, 0.1888],
         [0.6692, 0.5658]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0012662455124815629
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: 0.002143772439992498
Average Adjusted Rand Index: 0.0031328364720643585
[0.002143772439992498, 0.002143772439992498] [0.0031328364720643585, 0.0031328364720643585] [10187.7548828125, 10187.755859375]
-------------------------------------
This iteration is 53
True Objective function: Loss = -9842.477496766836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20992.26953125
inf tensor(20992.2695, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9737.9599609375
tensor(20992.2695, grad_fn=<NegBackward0>) tensor(9737.9600, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9737.328125
tensor(9737.9600, grad_fn=<NegBackward0>) tensor(9737.3281, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9736.72265625
tensor(9737.3281, grad_fn=<NegBackward0>) tensor(9736.7227, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9736.6083984375
tensor(9736.7227, grad_fn=<NegBackward0>) tensor(9736.6084, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9736.5576171875
tensor(9736.6084, grad_fn=<NegBackward0>) tensor(9736.5576, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9736.5244140625
tensor(9736.5576, grad_fn=<NegBackward0>) tensor(9736.5244, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9736.4931640625
tensor(9736.5244, grad_fn=<NegBackward0>) tensor(9736.4932, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9736.4658203125
tensor(9736.4932, grad_fn=<NegBackward0>) tensor(9736.4658, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9736.43359375
tensor(9736.4658, grad_fn=<NegBackward0>) tensor(9736.4336, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9736.392578125
tensor(9736.4336, grad_fn=<NegBackward0>) tensor(9736.3926, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9736.3447265625
tensor(9736.3926, grad_fn=<NegBackward0>) tensor(9736.3447, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9736.29296875
tensor(9736.3447, grad_fn=<NegBackward0>) tensor(9736.2930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9736.2412109375
tensor(9736.2930, grad_fn=<NegBackward0>) tensor(9736.2412, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9736.189453125
tensor(9736.2412, grad_fn=<NegBackward0>) tensor(9736.1895, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9736.130859375
tensor(9736.1895, grad_fn=<NegBackward0>) tensor(9736.1309, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9736.056640625
tensor(9736.1309, grad_fn=<NegBackward0>) tensor(9736.0566, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9735.91015625
tensor(9736.0566, grad_fn=<NegBackward0>) tensor(9735.9102, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9735.29296875
tensor(9735.9102, grad_fn=<NegBackward0>) tensor(9735.2930, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9733.40625
tensor(9735.2930, grad_fn=<NegBackward0>) tensor(9733.4062, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9733.033203125
tensor(9733.4062, grad_fn=<NegBackward0>) tensor(9733.0332, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9732.9208984375
tensor(9733.0332, grad_fn=<NegBackward0>) tensor(9732.9209, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9732.861328125
tensor(9732.9209, grad_fn=<NegBackward0>) tensor(9732.8613, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9732.8330078125
tensor(9732.8613, grad_fn=<NegBackward0>) tensor(9732.8330, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9732.814453125
tensor(9732.8330, grad_fn=<NegBackward0>) tensor(9732.8145, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9732.7998046875
tensor(9732.8145, grad_fn=<NegBackward0>) tensor(9732.7998, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9732.7890625
tensor(9732.7998, grad_fn=<NegBackward0>) tensor(9732.7891, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9732.78125
tensor(9732.7891, grad_fn=<NegBackward0>) tensor(9732.7812, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9732.7763671875
tensor(9732.7812, grad_fn=<NegBackward0>) tensor(9732.7764, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9732.771484375
tensor(9732.7764, grad_fn=<NegBackward0>) tensor(9732.7715, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9732.7744140625
tensor(9732.7715, grad_fn=<NegBackward0>) tensor(9732.7744, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9732.7646484375
tensor(9732.7715, grad_fn=<NegBackward0>) tensor(9732.7646, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9732.759765625
tensor(9732.7646, grad_fn=<NegBackward0>) tensor(9732.7598, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9732.7548828125
tensor(9732.7598, grad_fn=<NegBackward0>) tensor(9732.7549, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9732.7490234375
tensor(9732.7549, grad_fn=<NegBackward0>) tensor(9732.7490, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9732.7373046875
tensor(9732.7490, grad_fn=<NegBackward0>) tensor(9732.7373, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9732.6884765625
tensor(9732.7373, grad_fn=<NegBackward0>) tensor(9732.6885, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9732.2041015625
tensor(9732.6885, grad_fn=<NegBackward0>) tensor(9732.2041, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9731.837890625
tensor(9732.2041, grad_fn=<NegBackward0>) tensor(9731.8379, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9731.7685546875
tensor(9731.8379, grad_fn=<NegBackward0>) tensor(9731.7686, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9731.7294921875
tensor(9731.7686, grad_fn=<NegBackward0>) tensor(9731.7295, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9731.71484375
tensor(9731.7295, grad_fn=<NegBackward0>) tensor(9731.7148, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9731.708984375
tensor(9731.7148, grad_fn=<NegBackward0>) tensor(9731.7090, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9731.705078125
tensor(9731.7090, grad_fn=<NegBackward0>) tensor(9731.7051, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9731.703125
tensor(9731.7051, grad_fn=<NegBackward0>) tensor(9731.7031, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9731.7021484375
tensor(9731.7031, grad_fn=<NegBackward0>) tensor(9731.7021, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9731.701171875
tensor(9731.7021, grad_fn=<NegBackward0>) tensor(9731.7012, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9731.7001953125
tensor(9731.7012, grad_fn=<NegBackward0>) tensor(9731.7002, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9731.69921875
tensor(9731.7002, grad_fn=<NegBackward0>) tensor(9731.6992, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9731.6982421875
tensor(9731.6992, grad_fn=<NegBackward0>) tensor(9731.6982, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9731.6962890625
tensor(9731.6982, grad_fn=<NegBackward0>) tensor(9731.6963, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9731.697265625
tensor(9731.6963, grad_fn=<NegBackward0>) tensor(9731.6973, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9731.6953125
tensor(9731.6963, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9731.6953125
tensor(9731.6953, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9731.6962890625
tensor(9731.6953, grad_fn=<NegBackward0>) tensor(9731.6963, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9731.6943359375
tensor(9731.6953, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9731.6943359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9731.6943359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9731.6953125
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9731.6943359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9731.6943359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9731.6943359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9731.693359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9731.693359375
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9731.6923828125
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9731.6943359375
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9731.6923828125
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9731.693359375
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9731.69140625
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9731.693359375
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9731.693359375
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -9731.69140625
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9731.69140625
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9731.69140625
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9731.69140625
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9731.693359375
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9731.689453125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6895, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9731.6923828125
tensor(9731.6895, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9731.6923828125
tensor(9731.6895, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9731.6923828125
tensor(9731.6895, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -9731.697265625
tensor(9731.6895, grad_fn=<NegBackward0>) tensor(9731.6973, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -9731.69140625
tensor(9731.6895, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.5832e-01, 4.1676e-02],
        [9.9986e-01, 1.3690e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9288, 0.0712], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1350, 0.1656],
         [0.7113, 0.1957]],

        [[0.6904, 0.1346],
         [0.6192, 0.6697]],

        [[0.7310, 0.0506],
         [0.6862, 0.7085]],

        [[0.7118, 0.0537],
         [0.6906, 0.5312]],

        [[0.5299, 0.1596],
         [0.6540, 0.5043]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.008987974340494725
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.014344415049389271
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003316657479681342
Average Adjusted Rand Index: 0.0010712881417789092
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21280.68359375
inf tensor(21280.6836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9739.263671875
tensor(21280.6836, grad_fn=<NegBackward0>) tensor(9739.2637, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9737.7587890625
tensor(9739.2637, grad_fn=<NegBackward0>) tensor(9737.7588, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9736.9951171875
tensor(9737.7588, grad_fn=<NegBackward0>) tensor(9736.9951, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9736.67578125
tensor(9736.9951, grad_fn=<NegBackward0>) tensor(9736.6758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9736.5576171875
tensor(9736.6758, grad_fn=<NegBackward0>) tensor(9736.5576, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9736.4892578125
tensor(9736.5576, grad_fn=<NegBackward0>) tensor(9736.4893, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9736.443359375
tensor(9736.4893, grad_fn=<NegBackward0>) tensor(9736.4434, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9736.4091796875
tensor(9736.4434, grad_fn=<NegBackward0>) tensor(9736.4092, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9736.37890625
tensor(9736.4092, grad_fn=<NegBackward0>) tensor(9736.3789, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9736.349609375
tensor(9736.3789, grad_fn=<NegBackward0>) tensor(9736.3496, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9736.318359375
tensor(9736.3496, grad_fn=<NegBackward0>) tensor(9736.3184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9736.28125
tensor(9736.3184, grad_fn=<NegBackward0>) tensor(9736.2812, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9736.23828125
tensor(9736.2812, grad_fn=<NegBackward0>) tensor(9736.2383, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9736.169921875
tensor(9736.2383, grad_fn=<NegBackward0>) tensor(9736.1699, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9736.0556640625
tensor(9736.1699, grad_fn=<NegBackward0>) tensor(9736.0557, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9735.6572265625
tensor(9736.0557, grad_fn=<NegBackward0>) tensor(9735.6572, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9733.9072265625
tensor(9735.6572, grad_fn=<NegBackward0>) tensor(9733.9072, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9733.240234375
tensor(9733.9072, grad_fn=<NegBackward0>) tensor(9733.2402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9733.107421875
tensor(9733.2402, grad_fn=<NegBackward0>) tensor(9733.1074, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9733.0322265625
tensor(9733.1074, grad_fn=<NegBackward0>) tensor(9733.0322, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9732.98046875
tensor(9733.0322, grad_fn=<NegBackward0>) tensor(9732.9805, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9732.939453125
tensor(9732.9805, grad_fn=<NegBackward0>) tensor(9732.9395, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9732.9091796875
tensor(9732.9395, grad_fn=<NegBackward0>) tensor(9732.9092, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9732.8828125
tensor(9732.9092, grad_fn=<NegBackward0>) tensor(9732.8828, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9732.8603515625
tensor(9732.8828, grad_fn=<NegBackward0>) tensor(9732.8604, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9732.8408203125
tensor(9732.8604, grad_fn=<NegBackward0>) tensor(9732.8408, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9732.8251953125
tensor(9732.8408, grad_fn=<NegBackward0>) tensor(9732.8252, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9732.8125
tensor(9732.8252, grad_fn=<NegBackward0>) tensor(9732.8125, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9732.8017578125
tensor(9732.8125, grad_fn=<NegBackward0>) tensor(9732.8018, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9732.794921875
tensor(9732.8018, grad_fn=<NegBackward0>) tensor(9732.7949, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9732.7880859375
tensor(9732.7949, grad_fn=<NegBackward0>) tensor(9732.7881, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9732.7822265625
tensor(9732.7881, grad_fn=<NegBackward0>) tensor(9732.7822, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9732.77734375
tensor(9732.7822, grad_fn=<NegBackward0>) tensor(9732.7773, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9732.7724609375
tensor(9732.7773, grad_fn=<NegBackward0>) tensor(9732.7725, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9732.7685546875
tensor(9732.7725, grad_fn=<NegBackward0>) tensor(9732.7686, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9732.765625
tensor(9732.7686, grad_fn=<NegBackward0>) tensor(9732.7656, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9732.763671875
tensor(9732.7656, grad_fn=<NegBackward0>) tensor(9732.7637, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9732.7607421875
tensor(9732.7637, grad_fn=<NegBackward0>) tensor(9732.7607, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9732.759765625
tensor(9732.7607, grad_fn=<NegBackward0>) tensor(9732.7598, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9732.7568359375
tensor(9732.7598, grad_fn=<NegBackward0>) tensor(9732.7568, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9732.755859375
tensor(9732.7568, grad_fn=<NegBackward0>) tensor(9732.7559, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9732.751953125
tensor(9732.7559, grad_fn=<NegBackward0>) tensor(9732.7520, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9732.748046875
tensor(9732.7520, grad_fn=<NegBackward0>) tensor(9732.7480, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9732.7451171875
tensor(9732.7480, grad_fn=<NegBackward0>) tensor(9732.7451, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9732.73828125
tensor(9732.7451, grad_fn=<NegBackward0>) tensor(9732.7383, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9732.712890625
tensor(9732.7383, grad_fn=<NegBackward0>) tensor(9732.7129, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9732.5625
tensor(9732.7129, grad_fn=<NegBackward0>) tensor(9732.5625, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9732.0283203125
tensor(9732.5625, grad_fn=<NegBackward0>) tensor(9732.0283, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9731.8251953125
tensor(9732.0283, grad_fn=<NegBackward0>) tensor(9731.8252, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9731.740234375
tensor(9731.8252, grad_fn=<NegBackward0>) tensor(9731.7402, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9731.7119140625
tensor(9731.7402, grad_fn=<NegBackward0>) tensor(9731.7119, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9731.7041015625
tensor(9731.7119, grad_fn=<NegBackward0>) tensor(9731.7041, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9731.703125
tensor(9731.7041, grad_fn=<NegBackward0>) tensor(9731.7031, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9731.701171875
tensor(9731.7031, grad_fn=<NegBackward0>) tensor(9731.7012, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9731.69921875
tensor(9731.7012, grad_fn=<NegBackward0>) tensor(9731.6992, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9731.697265625
tensor(9731.6992, grad_fn=<NegBackward0>) tensor(9731.6973, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9731.697265625
tensor(9731.6973, grad_fn=<NegBackward0>) tensor(9731.6973, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9731.6953125
tensor(9731.6973, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9731.697265625
tensor(9731.6953, grad_fn=<NegBackward0>) tensor(9731.6973, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9731.6943359375
tensor(9731.6953, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9731.6953125
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9731.6953125
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9731.6953125
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6953, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -9731.693359375
tensor(9731.6943, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9731.693359375
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9731.6943359375
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9731.693359375
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9731.6923828125
tensor(9731.6934, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9731.693359375
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9731.6923828125
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9731.693359375
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9731.6943359375
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6943, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9731.6923828125
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9731.69140625
tensor(9731.6924, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9731.693359375
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9731.69140625
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6914, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9731.693359375
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6934, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -9731.841796875
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.8418, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -9731.6923828125
tensor(9731.6914, grad_fn=<NegBackward0>) tensor(9731.6924, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[9.5834e-01, 4.1664e-02],
        [9.9978e-01, 2.1751e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9289, 0.0711], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1350, 0.1656],
         [0.5233, 0.1957]],

        [[0.6641, 0.1346],
         [0.5902, 0.5021]],

        [[0.7240, 0.0506],
         [0.6537, 0.7291]],

        [[0.5915, 0.0537],
         [0.5436, 0.5580]],

        [[0.6619, 0.1596],
         [0.6757, 0.5074]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.008987974340494725
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.014344415049389271
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003316657479681342
Average Adjusted Rand Index: 0.0010712881417789092
[-0.0003316657479681342, -0.0003316657479681342] [0.0010712881417789092, 0.0010712881417789092] [9731.69140625, 9731.6923828125]
-------------------------------------
This iteration is 54
True Objective function: Loss = -9971.613218430504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24228.875
inf tensor(24228.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9823.1220703125
tensor(24228.8750, grad_fn=<NegBackward0>) tensor(9823.1221, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9821.6220703125
tensor(9823.1221, grad_fn=<NegBackward0>) tensor(9821.6221, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9821.34375
tensor(9821.6221, grad_fn=<NegBackward0>) tensor(9821.3438, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9821.23046875
tensor(9821.3438, grad_fn=<NegBackward0>) tensor(9821.2305, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9821.16796875
tensor(9821.2305, grad_fn=<NegBackward0>) tensor(9821.1680, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9821.126953125
tensor(9821.1680, grad_fn=<NegBackward0>) tensor(9821.1270, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9821.09375
tensor(9821.1270, grad_fn=<NegBackward0>) tensor(9821.0938, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9821.0615234375
tensor(9821.0938, grad_fn=<NegBackward0>) tensor(9821.0615, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9821.0302734375
tensor(9821.0615, grad_fn=<NegBackward0>) tensor(9821.0303, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9820.994140625
tensor(9821.0303, grad_fn=<NegBackward0>) tensor(9820.9941, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9820.9453125
tensor(9820.9941, grad_fn=<NegBackward0>) tensor(9820.9453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9820.8310546875
tensor(9820.9453, grad_fn=<NegBackward0>) tensor(9820.8311, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9819.744140625
tensor(9820.8311, grad_fn=<NegBackward0>) tensor(9819.7441, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9819.3681640625
tensor(9819.7441, grad_fn=<NegBackward0>) tensor(9819.3682, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9819.2216796875
tensor(9819.3682, grad_fn=<NegBackward0>) tensor(9819.2217, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9818.9560546875
tensor(9819.2217, grad_fn=<NegBackward0>) tensor(9818.9561, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9818.76171875
tensor(9818.9561, grad_fn=<NegBackward0>) tensor(9818.7617, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9818.6875
tensor(9818.7617, grad_fn=<NegBackward0>) tensor(9818.6875, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9818.6474609375
tensor(9818.6875, grad_fn=<NegBackward0>) tensor(9818.6475, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9818.6298828125
tensor(9818.6475, grad_fn=<NegBackward0>) tensor(9818.6299, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9818.615234375
tensor(9818.6299, grad_fn=<NegBackward0>) tensor(9818.6152, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9818.6044921875
tensor(9818.6152, grad_fn=<NegBackward0>) tensor(9818.6045, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9818.5966796875
tensor(9818.6045, grad_fn=<NegBackward0>) tensor(9818.5967, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9818.58984375
tensor(9818.5967, grad_fn=<NegBackward0>) tensor(9818.5898, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9818.583984375
tensor(9818.5898, grad_fn=<NegBackward0>) tensor(9818.5840, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9818.5791015625
tensor(9818.5840, grad_fn=<NegBackward0>) tensor(9818.5791, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9818.5771484375
tensor(9818.5791, grad_fn=<NegBackward0>) tensor(9818.5771, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9818.5751953125
tensor(9818.5771, grad_fn=<NegBackward0>) tensor(9818.5752, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9818.5732421875
tensor(9818.5752, grad_fn=<NegBackward0>) tensor(9818.5732, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9818.572265625
tensor(9818.5732, grad_fn=<NegBackward0>) tensor(9818.5723, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9818.5712890625
tensor(9818.5723, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9818.5712890625
tensor(9818.5713, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9818.5703125
tensor(9818.5713, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9818.572265625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5723, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9818.5732421875
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5732, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9818.5703125
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9818.5712890625
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9818.5693359375
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5693, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9818.5712890625
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9818.5712890625
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -9818.5693359375
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5693, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.4662, 0.5338],
        [0.0155, 0.9845]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0706, 0.9294], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.1650],
         [0.7285, 0.1325]],

        [[0.5439, 0.1999],
         [0.7149, 0.6101]],

        [[0.5609, 0.1458],
         [0.6496, 0.5368]],

        [[0.6852, 0.1145],
         [0.6782, 0.6957]],

        [[0.5007, 0.2093],
         [0.6955, 0.6479]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 35
Adjusted Rand Index: 0.0155095116927178
Global Adjusted Rand Index: 0.007484037302362982
Average Adjusted Rand Index: 0.005870131340926243
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21529.931640625
inf tensor(21529.9316, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9822.0693359375
tensor(21529.9316, grad_fn=<NegBackward0>) tensor(9822.0693, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9821.4072265625
tensor(9822.0693, grad_fn=<NegBackward0>) tensor(9821.4072, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9821.0009765625
tensor(9821.4072, grad_fn=<NegBackward0>) tensor(9821.0010, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9820.33203125
tensor(9821.0010, grad_fn=<NegBackward0>) tensor(9820.3320, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9820.0546875
tensor(9820.3320, grad_fn=<NegBackward0>) tensor(9820.0547, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9819.900390625
tensor(9820.0547, grad_fn=<NegBackward0>) tensor(9819.9004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9819.7509765625
tensor(9819.9004, grad_fn=<NegBackward0>) tensor(9819.7510, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9819.6298828125
tensor(9819.7510, grad_fn=<NegBackward0>) tensor(9819.6299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9819.5556640625
tensor(9819.6299, grad_fn=<NegBackward0>) tensor(9819.5557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9819.4970703125
tensor(9819.5557, grad_fn=<NegBackward0>) tensor(9819.4971, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9819.4365234375
tensor(9819.4971, grad_fn=<NegBackward0>) tensor(9819.4365, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9819.353515625
tensor(9819.4365, grad_fn=<NegBackward0>) tensor(9819.3535, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9819.203125
tensor(9819.3535, grad_fn=<NegBackward0>) tensor(9819.2031, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9818.9580078125
tensor(9819.2031, grad_fn=<NegBackward0>) tensor(9818.9580, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9818.7939453125
tensor(9818.9580, grad_fn=<NegBackward0>) tensor(9818.7939, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9818.7119140625
tensor(9818.7939, grad_fn=<NegBackward0>) tensor(9818.7119, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9818.6630859375
tensor(9818.7119, grad_fn=<NegBackward0>) tensor(9818.6631, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9818.6328125
tensor(9818.6631, grad_fn=<NegBackward0>) tensor(9818.6328, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9818.61328125
tensor(9818.6328, grad_fn=<NegBackward0>) tensor(9818.6133, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9818.5986328125
tensor(9818.6133, grad_fn=<NegBackward0>) tensor(9818.5986, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9818.5888671875
tensor(9818.5986, grad_fn=<NegBackward0>) tensor(9818.5889, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9818.583984375
tensor(9818.5889, grad_fn=<NegBackward0>) tensor(9818.5840, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9818.5791015625
tensor(9818.5840, grad_fn=<NegBackward0>) tensor(9818.5791, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9818.5791015625
tensor(9818.5791, grad_fn=<NegBackward0>) tensor(9818.5791, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9818.5751953125
tensor(9818.5791, grad_fn=<NegBackward0>) tensor(9818.5752, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9818.57421875
tensor(9818.5752, grad_fn=<NegBackward0>) tensor(9818.5742, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9818.57421875
tensor(9818.5742, grad_fn=<NegBackward0>) tensor(9818.5742, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9818.572265625
tensor(9818.5742, grad_fn=<NegBackward0>) tensor(9818.5723, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9818.5712890625
tensor(9818.5723, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9818.572265625
tensor(9818.5713, grad_fn=<NegBackward0>) tensor(9818.5723, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9818.5703125
tensor(9818.5713, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9818.5693359375
tensor(9818.5703, grad_fn=<NegBackward0>) tensor(9818.5693, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9818.5712890625
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -9818.5712890625
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5713, grad_fn=<NegBackward0>)
3
Iteration 3600: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
4
Iteration 3700: Loss = -9818.5703125
tensor(9818.5693, grad_fn=<NegBackward0>) tensor(9818.5703, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3700 due to no improvement.
pi: tensor([[0.4644, 0.5356],
        [0.0154, 0.9846]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0707, 0.9293], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2001, 0.1649],
         [0.5304, 0.1325]],

        [[0.5586, 0.1999],
         [0.7064, 0.6192]],

        [[0.7009, 0.1458],
         [0.5030, 0.5351]],

        [[0.6832, 0.1143],
         [0.5386, 0.6586]],

        [[0.6452, 0.2095],
         [0.6587, 0.5581]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 35
Adjusted Rand Index: 0.0155095116927178
Global Adjusted Rand Index: 0.007484037302362982
Average Adjusted Rand Index: 0.005870131340926243
[0.007484037302362982, 0.007484037302362982] [0.005870131340926243, 0.005870131340926243] [9818.5703125, 9818.5703125]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10075.32409435901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19982.47265625
inf tensor(19982.4727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9992.5048828125
tensor(19982.4727, grad_fn=<NegBackward0>) tensor(9992.5049, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9987.39453125
tensor(9992.5049, grad_fn=<NegBackward0>) tensor(9987.3945, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9984.2939453125
tensor(9987.3945, grad_fn=<NegBackward0>) tensor(9984.2939, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9980.7978515625
tensor(9984.2939, grad_fn=<NegBackward0>) tensor(9980.7979, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9978.33984375
tensor(9980.7979, grad_fn=<NegBackward0>) tensor(9978.3398, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9976.3779296875
tensor(9978.3398, grad_fn=<NegBackward0>) tensor(9976.3779, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9970.7783203125
tensor(9976.3779, grad_fn=<NegBackward0>) tensor(9970.7783, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9968.533203125
tensor(9970.7783, grad_fn=<NegBackward0>) tensor(9968.5332, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9967.537109375
tensor(9968.5332, grad_fn=<NegBackward0>) tensor(9967.5371, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9967.1318359375
tensor(9967.5371, grad_fn=<NegBackward0>) tensor(9967.1318, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9966.9931640625
tensor(9967.1318, grad_fn=<NegBackward0>) tensor(9966.9932, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9966.9658203125
tensor(9966.9932, grad_fn=<NegBackward0>) tensor(9966.9658, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9966.9443359375
tensor(9966.9658, grad_fn=<NegBackward0>) tensor(9966.9443, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9966.9365234375
tensor(9966.9443, grad_fn=<NegBackward0>) tensor(9966.9365, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9966.931640625
tensor(9966.9365, grad_fn=<NegBackward0>) tensor(9966.9316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9966.9287109375
tensor(9966.9316, grad_fn=<NegBackward0>) tensor(9966.9287, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9966.92578125
tensor(9966.9287, grad_fn=<NegBackward0>) tensor(9966.9258, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9966.9228515625
tensor(9966.9258, grad_fn=<NegBackward0>) tensor(9966.9229, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9966.9208984375
tensor(9966.9229, grad_fn=<NegBackward0>) tensor(9966.9209, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9966.9189453125
tensor(9966.9209, grad_fn=<NegBackward0>) tensor(9966.9189, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9966.91796875
tensor(9966.9189, grad_fn=<NegBackward0>) tensor(9966.9180, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9966.9169921875
tensor(9966.9180, grad_fn=<NegBackward0>) tensor(9966.9170, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9966.9150390625
tensor(9966.9170, grad_fn=<NegBackward0>) tensor(9966.9150, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9966.9140625
tensor(9966.9150, grad_fn=<NegBackward0>) tensor(9966.9141, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9966.9140625
tensor(9966.9141, grad_fn=<NegBackward0>) tensor(9966.9141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9966.912109375
tensor(9966.9141, grad_fn=<NegBackward0>) tensor(9966.9121, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9966.912109375
tensor(9966.9121, grad_fn=<NegBackward0>) tensor(9966.9121, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9966.912109375
tensor(9966.9121, grad_fn=<NegBackward0>) tensor(9966.9121, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9966.91015625
tensor(9966.9121, grad_fn=<NegBackward0>) tensor(9966.9102, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9966.91015625
tensor(9966.9102, grad_fn=<NegBackward0>) tensor(9966.9102, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9966.91015625
tensor(9966.9102, grad_fn=<NegBackward0>) tensor(9966.9102, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9966.9091796875
tensor(9966.9102, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9966.9091796875
tensor(9966.9092, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9966.9091796875
tensor(9966.9092, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9966.908203125
tensor(9966.9092, grad_fn=<NegBackward0>) tensor(9966.9082, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9966.9091796875
tensor(9966.9082, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9966.9091796875
tensor(9966.9082, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -9966.908203125
tensor(9966.9082, grad_fn=<NegBackward0>) tensor(9966.9082, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9966.9072265625
tensor(9966.9082, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9966.9072265625
tensor(9966.9072, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9966.90625
tensor(9966.9072, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9966.9072265625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9966.9072265625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9966.9111328125
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9111, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9966.9072265625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9966.90625
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9966.9052734375
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9053, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9966.9052734375
tensor(9966.9053, grad_fn=<NegBackward0>) tensor(9966.9053, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9966.9052734375
tensor(9966.9053, grad_fn=<NegBackward0>) tensor(9966.9053, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9966.9052734375
tensor(9966.9053, grad_fn=<NegBackward0>) tensor(9966.9053, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9966.904296875
tensor(9966.9053, grad_fn=<NegBackward0>) tensor(9966.9043, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9966.9033203125
tensor(9966.9043, grad_fn=<NegBackward0>) tensor(9966.9033, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9966.90625
tensor(9966.9033, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9966.9072265625
tensor(9966.9033, grad_fn=<NegBackward0>) tensor(9966.9072, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9966.916015625
tensor(9966.9033, grad_fn=<NegBackward0>) tensor(9966.9160, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9966.9052734375
tensor(9966.9033, grad_fn=<NegBackward0>) tensor(9966.9053, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -9966.904296875
tensor(9966.9033, grad_fn=<NegBackward0>) tensor(9966.9043, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.8741, 0.1259],
        [0.2882, 0.7118]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.8535e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2091, 0.1299],
         [0.6754, 0.1268]],

        [[0.5485, 0.1388],
         [0.5084, 0.6186]],

        [[0.5538, 0.1195],
         [0.5274, 0.5062]],

        [[0.5755, 0.1145],
         [0.6523, 0.6736]],

        [[0.7088, 0.0967],
         [0.5901, 0.5150]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 31
Adjusted Rand Index: 0.13722463718870195
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 15
Adjusted Rand Index: 0.48484848484848486
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 11
Adjusted Rand Index: 0.6044273961975466
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 12
Adjusted Rand Index: 0.5733034502079607
Global Adjusted Rand Index: 0.2729497806939191
Average Adjusted Rand Index: 0.35996079368853884
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20442.046875
inf tensor(20442.0469, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9991.8955078125
tensor(20442.0469, grad_fn=<NegBackward0>) tensor(9991.8955, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9987.4658203125
tensor(9991.8955, grad_fn=<NegBackward0>) tensor(9987.4658, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9984.5673828125
tensor(9987.4658, grad_fn=<NegBackward0>) tensor(9984.5674, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9983.298828125
tensor(9984.5674, grad_fn=<NegBackward0>) tensor(9983.2988, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9982.041015625
tensor(9983.2988, grad_fn=<NegBackward0>) tensor(9982.0410, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9979.80859375
tensor(9982.0410, grad_fn=<NegBackward0>) tensor(9979.8086, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9978.2431640625
tensor(9979.8086, grad_fn=<NegBackward0>) tensor(9978.2432, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9977.12109375
tensor(9978.2432, grad_fn=<NegBackward0>) tensor(9977.1211, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9975.666015625
tensor(9977.1211, grad_fn=<NegBackward0>) tensor(9975.6660, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9970.8564453125
tensor(9975.6660, grad_fn=<NegBackward0>) tensor(9970.8564, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9968.3701171875
tensor(9970.8564, grad_fn=<NegBackward0>) tensor(9968.3701, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9967.5400390625
tensor(9968.3701, grad_fn=<NegBackward0>) tensor(9967.5400, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9967.177734375
tensor(9967.5400, grad_fn=<NegBackward0>) tensor(9967.1777, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9966.958984375
tensor(9967.1777, grad_fn=<NegBackward0>) tensor(9966.9590, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9966.927734375
tensor(9966.9590, grad_fn=<NegBackward0>) tensor(9966.9277, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9966.9189453125
tensor(9966.9277, grad_fn=<NegBackward0>) tensor(9966.9189, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9966.9140625
tensor(9966.9189, grad_fn=<NegBackward0>) tensor(9966.9141, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9966.9091796875
tensor(9966.9141, grad_fn=<NegBackward0>) tensor(9966.9092, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9966.90625
tensor(9966.9092, grad_fn=<NegBackward0>) tensor(9966.9062, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9966.904296875
tensor(9966.9062, grad_fn=<NegBackward0>) tensor(9966.9043, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9966.90234375
tensor(9966.9043, grad_fn=<NegBackward0>) tensor(9966.9023, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9966.900390625
tensor(9966.9023, grad_fn=<NegBackward0>) tensor(9966.9004, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9966.8994140625
tensor(9966.9004, grad_fn=<NegBackward0>) tensor(9966.8994, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9966.8974609375
tensor(9966.8994, grad_fn=<NegBackward0>) tensor(9966.8975, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9966.8984375
tensor(9966.8975, grad_fn=<NegBackward0>) tensor(9966.8984, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -9966.8984375
tensor(9966.8975, grad_fn=<NegBackward0>) tensor(9966.8984, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -9966.89453125
tensor(9966.8975, grad_fn=<NegBackward0>) tensor(9966.8945, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9966.8935546875
tensor(9966.8945, grad_fn=<NegBackward0>) tensor(9966.8936, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9966.890625
tensor(9966.8936, grad_fn=<NegBackward0>) tensor(9966.8906, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9966.8876953125
tensor(9966.8906, grad_fn=<NegBackward0>) tensor(9966.8877, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9966.8818359375
tensor(9966.8877, grad_fn=<NegBackward0>) tensor(9966.8818, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9966.8759765625
tensor(9966.8818, grad_fn=<NegBackward0>) tensor(9966.8760, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9966.8720703125
tensor(9966.8760, grad_fn=<NegBackward0>) tensor(9966.8721, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9966.8671875
tensor(9966.8721, grad_fn=<NegBackward0>) tensor(9966.8672, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9966.8662109375
tensor(9966.8672, grad_fn=<NegBackward0>) tensor(9966.8662, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9966.8671875
tensor(9966.8662, grad_fn=<NegBackward0>) tensor(9966.8672, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9966.8662109375
tensor(9966.8662, grad_fn=<NegBackward0>) tensor(9966.8662, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9966.8662109375
tensor(9966.8662, grad_fn=<NegBackward0>) tensor(9966.8662, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9966.865234375
tensor(9966.8662, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9966.8681640625
tensor(9966.8652, grad_fn=<NegBackward0>) tensor(9966.8682, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9966.865234375
tensor(9966.8652, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9966.865234375
tensor(9966.8652, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9966.8642578125
tensor(9966.8652, grad_fn=<NegBackward0>) tensor(9966.8643, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9966.865234375
tensor(9966.8643, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9966.865234375
tensor(9966.8643, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9966.8662109375
tensor(9966.8643, grad_fn=<NegBackward0>) tensor(9966.8662, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9966.865234375
tensor(9966.8643, grad_fn=<NegBackward0>) tensor(9966.8652, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9966.8662109375
tensor(9966.8643, grad_fn=<NegBackward0>) tensor(9966.8662, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.7130, 0.2870],
        [0.1257, 0.8743]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9908, 0.0092], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1272, 0.0951],
         [0.5780, 0.2091]],

        [[0.6113, 0.1383],
         [0.5097, 0.5173]],

        [[0.7298, 0.1189],
         [0.6379, 0.6289]],

        [[0.6775, 0.1143],
         [0.7289, 0.6890]],

        [[0.6183, 0.0967],
         [0.6649, 0.6319]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 67
Adjusted Rand Index: 0.10795050437358514
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 86
Adjusted Rand Index: 0.5135353535353535
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 89
Adjusted Rand Index: 0.6044273961975466
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 88
Adjusted Rand Index: 0.5733034502079607
Global Adjusted Rand Index: 0.26878225444588255
Average Adjusted Rand Index: 0.35984334086288916
[0.2729497806939191, 0.26878225444588255] [0.35996079368853884, 0.35984334086288916] [9966.904296875, 9966.8662109375]
-------------------------------------
This iteration is 56
True Objective function: Loss = -9810.287838403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21934.71484375
inf tensor(21934.7148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9687.8525390625
tensor(21934.7148, grad_fn=<NegBackward0>) tensor(9687.8525, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9682.9267578125
tensor(9687.8525, grad_fn=<NegBackward0>) tensor(9682.9268, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9681.51171875
tensor(9682.9268, grad_fn=<NegBackward0>) tensor(9681.5117, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9680.6025390625
tensor(9681.5117, grad_fn=<NegBackward0>) tensor(9680.6025, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9679.947265625
tensor(9680.6025, grad_fn=<NegBackward0>) tensor(9679.9473, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9679.56640625
tensor(9679.9473, grad_fn=<NegBackward0>) tensor(9679.5664, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9679.3671875
tensor(9679.5664, grad_fn=<NegBackward0>) tensor(9679.3672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9679.234375
tensor(9679.3672, grad_fn=<NegBackward0>) tensor(9679.2344, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9679.1357421875
tensor(9679.2344, grad_fn=<NegBackward0>) tensor(9679.1357, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9679.04296875
tensor(9679.1357, grad_fn=<NegBackward0>) tensor(9679.0430, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9678.9697265625
tensor(9679.0430, grad_fn=<NegBackward0>) tensor(9678.9697, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9678.908203125
tensor(9678.9697, grad_fn=<NegBackward0>) tensor(9678.9082, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9678.822265625
tensor(9678.9082, grad_fn=<NegBackward0>) tensor(9678.8223, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9678.49609375
tensor(9678.8223, grad_fn=<NegBackward0>) tensor(9678.4961, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9677.576171875
tensor(9678.4961, grad_fn=<NegBackward0>) tensor(9677.5762, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9676.947265625
tensor(9677.5762, grad_fn=<NegBackward0>) tensor(9676.9473, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9676.669921875
tensor(9676.9473, grad_fn=<NegBackward0>) tensor(9676.6699, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9676.5107421875
tensor(9676.6699, grad_fn=<NegBackward0>) tensor(9676.5107, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9676.40625
tensor(9676.5107, grad_fn=<NegBackward0>) tensor(9676.4062, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9676.328125
tensor(9676.4062, grad_fn=<NegBackward0>) tensor(9676.3281, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9676.263671875
tensor(9676.3281, grad_fn=<NegBackward0>) tensor(9676.2637, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9676.2001953125
tensor(9676.2637, grad_fn=<NegBackward0>) tensor(9676.2002, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9676.12109375
tensor(9676.2002, grad_fn=<NegBackward0>) tensor(9676.1211, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9676.00390625
tensor(9676.1211, grad_fn=<NegBackward0>) tensor(9676.0039, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9675.8681640625
tensor(9676.0039, grad_fn=<NegBackward0>) tensor(9675.8682, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9675.7578125
tensor(9675.8682, grad_fn=<NegBackward0>) tensor(9675.7578, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9675.7021484375
tensor(9675.7578, grad_fn=<NegBackward0>) tensor(9675.7021, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9675.6689453125
tensor(9675.7021, grad_fn=<NegBackward0>) tensor(9675.6689, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9675.6474609375
tensor(9675.6689, grad_fn=<NegBackward0>) tensor(9675.6475, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9675.62890625
tensor(9675.6475, grad_fn=<NegBackward0>) tensor(9675.6289, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9675.6142578125
tensor(9675.6289, grad_fn=<NegBackward0>) tensor(9675.6143, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9675.59375
tensor(9675.6143, grad_fn=<NegBackward0>) tensor(9675.5938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9675.5849609375
tensor(9675.5938, grad_fn=<NegBackward0>) tensor(9675.5850, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9675.576171875
tensor(9675.5850, grad_fn=<NegBackward0>) tensor(9675.5762, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9675.5693359375
tensor(9675.5762, grad_fn=<NegBackward0>) tensor(9675.5693, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9675.5615234375
tensor(9675.5693, grad_fn=<NegBackward0>) tensor(9675.5615, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9675.5556640625
tensor(9675.5615, grad_fn=<NegBackward0>) tensor(9675.5557, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9675.5478515625
tensor(9675.5557, grad_fn=<NegBackward0>) tensor(9675.5479, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9675.5439453125
tensor(9675.5479, grad_fn=<NegBackward0>) tensor(9675.5439, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9675.5390625
tensor(9675.5439, grad_fn=<NegBackward0>) tensor(9675.5391, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9675.53515625
tensor(9675.5391, grad_fn=<NegBackward0>) tensor(9675.5352, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9675.5341796875
tensor(9675.5352, grad_fn=<NegBackward0>) tensor(9675.5342, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9675.5322265625
tensor(9675.5342, grad_fn=<NegBackward0>) tensor(9675.5322, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9675.5283203125
tensor(9675.5322, grad_fn=<NegBackward0>) tensor(9675.5283, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9675.5263671875
tensor(9675.5283, grad_fn=<NegBackward0>) tensor(9675.5264, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9675.5244140625
tensor(9675.5264, grad_fn=<NegBackward0>) tensor(9675.5244, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9675.521484375
tensor(9675.5244, grad_fn=<NegBackward0>) tensor(9675.5215, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9675.5205078125
tensor(9675.5215, grad_fn=<NegBackward0>) tensor(9675.5205, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9675.517578125
tensor(9675.5205, grad_fn=<NegBackward0>) tensor(9675.5176, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9675.5166015625
tensor(9675.5176, grad_fn=<NegBackward0>) tensor(9675.5166, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9675.5146484375
tensor(9675.5166, grad_fn=<NegBackward0>) tensor(9675.5146, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9675.5126953125
tensor(9675.5146, grad_fn=<NegBackward0>) tensor(9675.5127, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9675.5126953125
tensor(9675.5127, grad_fn=<NegBackward0>) tensor(9675.5127, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9675.5107421875
tensor(9675.5127, grad_fn=<NegBackward0>) tensor(9675.5107, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9675.5107421875
tensor(9675.5107, grad_fn=<NegBackward0>) tensor(9675.5107, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9675.509765625
tensor(9675.5107, grad_fn=<NegBackward0>) tensor(9675.5098, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9675.5068359375
tensor(9675.5098, grad_fn=<NegBackward0>) tensor(9675.5068, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9675.5078125
tensor(9675.5068, grad_fn=<NegBackward0>) tensor(9675.5078, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9675.5068359375
tensor(9675.5068, grad_fn=<NegBackward0>) tensor(9675.5068, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9675.505859375
tensor(9675.5068, grad_fn=<NegBackward0>) tensor(9675.5059, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9675.50390625
tensor(9675.5059, grad_fn=<NegBackward0>) tensor(9675.5039, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9675.50390625
tensor(9675.5039, grad_fn=<NegBackward0>) tensor(9675.5039, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9675.5029296875
tensor(9675.5039, grad_fn=<NegBackward0>) tensor(9675.5029, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9675.50390625
tensor(9675.5029, grad_fn=<NegBackward0>) tensor(9675.5039, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9675.5009765625
tensor(9675.5029, grad_fn=<NegBackward0>) tensor(9675.5010, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9675.5
tensor(9675.5010, grad_fn=<NegBackward0>) tensor(9675.5000, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9675.501953125
tensor(9675.5000, grad_fn=<NegBackward0>) tensor(9675.5020, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9675.4990234375
tensor(9675.5000, grad_fn=<NegBackward0>) tensor(9675.4990, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9675.5
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.5000, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9675.4990234375
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.4990, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9675.4990234375
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.4990, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9675.498046875
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9675.4970703125
tensor(9675.4980, grad_fn=<NegBackward0>) tensor(9675.4971, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9675.498046875
tensor(9675.4971, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9675.49609375
tensor(9675.4971, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9675.4951171875
tensor(9675.4961, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9675.4951171875
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9675.4951171875
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9675.49609375
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9675.4951171875
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9675.494140625
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9675.4951171875
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9675.501953125
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.5020, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9675.498046875
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9675.4931640625
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9675.4931640625
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9675.4931640625
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9675.4921875
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9675.4931640625
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9675.494140625
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -9675.4931640625
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -9675.4921875
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9675.4921875
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9675.4921875
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9675.4931640625
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9675.4921875
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9675.4921875
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9675.4912109375
tensor(9675.4922, grad_fn=<NegBackward0>) tensor(9675.4912, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9675.4921875
tensor(9675.4912, grad_fn=<NegBackward0>) tensor(9675.4922, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9965e-01, 3.5066e-04],
        [1.5031e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0300, 0.9700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3820, 0.1551],
         [0.5292, 0.1304]],

        [[0.6977, 0.1424],
         [0.6208, 0.5772]],

        [[0.6605, 0.1777],
         [0.5858, 0.6265]],

        [[0.7285, 0.2640],
         [0.6536, 0.6070]],

        [[0.5520, 0.1016],
         [0.6965, 0.6780]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.014344415049389271
Global Adjusted Rand Index: 0.009113030634939328
Average Adjusted Rand Index: 0.00755223359374541
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20844.09375
inf tensor(20844.0938, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9688.71875
tensor(20844.0938, grad_fn=<NegBackward0>) tensor(9688.7188, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9682.1572265625
tensor(9688.7188, grad_fn=<NegBackward0>) tensor(9682.1572, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9680.3427734375
tensor(9682.1572, grad_fn=<NegBackward0>) tensor(9680.3428, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9679.7626953125
tensor(9680.3428, grad_fn=<NegBackward0>) tensor(9679.7627, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9679.40625
tensor(9679.7627, grad_fn=<NegBackward0>) tensor(9679.4062, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9679.220703125
tensor(9679.4062, grad_fn=<NegBackward0>) tensor(9679.2207, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9679.08984375
tensor(9679.2207, grad_fn=<NegBackward0>) tensor(9679.0898, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9679.00390625
tensor(9679.0898, grad_fn=<NegBackward0>) tensor(9679.0039, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9678.9453125
tensor(9679.0039, grad_fn=<NegBackward0>) tensor(9678.9453, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9678.8896484375
tensor(9678.9453, grad_fn=<NegBackward0>) tensor(9678.8896, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9678.818359375
tensor(9678.8896, grad_fn=<NegBackward0>) tensor(9678.8184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9678.6328125
tensor(9678.8184, grad_fn=<NegBackward0>) tensor(9678.6328, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9677.7646484375
tensor(9678.6328, grad_fn=<NegBackward0>) tensor(9677.7646, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9676.9658203125
tensor(9677.7646, grad_fn=<NegBackward0>) tensor(9676.9658, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9676.6572265625
tensor(9676.9658, grad_fn=<NegBackward0>) tensor(9676.6572, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9676.51171875
tensor(9676.6572, grad_fn=<NegBackward0>) tensor(9676.5117, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9676.4169921875
tensor(9676.5117, grad_fn=<NegBackward0>) tensor(9676.4170, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9676.3486328125
tensor(9676.4170, grad_fn=<NegBackward0>) tensor(9676.3486, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9676.2890625
tensor(9676.3486, grad_fn=<NegBackward0>) tensor(9676.2891, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9676.2373046875
tensor(9676.2891, grad_fn=<NegBackward0>) tensor(9676.2373, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9676.189453125
tensor(9676.2373, grad_fn=<NegBackward0>) tensor(9676.1895, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9676.1337890625
tensor(9676.1895, grad_fn=<NegBackward0>) tensor(9676.1338, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9676.041015625
tensor(9676.1338, grad_fn=<NegBackward0>) tensor(9676.0410, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9675.8984375
tensor(9676.0410, grad_fn=<NegBackward0>) tensor(9675.8984, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9675.7861328125
tensor(9675.8984, grad_fn=<NegBackward0>) tensor(9675.7861, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9675.72265625
tensor(9675.7861, grad_fn=<NegBackward0>) tensor(9675.7227, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9675.6435546875
tensor(9675.7227, grad_fn=<NegBackward0>) tensor(9675.6436, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9675.62109375
tensor(9675.6436, grad_fn=<NegBackward0>) tensor(9675.6211, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9675.6064453125
tensor(9675.6211, grad_fn=<NegBackward0>) tensor(9675.6064, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9675.5927734375
tensor(9675.6064, grad_fn=<NegBackward0>) tensor(9675.5928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9675.5830078125
tensor(9675.5928, grad_fn=<NegBackward0>) tensor(9675.5830, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9675.576171875
tensor(9675.5830, grad_fn=<NegBackward0>) tensor(9675.5762, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9675.5625
tensor(9675.5762, grad_fn=<NegBackward0>) tensor(9675.5625, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9675.5517578125
tensor(9675.5625, grad_fn=<NegBackward0>) tensor(9675.5518, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9675.5478515625
tensor(9675.5518, grad_fn=<NegBackward0>) tensor(9675.5479, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9675.5419921875
tensor(9675.5479, grad_fn=<NegBackward0>) tensor(9675.5420, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9675.5400390625
tensor(9675.5420, grad_fn=<NegBackward0>) tensor(9675.5400, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9675.5361328125
tensor(9675.5400, grad_fn=<NegBackward0>) tensor(9675.5361, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9675.533203125
tensor(9675.5361, grad_fn=<NegBackward0>) tensor(9675.5332, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9675.5302734375
tensor(9675.5332, grad_fn=<NegBackward0>) tensor(9675.5303, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9675.5283203125
tensor(9675.5303, grad_fn=<NegBackward0>) tensor(9675.5283, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9675.5263671875
tensor(9675.5283, grad_fn=<NegBackward0>) tensor(9675.5264, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9675.5234375
tensor(9675.5264, grad_fn=<NegBackward0>) tensor(9675.5234, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9675.521484375
tensor(9675.5234, grad_fn=<NegBackward0>) tensor(9675.5215, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9675.5205078125
tensor(9675.5215, grad_fn=<NegBackward0>) tensor(9675.5205, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9675.517578125
tensor(9675.5205, grad_fn=<NegBackward0>) tensor(9675.5176, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9675.5166015625
tensor(9675.5176, grad_fn=<NegBackward0>) tensor(9675.5166, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9675.51171875
tensor(9675.5166, grad_fn=<NegBackward0>) tensor(9675.5117, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9675.5107421875
tensor(9675.5117, grad_fn=<NegBackward0>) tensor(9675.5107, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9675.5087890625
tensor(9675.5107, grad_fn=<NegBackward0>) tensor(9675.5088, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9675.509765625
tensor(9675.5088, grad_fn=<NegBackward0>) tensor(9675.5098, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9675.5068359375
tensor(9675.5088, grad_fn=<NegBackward0>) tensor(9675.5068, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9675.5068359375
tensor(9675.5068, grad_fn=<NegBackward0>) tensor(9675.5068, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9675.5048828125
tensor(9675.5068, grad_fn=<NegBackward0>) tensor(9675.5049, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9675.5048828125
tensor(9675.5049, grad_fn=<NegBackward0>) tensor(9675.5049, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9675.50390625
tensor(9675.5049, grad_fn=<NegBackward0>) tensor(9675.5039, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9675.50390625
tensor(9675.5039, grad_fn=<NegBackward0>) tensor(9675.5039, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9675.501953125
tensor(9675.5039, grad_fn=<NegBackward0>) tensor(9675.5020, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9675.501953125
tensor(9675.5020, grad_fn=<NegBackward0>) tensor(9675.5020, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9675.501953125
tensor(9675.5020, grad_fn=<NegBackward0>) tensor(9675.5020, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9675.5009765625
tensor(9675.5020, grad_fn=<NegBackward0>) tensor(9675.5010, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9675.5009765625
tensor(9675.5010, grad_fn=<NegBackward0>) tensor(9675.5010, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9675.5
tensor(9675.5010, grad_fn=<NegBackward0>) tensor(9675.5000, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9675.4990234375
tensor(9675.5000, grad_fn=<NegBackward0>) tensor(9675.4990, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9675.5
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.5000, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9675.498046875
tensor(9675.4990, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9675.498046875
tensor(9675.4980, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9675.4970703125
tensor(9675.4980, grad_fn=<NegBackward0>) tensor(9675.4971, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9675.498046875
tensor(9675.4971, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9675.498046875
tensor(9675.4971, grad_fn=<NegBackward0>) tensor(9675.4980, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9675.49609375
tensor(9675.4971, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9675.4970703125
tensor(9675.4961, grad_fn=<NegBackward0>) tensor(9675.4971, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9675.49609375
tensor(9675.4961, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9675.49609375
tensor(9675.4961, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9675.4951171875
tensor(9675.4961, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9675.4951171875
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9675.494140625
tensor(9675.4951, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9675.494140625
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9675.4951171875
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9675.4951171875
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4951, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9675.49609375
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4961, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -9675.494140625
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9675.4931640625
tensor(9675.4941, grad_fn=<NegBackward0>) tensor(9675.4932, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9675.494140625
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9675.5205078125
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.5205, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9675.494140625
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9675.5029296875
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.5029, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9675.494140625
tensor(9675.4932, grad_fn=<NegBackward0>) tensor(9675.4941, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[9.9963e-01, 3.6951e-04],
        [2.4277e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0300, 0.9700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3824, 0.1551],
         [0.5976, 0.1303]],

        [[0.5869, 0.1424],
         [0.7071, 0.6976]],

        [[0.6568, 0.1777],
         [0.5889, 0.7041]],

        [[0.5889, 0.2641],
         [0.5829, 0.6088]],

        [[0.5830, 0.1016],
         [0.6531, 0.5011]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.014344415049389271
Global Adjusted Rand Index: 0.009113030634939328
Average Adjusted Rand Index: 0.00755223359374541
[0.009113030634939328, 0.009113030634939328] [0.00755223359374541, 0.00755223359374541] [9675.4912109375, 9675.494140625]
-------------------------------------
This iteration is 57
True Objective function: Loss = -9920.716405674195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23043.490234375
inf tensor(23043.4902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9780.23828125
tensor(23043.4902, grad_fn=<NegBackward0>) tensor(9780.2383, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9779.0439453125
tensor(9780.2383, grad_fn=<NegBackward0>) tensor(9779.0439, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9778.8564453125
tensor(9779.0439, grad_fn=<NegBackward0>) tensor(9778.8564, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9778.7158203125
tensor(9778.8564, grad_fn=<NegBackward0>) tensor(9778.7158, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9778.5859375
tensor(9778.7158, grad_fn=<NegBackward0>) tensor(9778.5859, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9778.4443359375
tensor(9778.5859, grad_fn=<NegBackward0>) tensor(9778.4443, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9778.107421875
tensor(9778.4443, grad_fn=<NegBackward0>) tensor(9778.1074, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9777.5751953125
tensor(9778.1074, grad_fn=<NegBackward0>) tensor(9777.5752, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9776.1181640625
tensor(9777.5752, grad_fn=<NegBackward0>) tensor(9776.1182, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9775.4296875
tensor(9776.1182, grad_fn=<NegBackward0>) tensor(9775.4297, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9774.86328125
tensor(9775.4297, grad_fn=<NegBackward0>) tensor(9774.8633, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9774.5595703125
tensor(9774.8633, grad_fn=<NegBackward0>) tensor(9774.5596, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9774.4052734375
tensor(9774.5596, grad_fn=<NegBackward0>) tensor(9774.4053, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9774.3623046875
tensor(9774.4053, grad_fn=<NegBackward0>) tensor(9774.3623, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9774.349609375
tensor(9774.3623, grad_fn=<NegBackward0>) tensor(9774.3496, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9774.345703125
tensor(9774.3496, grad_fn=<NegBackward0>) tensor(9774.3457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9774.345703125
tensor(9774.3457, grad_fn=<NegBackward0>) tensor(9774.3457, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9774.345703125
tensor(9774.3457, grad_fn=<NegBackward0>) tensor(9774.3457, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9774.34765625
tensor(9774.3457, grad_fn=<NegBackward0>) tensor(9774.3477, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -9774.34375
tensor(9774.3457, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9774.34375
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9774.34375
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9774.3447265625
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -9774.3447265625
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
2
Iteration 2500: Loss = -9774.3447265625
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
3
Iteration 2600: Loss = -9774.34375
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9774.3427734375
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3428, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9774.3447265625
tensor(9774.3428, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -9774.3447265625
tensor(9774.3428, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
2
Iteration 3000: Loss = -9774.3447265625
tensor(9774.3428, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
3
Iteration 3100: Loss = -9774.34375
tensor(9774.3428, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
4
Iteration 3200: Loss = -9774.3447265625
tensor(9774.3428, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3200 due to no improvement.
pi: tensor([[0.5386, 0.4614],
        [0.1333, 0.8667]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3068, 0.6932], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2037, 0.1450],
         [0.6727, 0.1171]],

        [[0.5325, 0.1516],
         [0.6713, 0.6259]],

        [[0.6586, 0.1562],
         [0.6942, 0.5517]],

        [[0.6485, 0.1449],
         [0.6361, 0.5495]],

        [[0.7028, 0.1471],
         [0.6812, 0.5031]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.022626262626262626
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.008486976880304361
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.0496599956582707
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.04403151370505218
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.006156552330694811
Global Adjusted Rand Index: 0.02436037223189212
Average Adjusted Rand Index: 0.023729639307839008
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23771.80078125
inf tensor(23771.8008, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9782.396484375
tensor(23771.8008, grad_fn=<NegBackward0>) tensor(9782.3965, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9780.521484375
tensor(9782.3965, grad_fn=<NegBackward0>) tensor(9780.5215, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9779.8955078125
tensor(9780.5215, grad_fn=<NegBackward0>) tensor(9779.8955, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9779.373046875
tensor(9779.8955, grad_fn=<NegBackward0>) tensor(9779.3730, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9779.087890625
tensor(9779.3730, grad_fn=<NegBackward0>) tensor(9779.0879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9778.9052734375
tensor(9779.0879, grad_fn=<NegBackward0>) tensor(9778.9053, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9778.76953125
tensor(9778.9053, grad_fn=<NegBackward0>) tensor(9778.7695, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9778.6748046875
tensor(9778.7695, grad_fn=<NegBackward0>) tensor(9778.6748, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9778.6044921875
tensor(9778.6748, grad_fn=<NegBackward0>) tensor(9778.6045, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9778.5458984375
tensor(9778.6045, grad_fn=<NegBackward0>) tensor(9778.5459, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9778.48828125
tensor(9778.5459, grad_fn=<NegBackward0>) tensor(9778.4883, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9778.4208984375
tensor(9778.4883, grad_fn=<NegBackward0>) tensor(9778.4209, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9778.3447265625
tensor(9778.4209, grad_fn=<NegBackward0>) tensor(9778.3447, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9778.26953125
tensor(9778.3447, grad_fn=<NegBackward0>) tensor(9778.2695, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9778.1962890625
tensor(9778.2695, grad_fn=<NegBackward0>) tensor(9778.1963, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9778.115234375
tensor(9778.1963, grad_fn=<NegBackward0>) tensor(9778.1152, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9777.8955078125
tensor(9778.1152, grad_fn=<NegBackward0>) tensor(9777.8955, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9776.9052734375
tensor(9777.8955, grad_fn=<NegBackward0>) tensor(9776.9053, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9776.4609375
tensor(9776.9053, grad_fn=<NegBackward0>) tensor(9776.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9776.232421875
tensor(9776.4609, grad_fn=<NegBackward0>) tensor(9776.2324, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9776.0888671875
tensor(9776.2324, grad_fn=<NegBackward0>) tensor(9776.0889, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9776.0029296875
tensor(9776.0889, grad_fn=<NegBackward0>) tensor(9776.0029, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9775.9443359375
tensor(9776.0029, grad_fn=<NegBackward0>) tensor(9775.9443, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9775.90234375
tensor(9775.9443, grad_fn=<NegBackward0>) tensor(9775.9023, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9775.8740234375
tensor(9775.9023, grad_fn=<NegBackward0>) tensor(9775.8740, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9775.8486328125
tensor(9775.8740, grad_fn=<NegBackward0>) tensor(9775.8486, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9775.828125
tensor(9775.8486, grad_fn=<NegBackward0>) tensor(9775.8281, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9775.78515625
tensor(9775.8281, grad_fn=<NegBackward0>) tensor(9775.7852, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9775.689453125
tensor(9775.7852, grad_fn=<NegBackward0>) tensor(9775.6895, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9775.5888671875
tensor(9775.6895, grad_fn=<NegBackward0>) tensor(9775.5889, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9775.478515625
tensor(9775.5889, grad_fn=<NegBackward0>) tensor(9775.4785, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9775.365234375
tensor(9775.4785, grad_fn=<NegBackward0>) tensor(9775.3652, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9775.2548828125
tensor(9775.3652, grad_fn=<NegBackward0>) tensor(9775.2549, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9775.15234375
tensor(9775.2549, grad_fn=<NegBackward0>) tensor(9775.1523, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9775.0576171875
tensor(9775.1523, grad_fn=<NegBackward0>) tensor(9775.0576, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9774.9736328125
tensor(9775.0576, grad_fn=<NegBackward0>) tensor(9774.9736, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9774.900390625
tensor(9774.9736, grad_fn=<NegBackward0>) tensor(9774.9004, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9774.8349609375
tensor(9774.9004, grad_fn=<NegBackward0>) tensor(9774.8350, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9774.7783203125
tensor(9774.8350, grad_fn=<NegBackward0>) tensor(9774.7783, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9774.728515625
tensor(9774.7783, grad_fn=<NegBackward0>) tensor(9774.7285, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9774.6875
tensor(9774.7285, grad_fn=<NegBackward0>) tensor(9774.6875, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9774.6494140625
tensor(9774.6875, grad_fn=<NegBackward0>) tensor(9774.6494, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9774.6162109375
tensor(9774.6494, grad_fn=<NegBackward0>) tensor(9774.6162, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9774.587890625
tensor(9774.6162, grad_fn=<NegBackward0>) tensor(9774.5879, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9774.5625
tensor(9774.5879, grad_fn=<NegBackward0>) tensor(9774.5625, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9774.541015625
tensor(9774.5625, grad_fn=<NegBackward0>) tensor(9774.5410, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9774.51953125
tensor(9774.5410, grad_fn=<NegBackward0>) tensor(9774.5195, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9774.50390625
tensor(9774.5195, grad_fn=<NegBackward0>) tensor(9774.5039, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9774.48828125
tensor(9774.5039, grad_fn=<NegBackward0>) tensor(9774.4883, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9774.47265625
tensor(9774.4883, grad_fn=<NegBackward0>) tensor(9774.4727, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9774.458984375
tensor(9774.4727, grad_fn=<NegBackward0>) tensor(9774.4590, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9774.44921875
tensor(9774.4590, grad_fn=<NegBackward0>) tensor(9774.4492, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9774.4375
tensor(9774.4492, grad_fn=<NegBackward0>) tensor(9774.4375, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9774.4296875
tensor(9774.4375, grad_fn=<NegBackward0>) tensor(9774.4297, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9774.4208984375
tensor(9774.4297, grad_fn=<NegBackward0>) tensor(9774.4209, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9774.4140625
tensor(9774.4209, grad_fn=<NegBackward0>) tensor(9774.4141, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9774.4072265625
tensor(9774.4141, grad_fn=<NegBackward0>) tensor(9774.4072, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9774.3994140625
tensor(9774.4072, grad_fn=<NegBackward0>) tensor(9774.3994, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9774.3935546875
tensor(9774.3994, grad_fn=<NegBackward0>) tensor(9774.3936, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9774.38671875
tensor(9774.3936, grad_fn=<NegBackward0>) tensor(9774.3867, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9774.3837890625
tensor(9774.3867, grad_fn=<NegBackward0>) tensor(9774.3838, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9774.3779296875
tensor(9774.3838, grad_fn=<NegBackward0>) tensor(9774.3779, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9774.373046875
tensor(9774.3779, grad_fn=<NegBackward0>) tensor(9774.3730, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9774.3701171875
tensor(9774.3730, grad_fn=<NegBackward0>) tensor(9774.3701, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9774.3681640625
tensor(9774.3701, grad_fn=<NegBackward0>) tensor(9774.3682, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9774.36328125
tensor(9774.3682, grad_fn=<NegBackward0>) tensor(9774.3633, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9774.361328125
tensor(9774.3633, grad_fn=<NegBackward0>) tensor(9774.3613, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9774.357421875
tensor(9774.3613, grad_fn=<NegBackward0>) tensor(9774.3574, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9774.3544921875
tensor(9774.3574, grad_fn=<NegBackward0>) tensor(9774.3545, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9774.353515625
tensor(9774.3545, grad_fn=<NegBackward0>) tensor(9774.3535, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9774.349609375
tensor(9774.3535, grad_fn=<NegBackward0>) tensor(9774.3496, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9774.3486328125
tensor(9774.3496, grad_fn=<NegBackward0>) tensor(9774.3486, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9774.3466796875
tensor(9774.3486, grad_fn=<NegBackward0>) tensor(9774.3467, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9774.345703125
tensor(9774.3467, grad_fn=<NegBackward0>) tensor(9774.3457, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9774.34375
tensor(9774.3457, grad_fn=<NegBackward0>) tensor(9774.3438, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9774.341796875
tensor(9774.3438, grad_fn=<NegBackward0>) tensor(9774.3418, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9774.3388671875
tensor(9774.3418, grad_fn=<NegBackward0>) tensor(9774.3389, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9774.337890625
tensor(9774.3389, grad_fn=<NegBackward0>) tensor(9774.3379, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9774.3369140625
tensor(9774.3379, grad_fn=<NegBackward0>) tensor(9774.3369, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9774.3349609375
tensor(9774.3369, grad_fn=<NegBackward0>) tensor(9774.3350, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9774.3330078125
tensor(9774.3350, grad_fn=<NegBackward0>) tensor(9774.3330, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9774.33203125
tensor(9774.3330, grad_fn=<NegBackward0>) tensor(9774.3320, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9774.33203125
tensor(9774.3320, grad_fn=<NegBackward0>) tensor(9774.3320, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9774.3427734375
tensor(9774.3320, grad_fn=<NegBackward0>) tensor(9774.3428, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9774.330078125
tensor(9774.3320, grad_fn=<NegBackward0>) tensor(9774.3301, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9774.3466796875
tensor(9774.3301, grad_fn=<NegBackward0>) tensor(9774.3467, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9774.3271484375
tensor(9774.3301, grad_fn=<NegBackward0>) tensor(9774.3271, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9774.3291015625
tensor(9774.3271, grad_fn=<NegBackward0>) tensor(9774.3291, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9774.328125
tensor(9774.3271, grad_fn=<NegBackward0>) tensor(9774.3281, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -9774.3271484375
tensor(9774.3271, grad_fn=<NegBackward0>) tensor(9774.3271, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9774.3271484375
tensor(9774.3271, grad_fn=<NegBackward0>) tensor(9774.3271, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9774.326171875
tensor(9774.3271, grad_fn=<NegBackward0>) tensor(9774.3262, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9774.3251953125
tensor(9774.3262, grad_fn=<NegBackward0>) tensor(9774.3252, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9774.32421875
tensor(9774.3252, grad_fn=<NegBackward0>) tensor(9774.3242, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9774.333984375
tensor(9774.3242, grad_fn=<NegBackward0>) tensor(9774.3340, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9774.3251953125
tensor(9774.3242, grad_fn=<NegBackward0>) tensor(9774.3252, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -9774.32421875
tensor(9774.3242, grad_fn=<NegBackward0>) tensor(9774.3242, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9774.5986328125
tensor(9774.3242, grad_fn=<NegBackward0>) tensor(9774.5986, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9774.322265625
tensor(9774.3242, grad_fn=<NegBackward0>) tensor(9774.3223, grad_fn=<NegBackward0>)
pi: tensor([[8.3275e-01, 1.6725e-01],
        [6.9525e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9951, 0.1905],
         [0.5475, 0.1339]],

        [[0.5558, 0.0663],
         [0.6068, 0.5874]],

        [[0.6238, 0.1592],
         [0.6371, 0.6188]],

        [[0.6657, 0.1615],
         [0.5453, 0.6781]],

        [[0.6224, 0.2424],
         [0.5008, 0.6427]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.00048520659786272343
Average Adjusted Rand Index: -0.00017049731843534407
[0.02436037223189212, -0.00048520659786272343] [0.023729639307839008, -0.00017049731843534407] [9774.3447265625, 9774.3232421875]
-------------------------------------
This iteration is 58
True Objective function: Loss = -10018.323622310876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21719.263671875
inf tensor(21719.2637, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9912.7470703125
tensor(21719.2637, grad_fn=<NegBackward0>) tensor(9912.7471, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9911.26953125
tensor(9912.7471, grad_fn=<NegBackward0>) tensor(9911.2695, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9910.70703125
tensor(9911.2695, grad_fn=<NegBackward0>) tensor(9910.7070, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9910.25390625
tensor(9910.7070, grad_fn=<NegBackward0>) tensor(9910.2539, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9909.9423828125
tensor(9910.2539, grad_fn=<NegBackward0>) tensor(9909.9424, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9909.6279296875
tensor(9909.9424, grad_fn=<NegBackward0>) tensor(9909.6279, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9909.263671875
tensor(9909.6279, grad_fn=<NegBackward0>) tensor(9909.2637, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9908.8427734375
tensor(9909.2637, grad_fn=<NegBackward0>) tensor(9908.8428, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9908.4736328125
tensor(9908.8428, grad_fn=<NegBackward0>) tensor(9908.4736, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9908.18359375
tensor(9908.4736, grad_fn=<NegBackward0>) tensor(9908.1836, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9907.818359375
tensor(9908.1836, grad_fn=<NegBackward0>) tensor(9907.8184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9906.8232421875
tensor(9907.8184, grad_fn=<NegBackward0>) tensor(9906.8232, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9906.4619140625
tensor(9906.8232, grad_fn=<NegBackward0>) tensor(9906.4619, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9906.2919921875
tensor(9906.4619, grad_fn=<NegBackward0>) tensor(9906.2920, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9906.18359375
tensor(9906.2920, grad_fn=<NegBackward0>) tensor(9906.1836, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9906.1123046875
tensor(9906.1836, grad_fn=<NegBackward0>) tensor(9906.1123, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9906.056640625
tensor(9906.1123, grad_fn=<NegBackward0>) tensor(9906.0566, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9906.0107421875
tensor(9906.0566, grad_fn=<NegBackward0>) tensor(9906.0107, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9905.97265625
tensor(9906.0107, grad_fn=<NegBackward0>) tensor(9905.9727, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9905.9375
tensor(9905.9727, grad_fn=<NegBackward0>) tensor(9905.9375, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9905.9052734375
tensor(9905.9375, grad_fn=<NegBackward0>) tensor(9905.9053, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9905.8779296875
tensor(9905.9053, grad_fn=<NegBackward0>) tensor(9905.8779, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9905.853515625
tensor(9905.8779, grad_fn=<NegBackward0>) tensor(9905.8535, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9905.826171875
tensor(9905.8535, grad_fn=<NegBackward0>) tensor(9905.8262, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9905.791015625
tensor(9905.8262, grad_fn=<NegBackward0>) tensor(9905.7910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9905.7294921875
tensor(9905.7910, grad_fn=<NegBackward0>) tensor(9905.7295, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9905.5888671875
tensor(9905.7295, grad_fn=<NegBackward0>) tensor(9905.5889, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9905.09765625
tensor(9905.5889, grad_fn=<NegBackward0>) tensor(9905.0977, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9904.0712890625
tensor(9905.0977, grad_fn=<NegBackward0>) tensor(9904.0713, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9903.59765625
tensor(9904.0713, grad_fn=<NegBackward0>) tensor(9903.5977, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9902.3115234375
tensor(9903.5977, grad_fn=<NegBackward0>) tensor(9902.3115, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9901.6298828125
tensor(9902.3115, grad_fn=<NegBackward0>) tensor(9901.6299, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9901.607421875
tensor(9901.6299, grad_fn=<NegBackward0>) tensor(9901.6074, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9901.603515625
tensor(9901.6074, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9901.6044921875
tensor(9901.6035, grad_fn=<NegBackward0>) tensor(9901.6045, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9901.6025390625
tensor(9901.6035, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9901.6015625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6016, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9901.6064453125
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6064, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -9901.603515625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -9901.6015625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6016, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9901.603515625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9901.619140625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6191, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9901.6015625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6016, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -9901.603515625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -9901.6103515625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.9167, 0.0833],
        [0.5664, 0.4336]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3676, 0.6324], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1259, 0.0927],
         [0.5037, 0.1947]],

        [[0.6975, 0.1491],
         [0.7116, 0.5914]],

        [[0.6892, 0.1732],
         [0.6451, 0.6358]],

        [[0.7279, 0.1325],
         [0.6913, 0.5488]],

        [[0.5561, 0.1578],
         [0.5450, 0.6984]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 71
Adjusted Rand Index: 0.1692763855744965
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.022723709951211306
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.04814522816701128
Average Adjusted Rand Index: 0.16256318846555803
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24795.953125
inf tensor(24795.9531, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9913.3115234375
tensor(24795.9531, grad_fn=<NegBackward0>) tensor(9913.3115, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9911.6318359375
tensor(9913.3115, grad_fn=<NegBackward0>) tensor(9911.6318, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9911.0751953125
tensor(9911.6318, grad_fn=<NegBackward0>) tensor(9911.0752, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9910.703125
tensor(9911.0752, grad_fn=<NegBackward0>) tensor(9910.7031, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9910.431640625
tensor(9910.7031, grad_fn=<NegBackward0>) tensor(9910.4316, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9910.0712890625
tensor(9910.4316, grad_fn=<NegBackward0>) tensor(9910.0713, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9909.525390625
tensor(9910.0713, grad_fn=<NegBackward0>) tensor(9909.5254, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9908.876953125
tensor(9909.5254, grad_fn=<NegBackward0>) tensor(9908.8770, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9908.0830078125
tensor(9908.8770, grad_fn=<NegBackward0>) tensor(9908.0830, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9907.369140625
tensor(9908.0830, grad_fn=<NegBackward0>) tensor(9907.3691, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9906.8642578125
tensor(9907.3691, grad_fn=<NegBackward0>) tensor(9906.8643, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9906.5126953125
tensor(9906.8643, grad_fn=<NegBackward0>) tensor(9906.5127, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9906.2890625
tensor(9906.5127, grad_fn=<NegBackward0>) tensor(9906.2891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9906.1640625
tensor(9906.2891, grad_fn=<NegBackward0>) tensor(9906.1641, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9906.0869140625
tensor(9906.1641, grad_fn=<NegBackward0>) tensor(9906.0869, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9906.0341796875
tensor(9906.0869, grad_fn=<NegBackward0>) tensor(9906.0342, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9905.9912109375
tensor(9906.0342, grad_fn=<NegBackward0>) tensor(9905.9912, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9905.951171875
tensor(9905.9912, grad_fn=<NegBackward0>) tensor(9905.9512, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9905.916015625
tensor(9905.9512, grad_fn=<NegBackward0>) tensor(9905.9160, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9905.88671875
tensor(9905.9160, grad_fn=<NegBackward0>) tensor(9905.8867, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9905.85546875
tensor(9905.8867, grad_fn=<NegBackward0>) tensor(9905.8555, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9905.8193359375
tensor(9905.8555, grad_fn=<NegBackward0>) tensor(9905.8193, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9905.7568359375
tensor(9905.8193, grad_fn=<NegBackward0>) tensor(9905.7568, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9905.578125
tensor(9905.7568, grad_fn=<NegBackward0>) tensor(9905.5781, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9904.720703125
tensor(9905.5781, grad_fn=<NegBackward0>) tensor(9904.7207, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9903.8134765625
tensor(9904.7207, grad_fn=<NegBackward0>) tensor(9903.8135, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9903.19140625
tensor(9903.8135, grad_fn=<NegBackward0>) tensor(9903.1914, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9901.6806640625
tensor(9903.1914, grad_fn=<NegBackward0>) tensor(9901.6807, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9901.609375
tensor(9901.6807, grad_fn=<NegBackward0>) tensor(9901.6094, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9901.6044921875
tensor(9901.6094, grad_fn=<NegBackward0>) tensor(9901.6045, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9901.603515625
tensor(9901.6045, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9901.6025390625
tensor(9901.6035, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9901.611328125
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6113, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9901.6044921875
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6045, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9901.615234375
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6152, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9901.6044921875
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6045, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9901.6025390625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9901.603515625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -9901.6015625
tensor(9901.6025, grad_fn=<NegBackward0>) tensor(9901.6016, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9901.603515625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6035, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9901.6025390625
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6025, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -9901.607421875
tensor(9901.6016, grad_fn=<NegBackward0>) tensor(9901.6074, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.9164, 0.0836],
        [0.5652, 0.4348]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3680, 0.6320], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1259, 0.0927],
         [0.5890, 0.1947]],

        [[0.5546, 0.1491],
         [0.6242, 0.5529]],

        [[0.5099, 0.1732],
         [0.5602, 0.7259]],

        [[0.7009, 0.1325],
         [0.6373, 0.7209]],

        [[0.7185, 0.1578],
         [0.6281, 0.5401]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 71
Adjusted Rand Index: 0.1692763855744965
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.022723709951211306
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.04814522816701128
Average Adjusted Rand Index: 0.16256318846555803
[0.04814522816701128, 0.04814522816701128] [0.16256318846555803, 0.16256318846555803] [9901.6103515625, 9901.607421875]
-------------------------------------
This iteration is 59
True Objective function: Loss = -9889.942988698027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21113.1640625
inf tensor(21113.1641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9734.64453125
tensor(21113.1641, grad_fn=<NegBackward0>) tensor(9734.6445, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9733.8134765625
tensor(9734.6445, grad_fn=<NegBackward0>) tensor(9733.8135, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9733.5869140625
tensor(9733.8135, grad_fn=<NegBackward0>) tensor(9733.5869, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9733.490234375
tensor(9733.5869, grad_fn=<NegBackward0>) tensor(9733.4902, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9733.4296875
tensor(9733.4902, grad_fn=<NegBackward0>) tensor(9733.4297, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9733.3798828125
tensor(9733.4297, grad_fn=<NegBackward0>) tensor(9733.3799, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9733.341796875
tensor(9733.3799, grad_fn=<NegBackward0>) tensor(9733.3418, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9733.3056640625
tensor(9733.3418, grad_fn=<NegBackward0>) tensor(9733.3057, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9733.2705078125
tensor(9733.3057, grad_fn=<NegBackward0>) tensor(9733.2705, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9733.23828125
tensor(9733.2705, grad_fn=<NegBackward0>) tensor(9733.2383, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9733.2041015625
tensor(9733.2383, grad_fn=<NegBackward0>) tensor(9733.2041, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9733.169921875
tensor(9733.2041, grad_fn=<NegBackward0>) tensor(9733.1699, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9733.1357421875
tensor(9733.1699, grad_fn=<NegBackward0>) tensor(9733.1357, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9733.1025390625
tensor(9733.1357, grad_fn=<NegBackward0>) tensor(9733.1025, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9733.0732421875
tensor(9733.1025, grad_fn=<NegBackward0>) tensor(9733.0732, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9733.04296875
tensor(9733.0732, grad_fn=<NegBackward0>) tensor(9733.0430, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9733.0126953125
tensor(9733.0430, grad_fn=<NegBackward0>) tensor(9733.0127, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9732.9814453125
tensor(9733.0127, grad_fn=<NegBackward0>) tensor(9732.9814, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9732.951171875
tensor(9732.9814, grad_fn=<NegBackward0>) tensor(9732.9512, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9732.9189453125
tensor(9732.9512, grad_fn=<NegBackward0>) tensor(9732.9189, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9732.890625
tensor(9732.9189, grad_fn=<NegBackward0>) tensor(9732.8906, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9732.8642578125
tensor(9732.8906, grad_fn=<NegBackward0>) tensor(9732.8643, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9732.8369140625
tensor(9732.8643, grad_fn=<NegBackward0>) tensor(9732.8369, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9732.8134765625
tensor(9732.8369, grad_fn=<NegBackward0>) tensor(9732.8135, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9732.7900390625
tensor(9732.8135, grad_fn=<NegBackward0>) tensor(9732.7900, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9732.7744140625
tensor(9732.7900, grad_fn=<NegBackward0>) tensor(9732.7744, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9732.76171875
tensor(9732.7744, grad_fn=<NegBackward0>) tensor(9732.7617, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9732.75390625
tensor(9732.7617, grad_fn=<NegBackward0>) tensor(9732.7539, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9732.7470703125
tensor(9732.7539, grad_fn=<NegBackward0>) tensor(9732.7471, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9732.7412109375
tensor(9732.7471, grad_fn=<NegBackward0>) tensor(9732.7412, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9732.7373046875
tensor(9732.7412, grad_fn=<NegBackward0>) tensor(9732.7373, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9732.7353515625
tensor(9732.7373, grad_fn=<NegBackward0>) tensor(9732.7354, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9732.7333984375
tensor(9732.7354, grad_fn=<NegBackward0>) tensor(9732.7334, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9732.7314453125
tensor(9732.7334, grad_fn=<NegBackward0>) tensor(9732.7314, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9732.73046875
tensor(9732.7314, grad_fn=<NegBackward0>) tensor(9732.7305, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9732.7294921875
tensor(9732.7305, grad_fn=<NegBackward0>) tensor(9732.7295, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9732.7275390625
tensor(9732.7295, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9732.7275390625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9732.7265625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9732.7275390625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9732.7265625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9732.7265625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9732.7275390625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9732.7255859375
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9732.7265625
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9732.724609375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7246, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9732.7275390625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -9732.724609375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7246, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9732.724609375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7246, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9732.728515625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7285, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.0118, 0.9882],
        [0.0299, 0.9701]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2349, 0.7651], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1581, 0.1445],
         [0.5750, 0.1325]],

        [[0.6783, 0.1322],
         [0.5591, 0.6855]],

        [[0.6209, 0.0939],
         [0.6965, 0.6990]],

        [[0.6808, 0.1339],
         [0.7310, 0.6513]],

        [[0.6541, 0.1626],
         [0.7287, 0.7020]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21284.671875
inf tensor(21284.6719, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9735.5625
tensor(21284.6719, grad_fn=<NegBackward0>) tensor(9735.5625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9734.1103515625
tensor(9735.5625, grad_fn=<NegBackward0>) tensor(9734.1104, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9733.732421875
tensor(9734.1104, grad_fn=<NegBackward0>) tensor(9733.7324, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9733.5751953125
tensor(9733.7324, grad_fn=<NegBackward0>) tensor(9733.5752, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9733.482421875
tensor(9733.5752, grad_fn=<NegBackward0>) tensor(9733.4824, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9733.41796875
tensor(9733.4824, grad_fn=<NegBackward0>) tensor(9733.4180, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9733.3701171875
tensor(9733.4180, grad_fn=<NegBackward0>) tensor(9733.3701, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9733.333984375
tensor(9733.3701, grad_fn=<NegBackward0>) tensor(9733.3340, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9733.302734375
tensor(9733.3340, grad_fn=<NegBackward0>) tensor(9733.3027, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9733.2724609375
tensor(9733.3027, grad_fn=<NegBackward0>) tensor(9733.2725, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9733.2451171875
tensor(9733.2725, grad_fn=<NegBackward0>) tensor(9733.2451, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9733.216796875
tensor(9733.2451, grad_fn=<NegBackward0>) tensor(9733.2168, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9733.1875
tensor(9733.2168, grad_fn=<NegBackward0>) tensor(9733.1875, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9733.15625
tensor(9733.1875, grad_fn=<NegBackward0>) tensor(9733.1562, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9733.126953125
tensor(9733.1562, grad_fn=<NegBackward0>) tensor(9733.1270, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9733.0947265625
tensor(9733.1270, grad_fn=<NegBackward0>) tensor(9733.0947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9733.0625
tensor(9733.0947, grad_fn=<NegBackward0>) tensor(9733.0625, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9733.0302734375
tensor(9733.0625, grad_fn=<NegBackward0>) tensor(9733.0303, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9732.99609375
tensor(9733.0303, grad_fn=<NegBackward0>) tensor(9732.9961, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9732.96484375
tensor(9732.9961, grad_fn=<NegBackward0>) tensor(9732.9648, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9732.9326171875
tensor(9732.9648, grad_fn=<NegBackward0>) tensor(9732.9326, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9732.90234375
tensor(9732.9326, grad_fn=<NegBackward0>) tensor(9732.9023, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9732.8740234375
tensor(9732.9023, grad_fn=<NegBackward0>) tensor(9732.8740, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9732.8486328125
tensor(9732.8740, grad_fn=<NegBackward0>) tensor(9732.8486, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9732.828125
tensor(9732.8486, grad_fn=<NegBackward0>) tensor(9732.8281, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9732.8076171875
tensor(9732.8281, grad_fn=<NegBackward0>) tensor(9732.8076, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9732.79296875
tensor(9732.8076, grad_fn=<NegBackward0>) tensor(9732.7930, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9732.779296875
tensor(9732.7930, grad_fn=<NegBackward0>) tensor(9732.7793, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9732.76953125
tensor(9732.7793, grad_fn=<NegBackward0>) tensor(9732.7695, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9732.7705078125
tensor(9732.7695, grad_fn=<NegBackward0>) tensor(9732.7705, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9732.7548828125
tensor(9732.7695, grad_fn=<NegBackward0>) tensor(9732.7549, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9732.748046875
tensor(9732.7549, grad_fn=<NegBackward0>) tensor(9732.7480, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9732.7431640625
tensor(9732.7480, grad_fn=<NegBackward0>) tensor(9732.7432, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9732.73828125
tensor(9732.7432, grad_fn=<NegBackward0>) tensor(9732.7383, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9732.736328125
tensor(9732.7383, grad_fn=<NegBackward0>) tensor(9732.7363, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9732.7333984375
tensor(9732.7363, grad_fn=<NegBackward0>) tensor(9732.7334, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9732.740234375
tensor(9732.7334, grad_fn=<NegBackward0>) tensor(9732.7402, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9732.73046875
tensor(9732.7334, grad_fn=<NegBackward0>) tensor(9732.7305, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9732.7275390625
tensor(9732.7305, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9732.7275390625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9732.7275390625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9732.7275390625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9732.7275390625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9732.728515625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7285, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9732.7265625
tensor(9732.7275, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9732.7265625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9732.7275390625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7275, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9732.7265625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9732.7265625
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9732.7255859375
tensor(9732.7266, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9732.7255859375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9732.7255859375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9732.7265625
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9732.7265625
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9732.7255859375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9732.7255859375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9732.724609375
tensor(9732.7256, grad_fn=<NegBackward0>) tensor(9732.7246, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9732.724609375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7246, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9732.7265625
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7266, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -9732.7255859375
tensor(9732.7246, grad_fn=<NegBackward0>) tensor(9732.7256, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.9702, 0.0298],
        [0.9887, 0.0113]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7640, 0.2360], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1325, 0.1446],
         [0.6105, 0.1582]],

        [[0.5564, 0.1322],
         [0.6278, 0.6870]],

        [[0.6047, 0.0939],
         [0.5964, 0.5488]],

        [[0.6981, 0.1339],
         [0.5774, 0.5866]],

        [[0.5258, 0.1626],
         [0.5203, 0.6591]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [9732.728515625, 9732.7255859375]
-------------------------------------
This iteration is 60
True Objective function: Loss = -9889.23967898354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21694.1328125
inf tensor(21694.1328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9780.1259765625
tensor(21694.1328, grad_fn=<NegBackward0>) tensor(9780.1260, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9773.685546875
tensor(9780.1260, grad_fn=<NegBackward0>) tensor(9773.6855, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9772.1923828125
tensor(9773.6855, grad_fn=<NegBackward0>) tensor(9772.1924, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9771.6201171875
tensor(9772.1924, grad_fn=<NegBackward0>) tensor(9771.6201, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9771.2626953125
tensor(9771.6201, grad_fn=<NegBackward0>) tensor(9771.2627, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9771.0244140625
tensor(9771.2627, grad_fn=<NegBackward0>) tensor(9771.0244, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9770.859375
tensor(9771.0244, grad_fn=<NegBackward0>) tensor(9770.8594, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9770.759765625
tensor(9770.8594, grad_fn=<NegBackward0>) tensor(9770.7598, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9770.6865234375
tensor(9770.7598, grad_fn=<NegBackward0>) tensor(9770.6865, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9770.630859375
tensor(9770.6865, grad_fn=<NegBackward0>) tensor(9770.6309, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9770.58984375
tensor(9770.6309, grad_fn=<NegBackward0>) tensor(9770.5898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9770.5556640625
tensor(9770.5898, grad_fn=<NegBackward0>) tensor(9770.5557, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9770.5185546875
tensor(9770.5557, grad_fn=<NegBackward0>) tensor(9770.5186, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9770.4833984375
tensor(9770.5186, grad_fn=<NegBackward0>) tensor(9770.4834, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9770.4482421875
tensor(9770.4834, grad_fn=<NegBackward0>) tensor(9770.4482, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9770.4140625
tensor(9770.4482, grad_fn=<NegBackward0>) tensor(9770.4141, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9770.37890625
tensor(9770.4141, grad_fn=<NegBackward0>) tensor(9770.3789, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9770.34375
tensor(9770.3789, grad_fn=<NegBackward0>) tensor(9770.3438, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9770.306640625
tensor(9770.3438, grad_fn=<NegBackward0>) tensor(9770.3066, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9770.2685546875
tensor(9770.3066, grad_fn=<NegBackward0>) tensor(9770.2686, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9770.2275390625
tensor(9770.2686, grad_fn=<NegBackward0>) tensor(9770.2275, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9770.185546875
tensor(9770.2275, grad_fn=<NegBackward0>) tensor(9770.1855, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9770.1416015625
tensor(9770.1855, grad_fn=<NegBackward0>) tensor(9770.1416, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9770.09765625
tensor(9770.1416, grad_fn=<NegBackward0>) tensor(9770.0977, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9770.0546875
tensor(9770.0977, grad_fn=<NegBackward0>) tensor(9770.0547, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9770.0166015625
tensor(9770.0547, grad_fn=<NegBackward0>) tensor(9770.0166, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9769.9814453125
tensor(9770.0166, grad_fn=<NegBackward0>) tensor(9769.9814, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9769.9541015625
tensor(9769.9814, grad_fn=<NegBackward0>) tensor(9769.9541, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9769.9365234375
tensor(9769.9541, grad_fn=<NegBackward0>) tensor(9769.9365, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9769.9248046875
tensor(9769.9365, grad_fn=<NegBackward0>) tensor(9769.9248, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9769.916015625
tensor(9769.9248, grad_fn=<NegBackward0>) tensor(9769.9160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9769.908203125
tensor(9769.9160, grad_fn=<NegBackward0>) tensor(9769.9082, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9769.904296875
tensor(9769.9082, grad_fn=<NegBackward0>) tensor(9769.9043, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9769.8994140625
tensor(9769.9043, grad_fn=<NegBackward0>) tensor(9769.8994, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9769.896484375
tensor(9769.8994, grad_fn=<NegBackward0>) tensor(9769.8965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9769.8955078125
tensor(9769.8965, grad_fn=<NegBackward0>) tensor(9769.8955, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9769.8935546875
tensor(9769.8955, grad_fn=<NegBackward0>) tensor(9769.8936, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9769.892578125
tensor(9769.8936, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9769.892578125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9769.8935546875
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8936, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9769.892578125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9769.892578125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9769.892578125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9769.8916015625
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9769.890625
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8906, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9769.8916015625
tensor(9769.8906, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9769.8916015625
tensor(9769.8906, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9769.8916015625
tensor(9769.8906, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9769.9052734375
tensor(9769.8906, grad_fn=<NegBackward0>) tensor(9769.9053, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -9769.892578125
tensor(9769.8906, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5000 due to no improvement.
pi: tensor([[0.1476, 0.8524],
        [0.1944, 0.8056]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0769, 0.9231], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.2178],
         [0.5584, 0.1225]],

        [[0.5778, 0.1527],
         [0.5411, 0.5140]],

        [[0.6997, 0.1530],
         [0.5683, 0.5001]],

        [[0.6044, 0.1454],
         [0.6103, 0.5548]],

        [[0.6920, 0.1576],
         [0.6213, 0.6862]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004130624939255516
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
Global Adjusted Rand Index: 0.00857815068081273
Average Adjusted Rand Index: 0.005736354683866684
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20931.466796875
inf tensor(20931.4668, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9778.8369140625
tensor(20931.4668, grad_fn=<NegBackward0>) tensor(9778.8369, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9773.060546875
tensor(9778.8369, grad_fn=<NegBackward0>) tensor(9773.0605, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9772.5283203125
tensor(9773.0605, grad_fn=<NegBackward0>) tensor(9772.5283, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9772.2099609375
tensor(9772.5283, grad_fn=<NegBackward0>) tensor(9772.2100, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9771.69140625
tensor(9772.2100, grad_fn=<NegBackward0>) tensor(9771.6914, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9771.076171875
tensor(9771.6914, grad_fn=<NegBackward0>) tensor(9771.0762, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9770.7744140625
tensor(9771.0762, grad_fn=<NegBackward0>) tensor(9770.7744, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9770.654296875
tensor(9770.7744, grad_fn=<NegBackward0>) tensor(9770.6543, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9770.5869140625
tensor(9770.6543, grad_fn=<NegBackward0>) tensor(9770.5869, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9770.5390625
tensor(9770.5869, grad_fn=<NegBackward0>) tensor(9770.5391, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9770.49609375
tensor(9770.5391, grad_fn=<NegBackward0>) tensor(9770.4961, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9770.4580078125
tensor(9770.4961, grad_fn=<NegBackward0>) tensor(9770.4580, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9770.4189453125
tensor(9770.4580, grad_fn=<NegBackward0>) tensor(9770.4189, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9770.3818359375
tensor(9770.4189, grad_fn=<NegBackward0>) tensor(9770.3818, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9770.3427734375
tensor(9770.3818, grad_fn=<NegBackward0>) tensor(9770.3428, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9770.30078125
tensor(9770.3428, grad_fn=<NegBackward0>) tensor(9770.3008, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9770.2587890625
tensor(9770.3008, grad_fn=<NegBackward0>) tensor(9770.2588, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9770.212890625
tensor(9770.2588, grad_fn=<NegBackward0>) tensor(9770.2129, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9770.1669921875
tensor(9770.2129, grad_fn=<NegBackward0>) tensor(9770.1670, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9770.119140625
tensor(9770.1670, grad_fn=<NegBackward0>) tensor(9770.1191, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9770.0732421875
tensor(9770.1191, grad_fn=<NegBackward0>) tensor(9770.0732, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9770.0302734375
tensor(9770.0732, grad_fn=<NegBackward0>) tensor(9770.0303, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9769.9931640625
tensor(9770.0303, grad_fn=<NegBackward0>) tensor(9769.9932, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9769.962890625
tensor(9769.9932, grad_fn=<NegBackward0>) tensor(9769.9629, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9769.9404296875
tensor(9769.9629, grad_fn=<NegBackward0>) tensor(9769.9404, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9769.92578125
tensor(9769.9404, grad_fn=<NegBackward0>) tensor(9769.9258, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9769.919921875
tensor(9769.9258, grad_fn=<NegBackward0>) tensor(9769.9199, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9769.9091796875
tensor(9769.9199, grad_fn=<NegBackward0>) tensor(9769.9092, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9769.9052734375
tensor(9769.9092, grad_fn=<NegBackward0>) tensor(9769.9053, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9769.9013671875
tensor(9769.9053, grad_fn=<NegBackward0>) tensor(9769.9014, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9769.8984375
tensor(9769.9014, grad_fn=<NegBackward0>) tensor(9769.8984, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9769.8955078125
tensor(9769.8984, grad_fn=<NegBackward0>) tensor(9769.8955, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9769.8955078125
tensor(9769.8955, grad_fn=<NegBackward0>) tensor(9769.8955, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9769.8955078125
tensor(9769.8955, grad_fn=<NegBackward0>) tensor(9769.8955, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9769.89453125
tensor(9769.8955, grad_fn=<NegBackward0>) tensor(9769.8945, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9769.892578125
tensor(9769.8945, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9769.89453125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8945, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9769.8935546875
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8936, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9769.8935546875
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8936, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -9769.8955078125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8955, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -9769.892578125
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9769.8916015625
tensor(9769.8926, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9769.892578125
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9769.8916015625
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9769.8916015625
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8916, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9769.892578125
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9769.8974609375
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8975, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9769.8935546875
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8936, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9769.89453125
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8945, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -9769.892578125
tensor(9769.8916, grad_fn=<NegBackward0>) tensor(9769.8926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5000 due to no improvement.
pi: tensor([[0.1473, 0.8527],
        [0.1943, 0.8057]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0769, 0.9231], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.2178],
         [0.5824, 0.1225]],

        [[0.7118, 0.1527],
         [0.6141, 0.5576]],

        [[0.6060, 0.1530],
         [0.7231, 0.6732]],

        [[0.5876, 0.1454],
         [0.5552, 0.6236]],

        [[0.6538, 0.1576],
         [0.7217, 0.5863]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004130624939255516
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
Global Adjusted Rand Index: 0.00857815068081273
Average Adjusted Rand Index: 0.005736354683866684
[0.00857815068081273, 0.00857815068081273] [0.005736354683866684, 0.005736354683866684] [9769.892578125, 9769.892578125]
-------------------------------------
This iteration is 61
True Objective function: Loss = -10201.873632236695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24487.984375
inf tensor(24487.9844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10070.6884765625
tensor(24487.9844, grad_fn=<NegBackward0>) tensor(10070.6885, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10069.58984375
tensor(10070.6885, grad_fn=<NegBackward0>) tensor(10069.5898, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10069.0634765625
tensor(10069.5898, grad_fn=<NegBackward0>) tensor(10069.0635, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10068.49609375
tensor(10069.0635, grad_fn=<NegBackward0>) tensor(10068.4961, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10068.02734375
tensor(10068.4961, grad_fn=<NegBackward0>) tensor(10068.0273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10067.4501953125
tensor(10068.0273, grad_fn=<NegBackward0>) tensor(10067.4502, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10067.2080078125
tensor(10067.4502, grad_fn=<NegBackward0>) tensor(10067.2080, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10067.0400390625
tensor(10067.2080, grad_fn=<NegBackward0>) tensor(10067.0400, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10066.8828125
tensor(10067.0400, grad_fn=<NegBackward0>) tensor(10066.8828, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10066.7451171875
tensor(10066.8828, grad_fn=<NegBackward0>) tensor(10066.7451, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10066.6044921875
tensor(10066.7451, grad_fn=<NegBackward0>) tensor(10066.6045, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10066.4619140625
tensor(10066.6045, grad_fn=<NegBackward0>) tensor(10066.4619, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10066.3251953125
tensor(10066.4619, grad_fn=<NegBackward0>) tensor(10066.3252, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10066.193359375
tensor(10066.3252, grad_fn=<NegBackward0>) tensor(10066.1934, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10066.05078125
tensor(10066.1934, grad_fn=<NegBackward0>) tensor(10066.0508, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10065.88671875
tensor(10066.0508, grad_fn=<NegBackward0>) tensor(10065.8867, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10065.6826171875
tensor(10065.8867, grad_fn=<NegBackward0>) tensor(10065.6826, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10065.384765625
tensor(10065.6826, grad_fn=<NegBackward0>) tensor(10065.3848, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10064.9638671875
tensor(10065.3848, grad_fn=<NegBackward0>) tensor(10064.9639, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10064.091796875
tensor(10064.9639, grad_fn=<NegBackward0>) tensor(10064.0918, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10062.087890625
tensor(10064.0918, grad_fn=<NegBackward0>) tensor(10062.0879, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10061.7412109375
tensor(10062.0879, grad_fn=<NegBackward0>) tensor(10061.7412, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10061.7216796875
tensor(10061.7412, grad_fn=<NegBackward0>) tensor(10061.7217, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10061.7197265625
tensor(10061.7217, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10061.7197265625
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10061.7197265625
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10061.7216796875
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7217, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -10061.7197265625
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10061.7197265625
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10061.71875
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10061.720703125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10061.732421875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7324, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10061.720703125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -10061.720703125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -10061.720703125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -10061.7216796875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7217, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10061.7236328125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7236, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.8246, 0.1754],
        [0.1616, 0.8384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7464, 0.2536], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1683, 0.1014],
         [0.7249, 0.1132]],

        [[0.6396, 0.1277],
         [0.5678, 0.6249]],

        [[0.6597, 0.1246],
         [0.7113, 0.6255]],

        [[0.5406, 0.1402],
         [0.5843, 0.5973]],

        [[0.5591, 0.1367],
         [0.6178, 0.5198]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 31
Adjusted Rand Index: 0.13278456907672462
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 25
Adjusted Rand Index: 0.23923195070886982
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 24
Adjusted Rand Index: 0.2631115028646981
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04854289723591032
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 29
Adjusted Rand Index: 0.16808080808080808
Global Adjusted Rand Index: 0.16772037095868123
Average Adjusted Rand Index: 0.17035034559340217
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23701.8046875
inf tensor(23701.8047, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10070.482421875
tensor(23701.8047, grad_fn=<NegBackward0>) tensor(10070.4824, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10069.8017578125
tensor(10070.4824, grad_fn=<NegBackward0>) tensor(10069.8018, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10069.44140625
tensor(10069.8018, grad_fn=<NegBackward0>) tensor(10069.4414, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10069.119140625
tensor(10069.4414, grad_fn=<NegBackward0>) tensor(10069.1191, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10068.7060546875
tensor(10069.1191, grad_fn=<NegBackward0>) tensor(10068.7061, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10068.2822265625
tensor(10068.7061, grad_fn=<NegBackward0>) tensor(10068.2822, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10067.7294921875
tensor(10068.2822, grad_fn=<NegBackward0>) tensor(10067.7295, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10067.435546875
tensor(10067.7295, grad_fn=<NegBackward0>) tensor(10067.4355, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10067.20703125
tensor(10067.4355, grad_fn=<NegBackward0>) tensor(10067.2070, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10067.05859375
tensor(10067.2070, grad_fn=<NegBackward0>) tensor(10067.0586, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10066.923828125
tensor(10067.0586, grad_fn=<NegBackward0>) tensor(10066.9238, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10066.7978515625
tensor(10066.9238, grad_fn=<NegBackward0>) tensor(10066.7979, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10066.681640625
tensor(10066.7979, grad_fn=<NegBackward0>) tensor(10066.6816, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10066.5830078125
tensor(10066.6816, grad_fn=<NegBackward0>) tensor(10066.5830, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10066.4970703125
tensor(10066.5830, grad_fn=<NegBackward0>) tensor(10066.4971, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10066.4208984375
tensor(10066.4971, grad_fn=<NegBackward0>) tensor(10066.4209, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10066.34765625
tensor(10066.4209, grad_fn=<NegBackward0>) tensor(10066.3477, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10066.271484375
tensor(10066.3477, grad_fn=<NegBackward0>) tensor(10066.2715, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10066.1982421875
tensor(10066.2715, grad_fn=<NegBackward0>) tensor(10066.1982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10066.125
tensor(10066.1982, grad_fn=<NegBackward0>) tensor(10066.1250, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10066.0419921875
tensor(10066.1250, grad_fn=<NegBackward0>) tensor(10066.0420, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10065.9453125
tensor(10066.0420, grad_fn=<NegBackward0>) tensor(10065.9453, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10065.8251953125
tensor(10065.9453, grad_fn=<NegBackward0>) tensor(10065.8252, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10065.6748046875
tensor(10065.8252, grad_fn=<NegBackward0>) tensor(10065.6748, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10065.46875
tensor(10065.6748, grad_fn=<NegBackward0>) tensor(10065.4688, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10065.1591796875
tensor(10065.4688, grad_fn=<NegBackward0>) tensor(10065.1592, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10064.5673828125
tensor(10065.1592, grad_fn=<NegBackward0>) tensor(10064.5674, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10062.5576171875
tensor(10064.5674, grad_fn=<NegBackward0>) tensor(10062.5576, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10061.8359375
tensor(10062.5576, grad_fn=<NegBackward0>) tensor(10061.8359, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10061.7470703125
tensor(10061.8359, grad_fn=<NegBackward0>) tensor(10061.7471, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10061.7216796875
tensor(10061.7471, grad_fn=<NegBackward0>) tensor(10061.7217, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10061.7197265625
tensor(10061.7217, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10061.720703125
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10061.7197265625
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10061.71875
tensor(10061.7197, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10061.71875
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7188, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -10061.720703125
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7207, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -10061.7197265625
tensor(10061.7188, grad_fn=<NegBackward0>) tensor(10061.7197, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.8242, 0.1758],
        [0.1620, 0.8380]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7467, 0.2533], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1683, 0.1014],
         [0.5625, 0.1132]],

        [[0.5205, 0.1277],
         [0.6202, 0.7174]],

        [[0.7098, 0.1246],
         [0.6294, 0.5517]],

        [[0.7097, 0.1402],
         [0.6967, 0.5068]],

        [[0.6247, 0.1367],
         [0.5832, 0.7078]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 31
Adjusted Rand Index: 0.13278456907672462
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 25
Adjusted Rand Index: 0.23923195070886982
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 24
Adjusted Rand Index: 0.2631115028646981
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.04854289723591032
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 29
Adjusted Rand Index: 0.16808080808080808
Global Adjusted Rand Index: 0.16772037095868123
Average Adjusted Rand Index: 0.17035034559340217
[0.16772037095868123, 0.16772037095868123] [0.17035034559340217, 0.17035034559340217] [10061.7197265625, 10061.7197265625]
-------------------------------------
This iteration is 62
True Objective function: Loss = -9712.897213403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25364.3828125
inf tensor(25364.3828, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9605.52734375
tensor(25364.3828, grad_fn=<NegBackward0>) tensor(9605.5273, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9603.1787109375
tensor(9605.5273, grad_fn=<NegBackward0>) tensor(9603.1787, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9602.478515625
tensor(9603.1787, grad_fn=<NegBackward0>) tensor(9602.4785, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9602.216796875
tensor(9602.4785, grad_fn=<NegBackward0>) tensor(9602.2168, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9602.103515625
tensor(9602.2168, grad_fn=<NegBackward0>) tensor(9602.1035, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9602.0498046875
tensor(9602.1035, grad_fn=<NegBackward0>) tensor(9602.0498, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9602.01171875
tensor(9602.0498, grad_fn=<NegBackward0>) tensor(9602.0117, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9601.986328125
tensor(9602.0117, grad_fn=<NegBackward0>) tensor(9601.9863, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9601.96484375
tensor(9601.9863, grad_fn=<NegBackward0>) tensor(9601.9648, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9601.9453125
tensor(9601.9648, grad_fn=<NegBackward0>) tensor(9601.9453, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9601.9296875
tensor(9601.9453, grad_fn=<NegBackward0>) tensor(9601.9297, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9601.9130859375
tensor(9601.9297, grad_fn=<NegBackward0>) tensor(9601.9131, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9601.8974609375
tensor(9601.9131, grad_fn=<NegBackward0>) tensor(9601.8975, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9601.8828125
tensor(9601.8975, grad_fn=<NegBackward0>) tensor(9601.8828, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9601.8701171875
tensor(9601.8828, grad_fn=<NegBackward0>) tensor(9601.8701, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9601.8583984375
tensor(9601.8701, grad_fn=<NegBackward0>) tensor(9601.8584, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9601.84375
tensor(9601.8584, grad_fn=<NegBackward0>) tensor(9601.8438, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9601.8330078125
tensor(9601.8438, grad_fn=<NegBackward0>) tensor(9601.8330, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9601.8193359375
tensor(9601.8330, grad_fn=<NegBackward0>) tensor(9601.8193, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9601.806640625
tensor(9601.8193, grad_fn=<NegBackward0>) tensor(9601.8066, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9601.7919921875
tensor(9601.8066, grad_fn=<NegBackward0>) tensor(9601.7920, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9601.77734375
tensor(9601.7920, grad_fn=<NegBackward0>) tensor(9601.7773, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9601.763671875
tensor(9601.7773, grad_fn=<NegBackward0>) tensor(9601.7637, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9601.7490234375
tensor(9601.7637, grad_fn=<NegBackward0>) tensor(9601.7490, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9601.732421875
tensor(9601.7490, grad_fn=<NegBackward0>) tensor(9601.7324, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9601.71484375
tensor(9601.7324, grad_fn=<NegBackward0>) tensor(9601.7148, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9601.6962890625
tensor(9601.7148, grad_fn=<NegBackward0>) tensor(9601.6963, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9601.673828125
tensor(9601.6963, grad_fn=<NegBackward0>) tensor(9601.6738, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9601.6484375
tensor(9601.6738, grad_fn=<NegBackward0>) tensor(9601.6484, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9601.6162109375
tensor(9601.6484, grad_fn=<NegBackward0>) tensor(9601.6162, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9601.578125
tensor(9601.6162, grad_fn=<NegBackward0>) tensor(9601.5781, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9601.5380859375
tensor(9601.5781, grad_fn=<NegBackward0>) tensor(9601.5381, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9601.4990234375
tensor(9601.5381, grad_fn=<NegBackward0>) tensor(9601.4990, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9601.462890625
tensor(9601.4990, grad_fn=<NegBackward0>) tensor(9601.4629, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9601.4345703125
tensor(9601.4629, grad_fn=<NegBackward0>) tensor(9601.4346, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9601.41015625
tensor(9601.4346, grad_fn=<NegBackward0>) tensor(9601.4102, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9601.3896484375
tensor(9601.4102, grad_fn=<NegBackward0>) tensor(9601.3896, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9601.37109375
tensor(9601.3896, grad_fn=<NegBackward0>) tensor(9601.3711, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9601.3564453125
tensor(9601.3711, grad_fn=<NegBackward0>) tensor(9601.3564, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9601.33984375
tensor(9601.3564, grad_fn=<NegBackward0>) tensor(9601.3398, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9601.3232421875
tensor(9601.3398, grad_fn=<NegBackward0>) tensor(9601.3232, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9601.3056640625
tensor(9601.3232, grad_fn=<NegBackward0>) tensor(9601.3057, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9601.2919921875
tensor(9601.3057, grad_fn=<NegBackward0>) tensor(9601.2920, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9601.2802734375
tensor(9601.2920, grad_fn=<NegBackward0>) tensor(9601.2803, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9601.26953125
tensor(9601.2803, grad_fn=<NegBackward0>) tensor(9601.2695, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9601.26171875
tensor(9601.2695, grad_fn=<NegBackward0>) tensor(9601.2617, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9601.251953125
tensor(9601.2617, grad_fn=<NegBackward0>) tensor(9601.2520, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9601.2421875
tensor(9601.2520, grad_fn=<NegBackward0>) tensor(9601.2422, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9601.2333984375
tensor(9601.2422, grad_fn=<NegBackward0>) tensor(9601.2334, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9601.2265625
tensor(9601.2334, grad_fn=<NegBackward0>) tensor(9601.2266, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9601.220703125
tensor(9601.2266, grad_fn=<NegBackward0>) tensor(9601.2207, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9601.2138671875
tensor(9601.2207, grad_fn=<NegBackward0>) tensor(9601.2139, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9601.208984375
tensor(9601.2139, grad_fn=<NegBackward0>) tensor(9601.2090, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9601.203125
tensor(9601.2090, grad_fn=<NegBackward0>) tensor(9601.2031, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9601.2001953125
tensor(9601.2031, grad_fn=<NegBackward0>) tensor(9601.2002, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9601.197265625
tensor(9601.2002, grad_fn=<NegBackward0>) tensor(9601.1973, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9601.1943359375
tensor(9601.1973, grad_fn=<NegBackward0>) tensor(9601.1943, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9601.1923828125
tensor(9601.1943, grad_fn=<NegBackward0>) tensor(9601.1924, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9601.1904296875
tensor(9601.1924, grad_fn=<NegBackward0>) tensor(9601.1904, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9601.1904296875
tensor(9601.1904, grad_fn=<NegBackward0>) tensor(9601.1904, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9601.1865234375
tensor(9601.1904, grad_fn=<NegBackward0>) tensor(9601.1865, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9601.177734375
tensor(9601.1865, grad_fn=<NegBackward0>) tensor(9601.1777, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9601.0537109375
tensor(9601.1777, grad_fn=<NegBackward0>) tensor(9601.0537, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9600.755859375
tensor(9601.0537, grad_fn=<NegBackward0>) tensor(9600.7559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9600.400390625
tensor(9600.7559, grad_fn=<NegBackward0>) tensor(9600.4004, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9600.3115234375
tensor(9600.4004, grad_fn=<NegBackward0>) tensor(9600.3115, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9600.26171875
tensor(9600.3115, grad_fn=<NegBackward0>) tensor(9600.2617, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9600.2333984375
tensor(9600.2617, grad_fn=<NegBackward0>) tensor(9600.2334, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9600.205078125
tensor(9600.2334, grad_fn=<NegBackward0>) tensor(9600.2051, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9600.1953125
tensor(9600.2051, grad_fn=<NegBackward0>) tensor(9600.1953, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9600.1865234375
tensor(9600.1953, grad_fn=<NegBackward0>) tensor(9600.1865, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9600.1796875
tensor(9600.1865, grad_fn=<NegBackward0>) tensor(9600.1797, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9600.169921875
tensor(9600.1797, grad_fn=<NegBackward0>) tensor(9600.1699, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9600.1513671875
tensor(9600.1699, grad_fn=<NegBackward0>) tensor(9600.1514, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9600.0615234375
tensor(9600.1514, grad_fn=<NegBackward0>) tensor(9600.0615, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9600.037109375
tensor(9600.0615, grad_fn=<NegBackward0>) tensor(9600.0371, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9600.0302734375
tensor(9600.0371, grad_fn=<NegBackward0>) tensor(9600.0303, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9600.02734375
tensor(9600.0303, grad_fn=<NegBackward0>) tensor(9600.0273, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9600.0263671875
tensor(9600.0273, grad_fn=<NegBackward0>) tensor(9600.0264, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9600.0224609375
tensor(9600.0264, grad_fn=<NegBackward0>) tensor(9600.0225, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9600.021484375
tensor(9600.0225, grad_fn=<NegBackward0>) tensor(9600.0215, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9600.0205078125
tensor(9600.0215, grad_fn=<NegBackward0>) tensor(9600.0205, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9600.0185546875
tensor(9600.0205, grad_fn=<NegBackward0>) tensor(9600.0186, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9600.017578125
tensor(9600.0186, grad_fn=<NegBackward0>) tensor(9600.0176, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9600.0166015625
tensor(9600.0176, grad_fn=<NegBackward0>) tensor(9600.0166, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9600.015625
tensor(9600.0166, grad_fn=<NegBackward0>) tensor(9600.0156, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9600.0166015625
tensor(9600.0156, grad_fn=<NegBackward0>) tensor(9600.0166, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9600.0302734375
tensor(9600.0156, grad_fn=<NegBackward0>) tensor(9600.0303, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -9600.0146484375
tensor(9600.0156, grad_fn=<NegBackward0>) tensor(9600.0146, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9600.0146484375
tensor(9600.0146, grad_fn=<NegBackward0>) tensor(9600.0146, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9600.0126953125
tensor(9600.0146, grad_fn=<NegBackward0>) tensor(9600.0127, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9600.025390625
tensor(9600.0127, grad_fn=<NegBackward0>) tensor(9600.0254, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9600.01171875
tensor(9600.0127, grad_fn=<NegBackward0>) tensor(9600.0117, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9600.0126953125
tensor(9600.0117, grad_fn=<NegBackward0>) tensor(9600.0127, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9600.0126953125
tensor(9600.0117, grad_fn=<NegBackward0>) tensor(9600.0127, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9600.0107421875
tensor(9600.0117, grad_fn=<NegBackward0>) tensor(9600.0107, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9600.01171875
tensor(9600.0107, grad_fn=<NegBackward0>) tensor(9600.0117, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9600.009765625
tensor(9600.0107, grad_fn=<NegBackward0>) tensor(9600.0098, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9600.0087890625
tensor(9600.0098, grad_fn=<NegBackward0>) tensor(9600.0088, grad_fn=<NegBackward0>)
pi: tensor([[9.9976e-01, 2.3768e-04],
        [8.2616e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0324, 0.9676], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2772, 0.0806],
         [0.6821, 0.1296]],

        [[0.6960, 0.1412],
         [0.7208, 0.5968]],

        [[0.6510, 0.1303],
         [0.5073, 0.6916]],

        [[0.7292, 0.1772],
         [0.6097, 0.7200]],

        [[0.6456, 0.1857],
         [0.7201, 0.5276]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005797822571485746
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: 0.0003612375232548945
Average Adjusted Rand Index: -0.002708210239941545
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22845.033203125
inf tensor(22845.0332, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9602.626953125
tensor(22845.0332, grad_fn=<NegBackward0>) tensor(9602.6270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9601.931640625
tensor(9602.6270, grad_fn=<NegBackward0>) tensor(9601.9316, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9601.75390625
tensor(9601.9316, grad_fn=<NegBackward0>) tensor(9601.7539, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9601.6162109375
tensor(9601.7539, grad_fn=<NegBackward0>) tensor(9601.6162, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9601.541015625
tensor(9601.6162, grad_fn=<NegBackward0>) tensor(9601.5410, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9601.4970703125
tensor(9601.5410, grad_fn=<NegBackward0>) tensor(9601.4971, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9601.46484375
tensor(9601.4971, grad_fn=<NegBackward0>) tensor(9601.4648, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9601.4384765625
tensor(9601.4648, grad_fn=<NegBackward0>) tensor(9601.4385, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9601.4130859375
tensor(9601.4385, grad_fn=<NegBackward0>) tensor(9601.4131, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9601.3916015625
tensor(9601.4131, grad_fn=<NegBackward0>) tensor(9601.3916, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9601.37109375
tensor(9601.3916, grad_fn=<NegBackward0>) tensor(9601.3711, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9601.3505859375
tensor(9601.3711, grad_fn=<NegBackward0>) tensor(9601.3506, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9601.3310546875
tensor(9601.3506, grad_fn=<NegBackward0>) tensor(9601.3311, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9601.3115234375
tensor(9601.3311, grad_fn=<NegBackward0>) tensor(9601.3115, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9601.2890625
tensor(9601.3115, grad_fn=<NegBackward0>) tensor(9601.2891, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9601.2685546875
tensor(9601.2891, grad_fn=<NegBackward0>) tensor(9601.2686, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9601.2509765625
tensor(9601.2686, grad_fn=<NegBackward0>) tensor(9601.2510, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9601.2373046875
tensor(9601.2510, grad_fn=<NegBackward0>) tensor(9601.2373, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9601.2265625
tensor(9601.2373, grad_fn=<NegBackward0>) tensor(9601.2266, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9601.2177734375
tensor(9601.2266, grad_fn=<NegBackward0>) tensor(9601.2178, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9601.20703125
tensor(9601.2178, grad_fn=<NegBackward0>) tensor(9601.2070, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9601.1953125
tensor(9601.2070, grad_fn=<NegBackward0>) tensor(9601.1953, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9601.1875
tensor(9601.1953, grad_fn=<NegBackward0>) tensor(9601.1875, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9601.1787109375
tensor(9601.1875, grad_fn=<NegBackward0>) tensor(9601.1787, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9601.173828125
tensor(9601.1787, grad_fn=<NegBackward0>) tensor(9601.1738, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9601.1728515625
tensor(9601.1738, grad_fn=<NegBackward0>) tensor(9601.1729, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9601.169921875
tensor(9601.1729, grad_fn=<NegBackward0>) tensor(9601.1699, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9601.1689453125
tensor(9601.1699, grad_fn=<NegBackward0>) tensor(9601.1689, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9601.16796875
tensor(9601.1689, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9601.1689453125
tensor(9601.1680, grad_fn=<NegBackward0>) tensor(9601.1689, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9601.1689453125
tensor(9601.1680, grad_fn=<NegBackward0>) tensor(9601.1689, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -9601.1669921875
tensor(9601.1680, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9601.16796875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9601.1689453125
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1689, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -9601.16796875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
3
Iteration 3600: Loss = -9601.1669921875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9601.1669921875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9601.16796875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9601.1669921875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9601.16796875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9601.1669921875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9601.1669921875
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9601.166015625
tensor(9601.1670, grad_fn=<NegBackward0>) tensor(9601.1660, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9601.16796875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9601.166015625
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1660, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9601.166015625
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1660, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9601.166015625
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1660, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9601.16796875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1680, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -9601.1669921875
tensor(9601.1660, grad_fn=<NegBackward0>) tensor(9601.1670, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.0333, 0.9667],
        [0.0248, 0.9752]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9987, 0.0013], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1304, 0.1320],
         [0.6252, 0.1313]],

        [[0.6972, 0.0861],
         [0.6515, 0.6101]],

        [[0.6573, 0.1362],
         [0.5836, 0.5303]],

        [[0.6012, 0.0967],
         [0.5866, 0.6890]],

        [[0.7309, 0.1873],
         [0.7150, 0.5247]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018366732645071341
Average Adjusted Rand Index: 0.0
[0.0003612375232548945, -0.0018366732645071341] [-0.002708210239941545, 0.0] [9600.01953125, 9601.1669921875]
-------------------------------------
This iteration is 63
True Objective function: Loss = -10068.839739882842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22677.6484375
inf tensor(22677.6484, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9972.646484375
tensor(22677.6484, grad_fn=<NegBackward0>) tensor(9972.6465, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9970.9033203125
tensor(9972.6465, grad_fn=<NegBackward0>) tensor(9970.9033, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9970.2373046875
tensor(9970.9033, grad_fn=<NegBackward0>) tensor(9970.2373, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9969.8046875
tensor(9970.2373, grad_fn=<NegBackward0>) tensor(9969.8047, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9969.525390625
tensor(9969.8047, grad_fn=<NegBackward0>) tensor(9969.5254, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9969.3583984375
tensor(9969.5254, grad_fn=<NegBackward0>) tensor(9969.3584, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9969.2509765625
tensor(9969.3584, grad_fn=<NegBackward0>) tensor(9969.2510, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9969.1748046875
tensor(9969.2510, grad_fn=<NegBackward0>) tensor(9969.1748, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9969.12109375
tensor(9969.1748, grad_fn=<NegBackward0>) tensor(9969.1211, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9969.080078125
tensor(9969.1211, grad_fn=<NegBackward0>) tensor(9969.0801, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9969.0498046875
tensor(9969.0801, grad_fn=<NegBackward0>) tensor(9969.0498, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9969.0263671875
tensor(9969.0498, grad_fn=<NegBackward0>) tensor(9969.0264, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9969.005859375
tensor(9969.0264, grad_fn=<NegBackward0>) tensor(9969.0059, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9968.9912109375
tensor(9969.0059, grad_fn=<NegBackward0>) tensor(9968.9912, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9968.9736328125
tensor(9968.9912, grad_fn=<NegBackward0>) tensor(9968.9736, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9968.9599609375
tensor(9968.9736, grad_fn=<NegBackward0>) tensor(9968.9600, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9968.9443359375
tensor(9968.9600, grad_fn=<NegBackward0>) tensor(9968.9443, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9968.9267578125
tensor(9968.9443, grad_fn=<NegBackward0>) tensor(9968.9268, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9968.91015625
tensor(9968.9268, grad_fn=<NegBackward0>) tensor(9968.9102, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9968.884765625
tensor(9968.9102, grad_fn=<NegBackward0>) tensor(9968.8848, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9968.853515625
tensor(9968.8848, grad_fn=<NegBackward0>) tensor(9968.8535, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9968.7958984375
tensor(9968.8535, grad_fn=<NegBackward0>) tensor(9968.7959, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9968.6953125
tensor(9968.7959, grad_fn=<NegBackward0>) tensor(9968.6953, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9968.46875
tensor(9968.6953, grad_fn=<NegBackward0>) tensor(9968.4688, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9967.76953125
tensor(9968.4688, grad_fn=<NegBackward0>) tensor(9967.7695, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9967.283203125
tensor(9967.7695, grad_fn=<NegBackward0>) tensor(9967.2832, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9967.1220703125
tensor(9967.2832, grad_fn=<NegBackward0>) tensor(9967.1221, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9967.0654296875
tensor(9967.1221, grad_fn=<NegBackward0>) tensor(9967.0654, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9967.037109375
tensor(9967.0654, grad_fn=<NegBackward0>) tensor(9967.0371, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9967.021484375
tensor(9967.0371, grad_fn=<NegBackward0>) tensor(9967.0215, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9967.0107421875
tensor(9967.0215, grad_fn=<NegBackward0>) tensor(9967.0107, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9967.0009765625
tensor(9967.0107, grad_fn=<NegBackward0>) tensor(9967.0010, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9966.99609375
tensor(9967.0010, grad_fn=<NegBackward0>) tensor(9966.9961, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9966.9912109375
tensor(9966.9961, grad_fn=<NegBackward0>) tensor(9966.9912, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9966.9892578125
tensor(9966.9912, grad_fn=<NegBackward0>) tensor(9966.9893, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9966.9873046875
tensor(9966.9893, grad_fn=<NegBackward0>) tensor(9966.9873, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9966.984375
tensor(9966.9873, grad_fn=<NegBackward0>) tensor(9966.9844, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9966.984375
tensor(9966.9844, grad_fn=<NegBackward0>) tensor(9966.9844, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9966.98046875
tensor(9966.9844, grad_fn=<NegBackward0>) tensor(9966.9805, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9966.98046875
tensor(9966.9805, grad_fn=<NegBackward0>) tensor(9966.9805, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9966.978515625
tensor(9966.9805, grad_fn=<NegBackward0>) tensor(9966.9785, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9966.9775390625
tensor(9966.9785, grad_fn=<NegBackward0>) tensor(9966.9775, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9966.9765625
tensor(9966.9775, grad_fn=<NegBackward0>) tensor(9966.9766, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9966.974609375
tensor(9966.9766, grad_fn=<NegBackward0>) tensor(9966.9746, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9966.9755859375
tensor(9966.9746, grad_fn=<NegBackward0>) tensor(9966.9756, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9966.9736328125
tensor(9966.9746, grad_fn=<NegBackward0>) tensor(9966.9736, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9966.97265625
tensor(9966.9736, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9966.97265625
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9966.9736328125
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9736, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9966.97265625
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9966.9716796875
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9966.9716796875
tensor(9966.9717, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9966.9716796875
tensor(9966.9717, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9966.9716796875
tensor(9966.9717, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9966.97265625
tensor(9966.9717, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9966.970703125
tensor(9966.9717, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9966.97265625
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9966.97265625
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9966.970703125
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9966.9716796875
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9966.970703125
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9966.9716796875
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9966.9716796875
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9966.9697265625
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9966.970703125
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9966.97265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9966.970703125
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9966.97265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9966.96875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9688, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9966.96875
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9688, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9966.98828125
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9883, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -9966.970703125
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -9966.970703125
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.9927, 0.0073],
        [0.3610, 0.6390]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9429, 0.0571], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1360, 0.1536],
         [0.7149, 0.1995]],

        [[0.5123, 0.2328],
         [0.7078, 0.6746]],

        [[0.6004, 0.1832],
         [0.6867, 0.7101]],

        [[0.6926, 0.0959],
         [0.6423, 0.5297]],

        [[0.6591, 0.1947],
         [0.6148, 0.7207]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0008899652698720131
Average Adjusted Rand Index: -0.005436699050721277
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21860.810546875
inf tensor(21860.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9972.447265625
tensor(21860.8105, grad_fn=<NegBackward0>) tensor(9972.4473, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9971.4638671875
tensor(9972.4473, grad_fn=<NegBackward0>) tensor(9971.4639, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9970.6708984375
tensor(9971.4639, grad_fn=<NegBackward0>) tensor(9970.6709, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9970.3291015625
tensor(9970.6709, grad_fn=<NegBackward0>) tensor(9970.3291, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9970.0390625
tensor(9970.3291, grad_fn=<NegBackward0>) tensor(9970.0391, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9969.6689453125
tensor(9970.0391, grad_fn=<NegBackward0>) tensor(9969.6689, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9969.48828125
tensor(9969.6689, grad_fn=<NegBackward0>) tensor(9969.4883, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9969.326171875
tensor(9969.4883, grad_fn=<NegBackward0>) tensor(9969.3262, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9969.0673828125
tensor(9969.3262, grad_fn=<NegBackward0>) tensor(9969.0674, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9968.9658203125
tensor(9969.0674, grad_fn=<NegBackward0>) tensor(9968.9658, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9968.9306640625
tensor(9968.9658, grad_fn=<NegBackward0>) tensor(9968.9307, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9968.896484375
tensor(9968.9307, grad_fn=<NegBackward0>) tensor(9968.8965, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9968.85546875
tensor(9968.8965, grad_fn=<NegBackward0>) tensor(9968.8555, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9968.78515625
tensor(9968.8555, grad_fn=<NegBackward0>) tensor(9968.7852, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9968.6396484375
tensor(9968.7852, grad_fn=<NegBackward0>) tensor(9968.6396, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9968.3466796875
tensor(9968.6396, grad_fn=<NegBackward0>) tensor(9968.3467, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9967.59765625
tensor(9968.3467, grad_fn=<NegBackward0>) tensor(9967.5977, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9967.1396484375
tensor(9967.5977, grad_fn=<NegBackward0>) tensor(9967.1396, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9967.0419921875
tensor(9967.1396, grad_fn=<NegBackward0>) tensor(9967.0420, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9967.0126953125
tensor(9967.0420, grad_fn=<NegBackward0>) tensor(9967.0127, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9967.0
tensor(9967.0127, grad_fn=<NegBackward0>) tensor(9967., grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9966.9931640625
tensor(9967., grad_fn=<NegBackward0>) tensor(9966.9932, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9966.98828125
tensor(9966.9932, grad_fn=<NegBackward0>) tensor(9966.9883, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9966.986328125
tensor(9966.9883, grad_fn=<NegBackward0>) tensor(9966.9863, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9966.984375
tensor(9966.9863, grad_fn=<NegBackward0>) tensor(9966.9844, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9966.982421875
tensor(9966.9844, grad_fn=<NegBackward0>) tensor(9966.9824, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9966.9814453125
tensor(9966.9824, grad_fn=<NegBackward0>) tensor(9966.9814, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9966.98046875
tensor(9966.9814, grad_fn=<NegBackward0>) tensor(9966.9805, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9966.98046875
tensor(9966.9805, grad_fn=<NegBackward0>) tensor(9966.9805, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9966.978515625
tensor(9966.9805, grad_fn=<NegBackward0>) tensor(9966.9785, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9966.978515625
tensor(9966.9785, grad_fn=<NegBackward0>) tensor(9966.9785, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9966.9755859375
tensor(9966.9785, grad_fn=<NegBackward0>) tensor(9966.9756, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9966.9755859375
tensor(9966.9756, grad_fn=<NegBackward0>) tensor(9966.9756, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9966.9755859375
tensor(9966.9756, grad_fn=<NegBackward0>) tensor(9966.9756, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9966.974609375
tensor(9966.9756, grad_fn=<NegBackward0>) tensor(9966.9746, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9966.974609375
tensor(9966.9746, grad_fn=<NegBackward0>) tensor(9966.9746, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9966.974609375
tensor(9966.9746, grad_fn=<NegBackward0>) tensor(9966.9746, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9966.9736328125
tensor(9966.9746, grad_fn=<NegBackward0>) tensor(9966.9736, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9966.9736328125
tensor(9966.9736, grad_fn=<NegBackward0>) tensor(9966.9736, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9966.974609375
tensor(9966.9736, grad_fn=<NegBackward0>) tensor(9966.9746, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9966.97265625
tensor(9966.9736, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9966.9736328125
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9736, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9966.97265625
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9966.97265625
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9966.970703125
tensor(9966.9727, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9966.970703125
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9966.9716796875
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9966.9716796875
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9966.97265625
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9727, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -9966.9697265625
tensor(9966.9707, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9966.9716796875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9717, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9966.9697265625
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9966.970703125
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9966.970703125
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9966.96875
tensor(9966.9697, grad_fn=<NegBackward0>) tensor(9966.9688, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9966.96875
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9688, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9966.970703125
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9707, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -9966.9697265625
tensor(9966.9688, grad_fn=<NegBackward0>) tensor(9966.9697, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[0.9927, 0.0073],
        [0.3610, 0.6390]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9429, 0.0571], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1360, 0.1536],
         [0.5724, 0.1995]],

        [[0.5800, 0.2328],
         [0.6357, 0.7076]],

        [[0.6440, 0.1832],
         [0.6986, 0.6450]],

        [[0.6447, 0.0959],
         [0.7070, 0.6439]],

        [[0.6111, 0.1947],
         [0.7062, 0.5837]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0008899652698720131
Average Adjusted Rand Index: -0.005436699050721277
[0.0008899652698720131, 0.0008899652698720131] [-0.005436699050721277, -0.005436699050721277] [9966.970703125, 9966.9697265625]
-------------------------------------
This iteration is 64
True Objective function: Loss = -10071.763342245691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22587.376953125
inf tensor(22587.3770, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9940.5927734375
tensor(22587.3770, grad_fn=<NegBackward0>) tensor(9940.5928, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9935.259765625
tensor(9940.5928, grad_fn=<NegBackward0>) tensor(9935.2598, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9930.05078125
tensor(9935.2598, grad_fn=<NegBackward0>) tensor(9930.0508, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9928.23828125
tensor(9930.0508, grad_fn=<NegBackward0>) tensor(9928.2383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9926.8515625
tensor(9928.2383, grad_fn=<NegBackward0>) tensor(9926.8516, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9925.853515625
tensor(9926.8516, grad_fn=<NegBackward0>) tensor(9925.8535, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9925.1904296875
tensor(9925.8535, grad_fn=<NegBackward0>) tensor(9925.1904, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9924.6083984375
tensor(9925.1904, grad_fn=<NegBackward0>) tensor(9924.6084, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9924.09375
tensor(9924.6084, grad_fn=<NegBackward0>) tensor(9924.0938, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9923.427734375
tensor(9924.0938, grad_fn=<NegBackward0>) tensor(9923.4277, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9922.6796875
tensor(9923.4277, grad_fn=<NegBackward0>) tensor(9922.6797, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9922.2841796875
tensor(9922.6797, grad_fn=<NegBackward0>) tensor(9922.2842, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9921.99609375
tensor(9922.2842, grad_fn=<NegBackward0>) tensor(9921.9961, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9921.8056640625
tensor(9921.9961, grad_fn=<NegBackward0>) tensor(9921.8057, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9921.6982421875
tensor(9921.8057, grad_fn=<NegBackward0>) tensor(9921.6982, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9921.6416015625
tensor(9921.6982, grad_fn=<NegBackward0>) tensor(9921.6416, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9921.599609375
tensor(9921.6416, grad_fn=<NegBackward0>) tensor(9921.5996, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9921.56640625
tensor(9921.5996, grad_fn=<NegBackward0>) tensor(9921.5664, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9921.541015625
tensor(9921.5664, grad_fn=<NegBackward0>) tensor(9921.5410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9921.5146484375
tensor(9921.5410, grad_fn=<NegBackward0>) tensor(9921.5146, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9921.4951171875
tensor(9921.5146, grad_fn=<NegBackward0>) tensor(9921.4951, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9921.48046875
tensor(9921.4951, grad_fn=<NegBackward0>) tensor(9921.4805, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9921.4697265625
tensor(9921.4805, grad_fn=<NegBackward0>) tensor(9921.4697, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9921.4599609375
tensor(9921.4697, grad_fn=<NegBackward0>) tensor(9921.4600, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9921.453125
tensor(9921.4600, grad_fn=<NegBackward0>) tensor(9921.4531, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9921.4482421875
tensor(9921.4531, grad_fn=<NegBackward0>) tensor(9921.4482, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9921.443359375
tensor(9921.4482, grad_fn=<NegBackward0>) tensor(9921.4434, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9921.439453125
tensor(9921.4434, grad_fn=<NegBackward0>) tensor(9921.4395, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9921.4345703125
tensor(9921.4395, grad_fn=<NegBackward0>) tensor(9921.4346, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9921.4306640625
tensor(9921.4346, grad_fn=<NegBackward0>) tensor(9921.4307, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9921.427734375
tensor(9921.4307, grad_fn=<NegBackward0>) tensor(9921.4277, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9921.4228515625
tensor(9921.4277, grad_fn=<NegBackward0>) tensor(9921.4229, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9921.421875
tensor(9921.4229, grad_fn=<NegBackward0>) tensor(9921.4219, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9921.41796875
tensor(9921.4219, grad_fn=<NegBackward0>) tensor(9921.4180, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9921.416015625
tensor(9921.4180, grad_fn=<NegBackward0>) tensor(9921.4160, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9921.4140625
tensor(9921.4160, grad_fn=<NegBackward0>) tensor(9921.4141, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9921.41015625
tensor(9921.4141, grad_fn=<NegBackward0>) tensor(9921.4102, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9921.41015625
tensor(9921.4102, grad_fn=<NegBackward0>) tensor(9921.4102, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9921.4072265625
tensor(9921.4102, grad_fn=<NegBackward0>) tensor(9921.4072, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9921.40625
tensor(9921.4072, grad_fn=<NegBackward0>) tensor(9921.4062, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9921.4033203125
tensor(9921.4062, grad_fn=<NegBackward0>) tensor(9921.4033, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9921.4033203125
tensor(9921.4033, grad_fn=<NegBackward0>) tensor(9921.4033, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9921.400390625
tensor(9921.4033, grad_fn=<NegBackward0>) tensor(9921.4004, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9921.3984375
tensor(9921.4004, grad_fn=<NegBackward0>) tensor(9921.3984, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9921.3994140625
tensor(9921.3984, grad_fn=<NegBackward0>) tensor(9921.3994, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9921.3974609375
tensor(9921.3984, grad_fn=<NegBackward0>) tensor(9921.3975, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9921.3955078125
tensor(9921.3975, grad_fn=<NegBackward0>) tensor(9921.3955, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9921.3955078125
tensor(9921.3955, grad_fn=<NegBackward0>) tensor(9921.3955, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9921.3935546875
tensor(9921.3955, grad_fn=<NegBackward0>) tensor(9921.3936, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9921.39453125
tensor(9921.3936, grad_fn=<NegBackward0>) tensor(9921.3945, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9921.3935546875
tensor(9921.3936, grad_fn=<NegBackward0>) tensor(9921.3936, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9921.3955078125
tensor(9921.3936, grad_fn=<NegBackward0>) tensor(9921.3955, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9921.390625
tensor(9921.3936, grad_fn=<NegBackward0>) tensor(9921.3906, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9921.390625
tensor(9921.3906, grad_fn=<NegBackward0>) tensor(9921.3906, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9921.3896484375
tensor(9921.3906, grad_fn=<NegBackward0>) tensor(9921.3896, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9921.388671875
tensor(9921.3896, grad_fn=<NegBackward0>) tensor(9921.3887, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9921.3876953125
tensor(9921.3887, grad_fn=<NegBackward0>) tensor(9921.3877, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9921.3896484375
tensor(9921.3877, grad_fn=<NegBackward0>) tensor(9921.3896, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9921.388671875
tensor(9921.3877, grad_fn=<NegBackward0>) tensor(9921.3887, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9921.3857421875
tensor(9921.3877, grad_fn=<NegBackward0>) tensor(9921.3857, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9921.38671875
tensor(9921.3857, grad_fn=<NegBackward0>) tensor(9921.3867, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9921.3857421875
tensor(9921.3857, grad_fn=<NegBackward0>) tensor(9921.3857, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9921.38671875
tensor(9921.3857, grad_fn=<NegBackward0>) tensor(9921.3867, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9921.384765625
tensor(9921.3857, grad_fn=<NegBackward0>) tensor(9921.3848, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9921.3857421875
tensor(9921.3848, grad_fn=<NegBackward0>) tensor(9921.3857, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9921.3837890625
tensor(9921.3848, grad_fn=<NegBackward0>) tensor(9921.3838, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9921.3837890625
tensor(9921.3838, grad_fn=<NegBackward0>) tensor(9921.3838, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9921.3828125
tensor(9921.3838, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9921.3837890625
tensor(9921.3828, grad_fn=<NegBackward0>) tensor(9921.3838, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9921.3818359375
tensor(9921.3828, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9921.3828125
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9921.3828125
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9921.3818359375
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9921.3818359375
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9921.380859375
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9921.3828125
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9921.3828125
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9921.380859375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9921.380859375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9921.380859375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9921.3798828125
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9921.37890625
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3789, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9921.3798828125
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9921.3798828125
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9921.3876953125
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3877, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9921.3798828125
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9921.3818359375
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[6.2268e-01, 3.7732e-01],
        [4.9135e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3562, 0.6438], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2393, 0.1560],
         [0.5687, 0.1257]],

        [[0.7257, 0.1602],
         [0.6917, 0.5193]],

        [[0.6791, 0.1543],
         [0.5056, 0.6704]],

        [[0.6045, 0.1638],
         [0.6957, 0.6753]],

        [[0.6317, 0.1251],
         [0.7268, 0.6524]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.03878787878787879
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.006336633663366337
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.02104340201665936
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.022626262626262626
Global Adjusted Rand Index: 0.02188463828244154
Average Adjusted Rand Index: 0.015925563710559056
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21516.291015625
inf tensor(21516.2910, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9940.0009765625
tensor(21516.2910, grad_fn=<NegBackward0>) tensor(9940.0010, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9937.3984375
tensor(9940.0010, grad_fn=<NegBackward0>) tensor(9937.3984, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9935.9697265625
tensor(9937.3984, grad_fn=<NegBackward0>) tensor(9935.9697, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9930.7646484375
tensor(9935.9697, grad_fn=<NegBackward0>) tensor(9930.7646, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9926.798828125
tensor(9930.7646, grad_fn=<NegBackward0>) tensor(9926.7988, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9924.0048828125
tensor(9926.7988, grad_fn=<NegBackward0>) tensor(9924.0049, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9922.8623046875
tensor(9924.0049, grad_fn=<NegBackward0>) tensor(9922.8623, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9921.78125
tensor(9922.8623, grad_fn=<NegBackward0>) tensor(9921.7812, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9921.5791015625
tensor(9921.7812, grad_fn=<NegBackward0>) tensor(9921.5791, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9921.48828125
tensor(9921.5791, grad_fn=<NegBackward0>) tensor(9921.4883, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9921.4423828125
tensor(9921.4883, grad_fn=<NegBackward0>) tensor(9921.4424, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9921.42578125
tensor(9921.4424, grad_fn=<NegBackward0>) tensor(9921.4258, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9921.4169921875
tensor(9921.4258, grad_fn=<NegBackward0>) tensor(9921.4170, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9921.41015625
tensor(9921.4170, grad_fn=<NegBackward0>) tensor(9921.4102, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9921.40625
tensor(9921.4102, grad_fn=<NegBackward0>) tensor(9921.4062, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9921.404296875
tensor(9921.4062, grad_fn=<NegBackward0>) tensor(9921.4043, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9921.3994140625
tensor(9921.4043, grad_fn=<NegBackward0>) tensor(9921.3994, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9921.3984375
tensor(9921.3994, grad_fn=<NegBackward0>) tensor(9921.3984, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9921.3955078125
tensor(9921.3984, grad_fn=<NegBackward0>) tensor(9921.3955, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9921.39453125
tensor(9921.3955, grad_fn=<NegBackward0>) tensor(9921.3945, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9921.392578125
tensor(9921.3945, grad_fn=<NegBackward0>) tensor(9921.3926, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9921.3916015625
tensor(9921.3926, grad_fn=<NegBackward0>) tensor(9921.3916, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9921.390625
tensor(9921.3916, grad_fn=<NegBackward0>) tensor(9921.3906, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9921.3896484375
tensor(9921.3906, grad_fn=<NegBackward0>) tensor(9921.3896, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9921.3876953125
tensor(9921.3896, grad_fn=<NegBackward0>) tensor(9921.3877, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9921.3876953125
tensor(9921.3877, grad_fn=<NegBackward0>) tensor(9921.3877, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9921.38671875
tensor(9921.3877, grad_fn=<NegBackward0>) tensor(9921.3867, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9921.38671875
tensor(9921.3867, grad_fn=<NegBackward0>) tensor(9921.3867, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9921.384765625
tensor(9921.3867, grad_fn=<NegBackward0>) tensor(9921.3848, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9921.384765625
tensor(9921.3848, grad_fn=<NegBackward0>) tensor(9921.3848, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9921.3837890625
tensor(9921.3848, grad_fn=<NegBackward0>) tensor(9921.3838, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9921.3828125
tensor(9921.3838, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9921.3828125
tensor(9921.3828, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9921.3818359375
tensor(9921.3828, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9921.3828125
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9921.3828125
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9921.3828125
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -9921.380859375
tensor(9921.3818, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9921.38671875
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3867, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9921.3818359375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -9921.380859375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9921.3818359375
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9921.3798828125
tensor(9921.3809, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9921.3828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3828, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9921.3818359375
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3818, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9921.380859375
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -9921.380859375
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9921.3955078125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3955, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9921.380859375
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3809, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9921.396484375
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3965, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9921.3798828125
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3799, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9921.37890625
tensor(9921.3799, grad_fn=<NegBackward0>) tensor(9921.3789, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9921.3779296875
tensor(9921.3789, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9921.37890625
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3789, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9921.37890625
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3789, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9921.3779296875
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9921.376953125
tensor(9921.3779, grad_fn=<NegBackward0>) tensor(9921.3770, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9921.3759765625
tensor(9921.3770, grad_fn=<NegBackward0>) tensor(9921.3760, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9921.3779296875
tensor(9921.3760, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9921.3779296875
tensor(9921.3760, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9921.3779296875
tensor(9921.3760, grad_fn=<NegBackward0>) tensor(9921.3779, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -9921.37890625
tensor(9921.3760, grad_fn=<NegBackward0>) tensor(9921.3789, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -9921.376953125
tensor(9921.3760, grad_fn=<NegBackward0>) tensor(9921.3770, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[6.2295e-01, 3.7705e-01],
        [2.0239e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3565, 0.6435], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2393, 0.1560],
         [0.5393, 0.1257]],

        [[0.7162, 0.1601],
         [0.6734, 0.5704]],

        [[0.5879, 0.1543],
         [0.6366, 0.5023]],

        [[0.5551, 0.1638],
         [0.5921, 0.6656]],

        [[0.6977, 0.1251],
         [0.6223, 0.5920]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.03878787878787879
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.006336633663366337
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.02104340201665936
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.022626262626262626
Global Adjusted Rand Index: 0.02188463828244154
Average Adjusted Rand Index: 0.015925563710559056
[0.02188463828244154, 0.02188463828244154] [0.015925563710559056, 0.015925563710559056] [9921.3818359375, 9921.376953125]
-------------------------------------
This iteration is 65
True Objective function: Loss = -10299.640521132842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21212.373046875
inf tensor(21212.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10194.6806640625
tensor(21212.3730, grad_fn=<NegBackward0>) tensor(10194.6807, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10193.8212890625
tensor(10194.6807, grad_fn=<NegBackward0>) tensor(10193.8213, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10193.578125
tensor(10193.8213, grad_fn=<NegBackward0>) tensor(10193.5781, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10193.44140625
tensor(10193.5781, grad_fn=<NegBackward0>) tensor(10193.4414, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10193.31640625
tensor(10193.4414, grad_fn=<NegBackward0>) tensor(10193.3164, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10193.177734375
tensor(10193.3164, grad_fn=<NegBackward0>) tensor(10193.1777, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10193.01171875
tensor(10193.1777, grad_fn=<NegBackward0>) tensor(10193.0117, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10192.81640625
tensor(10193.0117, grad_fn=<NegBackward0>) tensor(10192.8164, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10192.5947265625
tensor(10192.8164, grad_fn=<NegBackward0>) tensor(10192.5947, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10192.34375
tensor(10192.5947, grad_fn=<NegBackward0>) tensor(10192.3438, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10192.0615234375
tensor(10192.3438, grad_fn=<NegBackward0>) tensor(10192.0615, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10191.7421875
tensor(10192.0615, grad_fn=<NegBackward0>) tensor(10191.7422, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10191.3916015625
tensor(10191.7422, grad_fn=<NegBackward0>) tensor(10191.3916, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10191.0673828125
tensor(10191.3916, grad_fn=<NegBackward0>) tensor(10191.0674, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10190.8642578125
tensor(10191.0674, grad_fn=<NegBackward0>) tensor(10190.8643, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10190.7451171875
tensor(10190.8643, grad_fn=<NegBackward0>) tensor(10190.7451, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10190.673828125
tensor(10190.7451, grad_fn=<NegBackward0>) tensor(10190.6738, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10190.642578125
tensor(10190.6738, grad_fn=<NegBackward0>) tensor(10190.6426, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10190.623046875
tensor(10190.6426, grad_fn=<NegBackward0>) tensor(10190.6230, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10190.603515625
tensor(10190.6230, grad_fn=<NegBackward0>) tensor(10190.6035, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10190.5869140625
tensor(10190.6035, grad_fn=<NegBackward0>) tensor(10190.5869, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10190.578125
tensor(10190.5869, grad_fn=<NegBackward0>) tensor(10190.5781, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10190.57421875
tensor(10190.5781, grad_fn=<NegBackward0>) tensor(10190.5742, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10190.5712890625
tensor(10190.5742, grad_fn=<NegBackward0>) tensor(10190.5713, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10190.5703125
tensor(10190.5713, grad_fn=<NegBackward0>) tensor(10190.5703, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10190.568359375
tensor(10190.5703, grad_fn=<NegBackward0>) tensor(10190.5684, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10190.5673828125
tensor(10190.5684, grad_fn=<NegBackward0>) tensor(10190.5674, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10190.5673828125
tensor(10190.5674, grad_fn=<NegBackward0>) tensor(10190.5674, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10190.5654296875
tensor(10190.5674, grad_fn=<NegBackward0>) tensor(10190.5654, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10190.564453125
tensor(10190.5654, grad_fn=<NegBackward0>) tensor(10190.5645, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10190.5625
tensor(10190.5645, grad_fn=<NegBackward0>) tensor(10190.5625, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10190.560546875
tensor(10190.5625, grad_fn=<NegBackward0>) tensor(10190.5605, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10190.5576171875
tensor(10190.5605, grad_fn=<NegBackward0>) tensor(10190.5576, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10190.5546875
tensor(10190.5576, grad_fn=<NegBackward0>) tensor(10190.5547, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10190.55078125
tensor(10190.5547, grad_fn=<NegBackward0>) tensor(10190.5508, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10190.5439453125
tensor(10190.5508, grad_fn=<NegBackward0>) tensor(10190.5439, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10190.5361328125
tensor(10190.5439, grad_fn=<NegBackward0>) tensor(10190.5361, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10190.5234375
tensor(10190.5361, grad_fn=<NegBackward0>) tensor(10190.5234, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10190.5087890625
tensor(10190.5234, grad_fn=<NegBackward0>) tensor(10190.5088, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10190.4208984375
tensor(10190.5088, grad_fn=<NegBackward0>) tensor(10190.4209, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10186.4072265625
tensor(10190.4209, grad_fn=<NegBackward0>) tensor(10186.4072, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10166.3603515625
tensor(10186.4072, grad_fn=<NegBackward0>) tensor(10166.3604, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10165.125
tensor(10166.3604, grad_fn=<NegBackward0>) tensor(10165.1250, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10164.904296875
tensor(10165.1250, grad_fn=<NegBackward0>) tensor(10164.9043, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10164.8994140625
tensor(10164.9043, grad_fn=<NegBackward0>) tensor(10164.8994, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10164.89453125
tensor(10164.8994, grad_fn=<NegBackward0>) tensor(10164.8945, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10164.892578125
tensor(10164.8945, grad_fn=<NegBackward0>) tensor(10164.8926, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10164.8896484375
tensor(10164.8926, grad_fn=<NegBackward0>) tensor(10164.8896, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10164.8876953125
tensor(10164.8896, grad_fn=<NegBackward0>) tensor(10164.8877, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10164.8828125
tensor(10164.8877, grad_fn=<NegBackward0>) tensor(10164.8828, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10164.8798828125
tensor(10164.8828, grad_fn=<NegBackward0>) tensor(10164.8799, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10164.8779296875
tensor(10164.8799, grad_fn=<NegBackward0>) tensor(10164.8779, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10164.884765625
tensor(10164.8779, grad_fn=<NegBackward0>) tensor(10164.8848, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -10164.8779296875
tensor(10164.8779, grad_fn=<NegBackward0>) tensor(10164.8779, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10164.8759765625
tensor(10164.8779, grad_fn=<NegBackward0>) tensor(10164.8760, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10164.8740234375
tensor(10164.8760, grad_fn=<NegBackward0>) tensor(10164.8740, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10164.8740234375
tensor(10164.8740, grad_fn=<NegBackward0>) tensor(10164.8740, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10164.873046875
tensor(10164.8740, grad_fn=<NegBackward0>) tensor(10164.8730, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10164.873046875
tensor(10164.8730, grad_fn=<NegBackward0>) tensor(10164.8730, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10164.8759765625
tensor(10164.8730, grad_fn=<NegBackward0>) tensor(10164.8760, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10164.8720703125
tensor(10164.8730, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10164.8720703125
tensor(10164.8721, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10164.8720703125
tensor(10164.8721, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10164.8720703125
tensor(10164.8721, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10164.87109375
tensor(10164.8721, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10164.87109375
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10164.8720703125
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10164.87109375
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10164.8720703125
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8721, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10164.87109375
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10164.8740234375
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8740, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10164.8701171875
tensor(10164.8711, grad_fn=<NegBackward0>) tensor(10164.8701, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10164.87109375
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10164.8701171875
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8701, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10164.8701171875
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8701, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10164.87109375
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10164.87109375
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10164.87109375
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10165.01953125
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10165.0195, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -10164.87109375
tensor(10164.8701, grad_fn=<NegBackward0>) tensor(10164.8711, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.8769, 0.1231],
        [0.0674, 0.9326]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6986, 0.3014], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1855, 0.1017],
         [0.5582, 0.1558]],

        [[0.5196, 0.1107],
         [0.5525, 0.6097]],

        [[0.7115, 0.0966],
         [0.5075, 0.5586]],

        [[0.6449, 0.0973],
         [0.6742, 0.6060]],

        [[0.5481, 0.1276],
         [0.6656, 0.7284]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.1528117359413203
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.6970427238613282
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 14
Adjusted Rand Index: 0.5136203918066116
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 16
Adjusted Rand Index: 0.45696969696969697
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 27
Adjusted Rand Index: 0.20363636363636364
Global Adjusted Rand Index: 0.38295880052817366
Average Adjusted Rand Index: 0.40481618244306417
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21713.77734375
inf tensor(21713.7773, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10194.8193359375
tensor(21713.7773, grad_fn=<NegBackward0>) tensor(10194.8193, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10193.728515625
tensor(10194.8193, grad_fn=<NegBackward0>) tensor(10193.7285, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10193.486328125
tensor(10193.7285, grad_fn=<NegBackward0>) tensor(10193.4863, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10193.3603515625
tensor(10193.4863, grad_fn=<NegBackward0>) tensor(10193.3604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10193.2529296875
tensor(10193.3604, grad_fn=<NegBackward0>) tensor(10193.2529, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10193.146484375
tensor(10193.2529, grad_fn=<NegBackward0>) tensor(10193.1465, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10193.0263671875
tensor(10193.1465, grad_fn=<NegBackward0>) tensor(10193.0264, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10192.884765625
tensor(10193.0264, grad_fn=<NegBackward0>) tensor(10192.8848, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10192.7099609375
tensor(10192.8848, grad_fn=<NegBackward0>) tensor(10192.7100, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10192.4951171875
tensor(10192.7100, grad_fn=<NegBackward0>) tensor(10192.4951, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10192.2451171875
tensor(10192.4951, grad_fn=<NegBackward0>) tensor(10192.2451, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10191.9638671875
tensor(10192.2451, grad_fn=<NegBackward0>) tensor(10191.9639, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10191.662109375
tensor(10191.9639, grad_fn=<NegBackward0>) tensor(10191.6621, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10191.375
tensor(10191.6621, grad_fn=<NegBackward0>) tensor(10191.3750, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10191.1416015625
tensor(10191.3750, grad_fn=<NegBackward0>) tensor(10191.1416, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10190.9638671875
tensor(10191.1416, grad_fn=<NegBackward0>) tensor(10190.9639, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10190.8212890625
tensor(10190.9639, grad_fn=<NegBackward0>) tensor(10190.8213, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10190.7333984375
tensor(10190.8213, grad_fn=<NegBackward0>) tensor(10190.7334, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10190.689453125
tensor(10190.7334, grad_fn=<NegBackward0>) tensor(10190.6895, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10190.669921875
tensor(10190.6895, grad_fn=<NegBackward0>) tensor(10190.6699, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10190.6611328125
tensor(10190.6699, grad_fn=<NegBackward0>) tensor(10190.6611, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10190.6552734375
tensor(10190.6611, grad_fn=<NegBackward0>) tensor(10190.6553, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10190.6513671875
tensor(10190.6553, grad_fn=<NegBackward0>) tensor(10190.6514, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10190.650390625
tensor(10190.6514, grad_fn=<NegBackward0>) tensor(10190.6504, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10190.650390625
tensor(10190.6504, grad_fn=<NegBackward0>) tensor(10190.6504, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10190.6494140625
tensor(10190.6504, grad_fn=<NegBackward0>) tensor(10190.6494, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10190.6474609375
tensor(10190.6494, grad_fn=<NegBackward0>) tensor(10190.6475, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10190.6474609375
tensor(10190.6475, grad_fn=<NegBackward0>) tensor(10190.6475, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10190.6474609375
tensor(10190.6475, grad_fn=<NegBackward0>) tensor(10190.6475, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10190.646484375
tensor(10190.6475, grad_fn=<NegBackward0>) tensor(10190.6465, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10190.6435546875
tensor(10190.6465, grad_fn=<NegBackward0>) tensor(10190.6436, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10190.6435546875
tensor(10190.6436, grad_fn=<NegBackward0>) tensor(10190.6436, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10190.6416015625
tensor(10190.6436, grad_fn=<NegBackward0>) tensor(10190.6416, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10190.63671875
tensor(10190.6416, grad_fn=<NegBackward0>) tensor(10190.6367, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10190.6298828125
tensor(10190.6367, grad_fn=<NegBackward0>) tensor(10190.6299, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10190.6142578125
tensor(10190.6299, grad_fn=<NegBackward0>) tensor(10190.6143, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10190.552734375
tensor(10190.6143, grad_fn=<NegBackward0>) tensor(10190.5527, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10190.3671875
tensor(10190.5527, grad_fn=<NegBackward0>) tensor(10190.3672, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10190.3564453125
tensor(10190.3672, grad_fn=<NegBackward0>) tensor(10190.3564, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10190.3505859375
tensor(10190.3564, grad_fn=<NegBackward0>) tensor(10190.3506, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10190.34765625
tensor(10190.3506, grad_fn=<NegBackward0>) tensor(10190.3477, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10190.345703125
tensor(10190.3477, grad_fn=<NegBackward0>) tensor(10190.3457, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10190.341796875
tensor(10190.3457, grad_fn=<NegBackward0>) tensor(10190.3418, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10190.3466796875
tensor(10190.3418, grad_fn=<NegBackward0>) tensor(10190.3467, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10190.3408203125
tensor(10190.3418, grad_fn=<NegBackward0>) tensor(10190.3408, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10190.34375
tensor(10190.3408, grad_fn=<NegBackward0>) tensor(10190.3438, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10190.3388671875
tensor(10190.3408, grad_fn=<NegBackward0>) tensor(10190.3389, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10190.3388671875
tensor(10190.3389, grad_fn=<NegBackward0>) tensor(10190.3389, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10190.337890625
tensor(10190.3389, grad_fn=<NegBackward0>) tensor(10190.3379, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10190.337890625
tensor(10190.3379, grad_fn=<NegBackward0>) tensor(10190.3379, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10190.3408203125
tensor(10190.3379, grad_fn=<NegBackward0>) tensor(10190.3408, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10190.3359375
tensor(10190.3379, grad_fn=<NegBackward0>) tensor(10190.3359, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10190.3359375
tensor(10190.3359, grad_fn=<NegBackward0>) tensor(10190.3359, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10190.3349609375
tensor(10190.3359, grad_fn=<NegBackward0>) tensor(10190.3350, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10190.3349609375
tensor(10190.3350, grad_fn=<NegBackward0>) tensor(10190.3350, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10190.380859375
tensor(10190.3350, grad_fn=<NegBackward0>) tensor(10190.3809, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10190.333984375
tensor(10190.3350, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10190.333984375
tensor(10190.3340, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10190.333984375
tensor(10190.3340, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10190.3330078125
tensor(10190.3340, grad_fn=<NegBackward0>) tensor(10190.3330, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10190.333984375
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10190.3330078125
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3330, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10190.3330078125
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3330, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10190.333984375
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10190.3330078125
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3330, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10190.3310546875
tensor(10190.3330, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10190.3330078125
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3330, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10190.33203125
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3320, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10190.333984375
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3340, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10190.3310546875
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10190.33203125
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3320, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10190.33203125
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3320, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -10190.3310546875
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10190.3310546875
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10190.330078125
tensor(10190.3311, grad_fn=<NegBackward0>) tensor(10190.3301, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10190.3515625
tensor(10190.3301, grad_fn=<NegBackward0>) tensor(10190.3516, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10190.3310546875
tensor(10190.3301, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -10190.3896484375
tensor(10190.3301, grad_fn=<NegBackward0>) tensor(10190.3896, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -10190.3310546875
tensor(10190.3301, grad_fn=<NegBackward0>) tensor(10190.3311, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -10190.337890625
tensor(10190.3301, grad_fn=<NegBackward0>) tensor(10190.3379, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.0030, 0.9970],
        [0.2614, 0.7386]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0201, 0.9799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1725, 0.0756],
         [0.5737, 0.1393]],

        [[0.7019, 0.1659],
         [0.5208, 0.5961]],

        [[0.5619, 0.1494],
         [0.7196, 0.6986]],

        [[0.5740, 0.1490],
         [0.6575, 0.6817]],

        [[0.5684, 0.1462],
         [0.6699, 0.5422]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 65
Adjusted Rand Index: -0.03827662261580155
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002503066891692963
Average Adjusted Rand Index: -0.00765532452316031
[0.38295880052817366, -0.002503066891692963] [0.40481618244306417, -0.00765532452316031] [10164.87109375, 10190.337890625]
-------------------------------------
This iteration is 66
True Objective function: Loss = -10069.112910573027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20799.96484375
inf tensor(20799.9648, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9985.892578125
tensor(20799.9648, grad_fn=<NegBackward0>) tensor(9985.8926, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9982.9736328125
tensor(9985.8926, grad_fn=<NegBackward0>) tensor(9982.9736, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9978.98828125
tensor(9982.9736, grad_fn=<NegBackward0>) tensor(9978.9883, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9976.591796875
tensor(9978.9883, grad_fn=<NegBackward0>) tensor(9976.5918, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9975.63671875
tensor(9976.5918, grad_fn=<NegBackward0>) tensor(9975.6367, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9974.84765625
tensor(9975.6367, grad_fn=<NegBackward0>) tensor(9974.8477, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9974.15234375
tensor(9974.8477, grad_fn=<NegBackward0>) tensor(9974.1523, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9973.546875
tensor(9974.1523, grad_fn=<NegBackward0>) tensor(9973.5469, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9972.9384765625
tensor(9973.5469, grad_fn=<NegBackward0>) tensor(9972.9385, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9972.400390625
tensor(9972.9385, grad_fn=<NegBackward0>) tensor(9972.4004, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9971.40625
tensor(9972.4004, grad_fn=<NegBackward0>) tensor(9971.4062, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9965.677734375
tensor(9971.4062, grad_fn=<NegBackward0>) tensor(9965.6777, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9958.591796875
tensor(9965.6777, grad_fn=<NegBackward0>) tensor(9958.5918, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9957.349609375
tensor(9958.5918, grad_fn=<NegBackward0>) tensor(9957.3496, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9957.228515625
tensor(9957.3496, grad_fn=<NegBackward0>) tensor(9957.2285, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9957.18359375
tensor(9957.2285, grad_fn=<NegBackward0>) tensor(9957.1836, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9957.169921875
tensor(9957.1836, grad_fn=<NegBackward0>) tensor(9957.1699, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9957.1611328125
tensor(9957.1699, grad_fn=<NegBackward0>) tensor(9957.1611, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9957.15625
tensor(9957.1611, grad_fn=<NegBackward0>) tensor(9957.1562, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9957.15234375
tensor(9957.1562, grad_fn=<NegBackward0>) tensor(9957.1523, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9957.1494140625
tensor(9957.1523, grad_fn=<NegBackward0>) tensor(9957.1494, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9957.146484375
tensor(9957.1494, grad_fn=<NegBackward0>) tensor(9957.1465, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9957.1455078125
tensor(9957.1465, grad_fn=<NegBackward0>) tensor(9957.1455, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9957.142578125
tensor(9957.1455, grad_fn=<NegBackward0>) tensor(9957.1426, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9957.1396484375
tensor(9957.1426, grad_fn=<NegBackward0>) tensor(9957.1396, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9957.1357421875
tensor(9957.1396, grad_fn=<NegBackward0>) tensor(9957.1357, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9957.138671875
tensor(9957.1357, grad_fn=<NegBackward0>) tensor(9957.1387, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9957.1318359375
tensor(9957.1357, grad_fn=<NegBackward0>) tensor(9957.1318, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9957.12890625
tensor(9957.1318, grad_fn=<NegBackward0>) tensor(9957.1289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9957.125
tensor(9957.1289, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9957.125
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9957.125
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9957.125
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9957.125
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9957.1357421875
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1357, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9957.1240234375
tensor(9957.1250, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9957.125
tensor(9957.1240, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9957.125
tensor(9957.1240, grad_fn=<NegBackward0>) tensor(9957.1250, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9957.123046875
tensor(9957.1240, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9957.1240234375
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9957.123046875
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9957.123046875
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9957.1240234375
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9957.123046875
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9957.1240234375
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9957.123046875
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9957.134765625
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1348, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9957.123046875
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9957.1396484375
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1396, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9957.1220703125
tensor(9957.1230, grad_fn=<NegBackward0>) tensor(9957.1221, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9957.123046875
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9957.123046875
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9957.1220703125
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1221, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9957.123046875
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9957.1220703125
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1221, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9957.1220703125
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1221, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9957.1240234375
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9957.1220703125
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1221, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9957.12109375
tensor(9957.1221, grad_fn=<NegBackward0>) tensor(9957.1211, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9957.12109375
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1211, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9957.126953125
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1270, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9957.1279296875
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1279, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9957.1328125
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1328, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -9957.123046875
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1230, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -9957.1240234375
tensor(9957.1211, grad_fn=<NegBackward0>) tensor(9957.1240, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.8240, 0.1760],
        [0.1967, 0.8033]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5429, 0.4571], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1396, 0.1367],
         [0.6125, 0.2137]],

        [[0.5121, 0.0992],
         [0.5277, 0.7279]],

        [[0.5756, 0.0866],
         [0.5986, 0.6398]],

        [[0.5264, 0.1099],
         [0.6576, 0.6044]],

        [[0.5555, 0.1001],
         [0.7215, 0.6995]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.1855969351180473
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.4848060960754898
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
time is 3
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 86
Adjusted Rand Index: 0.5135048563803448
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 75
Adjusted Rand Index: 0.24242424242424243
Global Adjusted Rand Index: 0.4084160671677032
Average Adjusted Rand Index: 0.42578794996904906
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22162.3984375
inf tensor(22162.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9985.37109375
tensor(22162.3984, grad_fn=<NegBackward0>) tensor(9985.3711, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9982.9677734375
tensor(9985.3711, grad_fn=<NegBackward0>) tensor(9982.9678, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9982.2158203125
tensor(9982.9678, grad_fn=<NegBackward0>) tensor(9982.2158, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9981.6904296875
tensor(9982.2158, grad_fn=<NegBackward0>) tensor(9981.6904, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9980.94921875
tensor(9981.6904, grad_fn=<NegBackward0>) tensor(9980.9492, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9980.0361328125
tensor(9980.9492, grad_fn=<NegBackward0>) tensor(9980.0361, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9979.5595703125
tensor(9980.0361, grad_fn=<NegBackward0>) tensor(9979.5596, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9979.314453125
tensor(9979.5596, grad_fn=<NegBackward0>) tensor(9979.3145, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9979.1611328125
tensor(9979.3145, grad_fn=<NegBackward0>) tensor(9979.1611, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9979.06640625
tensor(9979.1611, grad_fn=<NegBackward0>) tensor(9979.0664, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9979.0087890625
tensor(9979.0664, grad_fn=<NegBackward0>) tensor(9979.0088, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9978.9755859375
tensor(9979.0088, grad_fn=<NegBackward0>) tensor(9978.9756, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9978.9560546875
tensor(9978.9756, grad_fn=<NegBackward0>) tensor(9978.9561, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9978.9453125
tensor(9978.9561, grad_fn=<NegBackward0>) tensor(9978.9453, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9978.9384765625
tensor(9978.9453, grad_fn=<NegBackward0>) tensor(9978.9385, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9978.93359375
tensor(9978.9385, grad_fn=<NegBackward0>) tensor(9978.9336, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9978.9287109375
tensor(9978.9336, grad_fn=<NegBackward0>) tensor(9978.9287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9978.92578125
tensor(9978.9287, grad_fn=<NegBackward0>) tensor(9978.9258, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9978.92578125
tensor(9978.9258, grad_fn=<NegBackward0>) tensor(9978.9258, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9978.921875
tensor(9978.9258, grad_fn=<NegBackward0>) tensor(9978.9219, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9978.919921875
tensor(9978.9219, grad_fn=<NegBackward0>) tensor(9978.9199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9978.919921875
tensor(9978.9199, grad_fn=<NegBackward0>) tensor(9978.9199, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9978.9150390625
tensor(9978.9199, grad_fn=<NegBackward0>) tensor(9978.9150, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9978.9140625
tensor(9978.9150, grad_fn=<NegBackward0>) tensor(9978.9141, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9978.9140625
tensor(9978.9141, grad_fn=<NegBackward0>) tensor(9978.9141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9978.9140625
tensor(9978.9141, grad_fn=<NegBackward0>) tensor(9978.9141, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9978.912109375
tensor(9978.9141, grad_fn=<NegBackward0>) tensor(9978.9121, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9978.9111328125
tensor(9978.9121, grad_fn=<NegBackward0>) tensor(9978.9111, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9978.912109375
tensor(9978.9111, grad_fn=<NegBackward0>) tensor(9978.9121, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9978.9111328125
tensor(9978.9111, grad_fn=<NegBackward0>) tensor(9978.9111, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9978.908203125
tensor(9978.9111, grad_fn=<NegBackward0>) tensor(9978.9082, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9978.9091796875
tensor(9978.9082, grad_fn=<NegBackward0>) tensor(9978.9092, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9978.9072265625
tensor(9978.9082, grad_fn=<NegBackward0>) tensor(9978.9072, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9978.90625
tensor(9978.9072, grad_fn=<NegBackward0>) tensor(9978.9062, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9978.9072265625
tensor(9978.9062, grad_fn=<NegBackward0>) tensor(9978.9072, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9978.904296875
tensor(9978.9062, grad_fn=<NegBackward0>) tensor(9978.9043, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9978.9072265625
tensor(9978.9043, grad_fn=<NegBackward0>) tensor(9978.9072, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9978.9052734375
tensor(9978.9043, grad_fn=<NegBackward0>) tensor(9978.9053, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9978.9033203125
tensor(9978.9043, grad_fn=<NegBackward0>) tensor(9978.9033, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9978.904296875
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9043, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9978.9033203125
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9033, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9978.904296875
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9043, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9978.9033203125
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9033, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9978.9052734375
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9053, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9978.90234375
tensor(9978.9033, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9978.90234375
tensor(9978.9023, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9978.90234375
tensor(9978.9023, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9978.90234375
tensor(9978.9023, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9978.90234375
tensor(9978.9023, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9978.9013671875
tensor(9978.9023, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9978.9013671875
tensor(9978.9014, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9978.9013671875
tensor(9978.9014, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9978.90234375
tensor(9978.9014, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9978.900390625
tensor(9978.9014, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9978.900390625
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9978.9013671875
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9978.900390625
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9978.9013671875
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9978.9013671875
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9978.900390625
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9978.90234375
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9023, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9978.900390625
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9978.9013671875
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.9014, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9978.8994140625
tensor(9978.9004, grad_fn=<NegBackward0>) tensor(9978.8994, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9978.8984375
tensor(9978.8994, grad_fn=<NegBackward0>) tensor(9978.8984, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9978.900390625
tensor(9978.8984, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9978.8994140625
tensor(9978.8984, grad_fn=<NegBackward0>) tensor(9978.8994, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -9978.8994140625
tensor(9978.8984, grad_fn=<NegBackward0>) tensor(9978.8994, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -9978.900390625
tensor(9978.8984, grad_fn=<NegBackward0>) tensor(9978.9004, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -9978.8994140625
tensor(9978.8984, grad_fn=<NegBackward0>) tensor(9978.8994, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[6.8661e-04, 9.9931e-01],
        [3.9375e-02, 9.6062e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2517, 0.7483], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.1623],
         [0.5648, 0.1373]],

        [[0.7007, 0.0562],
         [0.5769, 0.5616]],

        [[0.5281, 0.1021],
         [0.5906, 0.5704]],

        [[0.6155, 0.2029],
         [0.5980, 0.6285]],

        [[0.7292, 0.1100],
         [0.5592, 0.5872]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.030334812424364664
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001451180515704904
Average Adjusted Rand Index: -0.005325066578554487
[0.4084160671677032, -0.001451180515704904] [0.42578794996904906, -0.005325066578554487] [9957.1240234375, 9978.8994140625]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10077.210464203867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22629.875
inf tensor(22629.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9921.8779296875
tensor(22629.8750, grad_fn=<NegBackward0>) tensor(9921.8779, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9920.7041015625
tensor(9921.8779, grad_fn=<NegBackward0>) tensor(9920.7041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9920.5439453125
tensor(9920.7041, grad_fn=<NegBackward0>) tensor(9920.5439, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9920.365234375
tensor(9920.5439, grad_fn=<NegBackward0>) tensor(9920.3652, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9920.0087890625
tensor(9920.3652, grad_fn=<NegBackward0>) tensor(9920.0088, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9919.48046875
tensor(9920.0088, grad_fn=<NegBackward0>) tensor(9919.4805, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9919.119140625
tensor(9919.4805, grad_fn=<NegBackward0>) tensor(9919.1191, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9918.931640625
tensor(9919.1191, grad_fn=<NegBackward0>) tensor(9918.9316, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9918.828125
tensor(9918.9316, grad_fn=<NegBackward0>) tensor(9918.8281, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9918.765625
tensor(9918.8281, grad_fn=<NegBackward0>) tensor(9918.7656, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9918.7275390625
tensor(9918.7656, grad_fn=<NegBackward0>) tensor(9918.7275, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9918.701171875
tensor(9918.7275, grad_fn=<NegBackward0>) tensor(9918.7012, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9918.6845703125
tensor(9918.7012, grad_fn=<NegBackward0>) tensor(9918.6846, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9918.6708984375
tensor(9918.6846, grad_fn=<NegBackward0>) tensor(9918.6709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9918.662109375
tensor(9918.6709, grad_fn=<NegBackward0>) tensor(9918.6621, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9918.65625
tensor(9918.6621, grad_fn=<NegBackward0>) tensor(9918.6562, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9918.6494140625
tensor(9918.6562, grad_fn=<NegBackward0>) tensor(9918.6494, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9918.6455078125
tensor(9918.6494, grad_fn=<NegBackward0>) tensor(9918.6455, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9918.642578125
tensor(9918.6455, grad_fn=<NegBackward0>) tensor(9918.6426, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9918.6396484375
tensor(9918.6426, grad_fn=<NegBackward0>) tensor(9918.6396, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9918.634765625
tensor(9918.6396, grad_fn=<NegBackward0>) tensor(9918.6348, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9918.626953125
tensor(9918.6348, grad_fn=<NegBackward0>) tensor(9918.6270, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9918.58203125
tensor(9918.6270, grad_fn=<NegBackward0>) tensor(9918.5820, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9917.3798828125
tensor(9918.5820, grad_fn=<NegBackward0>) tensor(9917.3799, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9916.9853515625
tensor(9917.3799, grad_fn=<NegBackward0>) tensor(9916.9854, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9916.8662109375
tensor(9916.9854, grad_fn=<NegBackward0>) tensor(9916.8662, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9916.8115234375
tensor(9916.8662, grad_fn=<NegBackward0>) tensor(9916.8115, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9916.7734375
tensor(9916.8115, grad_fn=<NegBackward0>) tensor(9916.7734, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9916.755859375
tensor(9916.7734, grad_fn=<NegBackward0>) tensor(9916.7559, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9916.744140625
tensor(9916.7559, grad_fn=<NegBackward0>) tensor(9916.7441, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9916.7353515625
tensor(9916.7441, grad_fn=<NegBackward0>) tensor(9916.7354, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9916.73046875
tensor(9916.7354, grad_fn=<NegBackward0>) tensor(9916.7305, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9916.7265625
tensor(9916.7305, grad_fn=<NegBackward0>) tensor(9916.7266, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9916.72265625
tensor(9916.7266, grad_fn=<NegBackward0>) tensor(9916.7227, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9916.7177734375
tensor(9916.7227, grad_fn=<NegBackward0>) tensor(9916.7178, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9916.7138671875
tensor(9916.7178, grad_fn=<NegBackward0>) tensor(9916.7139, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9916.70703125
tensor(9916.7139, grad_fn=<NegBackward0>) tensor(9916.7070, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9916.7041015625
tensor(9916.7070, grad_fn=<NegBackward0>) tensor(9916.7041, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9916.7041015625
tensor(9916.7041, grad_fn=<NegBackward0>) tensor(9916.7041, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9916.7021484375
tensor(9916.7041, grad_fn=<NegBackward0>) tensor(9916.7021, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9916.701171875
tensor(9916.7021, grad_fn=<NegBackward0>) tensor(9916.7012, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9916.7001953125
tensor(9916.7012, grad_fn=<NegBackward0>) tensor(9916.7002, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9916.69921875
tensor(9916.7002, grad_fn=<NegBackward0>) tensor(9916.6992, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9916.697265625
tensor(9916.6992, grad_fn=<NegBackward0>) tensor(9916.6973, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9916.6962890625
tensor(9916.6973, grad_fn=<NegBackward0>) tensor(9916.6963, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9916.6962890625
tensor(9916.6963, grad_fn=<NegBackward0>) tensor(9916.6963, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9916.6943359375
tensor(9916.6963, grad_fn=<NegBackward0>) tensor(9916.6943, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9916.697265625
tensor(9916.6943, grad_fn=<NegBackward0>) tensor(9916.6973, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9916.6943359375
tensor(9916.6943, grad_fn=<NegBackward0>) tensor(9916.6943, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9916.6943359375
tensor(9916.6943, grad_fn=<NegBackward0>) tensor(9916.6943, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9916.693359375
tensor(9916.6943, grad_fn=<NegBackward0>) tensor(9916.6934, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9916.6923828125
tensor(9916.6934, grad_fn=<NegBackward0>) tensor(9916.6924, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9916.6923828125
tensor(9916.6924, grad_fn=<NegBackward0>) tensor(9916.6924, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9916.6923828125
tensor(9916.6924, grad_fn=<NegBackward0>) tensor(9916.6924, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9916.6923828125
tensor(9916.6924, grad_fn=<NegBackward0>) tensor(9916.6924, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9916.6904296875
tensor(9916.6924, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9916.6904296875
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9916.69140625
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6914, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9916.6904296875
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9916.6904296875
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9916.6904296875
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9916.6923828125
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6924, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9916.6904296875
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9916.689453125
tensor(9916.6904, grad_fn=<NegBackward0>) tensor(9916.6895, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9916.689453125
tensor(9916.6895, grad_fn=<NegBackward0>) tensor(9916.6895, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9916.6884765625
tensor(9916.6895, grad_fn=<NegBackward0>) tensor(9916.6885, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9916.6904296875
tensor(9916.6885, grad_fn=<NegBackward0>) tensor(9916.6904, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9916.6875
tensor(9916.6885, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9916.689453125
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6895, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9916.6884765625
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6885, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -9916.6875
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9916.6875
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9916.6875
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9916.6865234375
tensor(9916.6875, grad_fn=<NegBackward0>) tensor(9916.6865, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9916.6875
tensor(9916.6865, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9916.685546875
tensor(9916.6865, grad_fn=<NegBackward0>) tensor(9916.6855, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9916.6875
tensor(9916.6855, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9916.6875
tensor(9916.6855, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9916.6875
tensor(9916.6855, grad_fn=<NegBackward0>) tensor(9916.6875, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9916.6884765625
tensor(9916.6855, grad_fn=<NegBackward0>) tensor(9916.6885, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -9916.6865234375
tensor(9916.6855, grad_fn=<NegBackward0>) tensor(9916.6865, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[9.9999e-01, 7.1783e-06],
        [2.1333e-01, 7.8667e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9370, 0.0630], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1372, 0.1670],
         [0.5218, 0.1757]],

        [[0.6122, 0.1953],
         [0.7246, 0.6789]],

        [[0.5563, 0.1222],
         [0.6737, 0.6026]],

        [[0.6155, 0.1192],
         [0.5719, 0.5004]],

        [[0.6339, 0.0480],
         [0.6062, 0.7022]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0013298787015871155
Average Adjusted Rand Index: -0.0002802203228346845
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25431.638671875
inf tensor(25431.6387, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9924.6611328125
tensor(25431.6387, grad_fn=<NegBackward0>) tensor(9924.6611, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9921.95703125
tensor(9924.6611, grad_fn=<NegBackward0>) tensor(9921.9570, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9921.0283203125
tensor(9921.9570, grad_fn=<NegBackward0>) tensor(9921.0283, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9920.6171875
tensor(9921.0283, grad_fn=<NegBackward0>) tensor(9920.6172, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9920.365234375
tensor(9920.6172, grad_fn=<NegBackward0>) tensor(9920.3652, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9920.1748046875
tensor(9920.3652, grad_fn=<NegBackward0>) tensor(9920.1748, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9920.021484375
tensor(9920.1748, grad_fn=<NegBackward0>) tensor(9920.0215, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9919.892578125
tensor(9920.0215, grad_fn=<NegBackward0>) tensor(9919.8926, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9919.7783203125
tensor(9919.8926, grad_fn=<NegBackward0>) tensor(9919.7783, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9919.6748046875
tensor(9919.7783, grad_fn=<NegBackward0>) tensor(9919.6748, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9919.5849609375
tensor(9919.6748, grad_fn=<NegBackward0>) tensor(9919.5850, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9919.5029296875
tensor(9919.5850, grad_fn=<NegBackward0>) tensor(9919.5029, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9919.4306640625
tensor(9919.5029, grad_fn=<NegBackward0>) tensor(9919.4307, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9919.3671875
tensor(9919.4307, grad_fn=<NegBackward0>) tensor(9919.3672, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9919.310546875
tensor(9919.3672, grad_fn=<NegBackward0>) tensor(9919.3105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9919.2548828125
tensor(9919.3105, grad_fn=<NegBackward0>) tensor(9919.2549, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9919.18359375
tensor(9919.2549, grad_fn=<NegBackward0>) tensor(9919.1836, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9919.0625
tensor(9919.1836, grad_fn=<NegBackward0>) tensor(9919.0625, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9918.849609375
tensor(9919.0625, grad_fn=<NegBackward0>) tensor(9918.8496, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9918.6796875
tensor(9918.8496, grad_fn=<NegBackward0>) tensor(9918.6797, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9918.578125
tensor(9918.6797, grad_fn=<NegBackward0>) tensor(9918.5781, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9918.509765625
tensor(9918.5781, grad_fn=<NegBackward0>) tensor(9918.5098, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9918.4619140625
tensor(9918.5098, grad_fn=<NegBackward0>) tensor(9918.4619, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9918.4248046875
tensor(9918.4619, grad_fn=<NegBackward0>) tensor(9918.4248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9918.396484375
tensor(9918.4248, grad_fn=<NegBackward0>) tensor(9918.3965, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9918.3759765625
tensor(9918.3965, grad_fn=<NegBackward0>) tensor(9918.3760, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9918.3603515625
tensor(9918.3760, grad_fn=<NegBackward0>) tensor(9918.3604, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9918.345703125
tensor(9918.3604, grad_fn=<NegBackward0>) tensor(9918.3457, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9918.3330078125
tensor(9918.3457, grad_fn=<NegBackward0>) tensor(9918.3330, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9918.3212890625
tensor(9918.3330, grad_fn=<NegBackward0>) tensor(9918.3213, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9918.310546875
tensor(9918.3213, grad_fn=<NegBackward0>) tensor(9918.3105, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9918.302734375
tensor(9918.3105, grad_fn=<NegBackward0>) tensor(9918.3027, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9918.296875
tensor(9918.3027, grad_fn=<NegBackward0>) tensor(9918.2969, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9918.2900390625
tensor(9918.2969, grad_fn=<NegBackward0>) tensor(9918.2900, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9918.2880859375
tensor(9918.2900, grad_fn=<NegBackward0>) tensor(9918.2881, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9918.2841796875
tensor(9918.2881, grad_fn=<NegBackward0>) tensor(9918.2842, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9918.283203125
tensor(9918.2842, grad_fn=<NegBackward0>) tensor(9918.2832, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9918.2822265625
tensor(9918.2832, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9918.283203125
tensor(9918.2822, grad_fn=<NegBackward0>) tensor(9918.2832, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9918.2822265625
tensor(9918.2822, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9918.28125
tensor(9918.2822, grad_fn=<NegBackward0>) tensor(9918.2812, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9918.28125
tensor(9918.2812, grad_fn=<NegBackward0>) tensor(9918.2812, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9918.28125
tensor(9918.2812, grad_fn=<NegBackward0>) tensor(9918.2812, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9918.283203125
tensor(9918.2812, grad_fn=<NegBackward0>) tensor(9918.2832, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9918.2822265625
tensor(9918.2812, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9918.2802734375
tensor(9918.2812, grad_fn=<NegBackward0>) tensor(9918.2803, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9918.2802734375
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2803, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9918.2822265625
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9918.2822265625
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9918.2822265625
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -9918.28125
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2812, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -9918.2822265625
tensor(9918.2803, grad_fn=<NegBackward0>) tensor(9918.2822, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.8599, 0.1401],
        [0.7803, 0.2197]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7728, 0.2272], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1264, 0.1576],
         [0.5509, 0.2033]],

        [[0.6920, 0.1702],
         [0.6900, 0.6932]],

        [[0.6202, 0.1472],
         [0.6691, 0.5336]],

        [[0.6358, 0.1500],
         [0.6723, 0.5570]],

        [[0.6913, 0.1690],
         [0.5281, 0.6976]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.011460342675592873
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: 0.012179060144002517
Average Adjusted Rand Index: 0.009513600261240254
[0.0013298787015871155, 0.012179060144002517] [-0.0002802203228346845, 0.009513600261240254] [9916.6865234375, 9918.2822265625]
-------------------------------------
This iteration is 68
True Objective function: Loss = -9997.17099980866
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20231.841796875
inf tensor(20231.8418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9902.7451171875
tensor(20231.8418, grad_fn=<NegBackward0>) tensor(9902.7451, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9901.98046875
tensor(9902.7451, grad_fn=<NegBackward0>) tensor(9901.9805, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9901.7392578125
tensor(9901.9805, grad_fn=<NegBackward0>) tensor(9901.7393, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9901.4765625
tensor(9901.7393, grad_fn=<NegBackward0>) tensor(9901.4766, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9901.2412109375
tensor(9901.4766, grad_fn=<NegBackward0>) tensor(9901.2412, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9901.0283203125
tensor(9901.2412, grad_fn=<NegBackward0>) tensor(9901.0283, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9900.81640625
tensor(9901.0283, grad_fn=<NegBackward0>) tensor(9900.8164, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9900.591796875
tensor(9900.8164, grad_fn=<NegBackward0>) tensor(9900.5918, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9900.2998046875
tensor(9900.5918, grad_fn=<NegBackward0>) tensor(9900.2998, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9899.955078125
tensor(9900.2998, grad_fn=<NegBackward0>) tensor(9899.9551, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9899.408203125
tensor(9899.9551, grad_fn=<NegBackward0>) tensor(9899.4082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9897.59375
tensor(9899.4082, grad_fn=<NegBackward0>) tensor(9897.5938, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9896.4091796875
tensor(9897.5938, grad_fn=<NegBackward0>) tensor(9896.4092, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9895.4736328125
tensor(9896.4092, grad_fn=<NegBackward0>) tensor(9895.4736, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9894.6435546875
tensor(9895.4736, grad_fn=<NegBackward0>) tensor(9894.6436, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9893.48828125
tensor(9894.6436, grad_fn=<NegBackward0>) tensor(9893.4883, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9891.5068359375
tensor(9893.4883, grad_fn=<NegBackward0>) tensor(9891.5068, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9890.0673828125
tensor(9891.5068, grad_fn=<NegBackward0>) tensor(9890.0674, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9889.896484375
tensor(9890.0674, grad_fn=<NegBackward0>) tensor(9889.8965, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9889.8603515625
tensor(9889.8965, grad_fn=<NegBackward0>) tensor(9889.8604, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9889.8017578125
tensor(9889.8604, grad_fn=<NegBackward0>) tensor(9889.8018, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9889.796875
tensor(9889.8018, grad_fn=<NegBackward0>) tensor(9889.7969, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9889.79296875
tensor(9889.7969, grad_fn=<NegBackward0>) tensor(9889.7930, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9889.7919921875
tensor(9889.7930, grad_fn=<NegBackward0>) tensor(9889.7920, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9889.791015625
tensor(9889.7920, grad_fn=<NegBackward0>) tensor(9889.7910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9889.791015625
tensor(9889.7910, grad_fn=<NegBackward0>) tensor(9889.7910, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9889.796875
tensor(9889.7910, grad_fn=<NegBackward0>) tensor(9889.7969, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9889.791015625
tensor(9889.7910, grad_fn=<NegBackward0>) tensor(9889.7910, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9889.791015625
tensor(9889.7910, grad_fn=<NegBackward0>) tensor(9889.7910, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9889.7900390625
tensor(9889.7910, grad_fn=<NegBackward0>) tensor(9889.7900, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9889.7890625
tensor(9889.7900, grad_fn=<NegBackward0>) tensor(9889.7891, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9889.7900390625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7900, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9889.7900390625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7900, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9889.7919921875
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7920, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -9889.7890625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7891, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9889.7900390625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7900, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9889.7890625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7891, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9889.7890625
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7891, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9889.7880859375
tensor(9889.7891, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9889.7880859375
tensor(9889.7881, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9889.7880859375
tensor(9889.7881, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9889.787109375
tensor(9889.7881, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9889.787109375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -9889.787109375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9889.787109375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9889.7880859375
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9889.7861328125
tensor(9889.7871, grad_fn=<NegBackward0>) tensor(9889.7861, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9889.7890625
tensor(9889.7861, grad_fn=<NegBackward0>) tensor(9889.7891, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9889.787109375
tensor(9889.7861, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9889.7880859375
tensor(9889.7861, grad_fn=<NegBackward0>) tensor(9889.7881, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9889.787109375
tensor(9889.7861, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9889.787109375
tensor(9889.7861, grad_fn=<NegBackward0>) tensor(9889.7871, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.8557, 0.1443],
        [0.1247, 0.8753]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5569, 0.4431], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1269, 0.1252],
         [0.6885, 0.2015]],

        [[0.7175, 0.1001],
         [0.6854, 0.5185]],

        [[0.5585, 0.1190],
         [0.5726, 0.5405]],

        [[0.6819, 0.1246],
         [0.6824, 0.5385]],

        [[0.5760, 0.1037],
         [0.6980, 0.5955]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.30669281158420963
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.35355973108084815
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 74
Adjusted Rand Index: 0.22267080380334497
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 82
Adjusted Rand Index: 0.40361796286827584
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 83
Adjusted Rand Index: 0.42994589242244163
Global Adjusted Rand Index: 0.3444363873313292
Average Adjusted Rand Index: 0.3432974403518241
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22774.640625
inf tensor(22774.6406, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9903.4892578125
tensor(22774.6406, grad_fn=<NegBackward0>) tensor(9903.4893, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9902.705078125
tensor(9903.4893, grad_fn=<NegBackward0>) tensor(9902.7051, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9902.431640625
tensor(9902.7051, grad_fn=<NegBackward0>) tensor(9902.4316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9902.1953125
tensor(9902.4316, grad_fn=<NegBackward0>) tensor(9902.1953, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9901.9033203125
tensor(9902.1953, grad_fn=<NegBackward0>) tensor(9901.9033, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9901.6279296875
tensor(9901.9033, grad_fn=<NegBackward0>) tensor(9901.6279, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9901.421875
tensor(9901.6279, grad_fn=<NegBackward0>) tensor(9901.4219, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9901.2275390625
tensor(9901.4219, grad_fn=<NegBackward0>) tensor(9901.2275, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9901.0029296875
tensor(9901.2275, grad_fn=<NegBackward0>) tensor(9901.0029, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9900.703125
tensor(9901.0029, grad_fn=<NegBackward0>) tensor(9900.7031, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9900.310546875
tensor(9900.7031, grad_fn=<NegBackward0>) tensor(9900.3105, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9899.7802734375
tensor(9900.3105, grad_fn=<NegBackward0>) tensor(9899.7803, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9898.98828125
tensor(9899.7803, grad_fn=<NegBackward0>) tensor(9898.9883, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9898.1083984375
tensor(9898.9883, grad_fn=<NegBackward0>) tensor(9898.1084, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9897.07421875
tensor(9898.1084, grad_fn=<NegBackward0>) tensor(9897.0742, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9896.0869140625
tensor(9897.0742, grad_fn=<NegBackward0>) tensor(9896.0869, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9895.1923828125
tensor(9896.0869, grad_fn=<NegBackward0>) tensor(9895.1924, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9894.3310546875
tensor(9895.1924, grad_fn=<NegBackward0>) tensor(9894.3311, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9893.251953125
tensor(9894.3311, grad_fn=<NegBackward0>) tensor(9893.2520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9891.8173828125
tensor(9893.2520, grad_fn=<NegBackward0>) tensor(9891.8174, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9890.8251953125
tensor(9891.8174, grad_fn=<NegBackward0>) tensor(9890.8252, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9890.232421875
tensor(9890.8252, grad_fn=<NegBackward0>) tensor(9890.2324, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9890.1357421875
tensor(9890.2324, grad_fn=<NegBackward0>) tensor(9890.1357, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9890.123046875
tensor(9890.1357, grad_fn=<NegBackward0>) tensor(9890.1230, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9890.1201171875
tensor(9890.1230, grad_fn=<NegBackward0>) tensor(9890.1201, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9890.1171875
tensor(9890.1201, grad_fn=<NegBackward0>) tensor(9890.1172, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9890.1162109375
tensor(9890.1172, grad_fn=<NegBackward0>) tensor(9890.1162, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9890.1123046875
tensor(9890.1162, grad_fn=<NegBackward0>) tensor(9890.1123, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9890.109375
tensor(9890.1123, grad_fn=<NegBackward0>) tensor(9890.1094, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9890.1083984375
tensor(9890.1094, grad_fn=<NegBackward0>) tensor(9890.1084, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9890.107421875
tensor(9890.1084, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9890.1083984375
tensor(9890.1074, grad_fn=<NegBackward0>) tensor(9890.1084, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9890.109375
tensor(9890.1074, grad_fn=<NegBackward0>) tensor(9890.1094, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9890.1064453125
tensor(9890.1074, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9890.107421875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9890.107421875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -9890.1064453125
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9890.107421875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9890.107421875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -9890.107421875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -9890.1064453125
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9890.1064453125
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9890.1064453125
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9890.10546875
tensor(9890.1064, grad_fn=<NegBackward0>) tensor(9890.1055, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9890.107421875
tensor(9890.1055, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9890.107421875
tensor(9890.1055, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9890.107421875
tensor(9890.1055, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -9890.107421875
tensor(9890.1055, grad_fn=<NegBackward0>) tensor(9890.1074, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -9890.1064453125
tensor(9890.1055, grad_fn=<NegBackward0>) tensor(9890.1064, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.9336, 0.0664],
        [0.1515, 0.8485]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5384, 0.4616], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1752, 0.1315],
         [0.6019, 0.1215]],

        [[0.5750, 0.1050],
         [0.5351, 0.6537]],

        [[0.5158, 0.1238],
         [0.6687, 0.6459]],

        [[0.5753, 0.1183],
         [0.6028, 0.5185]],

        [[0.6303, 0.0972],
         [0.6760, 0.5345]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 28
Adjusted Rand Index: 0.18548526116186864
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 31
Adjusted Rand Index: 0.13604191237704688
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.12096177123852726
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 22
Adjusted Rand Index: 0.3067878673921474
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 18
Adjusted Rand Index: 0.40329971289587074
Global Adjusted Rand Index: 0.22506830103647965
Average Adjusted Rand Index: 0.2305153050130922
[0.3444363873313292, 0.22506830103647965] [0.3432974403518241, 0.2305153050130922] [9889.787109375, 9890.1064453125]
-------------------------------------
This iteration is 69
True Objective function: Loss = -9883.944777632372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21188.61328125
inf tensor(21188.6133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9776.0927734375
tensor(21188.6133, grad_fn=<NegBackward0>) tensor(9776.0928, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9775.18359375
tensor(9776.0928, grad_fn=<NegBackward0>) tensor(9775.1836, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9774.8955078125
tensor(9775.1836, grad_fn=<NegBackward0>) tensor(9774.8955, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9774.7509765625
tensor(9774.8955, grad_fn=<NegBackward0>) tensor(9774.7510, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9774.6533203125
tensor(9774.7510, grad_fn=<NegBackward0>) tensor(9774.6533, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9774.556640625
tensor(9774.6533, grad_fn=<NegBackward0>) tensor(9774.5566, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9774.44140625
tensor(9774.5566, grad_fn=<NegBackward0>) tensor(9774.4414, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9774.3193359375
tensor(9774.4414, grad_fn=<NegBackward0>) tensor(9774.3193, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9774.21875
tensor(9774.3193, grad_fn=<NegBackward0>) tensor(9774.2188, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9774.1416015625
tensor(9774.2188, grad_fn=<NegBackward0>) tensor(9774.1416, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9774.0771484375
tensor(9774.1416, grad_fn=<NegBackward0>) tensor(9774.0771, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9774.0185546875
tensor(9774.0771, grad_fn=<NegBackward0>) tensor(9774.0186, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9773.96484375
tensor(9774.0186, grad_fn=<NegBackward0>) tensor(9773.9648, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9773.9130859375
tensor(9773.9648, grad_fn=<NegBackward0>) tensor(9773.9131, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9773.865234375
tensor(9773.9131, grad_fn=<NegBackward0>) tensor(9773.8652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9773.8193359375
tensor(9773.8652, grad_fn=<NegBackward0>) tensor(9773.8193, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9773.77734375
tensor(9773.8193, grad_fn=<NegBackward0>) tensor(9773.7773, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9773.740234375
tensor(9773.7773, grad_fn=<NegBackward0>) tensor(9773.7402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9773.705078125
tensor(9773.7402, grad_fn=<NegBackward0>) tensor(9773.7051, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9773.6708984375
tensor(9773.7051, grad_fn=<NegBackward0>) tensor(9773.6709, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9773.634765625
tensor(9773.6709, grad_fn=<NegBackward0>) tensor(9773.6348, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9773.5888671875
tensor(9773.6348, grad_fn=<NegBackward0>) tensor(9773.5889, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9773.5224609375
tensor(9773.5889, grad_fn=<NegBackward0>) tensor(9773.5225, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9773.4228515625
tensor(9773.5225, grad_fn=<NegBackward0>) tensor(9773.4229, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9773.142578125
tensor(9773.4229, grad_fn=<NegBackward0>) tensor(9773.1426, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9771.69140625
tensor(9773.1426, grad_fn=<NegBackward0>) tensor(9771.6914, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9771.0751953125
tensor(9771.6914, grad_fn=<NegBackward0>) tensor(9771.0752, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9770.791015625
tensor(9771.0752, grad_fn=<NegBackward0>) tensor(9770.7910, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9770.52734375
tensor(9770.7910, grad_fn=<NegBackward0>) tensor(9770.5273, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9770.29296875
tensor(9770.5273, grad_fn=<NegBackward0>) tensor(9770.2930, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9770.0927734375
tensor(9770.2930, grad_fn=<NegBackward0>) tensor(9770.0928, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9769.92578125
tensor(9770.0928, grad_fn=<NegBackward0>) tensor(9769.9258, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9769.79296875
tensor(9769.9258, grad_fn=<NegBackward0>) tensor(9769.7930, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9769.6767578125
tensor(9769.7930, grad_fn=<NegBackward0>) tensor(9769.6768, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9769.6005859375
tensor(9769.6768, grad_fn=<NegBackward0>) tensor(9769.6006, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9769.53515625
tensor(9769.6006, grad_fn=<NegBackward0>) tensor(9769.5352, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9769.4833984375
tensor(9769.5352, grad_fn=<NegBackward0>) tensor(9769.4834, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9769.44140625
tensor(9769.4834, grad_fn=<NegBackward0>) tensor(9769.4414, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9769.4052734375
tensor(9769.4414, grad_fn=<NegBackward0>) tensor(9769.4053, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9769.37890625
tensor(9769.4053, grad_fn=<NegBackward0>) tensor(9769.3789, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9769.353515625
tensor(9769.3789, grad_fn=<NegBackward0>) tensor(9769.3535, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9769.3330078125
tensor(9769.3535, grad_fn=<NegBackward0>) tensor(9769.3330, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9769.31640625
tensor(9769.3330, grad_fn=<NegBackward0>) tensor(9769.3164, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9769.3017578125
tensor(9769.3164, grad_fn=<NegBackward0>) tensor(9769.3018, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9769.2880859375
tensor(9769.3018, grad_fn=<NegBackward0>) tensor(9769.2881, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9769.2763671875
tensor(9769.2881, grad_fn=<NegBackward0>) tensor(9769.2764, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9769.2685546875
tensor(9769.2764, grad_fn=<NegBackward0>) tensor(9769.2686, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9769.2607421875
tensor(9769.2686, grad_fn=<NegBackward0>) tensor(9769.2607, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9769.2529296875
tensor(9769.2607, grad_fn=<NegBackward0>) tensor(9769.2529, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9769.24609375
tensor(9769.2529, grad_fn=<NegBackward0>) tensor(9769.2461, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9769.240234375
tensor(9769.2461, grad_fn=<NegBackward0>) tensor(9769.2402, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9769.2353515625
tensor(9769.2402, grad_fn=<NegBackward0>) tensor(9769.2354, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9769.23046875
tensor(9769.2354, grad_fn=<NegBackward0>) tensor(9769.2305, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9769.2255859375
tensor(9769.2305, grad_fn=<NegBackward0>) tensor(9769.2256, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9769.2216796875
tensor(9769.2256, grad_fn=<NegBackward0>) tensor(9769.2217, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9769.2177734375
tensor(9769.2217, grad_fn=<NegBackward0>) tensor(9769.2178, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9769.2138671875
tensor(9769.2178, grad_fn=<NegBackward0>) tensor(9769.2139, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9769.212890625
tensor(9769.2139, grad_fn=<NegBackward0>) tensor(9769.2129, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9769.2099609375
tensor(9769.2129, grad_fn=<NegBackward0>) tensor(9769.2100, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9769.205078125
tensor(9769.2100, grad_fn=<NegBackward0>) tensor(9769.2051, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9769.2041015625
tensor(9769.2051, grad_fn=<NegBackward0>) tensor(9769.2041, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9769.2021484375
tensor(9769.2041, grad_fn=<NegBackward0>) tensor(9769.2021, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9769.2001953125
tensor(9769.2021, grad_fn=<NegBackward0>) tensor(9769.2002, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9769.1982421875
tensor(9769.2002, grad_fn=<NegBackward0>) tensor(9769.1982, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9769.197265625
tensor(9769.1982, grad_fn=<NegBackward0>) tensor(9769.1973, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9769.1953125
tensor(9769.1973, grad_fn=<NegBackward0>) tensor(9769.1953, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9769.1943359375
tensor(9769.1953, grad_fn=<NegBackward0>) tensor(9769.1943, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9769.1943359375
tensor(9769.1943, grad_fn=<NegBackward0>) tensor(9769.1943, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9769.1923828125
tensor(9769.1943, grad_fn=<NegBackward0>) tensor(9769.1924, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9769.19140625
tensor(9769.1924, grad_fn=<NegBackward0>) tensor(9769.1914, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9769.1904296875
tensor(9769.1914, grad_fn=<NegBackward0>) tensor(9769.1904, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9769.1904296875
tensor(9769.1904, grad_fn=<NegBackward0>) tensor(9769.1904, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9769.1884765625
tensor(9769.1904, grad_fn=<NegBackward0>) tensor(9769.1885, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9769.1884765625
tensor(9769.1885, grad_fn=<NegBackward0>) tensor(9769.1885, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9769.1884765625
tensor(9769.1885, grad_fn=<NegBackward0>) tensor(9769.1885, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9769.1875
tensor(9769.1885, grad_fn=<NegBackward0>) tensor(9769.1875, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9769.185546875
tensor(9769.1875, grad_fn=<NegBackward0>) tensor(9769.1855, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9769.185546875
tensor(9769.1855, grad_fn=<NegBackward0>) tensor(9769.1855, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9769.1875
tensor(9769.1855, grad_fn=<NegBackward0>) tensor(9769.1875, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9769.1845703125
tensor(9769.1855, grad_fn=<NegBackward0>) tensor(9769.1846, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9769.212890625
tensor(9769.1846, grad_fn=<NegBackward0>) tensor(9769.2129, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9769.18359375
tensor(9769.1846, grad_fn=<NegBackward0>) tensor(9769.1836, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9769.3603515625
tensor(9769.1836, grad_fn=<NegBackward0>) tensor(9769.3604, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9769.18359375
tensor(9769.1836, grad_fn=<NegBackward0>) tensor(9769.1836, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9769.1845703125
tensor(9769.1836, grad_fn=<NegBackward0>) tensor(9769.1846, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9769.181640625
tensor(9769.1836, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9769.181640625
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9769.2197265625
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.2197, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9769.181640625
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9769.1826171875
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.1826, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9769.181640625
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9769.1806640625
tensor(9769.1816, grad_fn=<NegBackward0>) tensor(9769.1807, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9769.181640625
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9769.181640625
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1816, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -9769.1806640625
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1807, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9769.1806640625
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1807, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9769.185546875
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1855, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9769.17578125
tensor(9769.1807, grad_fn=<NegBackward0>) tensor(9769.1758, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9769.1845703125
tensor(9769.1758, grad_fn=<NegBackward0>) tensor(9769.1846, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9987e-01, 1.3032e-04],
        [3.9778e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0222, 0.9778], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0066, 0.0936],
         [0.6437, 0.1359]],

        [[0.5226, 0.1500],
         [0.5718, 0.5485]],

        [[0.5416, 0.0666],
         [0.6370, 0.5838]],

        [[0.5979, 0.0640],
         [0.5841, 0.6774]],

        [[0.6873, 0.1577],
         [0.5042, 0.5743]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.004267232452421997
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
Global Adjusted Rand Index: -0.00010450901519039915
Average Adjusted Rand Index: 0.0008810615899918411
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21306.15625
inf tensor(21306.1562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9775.47265625
tensor(21306.1562, grad_fn=<NegBackward0>) tensor(9775.4727, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9774.8740234375
tensor(9775.4727, grad_fn=<NegBackward0>) tensor(9774.8740, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9774.6650390625
tensor(9774.8740, grad_fn=<NegBackward0>) tensor(9774.6650, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9774.548828125
tensor(9774.6650, grad_fn=<NegBackward0>) tensor(9774.5488, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9774.4677734375
tensor(9774.5488, grad_fn=<NegBackward0>) tensor(9774.4678, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9774.400390625
tensor(9774.4678, grad_fn=<NegBackward0>) tensor(9774.4004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9774.3447265625
tensor(9774.4004, grad_fn=<NegBackward0>) tensor(9774.3447, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9774.2939453125
tensor(9774.3447, grad_fn=<NegBackward0>) tensor(9774.2939, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9774.2373046875
tensor(9774.2939, grad_fn=<NegBackward0>) tensor(9774.2373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9774.169921875
tensor(9774.2373, grad_fn=<NegBackward0>) tensor(9774.1699, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9774.0703125
tensor(9774.1699, grad_fn=<NegBackward0>) tensor(9774.0703, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9773.9697265625
tensor(9774.0703, grad_fn=<NegBackward0>) tensor(9773.9697, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9773.9208984375
tensor(9773.9697, grad_fn=<NegBackward0>) tensor(9773.9209, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9773.892578125
tensor(9773.9209, grad_fn=<NegBackward0>) tensor(9773.8926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9773.865234375
tensor(9773.8926, grad_fn=<NegBackward0>) tensor(9773.8652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9773.837890625
tensor(9773.8652, grad_fn=<NegBackward0>) tensor(9773.8379, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9773.8095703125
tensor(9773.8379, grad_fn=<NegBackward0>) tensor(9773.8096, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9773.7783203125
tensor(9773.8096, grad_fn=<NegBackward0>) tensor(9773.7783, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9773.744140625
tensor(9773.7783, grad_fn=<NegBackward0>) tensor(9773.7441, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9773.7099609375
tensor(9773.7441, grad_fn=<NegBackward0>) tensor(9773.7100, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9773.6767578125
tensor(9773.7100, grad_fn=<NegBackward0>) tensor(9773.6768, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9773.6533203125
tensor(9773.6768, grad_fn=<NegBackward0>) tensor(9773.6533, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9773.638671875
tensor(9773.6533, grad_fn=<NegBackward0>) tensor(9773.6387, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9773.626953125
tensor(9773.6387, grad_fn=<NegBackward0>) tensor(9773.6270, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9773.62109375
tensor(9773.6270, grad_fn=<NegBackward0>) tensor(9773.6211, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9773.6181640625
tensor(9773.6211, grad_fn=<NegBackward0>) tensor(9773.6182, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9773.6181640625
tensor(9773.6182, grad_fn=<NegBackward0>) tensor(9773.6182, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9773.6162109375
tensor(9773.6182, grad_fn=<NegBackward0>) tensor(9773.6162, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9773.6181640625
tensor(9773.6162, grad_fn=<NegBackward0>) tensor(9773.6182, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9773.6171875
tensor(9773.6162, grad_fn=<NegBackward0>) tensor(9773.6172, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -9773.6171875
tensor(9773.6162, grad_fn=<NegBackward0>) tensor(9773.6172, grad_fn=<NegBackward0>)
3
Iteration 3200: Loss = -9773.6162109375
tensor(9773.6162, grad_fn=<NegBackward0>) tensor(9773.6162, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9773.615234375
tensor(9773.6162, grad_fn=<NegBackward0>) tensor(9773.6152, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9773.6171875
tensor(9773.6152, grad_fn=<NegBackward0>) tensor(9773.6172, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9773.6171875
tensor(9773.6152, grad_fn=<NegBackward0>) tensor(9773.6172, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -9773.6162109375
tensor(9773.6152, grad_fn=<NegBackward0>) tensor(9773.6162, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -9773.6162109375
tensor(9773.6152, grad_fn=<NegBackward0>) tensor(9773.6162, grad_fn=<NegBackward0>)
4
Iteration 3800: Loss = -9773.6162109375
tensor(9773.6152, grad_fn=<NegBackward0>) tensor(9773.6162, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3800 due to no improvement.
pi: tensor([[0.9538, 0.0462],
        [0.9936, 0.0064]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8799, 0.1201], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1317, 0.1505],
         [0.5341, 0.1710]],

        [[0.5774, 0.2039],
         [0.5742, 0.6529]],

        [[0.5108, 0.1406],
         [0.5142, 0.5782]],

        [[0.5514, 0.1394],
         [0.5369, 0.6088]],

        [[0.6400, 0.1594],
         [0.7136, 0.5028]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.764727547676425e-05
Average Adjusted Rand Index: -0.0006490834186426951
[-0.00010450901519039915, 6.764727547676425e-05] [0.0008810615899918411, -0.0006490834186426951] [9769.17578125, 9773.6162109375]
-------------------------------------
This iteration is 70
True Objective function: Loss = -9859.272418641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21612.10546875
inf tensor(21612.1055, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9738.7001953125
tensor(21612.1055, grad_fn=<NegBackward0>) tensor(9738.7002, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9737.173828125
tensor(9738.7002, grad_fn=<NegBackward0>) tensor(9737.1738, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9736.552734375
tensor(9737.1738, grad_fn=<NegBackward0>) tensor(9736.5527, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9736.037109375
tensor(9736.5527, grad_fn=<NegBackward0>) tensor(9736.0371, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9735.5400390625
tensor(9736.0371, grad_fn=<NegBackward0>) tensor(9735.5400, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9734.9052734375
tensor(9735.5400, grad_fn=<NegBackward0>) tensor(9734.9053, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9733.3291015625
tensor(9734.9053, grad_fn=<NegBackward0>) tensor(9733.3291, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9732.1708984375
tensor(9733.3291, grad_fn=<NegBackward0>) tensor(9732.1709, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9731.517578125
tensor(9732.1709, grad_fn=<NegBackward0>) tensor(9731.5176, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9731.01953125
tensor(9731.5176, grad_fn=<NegBackward0>) tensor(9731.0195, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9730.376953125
tensor(9731.0195, grad_fn=<NegBackward0>) tensor(9730.3770, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9729.578125
tensor(9730.3770, grad_fn=<NegBackward0>) tensor(9729.5781, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9728.7451171875
tensor(9729.5781, grad_fn=<NegBackward0>) tensor(9728.7451, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9728.5419921875
tensor(9728.7451, grad_fn=<NegBackward0>) tensor(9728.5420, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9728.328125
tensor(9728.5420, grad_fn=<NegBackward0>) tensor(9728.3281, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9728.2626953125
tensor(9728.3281, grad_fn=<NegBackward0>) tensor(9728.2627, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9728.2138671875
tensor(9728.2627, grad_fn=<NegBackward0>) tensor(9728.2139, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9728.1787109375
tensor(9728.2139, grad_fn=<NegBackward0>) tensor(9728.1787, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9728.15234375
tensor(9728.1787, grad_fn=<NegBackward0>) tensor(9728.1523, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9728.1337890625
tensor(9728.1523, grad_fn=<NegBackward0>) tensor(9728.1338, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9728.1123046875
tensor(9728.1338, grad_fn=<NegBackward0>) tensor(9728.1123, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9727.9814453125
tensor(9728.1123, grad_fn=<NegBackward0>) tensor(9727.9814, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9727.962890625
tensor(9727.9814, grad_fn=<NegBackward0>) tensor(9727.9629, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9727.9453125
tensor(9727.9629, grad_fn=<NegBackward0>) tensor(9727.9453, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9727.7763671875
tensor(9727.9453, grad_fn=<NegBackward0>) tensor(9727.7764, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9727.61328125
tensor(9727.7764, grad_fn=<NegBackward0>) tensor(9727.6133, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9727.5146484375
tensor(9727.6133, grad_fn=<NegBackward0>) tensor(9727.5146, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9727.490234375
tensor(9727.5146, grad_fn=<NegBackward0>) tensor(9727.4902, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9727.458984375
tensor(9727.4902, grad_fn=<NegBackward0>) tensor(9727.4590, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9727.43359375
tensor(9727.4590, grad_fn=<NegBackward0>) tensor(9727.4336, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9727.4267578125
tensor(9727.4336, grad_fn=<NegBackward0>) tensor(9727.4268, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9727.4208984375
tensor(9727.4268, grad_fn=<NegBackward0>) tensor(9727.4209, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9727.41796875
tensor(9727.4209, grad_fn=<NegBackward0>) tensor(9727.4180, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9727.4130859375
tensor(9727.4180, grad_fn=<NegBackward0>) tensor(9727.4131, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9727.4091796875
tensor(9727.4131, grad_fn=<NegBackward0>) tensor(9727.4092, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9727.3994140625
tensor(9727.4092, grad_fn=<NegBackward0>) tensor(9727.3994, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9727.359375
tensor(9727.3994, grad_fn=<NegBackward0>) tensor(9727.3594, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9727.3505859375
tensor(9727.3594, grad_fn=<NegBackward0>) tensor(9727.3506, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9727.341796875
tensor(9727.3506, grad_fn=<NegBackward0>) tensor(9727.3418, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9727.3388671875
tensor(9727.3418, grad_fn=<NegBackward0>) tensor(9727.3389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9727.3271484375
tensor(9727.3389, grad_fn=<NegBackward0>) tensor(9727.3271, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9727.322265625
tensor(9727.3271, grad_fn=<NegBackward0>) tensor(9727.3223, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9727.2392578125
tensor(9727.3223, grad_fn=<NegBackward0>) tensor(9727.2393, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9727.1767578125
tensor(9727.2393, grad_fn=<NegBackward0>) tensor(9727.1768, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9727.173828125
tensor(9727.1768, grad_fn=<NegBackward0>) tensor(9727.1738, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9727.171875
tensor(9727.1738, grad_fn=<NegBackward0>) tensor(9727.1719, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9727.169921875
tensor(9727.1719, grad_fn=<NegBackward0>) tensor(9727.1699, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9727.171875
tensor(9727.1699, grad_fn=<NegBackward0>) tensor(9727.1719, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9727.1689453125
tensor(9727.1699, grad_fn=<NegBackward0>) tensor(9727.1689, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9727.16796875
tensor(9727.1689, grad_fn=<NegBackward0>) tensor(9727.1680, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9727.1689453125
tensor(9727.1680, grad_fn=<NegBackward0>) tensor(9727.1689, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9727.166015625
tensor(9727.1680, grad_fn=<NegBackward0>) tensor(9727.1660, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9727.162109375
tensor(9727.1660, grad_fn=<NegBackward0>) tensor(9727.1621, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9727.15625
tensor(9727.1621, grad_fn=<NegBackward0>) tensor(9727.1562, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9727.1484375
tensor(9727.1562, grad_fn=<NegBackward0>) tensor(9727.1484, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9727.146484375
tensor(9727.1484, grad_fn=<NegBackward0>) tensor(9727.1465, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9727.146484375
tensor(9727.1465, grad_fn=<NegBackward0>) tensor(9727.1465, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9727.1455078125
tensor(9727.1465, grad_fn=<NegBackward0>) tensor(9727.1455, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9727.1455078125
tensor(9727.1455, grad_fn=<NegBackward0>) tensor(9727.1455, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9727.1435546875
tensor(9727.1455, grad_fn=<NegBackward0>) tensor(9727.1436, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9727.1416015625
tensor(9727.1436, grad_fn=<NegBackward0>) tensor(9727.1416, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9727.1328125
tensor(9727.1416, grad_fn=<NegBackward0>) tensor(9727.1328, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9727.1318359375
tensor(9727.1328, grad_fn=<NegBackward0>) tensor(9727.1318, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9727.1318359375
tensor(9727.1318, grad_fn=<NegBackward0>) tensor(9727.1318, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9727.1298828125
tensor(9727.1318, grad_fn=<NegBackward0>) tensor(9727.1299, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9727.1171875
tensor(9727.1299, grad_fn=<NegBackward0>) tensor(9727.1172, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9727.1162109375
tensor(9727.1172, grad_fn=<NegBackward0>) tensor(9727.1162, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9727.115234375
tensor(9727.1162, grad_fn=<NegBackward0>) tensor(9727.1152, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9727.115234375
tensor(9727.1152, grad_fn=<NegBackward0>) tensor(9727.1152, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9727.1123046875
tensor(9727.1152, grad_fn=<NegBackward0>) tensor(9727.1123, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9727.111328125
tensor(9727.1123, grad_fn=<NegBackward0>) tensor(9727.1113, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9727.1123046875
tensor(9727.1113, grad_fn=<NegBackward0>) tensor(9727.1123, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9727.1103515625
tensor(9727.1113, grad_fn=<NegBackward0>) tensor(9727.1104, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9727.1044921875
tensor(9727.1104, grad_fn=<NegBackward0>) tensor(9727.1045, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9727.0791015625
tensor(9727.1045, grad_fn=<NegBackward0>) tensor(9727.0791, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9727.078125
tensor(9727.0791, grad_fn=<NegBackward0>) tensor(9727.0781, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9727.078125
tensor(9727.0781, grad_fn=<NegBackward0>) tensor(9727.0781, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9727.0771484375
tensor(9727.0781, grad_fn=<NegBackward0>) tensor(9727.0771, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9727.0751953125
tensor(9727.0771, grad_fn=<NegBackward0>) tensor(9727.0752, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9727.076171875
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0762, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9727.0751953125
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0752, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9727.091796875
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0918, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9727.0751953125
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0752, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9727.0751953125
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0752, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9727.0732421875
tensor(9727.0752, grad_fn=<NegBackward0>) tensor(9727.0732, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9727.08984375
tensor(9727.0732, grad_fn=<NegBackward0>) tensor(9727.0898, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9727.0703125
tensor(9727.0732, grad_fn=<NegBackward0>) tensor(9727.0703, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9727.080078125
tensor(9727.0703, grad_fn=<NegBackward0>) tensor(9727.0801, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9727.0673828125
tensor(9727.0703, grad_fn=<NegBackward0>) tensor(9727.0674, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9727.0634765625
tensor(9727.0674, grad_fn=<NegBackward0>) tensor(9727.0635, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9727.3349609375
tensor(9727.0635, grad_fn=<NegBackward0>) tensor(9727.3350, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9727.0595703125
tensor(9727.0635, grad_fn=<NegBackward0>) tensor(9727.0596, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9727.0546875
tensor(9727.0596, grad_fn=<NegBackward0>) tensor(9727.0547, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9727.0595703125
tensor(9727.0547, grad_fn=<NegBackward0>) tensor(9727.0596, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9727.0517578125
tensor(9727.0547, grad_fn=<NegBackward0>) tensor(9727.0518, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9727.052734375
tensor(9727.0518, grad_fn=<NegBackward0>) tensor(9727.0527, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -9727.0498046875
tensor(9727.0518, grad_fn=<NegBackward0>) tensor(9727.0498, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9727.046875
tensor(9727.0498, grad_fn=<NegBackward0>) tensor(9727.0469, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9727.0419921875
tensor(9727.0469, grad_fn=<NegBackward0>) tensor(9727.0420, grad_fn=<NegBackward0>)
pi: tensor([[9.7370e-01, 2.6305e-02],
        [7.6220e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9826, 0.0174], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1288, 0.1674],
         [0.7308, 0.4253]],

        [[0.7087, 0.1866],
         [0.5336, 0.6520]],

        [[0.5194, 0.1413],
         [0.7295, 0.5009]],

        [[0.6778, 0.1695],
         [0.5973, 0.5527]],

        [[0.6835, 0.1437],
         [0.6453, 0.6611]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.035323271006983556
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0056450870649491815
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.004097011376522661
Global Adjusted Rand Index: 0.01472405447500091
Average Adjusted Rand Index: 0.010511793708766653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21437.99609375
inf tensor(21437.9961, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9738.560546875
tensor(21437.9961, grad_fn=<NegBackward0>) tensor(9738.5605, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9737.2685546875
tensor(9738.5605, grad_fn=<NegBackward0>) tensor(9737.2686, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9736.927734375
tensor(9737.2686, grad_fn=<NegBackward0>) tensor(9736.9277, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9736.677734375
tensor(9736.9277, grad_fn=<NegBackward0>) tensor(9736.6777, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9736.2587890625
tensor(9736.6777, grad_fn=<NegBackward0>) tensor(9736.2588, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9735.8017578125
tensor(9736.2588, grad_fn=<NegBackward0>) tensor(9735.8018, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9735.58203125
tensor(9735.8018, grad_fn=<NegBackward0>) tensor(9735.5820, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9735.30859375
tensor(9735.5820, grad_fn=<NegBackward0>) tensor(9735.3086, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9734.46875
tensor(9735.3086, grad_fn=<NegBackward0>) tensor(9734.4688, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9732.474609375
tensor(9734.4688, grad_fn=<NegBackward0>) tensor(9732.4746, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9730.818359375
tensor(9732.4746, grad_fn=<NegBackward0>) tensor(9730.8184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9729.9228515625
tensor(9730.8184, grad_fn=<NegBackward0>) tensor(9729.9229, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9729.369140625
tensor(9729.9229, grad_fn=<NegBackward0>) tensor(9729.3691, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9728.8701171875
tensor(9729.3691, grad_fn=<NegBackward0>) tensor(9728.8701, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9728.4833984375
tensor(9728.8701, grad_fn=<NegBackward0>) tensor(9728.4834, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9728.3134765625
tensor(9728.4834, grad_fn=<NegBackward0>) tensor(9728.3135, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9728.21875
tensor(9728.3135, grad_fn=<NegBackward0>) tensor(9728.2188, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9728.0810546875
tensor(9728.2188, grad_fn=<NegBackward0>) tensor(9728.0811, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9728.044921875
tensor(9728.0811, grad_fn=<NegBackward0>) tensor(9728.0449, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9728.021484375
tensor(9728.0449, grad_fn=<NegBackward0>) tensor(9728.0215, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9728.0009765625
tensor(9728.0215, grad_fn=<NegBackward0>) tensor(9728.0010, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9727.98828125
tensor(9728.0010, grad_fn=<NegBackward0>) tensor(9727.9883, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9727.9765625
tensor(9727.9883, grad_fn=<NegBackward0>) tensor(9727.9766, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9727.9658203125
tensor(9727.9766, grad_fn=<NegBackward0>) tensor(9727.9658, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9727.94921875
tensor(9727.9658, grad_fn=<NegBackward0>) tensor(9727.9492, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9727.919921875
tensor(9727.9492, grad_fn=<NegBackward0>) tensor(9727.9199, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9727.9111328125
tensor(9727.9199, grad_fn=<NegBackward0>) tensor(9727.9111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9727.8994140625
tensor(9727.9111, grad_fn=<NegBackward0>) tensor(9727.8994, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9727.892578125
tensor(9727.8994, grad_fn=<NegBackward0>) tensor(9727.8926, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9727.8681640625
tensor(9727.8926, grad_fn=<NegBackward0>) tensor(9727.8682, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9727.8486328125
tensor(9727.8682, grad_fn=<NegBackward0>) tensor(9727.8486, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9727.7099609375
tensor(9727.8486, grad_fn=<NegBackward0>) tensor(9727.7100, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9727.5263671875
tensor(9727.7100, grad_fn=<NegBackward0>) tensor(9727.5264, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9727.4140625
tensor(9727.5264, grad_fn=<NegBackward0>) tensor(9727.4141, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9727.408203125
tensor(9727.4141, grad_fn=<NegBackward0>) tensor(9727.4082, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9727.400390625
tensor(9727.4082, grad_fn=<NegBackward0>) tensor(9727.4004, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9727.396484375
tensor(9727.4004, grad_fn=<NegBackward0>) tensor(9727.3965, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9727.39453125
tensor(9727.3965, grad_fn=<NegBackward0>) tensor(9727.3945, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9727.3935546875
tensor(9727.3945, grad_fn=<NegBackward0>) tensor(9727.3936, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9727.392578125
tensor(9727.3936, grad_fn=<NegBackward0>) tensor(9727.3926, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9727.390625
tensor(9727.3926, grad_fn=<NegBackward0>) tensor(9727.3906, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9727.3896484375
tensor(9727.3906, grad_fn=<NegBackward0>) tensor(9727.3896, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9727.38671875
tensor(9727.3896, grad_fn=<NegBackward0>) tensor(9727.3867, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9727.3857421875
tensor(9727.3867, grad_fn=<NegBackward0>) tensor(9727.3857, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9727.3818359375
tensor(9727.3857, grad_fn=<NegBackward0>) tensor(9727.3818, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9727.3798828125
tensor(9727.3818, grad_fn=<NegBackward0>) tensor(9727.3799, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9727.37890625
tensor(9727.3799, grad_fn=<NegBackward0>) tensor(9727.3789, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9727.376953125
tensor(9727.3789, grad_fn=<NegBackward0>) tensor(9727.3770, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9727.375
tensor(9727.3770, grad_fn=<NegBackward0>) tensor(9727.3750, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9727.3720703125
tensor(9727.3750, grad_fn=<NegBackward0>) tensor(9727.3721, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9727.3681640625
tensor(9727.3721, grad_fn=<NegBackward0>) tensor(9727.3682, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9727.3671875
tensor(9727.3682, grad_fn=<NegBackward0>) tensor(9727.3672, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9727.3662109375
tensor(9727.3672, grad_fn=<NegBackward0>) tensor(9727.3662, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9727.3671875
tensor(9727.3662, grad_fn=<NegBackward0>) tensor(9727.3672, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9727.3662109375
tensor(9727.3662, grad_fn=<NegBackward0>) tensor(9727.3662, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9727.365234375
tensor(9727.3662, grad_fn=<NegBackward0>) tensor(9727.3652, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9727.365234375
tensor(9727.3652, grad_fn=<NegBackward0>) tensor(9727.3652, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9727.365234375
tensor(9727.3652, grad_fn=<NegBackward0>) tensor(9727.3652, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9727.36328125
tensor(9727.3652, grad_fn=<NegBackward0>) tensor(9727.3633, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9727.36328125
tensor(9727.3633, grad_fn=<NegBackward0>) tensor(9727.3633, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9727.36328125
tensor(9727.3633, grad_fn=<NegBackward0>) tensor(9727.3633, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9727.361328125
tensor(9727.3633, grad_fn=<NegBackward0>) tensor(9727.3613, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9727.36328125
tensor(9727.3613, grad_fn=<NegBackward0>) tensor(9727.3633, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9727.3623046875
tensor(9727.3613, grad_fn=<NegBackward0>) tensor(9727.3623, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9727.3623046875
tensor(9727.3613, grad_fn=<NegBackward0>) tensor(9727.3623, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -9727.3505859375
tensor(9727.3613, grad_fn=<NegBackward0>) tensor(9727.3506, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9727.349609375
tensor(9727.3506, grad_fn=<NegBackward0>) tensor(9727.3496, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9727.3505859375
tensor(9727.3496, grad_fn=<NegBackward0>) tensor(9727.3506, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9727.349609375
tensor(9727.3496, grad_fn=<NegBackward0>) tensor(9727.3496, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9727.3505859375
tensor(9727.3496, grad_fn=<NegBackward0>) tensor(9727.3506, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9727.34765625
tensor(9727.3496, grad_fn=<NegBackward0>) tensor(9727.3477, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9727.3466796875
tensor(9727.3477, grad_fn=<NegBackward0>) tensor(9727.3467, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9727.3466796875
tensor(9727.3467, grad_fn=<NegBackward0>) tensor(9727.3467, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9727.34765625
tensor(9727.3467, grad_fn=<NegBackward0>) tensor(9727.3477, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9727.3466796875
tensor(9727.3467, grad_fn=<NegBackward0>) tensor(9727.3467, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9727.3271484375
tensor(9727.3467, grad_fn=<NegBackward0>) tensor(9727.3271, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9727.326171875
tensor(9727.3271, grad_fn=<NegBackward0>) tensor(9727.3262, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9727.32421875
tensor(9727.3262, grad_fn=<NegBackward0>) tensor(9727.3242, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9727.294921875
tensor(9727.3242, grad_fn=<NegBackward0>) tensor(9727.2949, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9727.283203125
tensor(9727.2949, grad_fn=<NegBackward0>) tensor(9727.2832, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9727.283203125
tensor(9727.2832, grad_fn=<NegBackward0>) tensor(9727.2832, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9727.283203125
tensor(9727.2832, grad_fn=<NegBackward0>) tensor(9727.2832, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9727.2822265625
tensor(9727.2832, grad_fn=<NegBackward0>) tensor(9727.2822, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9727.283203125
tensor(9727.2822, grad_fn=<NegBackward0>) tensor(9727.2832, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9727.287109375
tensor(9727.2822, grad_fn=<NegBackward0>) tensor(9727.2871, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9727.279296875
tensor(9727.2822, grad_fn=<NegBackward0>) tensor(9727.2793, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9727.2783203125
tensor(9727.2793, grad_fn=<NegBackward0>) tensor(9727.2783, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9727.1875
tensor(9727.2783, grad_fn=<NegBackward0>) tensor(9727.1875, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9727.1875
tensor(9727.1875, grad_fn=<NegBackward0>) tensor(9727.1875, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9727.1865234375
tensor(9727.1875, grad_fn=<NegBackward0>) tensor(9727.1865, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9727.1865234375
tensor(9727.1865, grad_fn=<NegBackward0>) tensor(9727.1865, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9727.185546875
tensor(9727.1865, grad_fn=<NegBackward0>) tensor(9727.1855, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9727.1806640625
tensor(9727.1855, grad_fn=<NegBackward0>) tensor(9727.1807, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9727.181640625
tensor(9727.1807, grad_fn=<NegBackward0>) tensor(9727.1816, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9727.12890625
tensor(9727.1807, grad_fn=<NegBackward0>) tensor(9727.1289, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9727.126953125
tensor(9727.1289, grad_fn=<NegBackward0>) tensor(9727.1270, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9727.1337890625
tensor(9727.1270, grad_fn=<NegBackward0>) tensor(9727.1338, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9727.10546875
tensor(9727.1270, grad_fn=<NegBackward0>) tensor(9727.1055, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9727.1064453125
tensor(9727.1055, grad_fn=<NegBackward0>) tensor(9727.1064, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9992e-01, 8.0693e-05],
        [2.5789e-02, 9.7421e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0172, 0.9828], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4283, 0.1675],
         [0.5156, 0.1288]],

        [[0.5745, 0.1877],
         [0.7015, 0.6589]],

        [[0.6193, 0.1420],
         [0.6164, 0.6735]],

        [[0.6407, 0.1702],
         [0.6927, 0.6591]],

        [[0.5513, 0.1440],
         [0.6951, 0.7177]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.035323271006983556
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0056450870649491815
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.004097011376522661
Global Adjusted Rand Index: 0.01472405447500091
Average Adjusted Rand Index: 0.010511793708766653
[0.01472405447500091, 0.01472405447500091] [0.010511793708766653, 0.010511793708766653] [9727.0419921875, 9727.1005859375]
-------------------------------------
This iteration is 71
True Objective function: Loss = -9880.192135278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23440.0546875
inf tensor(23440.0547, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9804.751953125
tensor(23440.0547, grad_fn=<NegBackward0>) tensor(9804.7520, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9799.9765625
tensor(9804.7520, grad_fn=<NegBackward0>) tensor(9799.9766, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9795.025390625
tensor(9799.9766, grad_fn=<NegBackward0>) tensor(9795.0254, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9793.7138671875
tensor(9795.0254, grad_fn=<NegBackward0>) tensor(9793.7139, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9792.6201171875
tensor(9793.7139, grad_fn=<NegBackward0>) tensor(9792.6201, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9791.166015625
tensor(9792.6201, grad_fn=<NegBackward0>) tensor(9791.1660, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9789.6298828125
tensor(9791.1660, grad_fn=<NegBackward0>) tensor(9789.6299, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9788.5625
tensor(9789.6299, grad_fn=<NegBackward0>) tensor(9788.5625, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9787.57421875
tensor(9788.5625, grad_fn=<NegBackward0>) tensor(9787.5742, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9786.6435546875
tensor(9787.5742, grad_fn=<NegBackward0>) tensor(9786.6436, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9785.7490234375
tensor(9786.6436, grad_fn=<NegBackward0>) tensor(9785.7490, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9784.8955078125
tensor(9785.7490, grad_fn=<NegBackward0>) tensor(9784.8955, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9784.3671875
tensor(9784.8955, grad_fn=<NegBackward0>) tensor(9784.3672, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9784.005859375
tensor(9784.3672, grad_fn=<NegBackward0>) tensor(9784.0059, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9783.873046875
tensor(9784.0059, grad_fn=<NegBackward0>) tensor(9783.8730, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9783.8134765625
tensor(9783.8730, grad_fn=<NegBackward0>) tensor(9783.8135, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9783.767578125
tensor(9783.8135, grad_fn=<NegBackward0>) tensor(9783.7676, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9783.7138671875
tensor(9783.7676, grad_fn=<NegBackward0>) tensor(9783.7139, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9783.66796875
tensor(9783.7139, grad_fn=<NegBackward0>) tensor(9783.6680, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9783.625
tensor(9783.6680, grad_fn=<NegBackward0>) tensor(9783.6250, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9783.58203125
tensor(9783.6250, grad_fn=<NegBackward0>) tensor(9783.5820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9783.5380859375
tensor(9783.5820, grad_fn=<NegBackward0>) tensor(9783.5381, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9783.501953125
tensor(9783.5381, grad_fn=<NegBackward0>) tensor(9783.5020, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9783.46875
tensor(9783.5020, grad_fn=<NegBackward0>) tensor(9783.4688, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9783.4130859375
tensor(9783.4688, grad_fn=<NegBackward0>) tensor(9783.4131, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9783.34765625
tensor(9783.4131, grad_fn=<NegBackward0>) tensor(9783.3477, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9783.3173828125
tensor(9783.3477, grad_fn=<NegBackward0>) tensor(9783.3174, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9783.2890625
tensor(9783.3174, grad_fn=<NegBackward0>) tensor(9783.2891, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9783.2431640625
tensor(9783.2891, grad_fn=<NegBackward0>) tensor(9783.2432, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9783.1142578125
tensor(9783.2432, grad_fn=<NegBackward0>) tensor(9783.1143, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9782.1416015625
tensor(9783.1143, grad_fn=<NegBackward0>) tensor(9782.1416, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9780.3173828125
tensor(9782.1416, grad_fn=<NegBackward0>) tensor(9780.3174, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9778.8935546875
tensor(9780.3174, grad_fn=<NegBackward0>) tensor(9778.8936, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9778.4931640625
tensor(9778.8936, grad_fn=<NegBackward0>) tensor(9778.4932, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9778.375
tensor(9778.4932, grad_fn=<NegBackward0>) tensor(9778.3750, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9778.3115234375
tensor(9778.3750, grad_fn=<NegBackward0>) tensor(9778.3115, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9778.267578125
tensor(9778.3115, grad_fn=<NegBackward0>) tensor(9778.2676, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9778.2353515625
tensor(9778.2676, grad_fn=<NegBackward0>) tensor(9778.2354, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9778.2119140625
tensor(9778.2354, grad_fn=<NegBackward0>) tensor(9778.2119, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9778.1943359375
tensor(9778.2119, grad_fn=<NegBackward0>) tensor(9778.1943, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9778.1806640625
tensor(9778.1943, grad_fn=<NegBackward0>) tensor(9778.1807, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9778.169921875
tensor(9778.1807, grad_fn=<NegBackward0>) tensor(9778.1699, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9778.16015625
tensor(9778.1699, grad_fn=<NegBackward0>) tensor(9778.1602, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9778.15234375
tensor(9778.1602, grad_fn=<NegBackward0>) tensor(9778.1523, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9778.146484375
tensor(9778.1523, grad_fn=<NegBackward0>) tensor(9778.1465, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9778.1416015625
tensor(9778.1465, grad_fn=<NegBackward0>) tensor(9778.1416, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9778.1357421875
tensor(9778.1416, grad_fn=<NegBackward0>) tensor(9778.1357, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9778.1318359375
tensor(9778.1357, grad_fn=<NegBackward0>) tensor(9778.1318, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9778.12890625
tensor(9778.1318, grad_fn=<NegBackward0>) tensor(9778.1289, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9778.1259765625
tensor(9778.1289, grad_fn=<NegBackward0>) tensor(9778.1260, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9778.123046875
tensor(9778.1260, grad_fn=<NegBackward0>) tensor(9778.1230, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9778.1201171875
tensor(9778.1230, grad_fn=<NegBackward0>) tensor(9778.1201, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9778.1171875
tensor(9778.1201, grad_fn=<NegBackward0>) tensor(9778.1172, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9778.1142578125
tensor(9778.1172, grad_fn=<NegBackward0>) tensor(9778.1143, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9778.111328125
tensor(9778.1143, grad_fn=<NegBackward0>) tensor(9778.1113, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9778.1123046875
tensor(9778.1113, grad_fn=<NegBackward0>) tensor(9778.1123, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9778.1083984375
tensor(9778.1113, grad_fn=<NegBackward0>) tensor(9778.1084, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9778.107421875
tensor(9778.1084, grad_fn=<NegBackward0>) tensor(9778.1074, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9778.107421875
tensor(9778.1074, grad_fn=<NegBackward0>) tensor(9778.1074, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9778.1064453125
tensor(9778.1074, grad_fn=<NegBackward0>) tensor(9778.1064, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9778.10546875
tensor(9778.1064, grad_fn=<NegBackward0>) tensor(9778.1055, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9778.103515625
tensor(9778.1055, grad_fn=<NegBackward0>) tensor(9778.1035, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9778.103515625
tensor(9778.1035, grad_fn=<NegBackward0>) tensor(9778.1035, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9778.103515625
tensor(9778.1035, grad_fn=<NegBackward0>) tensor(9778.1035, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9778.1015625
tensor(9778.1035, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9778.1015625
tensor(9778.1016, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9778.099609375
tensor(9778.1016, grad_fn=<NegBackward0>) tensor(9778.0996, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9778.0986328125
tensor(9778.0996, grad_fn=<NegBackward0>) tensor(9778.0986, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9778.1162109375
tensor(9778.0986, grad_fn=<NegBackward0>) tensor(9778.1162, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9778.09765625
tensor(9778.0986, grad_fn=<NegBackward0>) tensor(9778.0977, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9778.0966796875
tensor(9778.0977, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9778.09765625
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0977, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9778.0966796875
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9778.095703125
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0957, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9778.0966796875
tensor(9778.0957, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9778.0947265625
tensor(9778.0957, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9778.0947265625
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9778.0947265625
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9778.0947265625
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9778.09375
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9778.09375
tensor(9778.0938, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9778.09375
tensor(9778.0938, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9778.0927734375
tensor(9778.0938, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9778.0927734375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9778.0947265625
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9778.091796875
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9778.0927734375
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9778.0927734375
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -9778.091796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9778.091796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9778.091796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9778.1064453125
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.1064, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9778.091796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9778.107421875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.1074, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9778.0966796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9778.091796875
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9778.08984375
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0898, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9778.0908203125
tensor(9778.0898, grad_fn=<NegBackward0>) tensor(9778.0908, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -9778.103515625
tensor(9778.0898, grad_fn=<NegBackward0>) tensor(9778.1035, grad_fn=<NegBackward0>)
2
pi: tensor([[7.6557e-01, 2.3443e-01],
        [4.5530e-05, 9.9995e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5317, 0.4683], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2171, 0.1112],
         [0.6202, 0.1227]],

        [[0.6880, 0.1345],
         [0.7038, 0.5151]],

        [[0.5426, 0.1314],
         [0.5794, 0.5667]],

        [[0.5617, 0.1054],
         [0.5445, 0.5331]],

        [[0.6972, 0.1361],
         [0.7167, 0.5710]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 15
Adjusted Rand Index: 0.48494736486463325
time is 1
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 24
Adjusted Rand Index: 0.2624523654899462
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 28
Adjusted Rand Index: 0.18673426277989016
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 24
Adjusted Rand Index: 0.2620244697149516
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.05203261628170682
Global Adjusted Rand Index: 0.23236667632411606
Average Adjusted Rand Index: 0.24963821582622558
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23437.75390625
inf tensor(23437.7539, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9801.5419921875
tensor(23437.7539, grad_fn=<NegBackward0>) tensor(9801.5420, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9795.798828125
tensor(9801.5420, grad_fn=<NegBackward0>) tensor(9795.7988, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9794.0869140625
tensor(9795.7988, grad_fn=<NegBackward0>) tensor(9794.0869, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9793.1044921875
tensor(9794.0869, grad_fn=<NegBackward0>) tensor(9793.1045, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9792.0078125
tensor(9793.1045, grad_fn=<NegBackward0>) tensor(9792.0078, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9790.2978515625
tensor(9792.0078, grad_fn=<NegBackward0>) tensor(9790.2979, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9789.1376953125
tensor(9790.2979, grad_fn=<NegBackward0>) tensor(9789.1377, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9788.1787109375
tensor(9789.1377, grad_fn=<NegBackward0>) tensor(9788.1787, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9787.3720703125
tensor(9788.1787, grad_fn=<NegBackward0>) tensor(9787.3721, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9786.5869140625
tensor(9787.3721, grad_fn=<NegBackward0>) tensor(9786.5869, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9785.845703125
tensor(9786.5869, grad_fn=<NegBackward0>) tensor(9785.8457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9785.041015625
tensor(9785.8457, grad_fn=<NegBackward0>) tensor(9785.0410, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9784.3701171875
tensor(9785.0410, grad_fn=<NegBackward0>) tensor(9784.3701, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9783.916015625
tensor(9784.3701, grad_fn=<NegBackward0>) tensor(9783.9160, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9783.775390625
tensor(9783.9160, grad_fn=<NegBackward0>) tensor(9783.7754, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9783.6943359375
tensor(9783.7754, grad_fn=<NegBackward0>) tensor(9783.6943, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9783.6181640625
tensor(9783.6943, grad_fn=<NegBackward0>) tensor(9783.6182, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9783.5498046875
tensor(9783.6182, grad_fn=<NegBackward0>) tensor(9783.5498, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9783.49609375
tensor(9783.5498, grad_fn=<NegBackward0>) tensor(9783.4961, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9783.4453125
tensor(9783.4961, grad_fn=<NegBackward0>) tensor(9783.4453, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9783.40234375
tensor(9783.4453, grad_fn=<NegBackward0>) tensor(9783.4023, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9783.3662109375
tensor(9783.4023, grad_fn=<NegBackward0>) tensor(9783.3662, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9783.337890625
tensor(9783.3662, grad_fn=<NegBackward0>) tensor(9783.3379, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9783.3125
tensor(9783.3379, grad_fn=<NegBackward0>) tensor(9783.3125, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9783.283203125
tensor(9783.3125, grad_fn=<NegBackward0>) tensor(9783.2832, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9783.2421875
tensor(9783.2832, grad_fn=<NegBackward0>) tensor(9783.2422, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9783.126953125
tensor(9783.2422, grad_fn=<NegBackward0>) tensor(9783.1270, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9782.369140625
tensor(9783.1270, grad_fn=<NegBackward0>) tensor(9782.3691, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9780.4140625
tensor(9782.3691, grad_fn=<NegBackward0>) tensor(9780.4141, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9778.9384765625
tensor(9780.4141, grad_fn=<NegBackward0>) tensor(9778.9385, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9778.5029296875
tensor(9778.9385, grad_fn=<NegBackward0>) tensor(9778.5029, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9778.380859375
tensor(9778.5029, grad_fn=<NegBackward0>) tensor(9778.3809, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9778.31640625
tensor(9778.3809, grad_fn=<NegBackward0>) tensor(9778.3164, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9778.271484375
tensor(9778.3164, grad_fn=<NegBackward0>) tensor(9778.2715, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9778.240234375
tensor(9778.2715, grad_fn=<NegBackward0>) tensor(9778.2402, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9778.2197265625
tensor(9778.2402, grad_fn=<NegBackward0>) tensor(9778.2197, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9778.1982421875
tensor(9778.2197, grad_fn=<NegBackward0>) tensor(9778.1982, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9778.18359375
tensor(9778.1982, grad_fn=<NegBackward0>) tensor(9778.1836, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9778.1728515625
tensor(9778.1836, grad_fn=<NegBackward0>) tensor(9778.1729, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9778.1630859375
tensor(9778.1729, grad_fn=<NegBackward0>) tensor(9778.1631, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9778.1533203125
tensor(9778.1631, grad_fn=<NegBackward0>) tensor(9778.1533, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9778.1484375
tensor(9778.1533, grad_fn=<NegBackward0>) tensor(9778.1484, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9778.142578125
tensor(9778.1484, grad_fn=<NegBackward0>) tensor(9778.1426, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9778.13671875
tensor(9778.1426, grad_fn=<NegBackward0>) tensor(9778.1367, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9778.1328125
tensor(9778.1367, grad_fn=<NegBackward0>) tensor(9778.1328, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9778.12890625
tensor(9778.1328, grad_fn=<NegBackward0>) tensor(9778.1289, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9778.1259765625
tensor(9778.1289, grad_fn=<NegBackward0>) tensor(9778.1260, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9778.123046875
tensor(9778.1260, grad_fn=<NegBackward0>) tensor(9778.1230, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9778.1201171875
tensor(9778.1230, grad_fn=<NegBackward0>) tensor(9778.1201, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9778.1181640625
tensor(9778.1201, grad_fn=<NegBackward0>) tensor(9778.1182, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9778.1162109375
tensor(9778.1182, grad_fn=<NegBackward0>) tensor(9778.1162, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9778.11328125
tensor(9778.1162, grad_fn=<NegBackward0>) tensor(9778.1133, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9778.1123046875
tensor(9778.1133, grad_fn=<NegBackward0>) tensor(9778.1123, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9778.111328125
tensor(9778.1123, grad_fn=<NegBackward0>) tensor(9778.1113, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9778.109375
tensor(9778.1113, grad_fn=<NegBackward0>) tensor(9778.1094, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9778.107421875
tensor(9778.1094, grad_fn=<NegBackward0>) tensor(9778.1074, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9778.1064453125
tensor(9778.1074, grad_fn=<NegBackward0>) tensor(9778.1064, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9778.10546875
tensor(9778.1064, grad_fn=<NegBackward0>) tensor(9778.1055, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9778.115234375
tensor(9778.1055, grad_fn=<NegBackward0>) tensor(9778.1152, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9778.1025390625
tensor(9778.1055, grad_fn=<NegBackward0>) tensor(9778.1025, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9778.1025390625
tensor(9778.1025, grad_fn=<NegBackward0>) tensor(9778.1025, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9778.1015625
tensor(9778.1025, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9778.1015625
tensor(9778.1016, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9778.099609375
tensor(9778.1016, grad_fn=<NegBackward0>) tensor(9778.0996, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9778.1015625
tensor(9778.0996, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9778.09765625
tensor(9778.0996, grad_fn=<NegBackward0>) tensor(9778.0977, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9778.09765625
tensor(9778.0977, grad_fn=<NegBackward0>) tensor(9778.0977, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9778.0986328125
tensor(9778.0977, grad_fn=<NegBackward0>) tensor(9778.0986, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9778.0966796875
tensor(9778.0977, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9778.09765625
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0977, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9778.0966796875
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0967, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9778.095703125
tensor(9778.0967, grad_fn=<NegBackward0>) tensor(9778.0957, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9778.095703125
tensor(9778.0957, grad_fn=<NegBackward0>) tensor(9778.0957, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9778.099609375
tensor(9778.0957, grad_fn=<NegBackward0>) tensor(9778.0996, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9778.0947265625
tensor(9778.0957, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9778.095703125
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0957, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9778.0947265625
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9778.09375
tensor(9778.0947, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9778.09375
tensor(9778.0938, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9778.0927734375
tensor(9778.0938, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9778.0927734375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9778.0927734375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9778.0927734375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9778.09375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0938, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9778.0927734375
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0928, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9778.091796875
tensor(9778.0928, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9778.0908203125
tensor(9778.0918, grad_fn=<NegBackward0>) tensor(9778.0908, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9778.0947265625
tensor(9778.0908, grad_fn=<NegBackward0>) tensor(9778.0947, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9778.091796875
tensor(9778.0908, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -9778.091796875
tensor(9778.0908, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -9778.1015625
tensor(9778.0908, grad_fn=<NegBackward0>) tensor(9778.1016, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -9778.091796875
tensor(9778.0908, grad_fn=<NegBackward0>) tensor(9778.0918, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[9.9994e-01, 5.8581e-05],
        [2.3440e-01, 7.6560e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4683, 0.5317], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1227, 0.1112],
         [0.6551, 0.2171]],

        [[0.6115, 0.1345],
         [0.5776, 0.7132]],

        [[0.7160, 0.1314],
         [0.6474, 0.5418]],

        [[0.7167, 0.1054],
         [0.6614, 0.7077]],

        [[0.6929, 0.1360],
         [0.6414, 0.5471]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 85
Adjusted Rand Index: 0.48494736486463325
time is 1
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 76
Adjusted Rand Index: 0.2624523654899462
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18673426277989016
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 76
Adjusted Rand Index: 0.2620244697149516
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.05203261628170682
Global Adjusted Rand Index: 0.23236667632411606
Average Adjusted Rand Index: 0.24963821582622558
[0.23236667632411606, 0.23236667632411606] [0.24963821582622558, 0.24963821582622558] [9778.0908203125, 9778.091796875]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10073.936316951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22459.916015625
inf tensor(22459.9160, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9951.80859375
tensor(22459.9160, grad_fn=<NegBackward0>) tensor(9951.8086, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9949.0771484375
tensor(9951.8086, grad_fn=<NegBackward0>) tensor(9949.0771, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9945.0087890625
tensor(9949.0771, grad_fn=<NegBackward0>) tensor(9945.0088, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9943.734375
tensor(9945.0088, grad_fn=<NegBackward0>) tensor(9943.7344, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9942.7822265625
tensor(9943.7344, grad_fn=<NegBackward0>) tensor(9942.7822, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9942.23046875
tensor(9942.7822, grad_fn=<NegBackward0>) tensor(9942.2305, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9941.7861328125
tensor(9942.2305, grad_fn=<NegBackward0>) tensor(9941.7861, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9941.51171875
tensor(9941.7861, grad_fn=<NegBackward0>) tensor(9941.5117, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9941.3134765625
tensor(9941.5117, grad_fn=<NegBackward0>) tensor(9941.3135, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9941.1640625
tensor(9941.3135, grad_fn=<NegBackward0>) tensor(9941.1641, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9941.033203125
tensor(9941.1641, grad_fn=<NegBackward0>) tensor(9941.0332, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9940.8818359375
tensor(9941.0332, grad_fn=<NegBackward0>) tensor(9940.8818, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9940.76953125
tensor(9940.8818, grad_fn=<NegBackward0>) tensor(9940.7695, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9940.703125
tensor(9940.7695, grad_fn=<NegBackward0>) tensor(9940.7031, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9940.65234375
tensor(9940.7031, grad_fn=<NegBackward0>) tensor(9940.6523, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9940.6044921875
tensor(9940.6523, grad_fn=<NegBackward0>) tensor(9940.6045, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9940.53125
tensor(9940.6045, grad_fn=<NegBackward0>) tensor(9940.5312, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9940.5009765625
tensor(9940.5312, grad_fn=<NegBackward0>) tensor(9940.5010, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9940.4794921875
tensor(9940.5010, grad_fn=<NegBackward0>) tensor(9940.4795, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9940.4619140625
tensor(9940.4795, grad_fn=<NegBackward0>) tensor(9940.4619, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9940.447265625
tensor(9940.4619, grad_fn=<NegBackward0>) tensor(9940.4473, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9940.43359375
tensor(9940.4473, grad_fn=<NegBackward0>) tensor(9940.4336, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9940.4228515625
tensor(9940.4336, grad_fn=<NegBackward0>) tensor(9940.4229, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9940.41015625
tensor(9940.4229, grad_fn=<NegBackward0>) tensor(9940.4102, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9940.400390625
tensor(9940.4102, grad_fn=<NegBackward0>) tensor(9940.4004, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9940.3818359375
tensor(9940.4004, grad_fn=<NegBackward0>) tensor(9940.3818, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9940.3720703125
tensor(9940.3818, grad_fn=<NegBackward0>) tensor(9940.3721, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9940.3662109375
tensor(9940.3721, grad_fn=<NegBackward0>) tensor(9940.3662, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9940.3583984375
tensor(9940.3662, grad_fn=<NegBackward0>) tensor(9940.3584, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9940.34765625
tensor(9940.3584, grad_fn=<NegBackward0>) tensor(9940.3477, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9940.330078125
tensor(9940.3477, grad_fn=<NegBackward0>) tensor(9940.3301, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9940.322265625
tensor(9940.3301, grad_fn=<NegBackward0>) tensor(9940.3223, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9940.3154296875
tensor(9940.3223, grad_fn=<NegBackward0>) tensor(9940.3154, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9940.3056640625
tensor(9940.3154, grad_fn=<NegBackward0>) tensor(9940.3057, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9940.294921875
tensor(9940.3057, grad_fn=<NegBackward0>) tensor(9940.2949, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9940.263671875
tensor(9940.2949, grad_fn=<NegBackward0>) tensor(9940.2637, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9940.2568359375
tensor(9940.2637, grad_fn=<NegBackward0>) tensor(9940.2568, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9940.2529296875
tensor(9940.2568, grad_fn=<NegBackward0>) tensor(9940.2529, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9940.25
tensor(9940.2529, grad_fn=<NegBackward0>) tensor(9940.2500, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9940.2451171875
tensor(9940.2500, grad_fn=<NegBackward0>) tensor(9940.2451, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9940.2412109375
tensor(9940.2451, grad_fn=<NegBackward0>) tensor(9940.2412, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9940.236328125
tensor(9940.2412, grad_fn=<NegBackward0>) tensor(9940.2363, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9940.23046875
tensor(9940.2363, grad_fn=<NegBackward0>) tensor(9940.2305, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9939.994140625
tensor(9940.2305, grad_fn=<NegBackward0>) tensor(9939.9941, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9938.93359375
tensor(9939.9941, grad_fn=<NegBackward0>) tensor(9938.9336, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9938.8876953125
tensor(9938.9336, grad_fn=<NegBackward0>) tensor(9938.8877, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9938.318359375
tensor(9938.8877, grad_fn=<NegBackward0>) tensor(9938.3184, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9938.291015625
tensor(9938.3184, grad_fn=<NegBackward0>) tensor(9938.2910, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9938.2783203125
tensor(9938.2910, grad_fn=<NegBackward0>) tensor(9938.2783, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9938.26953125
tensor(9938.2783, grad_fn=<NegBackward0>) tensor(9938.2695, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9938.263671875
tensor(9938.2695, grad_fn=<NegBackward0>) tensor(9938.2637, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9938.259765625
tensor(9938.2637, grad_fn=<NegBackward0>) tensor(9938.2598, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9938.2548828125
tensor(9938.2598, grad_fn=<NegBackward0>) tensor(9938.2549, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9938.2509765625
tensor(9938.2549, grad_fn=<NegBackward0>) tensor(9938.2510, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9938.2421875
tensor(9938.2510, grad_fn=<NegBackward0>) tensor(9938.2422, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9938.1416015625
tensor(9938.2422, grad_fn=<NegBackward0>) tensor(9938.1416, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9938.138671875
tensor(9938.1416, grad_fn=<NegBackward0>) tensor(9938.1387, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9938.138671875
tensor(9938.1387, grad_fn=<NegBackward0>) tensor(9938.1387, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9938.1357421875
tensor(9938.1387, grad_fn=<NegBackward0>) tensor(9938.1357, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9938.1337890625
tensor(9938.1357, grad_fn=<NegBackward0>) tensor(9938.1338, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9938.1337890625
tensor(9938.1338, grad_fn=<NegBackward0>) tensor(9938.1338, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9938.1328125
tensor(9938.1338, grad_fn=<NegBackward0>) tensor(9938.1328, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9938.1318359375
tensor(9938.1328, grad_fn=<NegBackward0>) tensor(9938.1318, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9938.1298828125
tensor(9938.1318, grad_fn=<NegBackward0>) tensor(9938.1299, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9938.12890625
tensor(9938.1299, grad_fn=<NegBackward0>) tensor(9938.1289, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9938.0439453125
tensor(9938.1289, grad_fn=<NegBackward0>) tensor(9938.0439, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9938.04296875
tensor(9938.0439, grad_fn=<NegBackward0>) tensor(9938.0430, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9938.0439453125
tensor(9938.0430, grad_fn=<NegBackward0>) tensor(9938.0439, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9938.04296875
tensor(9938.0430, grad_fn=<NegBackward0>) tensor(9938.0430, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9938.041015625
tensor(9938.0430, grad_fn=<NegBackward0>) tensor(9938.0410, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9938.02734375
tensor(9938.0410, grad_fn=<NegBackward0>) tensor(9938.0273, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9938.0029296875
tensor(9938.0273, grad_fn=<NegBackward0>) tensor(9938.0029, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9938.0009765625
tensor(9938.0029, grad_fn=<NegBackward0>) tensor(9938.0010, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9938.0009765625
tensor(9938.0010, grad_fn=<NegBackward0>) tensor(9938.0010, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9938.0
tensor(9938.0010, grad_fn=<NegBackward0>) tensor(9938., grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9937.9814453125
tensor(9938., grad_fn=<NegBackward0>) tensor(9937.9814, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9937.9794921875
tensor(9937.9814, grad_fn=<NegBackward0>) tensor(9937.9795, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9937.9404296875
tensor(9937.9795, grad_fn=<NegBackward0>) tensor(9937.9404, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9937.939453125
tensor(9937.9404, grad_fn=<NegBackward0>) tensor(9937.9395, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9937.8857421875
tensor(9937.9395, grad_fn=<NegBackward0>) tensor(9937.8857, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9937.904296875
tensor(9937.8857, grad_fn=<NegBackward0>) tensor(9937.9043, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9937.90625
tensor(9937.8857, grad_fn=<NegBackward0>) tensor(9937.9062, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9937.884765625
tensor(9937.8857, grad_fn=<NegBackward0>) tensor(9937.8848, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9937.8857421875
tensor(9937.8848, grad_fn=<NegBackward0>) tensor(9937.8857, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9937.8837890625
tensor(9937.8848, grad_fn=<NegBackward0>) tensor(9937.8838, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9937.884765625
tensor(9937.8838, grad_fn=<NegBackward0>) tensor(9937.8848, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9937.8828125
tensor(9937.8838, grad_fn=<NegBackward0>) tensor(9937.8828, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9937.8779296875
tensor(9937.8828, grad_fn=<NegBackward0>) tensor(9937.8779, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9937.8759765625
tensor(9937.8779, grad_fn=<NegBackward0>) tensor(9937.8760, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9937.9052734375
tensor(9937.8760, grad_fn=<NegBackward0>) tensor(9937.9053, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9937.875
tensor(9937.8760, grad_fn=<NegBackward0>) tensor(9937.8750, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9937.904296875
tensor(9937.8750, grad_fn=<NegBackward0>) tensor(9937.9043, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9937.8740234375
tensor(9937.8750, grad_fn=<NegBackward0>) tensor(9937.8740, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9937.859375
tensor(9937.8740, grad_fn=<NegBackward0>) tensor(9937.8594, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9937.857421875
tensor(9937.8594, grad_fn=<NegBackward0>) tensor(9937.8574, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9937.8447265625
tensor(9937.8574, grad_fn=<NegBackward0>) tensor(9937.8447, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9937.861328125
tensor(9937.8447, grad_fn=<NegBackward0>) tensor(9937.8613, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9937.8447265625
tensor(9937.8447, grad_fn=<NegBackward0>) tensor(9937.8447, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9937.8427734375
tensor(9937.8447, grad_fn=<NegBackward0>) tensor(9937.8428, grad_fn=<NegBackward0>)
pi: tensor([[1.0000e+00, 3.7132e-06],
        [1.8768e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9288, 0.0712], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1329, 0.1279],
         [0.6075, 0.3162]],

        [[0.6462, 0.1592],
         [0.6475, 0.7242]],

        [[0.5992, 0.1974],
         [0.6984, 0.6224]],

        [[0.7138, 0.1807],
         [0.6320, 0.7088]],

        [[0.6643, 0.1725],
         [0.7156, 0.6854]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.003702958125914224
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.010860348555184451
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.005311456456244215
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.005311456456244215
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.023323334729290386
Global Adjusted Rand Index: -0.0004563642551141373
Average Adjusted Rand Index: -0.0003725769728593439
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22906.603515625
inf tensor(22906.6035, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9949.5634765625
tensor(22906.6035, grad_fn=<NegBackward0>) tensor(9949.5635, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9947.8076171875
tensor(9949.5635, grad_fn=<NegBackward0>) tensor(9947.8076, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9945.599609375
tensor(9947.8076, grad_fn=<NegBackward0>) tensor(9945.5996, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9941.966796875
tensor(9945.5996, grad_fn=<NegBackward0>) tensor(9941.9668, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9941.294921875
tensor(9941.9668, grad_fn=<NegBackward0>) tensor(9941.2949, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9940.970703125
tensor(9941.2949, grad_fn=<NegBackward0>) tensor(9940.9707, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9940.767578125
tensor(9940.9707, grad_fn=<NegBackward0>) tensor(9940.7676, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9940.646484375
tensor(9940.7676, grad_fn=<NegBackward0>) tensor(9940.6465, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9940.5654296875
tensor(9940.6465, grad_fn=<NegBackward0>) tensor(9940.5654, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9940.5068359375
tensor(9940.5654, grad_fn=<NegBackward0>) tensor(9940.5068, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9940.4619140625
tensor(9940.5068, grad_fn=<NegBackward0>) tensor(9940.4619, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9940.42578125
tensor(9940.4619, grad_fn=<NegBackward0>) tensor(9940.4258, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9940.400390625
tensor(9940.4258, grad_fn=<NegBackward0>) tensor(9940.4004, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9940.376953125
tensor(9940.4004, grad_fn=<NegBackward0>) tensor(9940.3770, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9940.359375
tensor(9940.3770, grad_fn=<NegBackward0>) tensor(9940.3594, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9940.3427734375
tensor(9940.3594, grad_fn=<NegBackward0>) tensor(9940.3428, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9940.3291015625
tensor(9940.3428, grad_fn=<NegBackward0>) tensor(9940.3291, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9940.318359375
tensor(9940.3291, grad_fn=<NegBackward0>) tensor(9940.3184, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9940.3095703125
tensor(9940.3184, grad_fn=<NegBackward0>) tensor(9940.3096, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9940.2998046875
tensor(9940.3096, grad_fn=<NegBackward0>) tensor(9940.2998, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9940.28515625
tensor(9940.2998, grad_fn=<NegBackward0>) tensor(9940.2852, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9940.2763671875
tensor(9940.2852, grad_fn=<NegBackward0>) tensor(9940.2764, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9940.2705078125
tensor(9940.2764, grad_fn=<NegBackward0>) tensor(9940.2705, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9940.2607421875
tensor(9940.2705, grad_fn=<NegBackward0>) tensor(9940.2607, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9940.2470703125
tensor(9940.2607, grad_fn=<NegBackward0>) tensor(9940.2471, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9940.244140625
tensor(9940.2471, grad_fn=<NegBackward0>) tensor(9940.2441, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9940.2421875
tensor(9940.2441, grad_fn=<NegBackward0>) tensor(9940.2422, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9940.2392578125
tensor(9940.2422, grad_fn=<NegBackward0>) tensor(9940.2393, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9940.23828125
tensor(9940.2393, grad_fn=<NegBackward0>) tensor(9940.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9940.2353515625
tensor(9940.2383, grad_fn=<NegBackward0>) tensor(9940.2354, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9940.2353515625
tensor(9940.2354, grad_fn=<NegBackward0>) tensor(9940.2354, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9940.2333984375
tensor(9940.2354, grad_fn=<NegBackward0>) tensor(9940.2334, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9940.232421875
tensor(9940.2334, grad_fn=<NegBackward0>) tensor(9940.2324, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9940.228515625
tensor(9940.2324, grad_fn=<NegBackward0>) tensor(9940.2285, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9940.2275390625
tensor(9940.2285, grad_fn=<NegBackward0>) tensor(9940.2275, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9940.2275390625
tensor(9940.2275, grad_fn=<NegBackward0>) tensor(9940.2275, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9940.2265625
tensor(9940.2275, grad_fn=<NegBackward0>) tensor(9940.2266, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9940.2255859375
tensor(9940.2266, grad_fn=<NegBackward0>) tensor(9940.2256, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9940.2255859375
tensor(9940.2256, grad_fn=<NegBackward0>) tensor(9940.2256, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9940.2236328125
tensor(9940.2256, grad_fn=<NegBackward0>) tensor(9940.2236, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9940.22265625
tensor(9940.2236, grad_fn=<NegBackward0>) tensor(9940.2227, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9940.224609375
tensor(9940.2227, grad_fn=<NegBackward0>) tensor(9940.2246, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9940.22265625
tensor(9940.2227, grad_fn=<NegBackward0>) tensor(9940.2227, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9940.2216796875
tensor(9940.2227, grad_fn=<NegBackward0>) tensor(9940.2217, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9940.2216796875
tensor(9940.2217, grad_fn=<NegBackward0>) tensor(9940.2217, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9940.22265625
tensor(9940.2217, grad_fn=<NegBackward0>) tensor(9940.2227, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9940.220703125
tensor(9940.2217, grad_fn=<NegBackward0>) tensor(9940.2207, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9940.2216796875
tensor(9940.2207, grad_fn=<NegBackward0>) tensor(9940.2217, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9940.220703125
tensor(9940.2207, grad_fn=<NegBackward0>) tensor(9940.2207, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9940.2197265625
tensor(9940.2207, grad_fn=<NegBackward0>) tensor(9940.2197, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9940.21875
tensor(9940.2197, grad_fn=<NegBackward0>) tensor(9940.2188, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9940.2197265625
tensor(9940.2188, grad_fn=<NegBackward0>) tensor(9940.2197, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9940.21875
tensor(9940.2188, grad_fn=<NegBackward0>) tensor(9940.2188, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9940.21875
tensor(9940.2188, grad_fn=<NegBackward0>) tensor(9940.2188, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9940.21875
tensor(9940.2188, grad_fn=<NegBackward0>) tensor(9940.2188, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9940.2158203125
tensor(9940.2188, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9940.2177734375
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2178, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9940.2158203125
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9940.216796875
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2168, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9940.2158203125
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9940.2158203125
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9940.21484375
tensor(9940.2158, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9940.216796875
tensor(9940.2148, grad_fn=<NegBackward0>) tensor(9940.2168, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9940.2158203125
tensor(9940.2148, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9940.21484375
tensor(9940.2148, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9940.2138671875
tensor(9940.2148, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -9940.2138671875
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9940.216796875
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2168, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9940.2138671875
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9940.21484375
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -9940.212890625
tensor(9940.2139, grad_fn=<NegBackward0>) tensor(9940.2129, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9940.21484375
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9940.2138671875
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9940.212890625
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2129, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9940.2138671875
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9940.21484375
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9940.2158203125
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2158, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9940.21484375
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2148, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -9940.2138671875
tensor(9940.2129, grad_fn=<NegBackward0>) tensor(9940.2139, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[9.8983e-01, 1.0172e-02],
        [1.1945e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 8.2976e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1352, 0.1314],
         [0.6046, 0.2907]],

        [[0.5569, 0.2300],
         [0.6163, 0.6706]],

        [[0.6751, 0.2447],
         [0.6121, 0.7050]],

        [[0.7151, 0.1902],
         [0.6004, 0.6679]],

        [[0.6644, 0.1834],
         [0.6044, 0.6355]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: -0.001272648886512999
Average Adjusted Rand Index: 0.0002932066765850269
[-0.0004563642551141373, -0.001272648886512999] [-0.0003725769728593439, 0.0002932066765850269] [9937.84375, 9940.2138671875]
-------------------------------------
This iteration is 73
True Objective function: Loss = -9764.76909900153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25609.08984375
inf tensor(25609.0898, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9646.9716796875
tensor(25609.0898, grad_fn=<NegBackward0>) tensor(9646.9717, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9645.6201171875
tensor(9646.9717, grad_fn=<NegBackward0>) tensor(9645.6201, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9641.4013671875
tensor(9645.6201, grad_fn=<NegBackward0>) tensor(9641.4014, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9623.7138671875
tensor(9641.4014, grad_fn=<NegBackward0>) tensor(9623.7139, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9622.50390625
tensor(9623.7139, grad_fn=<NegBackward0>) tensor(9622.5039, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9622.3779296875
tensor(9622.5039, grad_fn=<NegBackward0>) tensor(9622.3779, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9622.314453125
tensor(9622.3779, grad_fn=<NegBackward0>) tensor(9622.3145, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9622.2919921875
tensor(9622.3145, grad_fn=<NegBackward0>) tensor(9622.2920, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9622.287109375
tensor(9622.2920, grad_fn=<NegBackward0>) tensor(9622.2871, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9622.28515625
tensor(9622.2871, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9622.2841796875
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9622.2841796875
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9622.2841796875
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9622.283203125
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2832, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9622.2841796875
tensor(9622.2832, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
1
Iteration 1600: Loss = -9622.2841796875
tensor(9622.2832, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
2
Iteration 1700: Loss = -9622.2841796875
tensor(9622.2832, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
3
Iteration 1800: Loss = -9622.28515625
tensor(9622.2832, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
4
Iteration 1900: Loss = -9622.2841796875
tensor(9622.2832, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
5
Stopping early at iteration 1900 due to no improvement.
pi: tensor([[0.7628, 0.2372],
        [0.1808, 0.8192]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9742, 0.0258], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1284, 0.2160],
         [0.5756, 0.2294]],

        [[0.5977, 0.1098],
         [0.6405, 0.6610]],

        [[0.5885, 0.0999],
         [0.7174, 0.7035]],

        [[0.5377, 0.1014],
         [0.7045, 0.6531]],

        [[0.5476, 0.0879],
         [0.5926, 0.6477]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 73
Adjusted Rand Index: 0.20363636363636364
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 83
Adjusted Rand Index: 0.4299793895002505
time is 3
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.7026542224128787
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.7026396121386246
Global Adjusted Rand Index: 0.3079064601816149
Average Adjusted Rand Index: 0.40762969243475256
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22042.53125
inf tensor(22042.5312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9649.041015625
tensor(22042.5312, grad_fn=<NegBackward0>) tensor(9649.0410, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9648.009765625
tensor(9649.0410, grad_fn=<NegBackward0>) tensor(9648.0098, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9647.611328125
tensor(9648.0098, grad_fn=<NegBackward0>) tensor(9647.6113, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9647.4052734375
tensor(9647.6113, grad_fn=<NegBackward0>) tensor(9647.4053, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9647.208984375
tensor(9647.4053, grad_fn=<NegBackward0>) tensor(9647.2090, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9647.02734375
tensor(9647.2090, grad_fn=<NegBackward0>) tensor(9647.0273, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9646.931640625
tensor(9647.0273, grad_fn=<NegBackward0>) tensor(9646.9316, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9646.837890625
tensor(9646.9316, grad_fn=<NegBackward0>) tensor(9646.8379, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9646.7431640625
tensor(9646.8379, grad_fn=<NegBackward0>) tensor(9646.7432, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9646.6494140625
tensor(9646.7432, grad_fn=<NegBackward0>) tensor(9646.6494, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9646.5537109375
tensor(9646.6494, grad_fn=<NegBackward0>) tensor(9646.5537, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9646.4560546875
tensor(9646.5537, grad_fn=<NegBackward0>) tensor(9646.4561, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9646.3466796875
tensor(9646.4561, grad_fn=<NegBackward0>) tensor(9646.3467, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9646.2216796875
tensor(9646.3467, grad_fn=<NegBackward0>) tensor(9646.2217, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9646.078125
tensor(9646.2217, grad_fn=<NegBackward0>) tensor(9646.0781, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9645.90625
tensor(9646.0781, grad_fn=<NegBackward0>) tensor(9645.9062, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9645.6962890625
tensor(9645.9062, grad_fn=<NegBackward0>) tensor(9645.6963, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9645.3759765625
tensor(9645.6963, grad_fn=<NegBackward0>) tensor(9645.3760, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9644.53125
tensor(9645.3760, grad_fn=<NegBackward0>) tensor(9644.5312, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9639.58984375
tensor(9644.5312, grad_fn=<NegBackward0>) tensor(9639.5898, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9623.369140625
tensor(9639.5898, grad_fn=<NegBackward0>) tensor(9623.3691, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9622.5634765625
tensor(9623.3691, grad_fn=<NegBackward0>) tensor(9622.5635, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9622.40625
tensor(9622.5635, grad_fn=<NegBackward0>) tensor(9622.4062, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9622.3671875
tensor(9622.4062, grad_fn=<NegBackward0>) tensor(9622.3672, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9622.3369140625
tensor(9622.3672, grad_fn=<NegBackward0>) tensor(9622.3369, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9622.3310546875
tensor(9622.3369, grad_fn=<NegBackward0>) tensor(9622.3311, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9622.328125
tensor(9622.3311, grad_fn=<NegBackward0>) tensor(9622.3281, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9622.3271484375
tensor(9622.3281, grad_fn=<NegBackward0>) tensor(9622.3271, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9622.3232421875
tensor(9622.3271, grad_fn=<NegBackward0>) tensor(9622.3232, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9622.3388671875
tensor(9622.3232, grad_fn=<NegBackward0>) tensor(9622.3389, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -9622.3212890625
tensor(9622.3232, grad_fn=<NegBackward0>) tensor(9622.3213, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9622.3203125
tensor(9622.3213, grad_fn=<NegBackward0>) tensor(9622.3203, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9622.3193359375
tensor(9622.3203, grad_fn=<NegBackward0>) tensor(9622.3193, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9622.318359375
tensor(9622.3193, grad_fn=<NegBackward0>) tensor(9622.3184, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9622.3173828125
tensor(9622.3184, grad_fn=<NegBackward0>) tensor(9622.3174, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9622.3173828125
tensor(9622.3174, grad_fn=<NegBackward0>) tensor(9622.3174, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9622.3173828125
tensor(9622.3174, grad_fn=<NegBackward0>) tensor(9622.3174, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9622.3173828125
tensor(9622.3174, grad_fn=<NegBackward0>) tensor(9622.3174, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9622.31640625
tensor(9622.3174, grad_fn=<NegBackward0>) tensor(9622.3164, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9622.314453125
tensor(9622.3164, grad_fn=<NegBackward0>) tensor(9622.3145, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9622.3134765625
tensor(9622.3145, grad_fn=<NegBackward0>) tensor(9622.3135, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9622.3115234375
tensor(9622.3135, grad_fn=<NegBackward0>) tensor(9622.3115, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9622.3095703125
tensor(9622.3115, grad_fn=<NegBackward0>) tensor(9622.3096, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9622.30859375
tensor(9622.3096, grad_fn=<NegBackward0>) tensor(9622.3086, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9622.306640625
tensor(9622.3086, grad_fn=<NegBackward0>) tensor(9622.3066, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9622.3056640625
tensor(9622.3066, grad_fn=<NegBackward0>) tensor(9622.3057, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9622.302734375
tensor(9622.3057, grad_fn=<NegBackward0>) tensor(9622.3027, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9622.302734375
tensor(9622.3027, grad_fn=<NegBackward0>) tensor(9622.3027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9622.302734375
tensor(9622.3027, grad_fn=<NegBackward0>) tensor(9622.3027, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9622.306640625
tensor(9622.3027, grad_fn=<NegBackward0>) tensor(9622.3066, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9622.30078125
tensor(9622.3027, grad_fn=<NegBackward0>) tensor(9622.3008, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9622.2998046875
tensor(9622.3008, grad_fn=<NegBackward0>) tensor(9622.2998, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9622.2998046875
tensor(9622.2998, grad_fn=<NegBackward0>) tensor(9622.2998, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9622.298828125
tensor(9622.2998, grad_fn=<NegBackward0>) tensor(9622.2988, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9622.298828125
tensor(9622.2988, grad_fn=<NegBackward0>) tensor(9622.2988, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9622.2978515625
tensor(9622.2988, grad_fn=<NegBackward0>) tensor(9622.2979, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9622.3037109375
tensor(9622.2979, grad_fn=<NegBackward0>) tensor(9622.3037, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9622.2978515625
tensor(9622.2979, grad_fn=<NegBackward0>) tensor(9622.2979, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9622.2958984375
tensor(9622.2979, grad_fn=<NegBackward0>) tensor(9622.2959, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9622.2958984375
tensor(9622.2959, grad_fn=<NegBackward0>) tensor(9622.2959, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9622.294921875
tensor(9622.2959, grad_fn=<NegBackward0>) tensor(9622.2949, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9622.2958984375
tensor(9622.2949, grad_fn=<NegBackward0>) tensor(9622.2959, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9622.294921875
tensor(9622.2949, grad_fn=<NegBackward0>) tensor(9622.2949, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9622.296875
tensor(9622.2949, grad_fn=<NegBackward0>) tensor(9622.2969, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9622.294921875
tensor(9622.2949, grad_fn=<NegBackward0>) tensor(9622.2949, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9622.2939453125
tensor(9622.2949, grad_fn=<NegBackward0>) tensor(9622.2939, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9622.294921875
tensor(9622.2939, grad_fn=<NegBackward0>) tensor(9622.2949, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9622.294921875
tensor(9622.2939, grad_fn=<NegBackward0>) tensor(9622.2949, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9622.30078125
tensor(9622.2939, grad_fn=<NegBackward0>) tensor(9622.3008, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -9622.2939453125
tensor(9622.2939, grad_fn=<NegBackward0>) tensor(9622.2939, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9622.29296875
tensor(9622.2939, grad_fn=<NegBackward0>) tensor(9622.2930, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9622.2939453125
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2939, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9622.2939453125
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2939, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9622.296875
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2969, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -9622.29296875
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2930, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9622.2939453125
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2939, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9622.29296875
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2930, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9622.2919921875
tensor(9622.2930, grad_fn=<NegBackward0>) tensor(9622.2920, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9622.2919921875
tensor(9622.2920, grad_fn=<NegBackward0>) tensor(9622.2920, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9622.2890625
tensor(9622.2920, grad_fn=<NegBackward0>) tensor(9622.2891, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9622.291015625
tensor(9622.2891, grad_fn=<NegBackward0>) tensor(9622.2910, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9622.2890625
tensor(9622.2891, grad_fn=<NegBackward0>) tensor(9622.2891, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9622.2900390625
tensor(9622.2891, grad_fn=<NegBackward0>) tensor(9622.2900, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9622.2900390625
tensor(9622.2891, grad_fn=<NegBackward0>) tensor(9622.2900, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -9622.287109375
tensor(9622.2891, grad_fn=<NegBackward0>) tensor(9622.2871, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9622.2861328125
tensor(9622.2871, grad_fn=<NegBackward0>) tensor(9622.2861, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9622.287109375
tensor(9622.2861, grad_fn=<NegBackward0>) tensor(9622.2871, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9622.2978515625
tensor(9622.2861, grad_fn=<NegBackward0>) tensor(9622.2979, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -9622.28515625
tensor(9622.2861, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9622.2919921875
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2920, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9622.2861328125
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2861, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -9622.287109375
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2871, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -9622.28515625
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9622.2841796875
tensor(9622.2852, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9622.4794921875
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.4795, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9622.2841796875
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2842, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9622.28515625
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9622.28515625
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2852, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9622.2890625
tensor(9622.2842, grad_fn=<NegBackward0>) tensor(9622.2891, grad_fn=<NegBackward0>)
3
pi: tensor([[0.8192, 0.1808],
        [0.2372, 0.7628]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0257, 0.9743], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2294, 0.2161],
         [0.6876, 0.1284]],

        [[0.6182, 0.1098],
         [0.5225, 0.6818]],

        [[0.7142, 0.0999],
         [0.6629, 0.7017]],

        [[0.6962, 0.1014],
         [0.6303, 0.7100]],

        [[0.6835, 0.0879],
         [0.6415, 0.6822]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 27
Adjusted Rand Index: 0.20363636363636364
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 17
Adjusted Rand Index: 0.4299793895002505
time is 3
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026542224128787
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026396121386246
Global Adjusted Rand Index: 0.3079064601816149
Average Adjusted Rand Index: 0.40762969243475256
[0.3079064601816149, 0.3079064601816149] [0.40762969243475256, 0.40762969243475256] [9622.2841796875, 9622.28515625]
-------------------------------------
This iteration is 74
True Objective function: Loss = -10043.897418641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23440.435546875
inf tensor(23440.4355, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9951.310546875
tensor(23440.4355, grad_fn=<NegBackward0>) tensor(9951.3105, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9950.6884765625
tensor(9951.3105, grad_fn=<NegBackward0>) tensor(9950.6885, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9950.4326171875
tensor(9950.6885, grad_fn=<NegBackward0>) tensor(9950.4326, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9950.1611328125
tensor(9950.4326, grad_fn=<NegBackward0>) tensor(9950.1611, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9949.7998046875
tensor(9950.1611, grad_fn=<NegBackward0>) tensor(9949.7998, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9949.515625
tensor(9949.7998, grad_fn=<NegBackward0>) tensor(9949.5156, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9949.34375
tensor(9949.5156, grad_fn=<NegBackward0>) tensor(9949.3438, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9949.150390625
tensor(9949.3438, grad_fn=<NegBackward0>) tensor(9949.1504, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9948.9423828125
tensor(9949.1504, grad_fn=<NegBackward0>) tensor(9948.9424, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9948.8271484375
tensor(9948.9424, grad_fn=<NegBackward0>) tensor(9948.8271, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9948.8037109375
tensor(9948.8271, grad_fn=<NegBackward0>) tensor(9948.8037, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9948.791015625
tensor(9948.8037, grad_fn=<NegBackward0>) tensor(9948.7910, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9948.7841796875
tensor(9948.7910, grad_fn=<NegBackward0>) tensor(9948.7842, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9948.775390625
tensor(9948.7842, grad_fn=<NegBackward0>) tensor(9948.7754, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9948.763671875
tensor(9948.7754, grad_fn=<NegBackward0>) tensor(9948.7637, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9948.7421875
tensor(9948.7637, grad_fn=<NegBackward0>) tensor(9948.7422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9948.7041015625
tensor(9948.7422, grad_fn=<NegBackward0>) tensor(9948.7041, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9948.638671875
tensor(9948.7041, grad_fn=<NegBackward0>) tensor(9948.6387, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9948.5869140625
tensor(9948.6387, grad_fn=<NegBackward0>) tensor(9948.5869, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9948.5654296875
tensor(9948.5869, grad_fn=<NegBackward0>) tensor(9948.5654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9948.556640625
tensor(9948.5654, grad_fn=<NegBackward0>) tensor(9948.5566, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9948.5517578125
tensor(9948.5566, grad_fn=<NegBackward0>) tensor(9948.5518, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9948.55078125
tensor(9948.5518, grad_fn=<NegBackward0>) tensor(9948.5508, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9948.546875
tensor(9948.5508, grad_fn=<NegBackward0>) tensor(9948.5469, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9948.5458984375
tensor(9948.5469, grad_fn=<NegBackward0>) tensor(9948.5459, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9948.544921875
tensor(9948.5459, grad_fn=<NegBackward0>) tensor(9948.5449, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9948.5458984375
tensor(9948.5449, grad_fn=<NegBackward0>) tensor(9948.5459, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9948.54296875
tensor(9948.5449, grad_fn=<NegBackward0>) tensor(9948.5430, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9948.54296875
tensor(9948.5430, grad_fn=<NegBackward0>) tensor(9948.5430, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9948.5419921875
tensor(9948.5430, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9948.5419921875
tensor(9948.5420, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9948.5419921875
tensor(9948.5420, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9948.5419921875
tensor(9948.5420, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9948.541015625
tensor(9948.5420, grad_fn=<NegBackward0>) tensor(9948.5410, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9948.5419921875
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9948.541015625
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5410, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9948.541015625
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9948.5419921875
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -9948.541015625
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5410, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9948.560546875
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5605, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9948.5400390625
tensor(9948.5410, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9948.544921875
tensor(9948.5400, grad_fn=<NegBackward0>) tensor(9948.5449, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9948.5400390625
tensor(9948.5400, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9948.5400390625
tensor(9948.5400, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9948.5400390625
tensor(9948.5400, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9948.5390625
tensor(9948.5400, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9948.5390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9948.5390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9948.5390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9948.5390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9948.5390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9948.5400390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9948.5400390625
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9948.5419921875
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9948.560546875
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5605, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -9948.537109375
tensor(9948.5391, grad_fn=<NegBackward0>) tensor(9948.5371, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9948.5380859375
tensor(9948.5371, grad_fn=<NegBackward0>) tensor(9948.5381, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9948.5400390625
tensor(9948.5371, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -9948.5400390625
tensor(9948.5371, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -9948.5400390625
tensor(9948.5371, grad_fn=<NegBackward0>) tensor(9948.5400, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -9948.5419921875
tensor(9948.5371, grad_fn=<NegBackward0>) tensor(9948.5420, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.3461, 0.6539],
        [0.1268, 0.8732]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.7574e-04, 9.9962e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.0992],
         [0.6339, 0.1309]],

        [[0.5115, 0.1763],
         [0.6445, 0.6994]],

        [[0.7151, 0.1551],
         [0.6457, 0.7192]],

        [[0.6940, 0.1608],
         [0.5693, 0.6784]],

        [[0.6647, 0.1530],
         [0.5297, 0.5946]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.020678759622205528
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.012285862605987194
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: -0.006137112529496403
Average Adjusted Rand Index: -0.006969169693697828
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24102.67578125
inf tensor(24102.6758, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9953.0791015625
tensor(24102.6758, grad_fn=<NegBackward0>) tensor(9953.0791, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9952.0849609375
tensor(9953.0791, grad_fn=<NegBackward0>) tensor(9952.0850, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9951.7177734375
tensor(9952.0850, grad_fn=<NegBackward0>) tensor(9951.7178, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9951.2421875
tensor(9951.7178, grad_fn=<NegBackward0>) tensor(9951.2422, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9950.8115234375
tensor(9951.2422, grad_fn=<NegBackward0>) tensor(9950.8115, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9950.4931640625
tensor(9950.8115, grad_fn=<NegBackward0>) tensor(9950.4932, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9950.2685546875
tensor(9950.4932, grad_fn=<NegBackward0>) tensor(9950.2686, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9950.1005859375
tensor(9950.2686, grad_fn=<NegBackward0>) tensor(9950.1006, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9949.9453125
tensor(9950.1006, grad_fn=<NegBackward0>) tensor(9949.9453, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9949.7841796875
tensor(9949.9453, grad_fn=<NegBackward0>) tensor(9949.7842, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9949.625
tensor(9949.7842, grad_fn=<NegBackward0>) tensor(9949.6250, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9949.4697265625
tensor(9949.6250, grad_fn=<NegBackward0>) tensor(9949.4697, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9949.3203125
tensor(9949.4697, grad_fn=<NegBackward0>) tensor(9949.3203, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9949.1767578125
tensor(9949.3203, grad_fn=<NegBackward0>) tensor(9949.1768, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9949.0390625
tensor(9949.1768, grad_fn=<NegBackward0>) tensor(9949.0391, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9948.9052734375
tensor(9949.0391, grad_fn=<NegBackward0>) tensor(9948.9053, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9948.7626953125
tensor(9948.9053, grad_fn=<NegBackward0>) tensor(9948.7627, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9948.435546875
tensor(9948.7627, grad_fn=<NegBackward0>) tensor(9948.4355, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9948.126953125
tensor(9948.4355, grad_fn=<NegBackward0>) tensor(9948.1270, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9947.9990234375
tensor(9948.1270, grad_fn=<NegBackward0>) tensor(9947.9990, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9947.9072265625
tensor(9947.9990, grad_fn=<NegBackward0>) tensor(9947.9072, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9947.833984375
tensor(9947.9072, grad_fn=<NegBackward0>) tensor(9947.8340, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9947.77734375
tensor(9947.8340, grad_fn=<NegBackward0>) tensor(9947.7773, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9947.7236328125
tensor(9947.7773, grad_fn=<NegBackward0>) tensor(9947.7236, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9947.6630859375
tensor(9947.7236, grad_fn=<NegBackward0>) tensor(9947.6631, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9947.587890625
tensor(9947.6631, grad_fn=<NegBackward0>) tensor(9947.5879, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9947.4775390625
tensor(9947.5879, grad_fn=<NegBackward0>) tensor(9947.4775, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9947.3291015625
tensor(9947.4775, grad_fn=<NegBackward0>) tensor(9947.3291, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9947.1748046875
tensor(9947.3291, grad_fn=<NegBackward0>) tensor(9947.1748, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9947.033203125
tensor(9947.1748, grad_fn=<NegBackward0>) tensor(9947.0332, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9946.8876953125
tensor(9947.0332, grad_fn=<NegBackward0>) tensor(9946.8877, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9946.6669921875
tensor(9946.8877, grad_fn=<NegBackward0>) tensor(9946.6670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9943.58984375
tensor(9946.6670, grad_fn=<NegBackward0>) tensor(9943.5898, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9939.109375
tensor(9943.5898, grad_fn=<NegBackward0>) tensor(9939.1094, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9938.7744140625
tensor(9939.1094, grad_fn=<NegBackward0>) tensor(9938.7744, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9938.66796875
tensor(9938.7744, grad_fn=<NegBackward0>) tensor(9938.6680, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9938.61328125
tensor(9938.6680, grad_fn=<NegBackward0>) tensor(9938.6133, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9938.5810546875
tensor(9938.6133, grad_fn=<NegBackward0>) tensor(9938.5811, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9938.5595703125
tensor(9938.5811, grad_fn=<NegBackward0>) tensor(9938.5596, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9938.544921875
tensor(9938.5596, grad_fn=<NegBackward0>) tensor(9938.5449, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9938.5322265625
tensor(9938.5449, grad_fn=<NegBackward0>) tensor(9938.5322, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9938.5224609375
tensor(9938.5322, grad_fn=<NegBackward0>) tensor(9938.5225, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9938.5146484375
tensor(9938.5225, grad_fn=<NegBackward0>) tensor(9938.5146, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9938.509765625
tensor(9938.5146, grad_fn=<NegBackward0>) tensor(9938.5098, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9938.5048828125
tensor(9938.5098, grad_fn=<NegBackward0>) tensor(9938.5049, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9938.5009765625
tensor(9938.5049, grad_fn=<NegBackward0>) tensor(9938.5010, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9938.4970703125
tensor(9938.5010, grad_fn=<NegBackward0>) tensor(9938.4971, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9938.4931640625
tensor(9938.4971, grad_fn=<NegBackward0>) tensor(9938.4932, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9938.490234375
tensor(9938.4932, grad_fn=<NegBackward0>) tensor(9938.4902, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9938.4912109375
tensor(9938.4902, grad_fn=<NegBackward0>) tensor(9938.4912, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9938.4873046875
tensor(9938.4902, grad_fn=<NegBackward0>) tensor(9938.4873, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9938.484375
tensor(9938.4873, grad_fn=<NegBackward0>) tensor(9938.4844, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9938.4833984375
tensor(9938.4844, grad_fn=<NegBackward0>) tensor(9938.4834, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9938.4814453125
tensor(9938.4834, grad_fn=<NegBackward0>) tensor(9938.4814, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9938.48046875
tensor(9938.4814, grad_fn=<NegBackward0>) tensor(9938.4805, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9938.478515625
tensor(9938.4805, grad_fn=<NegBackward0>) tensor(9938.4785, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9938.478515625
tensor(9938.4785, grad_fn=<NegBackward0>) tensor(9938.4785, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9938.4775390625
tensor(9938.4785, grad_fn=<NegBackward0>) tensor(9938.4775, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9938.4755859375
tensor(9938.4775, grad_fn=<NegBackward0>) tensor(9938.4756, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9938.49609375
tensor(9938.4756, grad_fn=<NegBackward0>) tensor(9938.4961, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9938.4755859375
tensor(9938.4756, grad_fn=<NegBackward0>) tensor(9938.4756, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9938.4736328125
tensor(9938.4756, grad_fn=<NegBackward0>) tensor(9938.4736, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9938.47265625
tensor(9938.4736, grad_fn=<NegBackward0>) tensor(9938.4727, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9938.47265625
tensor(9938.4727, grad_fn=<NegBackward0>) tensor(9938.4727, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9938.4736328125
tensor(9938.4727, grad_fn=<NegBackward0>) tensor(9938.4736, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9938.47265625
tensor(9938.4727, grad_fn=<NegBackward0>) tensor(9938.4727, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9938.4716796875
tensor(9938.4727, grad_fn=<NegBackward0>) tensor(9938.4717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9938.470703125
tensor(9938.4717, grad_fn=<NegBackward0>) tensor(9938.4707, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9938.470703125
tensor(9938.4707, grad_fn=<NegBackward0>) tensor(9938.4707, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9938.46875
tensor(9938.4707, grad_fn=<NegBackward0>) tensor(9938.4688, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9938.470703125
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4707, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9938.46875
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4688, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9938.470703125
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4707, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9938.46875
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4688, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9938.474609375
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4746, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9938.466796875
tensor(9938.4688, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9938.466796875
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9938.4951171875
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4951, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9938.4677734375
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4678, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9938.4677734375
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4678, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -9938.466796875
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9938.4677734375
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4678, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9938.4658203125
tensor(9938.4668, grad_fn=<NegBackward0>) tensor(9938.4658, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9938.4677734375
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4678, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9938.466796875
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9938.5185546875
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.5186, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9938.466796875
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9938.4658203125
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4658, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9938.4658203125
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4658, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9938.466796875
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4668, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9938.4658203125
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4658, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9938.4658203125
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4658, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9938.58984375
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.5898, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9938.46484375
tensor(9938.4658, grad_fn=<NegBackward0>) tensor(9938.4648, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9938.478515625
tensor(9938.4648, grad_fn=<NegBackward0>) tensor(9938.4785, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9938.46484375
tensor(9938.4648, grad_fn=<NegBackward0>) tensor(9938.4648, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9938.46484375
tensor(9938.4648, grad_fn=<NegBackward0>) tensor(9938.4648, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9938.46484375
tensor(9938.4648, grad_fn=<NegBackward0>) tensor(9938.4648, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9938.4638671875
tensor(9938.4648, grad_fn=<NegBackward0>) tensor(9938.4639, grad_fn=<NegBackward0>)
pi: tensor([[8.7829e-01, 1.2171e-01],
        [4.4643e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1749, 0.0881],
         [0.6375, 0.1323]],

        [[0.5707, 0.1175],
         [0.6261, 0.6081]],

        [[0.5769, 0.1096],
         [0.6514, 0.5500]],

        [[0.5885, 0.1367],
         [0.5224, 0.7301]],

        [[0.7231, 0.1281],
         [0.7040, 0.6403]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 25
Adjusted Rand Index: 0.24370826390048933
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28493472489018856
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.28444444444444444
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.12084754893158556
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.08080808080808081
Global Adjusted Rand Index: 0.1990156569634378
Average Adjusted Rand Index: 0.20294861259495772
[-0.006137112529496403, 0.1990156569634378] [-0.006969169693697828, 0.20294861259495772] [9948.5419921875, 9938.4697265625]
-------------------------------------
This iteration is 75
True Objective function: Loss = -9977.3720690645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22053.451171875
inf tensor(22053.4512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9844.1064453125
tensor(22053.4512, grad_fn=<NegBackward0>) tensor(9844.1064, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9843.42578125
tensor(9844.1064, grad_fn=<NegBackward0>) tensor(9843.4258, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9843.2353515625
tensor(9843.4258, grad_fn=<NegBackward0>) tensor(9843.2354, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9843.1015625
tensor(9843.2354, grad_fn=<NegBackward0>) tensor(9843.1016, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9842.982421875
tensor(9843.1016, grad_fn=<NegBackward0>) tensor(9842.9824, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9842.8779296875
tensor(9842.9824, grad_fn=<NegBackward0>) tensor(9842.8779, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9842.7861328125
tensor(9842.8779, grad_fn=<NegBackward0>) tensor(9842.7861, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9842.7001953125
tensor(9842.7861, grad_fn=<NegBackward0>) tensor(9842.7002, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9842.6171875
tensor(9842.7002, grad_fn=<NegBackward0>) tensor(9842.6172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9842.53125
tensor(9842.6172, grad_fn=<NegBackward0>) tensor(9842.5312, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9842.4384765625
tensor(9842.5312, grad_fn=<NegBackward0>) tensor(9842.4385, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9842.3330078125
tensor(9842.4385, grad_fn=<NegBackward0>) tensor(9842.3330, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9842.2158203125
tensor(9842.3330, grad_fn=<NegBackward0>) tensor(9842.2158, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9842.0810546875
tensor(9842.2158, grad_fn=<NegBackward0>) tensor(9842.0811, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9841.92578125
tensor(9842.0811, grad_fn=<NegBackward0>) tensor(9841.9258, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9841.748046875
tensor(9841.9258, grad_fn=<NegBackward0>) tensor(9841.7480, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9841.5498046875
tensor(9841.7480, grad_fn=<NegBackward0>) tensor(9841.5498, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9841.359375
tensor(9841.5498, grad_fn=<NegBackward0>) tensor(9841.3594, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9841.2021484375
tensor(9841.3594, grad_fn=<NegBackward0>) tensor(9841.2021, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9841.08984375
tensor(9841.2021, grad_fn=<NegBackward0>) tensor(9841.0898, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9841.0185546875
tensor(9841.0898, grad_fn=<NegBackward0>) tensor(9841.0186, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9840.974609375
tensor(9841.0186, grad_fn=<NegBackward0>) tensor(9840.9746, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9840.9443359375
tensor(9840.9746, grad_fn=<NegBackward0>) tensor(9840.9443, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9840.9267578125
tensor(9840.9443, grad_fn=<NegBackward0>) tensor(9840.9268, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9840.9140625
tensor(9840.9268, grad_fn=<NegBackward0>) tensor(9840.9141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9840.90625
tensor(9840.9141, grad_fn=<NegBackward0>) tensor(9840.9062, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9840.900390625
tensor(9840.9062, grad_fn=<NegBackward0>) tensor(9840.9004, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9840.8935546875
tensor(9840.9004, grad_fn=<NegBackward0>) tensor(9840.8936, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9840.892578125
tensor(9840.8936, grad_fn=<NegBackward0>) tensor(9840.8926, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9840.888671875
tensor(9840.8926, grad_fn=<NegBackward0>) tensor(9840.8887, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9840.8876953125
tensor(9840.8887, grad_fn=<NegBackward0>) tensor(9840.8877, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9840.884765625
tensor(9840.8877, grad_fn=<NegBackward0>) tensor(9840.8848, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9840.884765625
tensor(9840.8848, grad_fn=<NegBackward0>) tensor(9840.8848, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9840.8837890625
tensor(9840.8848, grad_fn=<NegBackward0>) tensor(9840.8838, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9840.8828125
tensor(9840.8838, grad_fn=<NegBackward0>) tensor(9840.8828, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9840.880859375
tensor(9840.8828, grad_fn=<NegBackward0>) tensor(9840.8809, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9840.8798828125
tensor(9840.8809, grad_fn=<NegBackward0>) tensor(9840.8799, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9840.87890625
tensor(9840.8799, grad_fn=<NegBackward0>) tensor(9840.8789, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9840.8779296875
tensor(9840.8789, grad_fn=<NegBackward0>) tensor(9840.8779, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9840.8779296875
tensor(9840.8779, grad_fn=<NegBackward0>) tensor(9840.8779, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9840.876953125
tensor(9840.8779, grad_fn=<NegBackward0>) tensor(9840.8770, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9840.876953125
tensor(9840.8770, grad_fn=<NegBackward0>) tensor(9840.8770, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9840.8740234375
tensor(9840.8770, grad_fn=<NegBackward0>) tensor(9840.8740, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9840.875
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8750, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9840.875
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8750, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -9840.8740234375
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8740, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9840.8740234375
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8740, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9840.875
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8750, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9840.8740234375
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8740, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9840.873046875
tensor(9840.8740, grad_fn=<NegBackward0>) tensor(9840.8730, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9840.873046875
tensor(9840.8730, grad_fn=<NegBackward0>) tensor(9840.8730, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9840.873046875
tensor(9840.8730, grad_fn=<NegBackward0>) tensor(9840.8730, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9840.873046875
tensor(9840.8730, grad_fn=<NegBackward0>) tensor(9840.8730, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9840.8720703125
tensor(9840.8730, grad_fn=<NegBackward0>) tensor(9840.8721, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9840.8720703125
tensor(9840.8721, grad_fn=<NegBackward0>) tensor(9840.8721, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9840.8720703125
tensor(9840.8721, grad_fn=<NegBackward0>) tensor(9840.8721, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9840.8720703125
tensor(9840.8721, grad_fn=<NegBackward0>) tensor(9840.8721, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9840.873046875
tensor(9840.8721, grad_fn=<NegBackward0>) tensor(9840.8730, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9840.8701171875
tensor(9840.8721, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9840.87109375
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9840.8720703125
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8721, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9840.87109375
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -9840.87109375
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -9840.869140625
tensor(9840.8701, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9840.8701171875
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9840.8701171875
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9840.87109375
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -9840.869140625
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9840.87109375
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9840.869140625
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9840.87109375
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9840.8701171875
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9840.8740234375
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8740, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -9840.8701171875
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -9840.869140625
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9840.8681640625
tensor(9840.8691, grad_fn=<NegBackward0>) tensor(9840.8682, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9840.8701171875
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9840.869140625
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9840.87109375
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8711, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9840.8681640625
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8682, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9840.8701171875
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9840.869140625
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9840.869140625
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8691, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -9840.892578125
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8926, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -9840.8701171875
tensor(9840.8682, grad_fn=<NegBackward0>) tensor(9840.8701, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.0638, 0.9362],
        [0.0429, 0.9571]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9577, 0.0423], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1447, 0.1404],
         [0.5648, 0.1323]],

        [[0.5607, 0.1451],
         [0.5135, 0.7199]],

        [[0.6499, 0.1791],
         [0.5460, 0.6521]],

        [[0.5647, 0.1229],
         [0.5575, 0.6734]],

        [[0.5820, 0.1579],
         [0.5407, 0.7118]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010082400599258376
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24453.92578125
inf tensor(24453.9258, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9843.2314453125
tensor(24453.9258, grad_fn=<NegBackward0>) tensor(9843.2314, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9842.0185546875
tensor(9843.2314, grad_fn=<NegBackward0>) tensor(9842.0186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9841.7861328125
tensor(9842.0186, grad_fn=<NegBackward0>) tensor(9841.7861, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9841.6025390625
tensor(9841.7861, grad_fn=<NegBackward0>) tensor(9841.6025, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9841.4267578125
tensor(9841.6025, grad_fn=<NegBackward0>) tensor(9841.4268, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9841.255859375
tensor(9841.4268, grad_fn=<NegBackward0>) tensor(9841.2559, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9841.0986328125
tensor(9841.2559, grad_fn=<NegBackward0>) tensor(9841.0986, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9840.9609375
tensor(9841.0986, grad_fn=<NegBackward0>) tensor(9840.9609, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9840.830078125
tensor(9840.9609, grad_fn=<NegBackward0>) tensor(9840.8301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9840.681640625
tensor(9840.8301, grad_fn=<NegBackward0>) tensor(9840.6816, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9840.5244140625
tensor(9840.6816, grad_fn=<NegBackward0>) tensor(9840.5244, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9840.3798828125
tensor(9840.5244, grad_fn=<NegBackward0>) tensor(9840.3799, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9840.271484375
tensor(9840.3799, grad_fn=<NegBackward0>) tensor(9840.2715, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9840.197265625
tensor(9840.2715, grad_fn=<NegBackward0>) tensor(9840.1973, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9840.1513671875
tensor(9840.1973, grad_fn=<NegBackward0>) tensor(9840.1514, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9840.1240234375
tensor(9840.1514, grad_fn=<NegBackward0>) tensor(9840.1240, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9840.1083984375
tensor(9840.1240, grad_fn=<NegBackward0>) tensor(9840.1084, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9840.0986328125
tensor(9840.1084, grad_fn=<NegBackward0>) tensor(9840.0986, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9840.0927734375
tensor(9840.0986, grad_fn=<NegBackward0>) tensor(9840.0928, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9840.08984375
tensor(9840.0928, grad_fn=<NegBackward0>) tensor(9840.0898, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9840.0869140625
tensor(9840.0898, grad_fn=<NegBackward0>) tensor(9840.0869, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9840.083984375
tensor(9840.0869, grad_fn=<NegBackward0>) tensor(9840.0840, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9840.083984375
tensor(9840.0840, grad_fn=<NegBackward0>) tensor(9840.0840, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9840.0830078125
tensor(9840.0840, grad_fn=<NegBackward0>) tensor(9840.0830, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9840.0830078125
tensor(9840.0830, grad_fn=<NegBackward0>) tensor(9840.0830, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9840.080078125
tensor(9840.0830, grad_fn=<NegBackward0>) tensor(9840.0801, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9840.0810546875
tensor(9840.0801, grad_fn=<NegBackward0>) tensor(9840.0811, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -9840.0810546875
tensor(9840.0801, grad_fn=<NegBackward0>) tensor(9840.0811, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -9840.0791015625
tensor(9840.0801, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9840.080078125
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0801, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9840.0791015625
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9840.078125
tensor(9840.0791, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9840.078125
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9840.078125
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9840.0791015625
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9840.078125
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9840.078125
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9840.0791015625
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9840.0791015625
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -9840.078125
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9840.0771484375
tensor(9840.0781, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9840.0791015625
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9840.0791015625
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -9840.0791015625
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9840.0771484375
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9840.078125
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -9840.076171875
tensor(9840.0771, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9840.0771484375
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9840.0771484375
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9840.080078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0801, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9840.0791015625
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9840.076171875
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0762, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9840.078125
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0781, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9840.0791015625
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -9840.0771484375
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -9840.0771484375
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0771, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -9840.0791015625
tensor(9840.0762, grad_fn=<NegBackward0>) tensor(9840.0791, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.9882, 0.0118],
        [0.9835, 0.0165]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0036, 0.9964], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1345, 0.1434],
         [0.6809, 0.1441]],

        [[0.6362, 0.0791],
         [0.6346, 0.5049]],

        [[0.6079, 0.1955],
         [0.5795, 0.6747]],

        [[0.5873, 0.0348],
         [0.6235, 0.6933]],

        [[0.7139, 0.1637],
         [0.5892, 0.6655]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013325989538133503
Average Adjusted Rand Index: -0.0007272727272727272
[-0.0010082400599258376, -0.0013325989538133503] [0.0, -0.0007272727272727272] [9840.8701171875, 9840.0791015625]
-------------------------------------
This iteration is 76
True Objective function: Loss = -9722.793636207021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21841.693359375
inf tensor(21841.6934, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9569.8154296875
tensor(21841.6934, grad_fn=<NegBackward0>) tensor(9569.8154, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9568.53515625
tensor(9569.8154, grad_fn=<NegBackward0>) tensor(9568.5352, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9568.0673828125
tensor(9568.5352, grad_fn=<NegBackward0>) tensor(9568.0674, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9567.8115234375
tensor(9568.0674, grad_fn=<NegBackward0>) tensor(9567.8115, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9567.55078125
tensor(9567.8115, grad_fn=<NegBackward0>) tensor(9567.5508, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9566.646484375
tensor(9567.5508, grad_fn=<NegBackward0>) tensor(9566.6465, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9565.7939453125
tensor(9566.6465, grad_fn=<NegBackward0>) tensor(9565.7939, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9565.134765625
tensor(9565.7939, grad_fn=<NegBackward0>) tensor(9565.1348, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9564.5625
tensor(9565.1348, grad_fn=<NegBackward0>) tensor(9564.5625, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9562.091796875
tensor(9564.5625, grad_fn=<NegBackward0>) tensor(9562.0918, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9561.0791015625
tensor(9562.0918, grad_fn=<NegBackward0>) tensor(9561.0791, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9560.732421875
tensor(9561.0791, grad_fn=<NegBackward0>) tensor(9560.7324, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9560.556640625
tensor(9560.7324, grad_fn=<NegBackward0>) tensor(9560.5566, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9560.4453125
tensor(9560.5566, grad_fn=<NegBackward0>) tensor(9560.4453, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9560.365234375
tensor(9560.4453, grad_fn=<NegBackward0>) tensor(9560.3652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9560.3037109375
tensor(9560.3652, grad_fn=<NegBackward0>) tensor(9560.3037, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9560.2548828125
tensor(9560.3037, grad_fn=<NegBackward0>) tensor(9560.2549, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9560.2177734375
tensor(9560.2549, grad_fn=<NegBackward0>) tensor(9560.2178, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9560.185546875
tensor(9560.2178, grad_fn=<NegBackward0>) tensor(9560.1855, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9560.1611328125
tensor(9560.1855, grad_fn=<NegBackward0>) tensor(9560.1611, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9560.1396484375
tensor(9560.1611, grad_fn=<NegBackward0>) tensor(9560.1396, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9560.12109375
tensor(9560.1396, grad_fn=<NegBackward0>) tensor(9560.1211, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9560.1064453125
tensor(9560.1211, grad_fn=<NegBackward0>) tensor(9560.1064, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9560.09375
tensor(9560.1064, grad_fn=<NegBackward0>) tensor(9560.0938, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9560.08203125
tensor(9560.0938, grad_fn=<NegBackward0>) tensor(9560.0820, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9560.0732421875
tensor(9560.0820, grad_fn=<NegBackward0>) tensor(9560.0732, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9560.064453125
tensor(9560.0732, grad_fn=<NegBackward0>) tensor(9560.0645, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9560.0556640625
tensor(9560.0645, grad_fn=<NegBackward0>) tensor(9560.0557, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9560.0498046875
tensor(9560.0557, grad_fn=<NegBackward0>) tensor(9560.0498, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9560.0439453125
tensor(9560.0498, grad_fn=<NegBackward0>) tensor(9560.0439, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9560.0380859375
tensor(9560.0439, grad_fn=<NegBackward0>) tensor(9560.0381, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9560.033203125
tensor(9560.0381, grad_fn=<NegBackward0>) tensor(9560.0332, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9560.0283203125
tensor(9560.0332, grad_fn=<NegBackward0>) tensor(9560.0283, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9560.0234375
tensor(9560.0283, grad_fn=<NegBackward0>) tensor(9560.0234, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9560.0205078125
tensor(9560.0234, grad_fn=<NegBackward0>) tensor(9560.0205, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9560.017578125
tensor(9560.0205, grad_fn=<NegBackward0>) tensor(9560.0176, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9560.013671875
tensor(9560.0176, grad_fn=<NegBackward0>) tensor(9560.0137, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9560.0107421875
tensor(9560.0137, grad_fn=<NegBackward0>) tensor(9560.0107, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9560.0068359375
tensor(9560.0107, grad_fn=<NegBackward0>) tensor(9560.0068, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9560.005859375
tensor(9560.0068, grad_fn=<NegBackward0>) tensor(9560.0059, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9560.001953125
tensor(9560.0059, grad_fn=<NegBackward0>) tensor(9560.0020, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9560.0009765625
tensor(9560.0020, grad_fn=<NegBackward0>) tensor(9560.0010, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9559.9990234375
tensor(9560.0010, grad_fn=<NegBackward0>) tensor(9559.9990, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9559.998046875
tensor(9559.9990, grad_fn=<NegBackward0>) tensor(9559.9980, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9559.9970703125
tensor(9559.9980, grad_fn=<NegBackward0>) tensor(9559.9971, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9559.9951171875
tensor(9559.9971, grad_fn=<NegBackward0>) tensor(9559.9951, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9559.9931640625
tensor(9559.9951, grad_fn=<NegBackward0>) tensor(9559.9932, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9559.9921875
tensor(9559.9932, grad_fn=<NegBackward0>) tensor(9559.9922, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9559.9892578125
tensor(9559.9922, grad_fn=<NegBackward0>) tensor(9559.9893, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9559.990234375
tensor(9559.9893, grad_fn=<NegBackward0>) tensor(9559.9902, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9559.9892578125
tensor(9559.9893, grad_fn=<NegBackward0>) tensor(9559.9893, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9559.9873046875
tensor(9559.9893, grad_fn=<NegBackward0>) tensor(9559.9873, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9559.986328125
tensor(9559.9873, grad_fn=<NegBackward0>) tensor(9559.9863, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9559.984375
tensor(9559.9863, grad_fn=<NegBackward0>) tensor(9559.9844, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9559.9853515625
tensor(9559.9844, grad_fn=<NegBackward0>) tensor(9559.9854, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9559.9833984375
tensor(9559.9844, grad_fn=<NegBackward0>) tensor(9559.9834, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9559.9833984375
tensor(9559.9834, grad_fn=<NegBackward0>) tensor(9559.9834, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9559.982421875
tensor(9559.9834, grad_fn=<NegBackward0>) tensor(9559.9824, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9559.9814453125
tensor(9559.9824, grad_fn=<NegBackward0>) tensor(9559.9814, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9559.98046875
tensor(9559.9814, grad_fn=<NegBackward0>) tensor(9559.9805, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9559.98046875
tensor(9559.9805, grad_fn=<NegBackward0>) tensor(9559.9805, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9559.978515625
tensor(9559.9805, grad_fn=<NegBackward0>) tensor(9559.9785, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9559.978515625
tensor(9559.9785, grad_fn=<NegBackward0>) tensor(9559.9785, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9559.978515625
tensor(9559.9785, grad_fn=<NegBackward0>) tensor(9559.9785, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9559.9794921875
tensor(9559.9785, grad_fn=<NegBackward0>) tensor(9559.9795, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9559.9775390625
tensor(9559.9785, grad_fn=<NegBackward0>) tensor(9559.9775, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9559.9765625
tensor(9559.9775, grad_fn=<NegBackward0>) tensor(9559.9766, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9559.9765625
tensor(9559.9766, grad_fn=<NegBackward0>) tensor(9559.9766, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9559.9755859375
tensor(9559.9766, grad_fn=<NegBackward0>) tensor(9559.9756, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9559.9755859375
tensor(9559.9756, grad_fn=<NegBackward0>) tensor(9559.9756, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9559.9736328125
tensor(9559.9756, grad_fn=<NegBackward0>) tensor(9559.9736, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9559.974609375
tensor(9559.9736, grad_fn=<NegBackward0>) tensor(9559.9746, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9559.974609375
tensor(9559.9736, grad_fn=<NegBackward0>) tensor(9559.9746, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9559.97265625
tensor(9559.9736, grad_fn=<NegBackward0>) tensor(9559.9727, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9559.9716796875
tensor(9559.9727, grad_fn=<NegBackward0>) tensor(9559.9717, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9559.9736328125
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9559.9736, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9559.9716796875
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9559.9717, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9559.9716796875
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9559.9717, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9559.9716796875
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9559.9717, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9560.0625
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9560.0625, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9559.9697265625
tensor(9559.9717, grad_fn=<NegBackward0>) tensor(9559.9697, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9559.96875
tensor(9559.9697, grad_fn=<NegBackward0>) tensor(9559.9688, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9559.966796875
tensor(9559.9688, grad_fn=<NegBackward0>) tensor(9559.9668, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9559.9599609375
tensor(9559.9668, grad_fn=<NegBackward0>) tensor(9559.9600, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9559.9404296875
tensor(9559.9600, grad_fn=<NegBackward0>) tensor(9559.9404, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9559.94140625
tensor(9559.9404, grad_fn=<NegBackward0>) tensor(9559.9414, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9559.9228515625
tensor(9559.9404, grad_fn=<NegBackward0>) tensor(9559.9229, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9559.9287109375
tensor(9559.9229, grad_fn=<NegBackward0>) tensor(9559.9287, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9559.90234375
tensor(9559.9229, grad_fn=<NegBackward0>) tensor(9559.9023, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9559.9052734375
tensor(9559.9023, grad_fn=<NegBackward0>) tensor(9559.9053, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9559.90234375
tensor(9559.9023, grad_fn=<NegBackward0>) tensor(9559.9023, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9559.978515625
tensor(9559.9023, grad_fn=<NegBackward0>) tensor(9559.9785, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9559.900390625
tensor(9559.9023, grad_fn=<NegBackward0>) tensor(9559.9004, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9559.904296875
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9043, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9559.90234375
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9023, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -9559.9013671875
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9014, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -9559.900390625
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9004, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -9559.900390625
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9004, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9559.900390625
tensor(9559.9004, grad_fn=<NegBackward0>) tensor(9559.9004, grad_fn=<NegBackward0>)
pi: tensor([[9.9836e-01, 1.6387e-03],
        [1.7880e-04, 9.9982e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1291, 0.0506],
         [0.7123, 0.5983]],

        [[0.5693, 0.2513],
         [0.6738, 0.5020]],

        [[0.5078, 0.1998],
         [0.5440, 0.6974]],

        [[0.6975, 0.2057],
         [0.5991, 0.5492]],

        [[0.6860, 0.1259],
         [0.6218, 0.5175]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
Global Adjusted Rand Index: -0.00047112684663770486
Average Adjusted Rand Index: 0.0025325607642163726
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22312.57421875
inf tensor(22312.5742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9569.701171875
tensor(22312.5742, grad_fn=<NegBackward0>) tensor(9569.7012, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9568.3642578125
tensor(9569.7012, grad_fn=<NegBackward0>) tensor(9568.3643, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9567.8056640625
tensor(9568.3643, grad_fn=<NegBackward0>) tensor(9567.8057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9567.3681640625
tensor(9567.8057, grad_fn=<NegBackward0>) tensor(9567.3682, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9566.4912109375
tensor(9567.3682, grad_fn=<NegBackward0>) tensor(9566.4912, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9565.8203125
tensor(9566.4912, grad_fn=<NegBackward0>) tensor(9565.8203, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9565.03125
tensor(9565.8203, grad_fn=<NegBackward0>) tensor(9565.0312, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9562.984375
tensor(9565.0312, grad_fn=<NegBackward0>) tensor(9562.9844, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9561.796875
tensor(9562.9844, grad_fn=<NegBackward0>) tensor(9561.7969, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9561.255859375
tensor(9561.7969, grad_fn=<NegBackward0>) tensor(9561.2559, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9560.943359375
tensor(9561.2559, grad_fn=<NegBackward0>) tensor(9560.9434, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9560.740234375
tensor(9560.9434, grad_fn=<NegBackward0>) tensor(9560.7402, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9560.5986328125
tensor(9560.7402, grad_fn=<NegBackward0>) tensor(9560.5986, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9560.4912109375
tensor(9560.5986, grad_fn=<NegBackward0>) tensor(9560.4912, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9560.4140625
tensor(9560.4912, grad_fn=<NegBackward0>) tensor(9560.4141, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9560.3505859375
tensor(9560.4141, grad_fn=<NegBackward0>) tensor(9560.3506, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9560.2998046875
tensor(9560.3506, grad_fn=<NegBackward0>) tensor(9560.2998, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9560.259765625
tensor(9560.2998, grad_fn=<NegBackward0>) tensor(9560.2598, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9560.224609375
tensor(9560.2598, grad_fn=<NegBackward0>) tensor(9560.2246, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9560.1953125
tensor(9560.2246, grad_fn=<NegBackward0>) tensor(9560.1953, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9560.1728515625
tensor(9560.1953, grad_fn=<NegBackward0>) tensor(9560.1729, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9560.15234375
tensor(9560.1729, grad_fn=<NegBackward0>) tensor(9560.1523, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9560.1328125
tensor(9560.1523, grad_fn=<NegBackward0>) tensor(9560.1328, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9560.115234375
tensor(9560.1328, grad_fn=<NegBackward0>) tensor(9560.1152, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9560.1025390625
tensor(9560.1152, grad_fn=<NegBackward0>) tensor(9560.1025, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9560.091796875
tensor(9560.1025, grad_fn=<NegBackward0>) tensor(9560.0918, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9560.0810546875
tensor(9560.0918, grad_fn=<NegBackward0>) tensor(9560.0811, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9560.0703125
tensor(9560.0811, grad_fn=<NegBackward0>) tensor(9560.0703, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9560.0625
tensor(9560.0703, grad_fn=<NegBackward0>) tensor(9560.0625, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9560.0537109375
tensor(9560.0625, grad_fn=<NegBackward0>) tensor(9560.0537, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9560.0458984375
tensor(9560.0537, grad_fn=<NegBackward0>) tensor(9560.0459, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9560.0400390625
tensor(9560.0459, grad_fn=<NegBackward0>) tensor(9560.0400, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9560.033203125
tensor(9560.0400, grad_fn=<NegBackward0>) tensor(9560.0332, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9560.0283203125
tensor(9560.0332, grad_fn=<NegBackward0>) tensor(9560.0283, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9560.0234375
tensor(9560.0283, grad_fn=<NegBackward0>) tensor(9560.0234, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9560.0185546875
tensor(9560.0234, grad_fn=<NegBackward0>) tensor(9560.0186, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9560.013671875
tensor(9560.0186, grad_fn=<NegBackward0>) tensor(9560.0137, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9560.0107421875
tensor(9560.0137, grad_fn=<NegBackward0>) tensor(9560.0107, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9560.005859375
tensor(9560.0107, grad_fn=<NegBackward0>) tensor(9560.0059, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9560.0009765625
tensor(9560.0059, grad_fn=<NegBackward0>) tensor(9560.0010, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9559.998046875
tensor(9560.0010, grad_fn=<NegBackward0>) tensor(9559.9980, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9559.994140625
tensor(9559.9980, grad_fn=<NegBackward0>) tensor(9559.9941, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9559.9912109375
tensor(9559.9941, grad_fn=<NegBackward0>) tensor(9559.9912, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9559.9853515625
tensor(9559.9912, grad_fn=<NegBackward0>) tensor(9559.9854, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9559.9814453125
tensor(9559.9854, grad_fn=<NegBackward0>) tensor(9559.9814, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9559.9765625
tensor(9559.9814, grad_fn=<NegBackward0>) tensor(9559.9766, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9559.970703125
tensor(9559.9766, grad_fn=<NegBackward0>) tensor(9559.9707, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9559.966796875
tensor(9559.9707, grad_fn=<NegBackward0>) tensor(9559.9668, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9559.9599609375
tensor(9559.9668, grad_fn=<NegBackward0>) tensor(9559.9600, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9559.9462890625
tensor(9559.9600, grad_fn=<NegBackward0>) tensor(9559.9463, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9559.9365234375
tensor(9559.9463, grad_fn=<NegBackward0>) tensor(9559.9365, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9559.9296875
tensor(9559.9365, grad_fn=<NegBackward0>) tensor(9559.9297, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9559.9267578125
tensor(9559.9297, grad_fn=<NegBackward0>) tensor(9559.9268, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9559.9228515625
tensor(9559.9268, grad_fn=<NegBackward0>) tensor(9559.9229, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9559.9228515625
tensor(9559.9229, grad_fn=<NegBackward0>) tensor(9559.9229, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9559.921875
 77%|███████▋  | 77/100 [17:24:25<6:03:22, 947.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 78%|███████▊  | 78/100 [17:39:46<5:44:38, 939.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 79%|███████▉  | 79/100 [17:55:33<5:29:42, 942.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 80%|████████  | 80/100 [18:10:40<5:10:31, 931.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 81%|████████  | 81/100 [18:29:04<5:11:20, 983.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 82%|████████▏ | 82/100 [18:38:47<4:18:58, 863.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 83%|████████▎ | 83/100 [18:55:01<4:13:58, 896.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 84%|████████▍ | 84/100 [19:09:33<3:57:06, 889.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 85%|████████▌ | 85/100 [19:27:38<3:56:56, 947.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 86%|████████▌ | 86/100 [19:34:39<3:04:17, 789.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 87%|████████▋ | 87/100 [19:43:19<2:33:35, 708.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 88%|████████▊ | 88/100 [19:59:50<2:38:43, 793.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 89%|████████▉ | 89/100 [20:09:33<2:13:53, 730.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 90%|█████████ | 90/100 [20:19:08<1:53:56, 683.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 91%|█████████ | 91/100 [20:37:31<2:01:27, 809.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 92%|█████████▏| 92/100 [20:51:57<1:50:11, 826.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 93%|█████████▎| 93/100 [21:10:13<1:45:52, 907.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 94%|█████████▍| 94/100 [21:20:12<1:21:28, 814.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 95%|█████████▌| 95/100 [21:33:58<1:08:10, 818.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 96%|█████████▌| 96/100 [21:52:25<1:00:19, 904.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 97%|█████████▋| 97/100 [22:06:52<44:40, 893.54s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 98%|█████████▊| 98/100 [22:23:27<30:47, 923.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 99%|█████████▉| 99/100 [22:37:17<14:55, 895.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
100%|██████████| 100/100 [22:50:21<00:00, 862.26s/it]100%|██████████| 100/100 [22:50:21<00:00, 822.22s/it]
tensor(9559.9229, grad_fn=<NegBackward0>) tensor(9559.9219, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9559.919921875
tensor(9559.9219, grad_fn=<NegBackward0>) tensor(9559.9199, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9559.919921875
tensor(9559.9199, grad_fn=<NegBackward0>) tensor(9559.9199, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9559.91796875
tensor(9559.9199, grad_fn=<NegBackward0>) tensor(9559.9180, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9559.9169921875
tensor(9559.9180, grad_fn=<NegBackward0>) tensor(9559.9170, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9559.91796875
tensor(9559.9170, grad_fn=<NegBackward0>) tensor(9559.9180, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9559.9169921875
tensor(9559.9170, grad_fn=<NegBackward0>) tensor(9559.9170, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9559.9150390625
tensor(9559.9170, grad_fn=<NegBackward0>) tensor(9559.9150, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9559.9169921875
tensor(9559.9150, grad_fn=<NegBackward0>) tensor(9559.9170, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9559.9150390625
tensor(9559.9150, grad_fn=<NegBackward0>) tensor(9559.9150, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9559.9150390625
tensor(9559.9150, grad_fn=<NegBackward0>) tensor(9559.9150, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9559.9150390625
tensor(9559.9150, grad_fn=<NegBackward0>) tensor(9559.9150, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9559.9140625
tensor(9559.9150, grad_fn=<NegBackward0>) tensor(9559.9141, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9559.9140625
tensor(9559.9141, grad_fn=<NegBackward0>) tensor(9559.9141, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9559.9130859375
tensor(9559.9141, grad_fn=<NegBackward0>) tensor(9559.9131, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9559.912109375
tensor(9559.9131, grad_fn=<NegBackward0>) tensor(9559.9121, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9559.912109375
tensor(9559.9121, grad_fn=<NegBackward0>) tensor(9559.9121, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9559.9130859375
tensor(9559.9121, grad_fn=<NegBackward0>) tensor(9559.9131, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9559.912109375
tensor(9559.9121, grad_fn=<NegBackward0>) tensor(9559.9121, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9559.9111328125
tensor(9559.9121, grad_fn=<NegBackward0>) tensor(9559.9111, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9559.9111328125
tensor(9559.9111, grad_fn=<NegBackward0>) tensor(9559.9111, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9559.91015625
tensor(9559.9111, grad_fn=<NegBackward0>) tensor(9559.9102, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9559.9091796875
tensor(9559.9102, grad_fn=<NegBackward0>) tensor(9559.9092, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9559.90625
tensor(9559.9092, grad_fn=<NegBackward0>) tensor(9559.9062, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9559.904296875
tensor(9559.9062, grad_fn=<NegBackward0>) tensor(9559.9043, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9559.8984375
tensor(9559.9043, grad_fn=<NegBackward0>) tensor(9559.8984, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9559.8974609375
tensor(9559.8984, grad_fn=<NegBackward0>) tensor(9559.8975, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9559.896484375
tensor(9559.8975, grad_fn=<NegBackward0>) tensor(9559.8965, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9559.8974609375
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8975, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9559.896484375
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8965, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9559.896484375
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8965, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9559.896484375
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8965, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9559.8974609375
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8975, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9559.8955078125
tensor(9559.8965, grad_fn=<NegBackward0>) tensor(9559.8955, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9559.8955078125
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.8955, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9559.904296875
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.9043, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9559.896484375
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.8965, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -9559.9013671875
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.9014, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -9559.8974609375
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.8975, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -9559.9013671875
tensor(9559.8955, grad_fn=<NegBackward0>) tensor(9559.9014, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.9974e-01, 2.5842e-04],
        [1.7061e-03, 9.9829e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5900, 0.0506],
         [0.5248, 0.1294]],

        [[0.5329, 0.2512],
         [0.6598, 0.5210]],

        [[0.5036, 0.1995],
         [0.5181, 0.7095]],

        [[0.5092, 0.2055],
         [0.6941, 0.5261]],

        [[0.6569, 0.1257],
         [0.7054, 0.6027]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
Global Adjusted Rand Index: -0.00047112684663770486
Average Adjusted Rand Index: 0.0025325607642163726
[-0.00047112684663770486, -0.00047112684663770486] [0.0025325607642163726, 0.0025325607642163726] [9559.9013671875, 9559.9013671875]
-------------------------------------
This iteration is 77
True Objective function: Loss = -9955.557533844172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20588.1328125
inf tensor(20588.1328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9851.0390625
tensor(20588.1328, grad_fn=<NegBackward0>) tensor(9851.0391, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9850.666015625
tensor(9851.0391, grad_fn=<NegBackward0>) tensor(9850.6660, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9850.494140625
tensor(9850.6660, grad_fn=<NegBackward0>) tensor(9850.4941, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9850.1953125
tensor(9850.4941, grad_fn=<NegBackward0>) tensor(9850.1953, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9848.5751953125
tensor(9850.1953, grad_fn=<NegBackward0>) tensor(9848.5752, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9848.091796875
tensor(9848.5752, grad_fn=<NegBackward0>) tensor(9848.0918, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9847.9775390625
tensor(9848.0918, grad_fn=<NegBackward0>) tensor(9847.9775, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9847.9072265625
tensor(9847.9775, grad_fn=<NegBackward0>) tensor(9847.9072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9847.85546875
tensor(9847.9072, grad_fn=<NegBackward0>) tensor(9847.8555, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9847.8134765625
tensor(9847.8555, grad_fn=<NegBackward0>) tensor(9847.8135, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9847.7763671875
tensor(9847.8135, grad_fn=<NegBackward0>) tensor(9847.7764, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9847.7470703125
tensor(9847.7764, grad_fn=<NegBackward0>) tensor(9847.7471, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9847.7216796875
tensor(9847.7471, grad_fn=<NegBackward0>) tensor(9847.7217, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9847.7001953125
tensor(9847.7217, grad_fn=<NegBackward0>) tensor(9847.7002, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9847.6826171875
tensor(9847.7002, grad_fn=<NegBackward0>) tensor(9847.6826, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9847.6689453125
tensor(9847.6826, grad_fn=<NegBackward0>) tensor(9847.6689, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9847.6572265625
tensor(9847.6689, grad_fn=<NegBackward0>) tensor(9847.6572, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9847.6494140625
tensor(9847.6572, grad_fn=<NegBackward0>) tensor(9847.6494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9847.6416015625
tensor(9847.6494, grad_fn=<NegBackward0>) tensor(9847.6416, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9847.6357421875
tensor(9847.6416, grad_fn=<NegBackward0>) tensor(9847.6357, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9847.6298828125
tensor(9847.6357, grad_fn=<NegBackward0>) tensor(9847.6299, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9847.625
tensor(9847.6299, grad_fn=<NegBackward0>) tensor(9847.6250, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9847.6220703125
tensor(9847.6250, grad_fn=<NegBackward0>) tensor(9847.6221, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9847.619140625
tensor(9847.6221, grad_fn=<NegBackward0>) tensor(9847.6191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9847.6162109375
tensor(9847.6191, grad_fn=<NegBackward0>) tensor(9847.6162, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9847.61328125
tensor(9847.6162, grad_fn=<NegBackward0>) tensor(9847.6133, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9847.6103515625
tensor(9847.6133, grad_fn=<NegBackward0>) tensor(9847.6104, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9847.6103515625
tensor(9847.6104, grad_fn=<NegBackward0>) tensor(9847.6104, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9847.607421875
tensor(9847.6104, grad_fn=<NegBackward0>) tensor(9847.6074, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9847.607421875
tensor(9847.6074, grad_fn=<NegBackward0>) tensor(9847.6074, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9847.6044921875
tensor(9847.6074, grad_fn=<NegBackward0>) tensor(9847.6045, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9847.603515625
tensor(9847.6045, grad_fn=<NegBackward0>) tensor(9847.6035, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9847.603515625
tensor(9847.6035, grad_fn=<NegBackward0>) tensor(9847.6035, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9847.6025390625
tensor(9847.6035, grad_fn=<NegBackward0>) tensor(9847.6025, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9847.6005859375
tensor(9847.6025, grad_fn=<NegBackward0>) tensor(9847.6006, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9847.599609375
tensor(9847.6006, grad_fn=<NegBackward0>) tensor(9847.5996, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9847.599609375
tensor(9847.5996, grad_fn=<NegBackward0>) tensor(9847.5996, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9847.59765625
tensor(9847.5996, grad_fn=<NegBackward0>) tensor(9847.5977, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9847.5986328125
tensor(9847.5977, grad_fn=<NegBackward0>) tensor(9847.5986, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9847.59765625
tensor(9847.5977, grad_fn=<NegBackward0>) tensor(9847.5977, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9847.5966796875
tensor(9847.5977, grad_fn=<NegBackward0>) tensor(9847.5967, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9847.595703125
tensor(9847.5967, grad_fn=<NegBackward0>) tensor(9847.5957, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9847.5947265625
tensor(9847.5957, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9847.5947265625
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9847.595703125
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5957, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9847.59375
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9847.59375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9847.59375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9847.5947265625
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9847.5927734375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9847.5927734375
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9847.59375
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9847.5927734375
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9847.591796875
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9847.591796875
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9847.5927734375
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9847.591796875
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9847.58984375
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9847.591796875
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9847.58984375
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9847.5908203125
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9847.591796875
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9847.5888671875
tensor(9847.5898, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9847.591796875
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -9847.5908203125
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[7.9904e-04, 9.9920e-01],
        [3.4932e-02, 9.6507e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0090, 0.9910], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2101, 0.2490],
         [0.6895, 0.1334]],

        [[0.5655, 0.1754],
         [0.5010, 0.6377]],

        [[0.7303, 0.1198],
         [0.5568, 0.5414]],

        [[0.6350, 0.1743],
         [0.6692, 0.6726]],

        [[0.6601, 0.2322],
         [0.5988, 0.6407]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: 0.001525737546010555
Average Adjusted Rand Index: 0.0004854192201740228
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21621.373046875
inf tensor(21621.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9851.1884765625
tensor(21621.3730, grad_fn=<NegBackward0>) tensor(9851.1885, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9849.95703125
tensor(9851.1885, grad_fn=<NegBackward0>) tensor(9849.9570, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9849.478515625
tensor(9849.9570, grad_fn=<NegBackward0>) tensor(9849.4785, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9849.20703125
tensor(9849.4785, grad_fn=<NegBackward0>) tensor(9849.2070, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9849.0068359375
tensor(9849.2070, grad_fn=<NegBackward0>) tensor(9849.0068, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9848.8662109375
tensor(9849.0068, grad_fn=<NegBackward0>) tensor(9848.8662, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9848.7626953125
tensor(9848.8662, grad_fn=<NegBackward0>) tensor(9848.7627, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9848.67578125
tensor(9848.7627, grad_fn=<NegBackward0>) tensor(9848.6758, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9848.595703125
tensor(9848.6758, grad_fn=<NegBackward0>) tensor(9848.5957, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9848.5234375
tensor(9848.5957, grad_fn=<NegBackward0>) tensor(9848.5234, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9848.45703125
tensor(9848.5234, grad_fn=<NegBackward0>) tensor(9848.4570, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9848.39453125
tensor(9848.4570, grad_fn=<NegBackward0>) tensor(9848.3945, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9848.3388671875
tensor(9848.3945, grad_fn=<NegBackward0>) tensor(9848.3389, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9848.2958984375
tensor(9848.3389, grad_fn=<NegBackward0>) tensor(9848.2959, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9848.271484375
tensor(9848.2959, grad_fn=<NegBackward0>) tensor(9848.2715, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9848.25390625
tensor(9848.2715, grad_fn=<NegBackward0>) tensor(9848.2539, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9848.2421875
tensor(9848.2539, grad_fn=<NegBackward0>) tensor(9848.2422, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9848.2333984375
tensor(9848.2422, grad_fn=<NegBackward0>) tensor(9848.2334, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9848.2275390625
tensor(9848.2334, grad_fn=<NegBackward0>) tensor(9848.2275, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9848.22265625
tensor(9848.2275, grad_fn=<NegBackward0>) tensor(9848.2227, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9848.21875
tensor(9848.2227, grad_fn=<NegBackward0>) tensor(9848.2188, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9848.216796875
tensor(9848.2188, grad_fn=<NegBackward0>) tensor(9848.2168, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9848.2138671875
tensor(9848.2168, grad_fn=<NegBackward0>) tensor(9848.2139, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9848.2119140625
tensor(9848.2139, grad_fn=<NegBackward0>) tensor(9848.2119, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9848.208984375
tensor(9848.2119, grad_fn=<NegBackward0>) tensor(9848.2090, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9848.2041015625
tensor(9848.2090, grad_fn=<NegBackward0>) tensor(9848.2041, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9848.1044921875
tensor(9848.2041, grad_fn=<NegBackward0>) tensor(9848.1045, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9847.7060546875
tensor(9848.1045, grad_fn=<NegBackward0>) tensor(9847.7061, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9847.6396484375
tensor(9847.7061, grad_fn=<NegBackward0>) tensor(9847.6396, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9847.62109375
tensor(9847.6396, grad_fn=<NegBackward0>) tensor(9847.6211, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9847.61328125
tensor(9847.6211, grad_fn=<NegBackward0>) tensor(9847.6133, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9847.609375
tensor(9847.6133, grad_fn=<NegBackward0>) tensor(9847.6094, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9847.6064453125
tensor(9847.6094, grad_fn=<NegBackward0>) tensor(9847.6064, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9847.6044921875
tensor(9847.6064, grad_fn=<NegBackward0>) tensor(9847.6045, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9847.603515625
tensor(9847.6045, grad_fn=<NegBackward0>) tensor(9847.6035, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9847.6005859375
tensor(9847.6035, grad_fn=<NegBackward0>) tensor(9847.6006, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9847.6005859375
tensor(9847.6006, grad_fn=<NegBackward0>) tensor(9847.6006, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9847.599609375
tensor(9847.6006, grad_fn=<NegBackward0>) tensor(9847.5996, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9847.5986328125
tensor(9847.5996, grad_fn=<NegBackward0>) tensor(9847.5986, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9847.5986328125
tensor(9847.5986, grad_fn=<NegBackward0>) tensor(9847.5986, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9847.5966796875
tensor(9847.5986, grad_fn=<NegBackward0>) tensor(9847.5967, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9847.59765625
tensor(9847.5967, grad_fn=<NegBackward0>) tensor(9847.5977, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9847.595703125
tensor(9847.5967, grad_fn=<NegBackward0>) tensor(9847.5957, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9847.5966796875
tensor(9847.5957, grad_fn=<NegBackward0>) tensor(9847.5967, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -9847.5947265625
tensor(9847.5957, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9847.5947265625
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9847.5947265625
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9847.59375
tensor(9847.5947, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9847.5947265625
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5947, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9847.59375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9847.59375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9847.5927734375
tensor(9847.5938, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9847.59375
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5938, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9847.591796875
tensor(9847.5928, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9847.591796875
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9847.5927734375
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5928, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9847.591796875
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9847.5908203125
tensor(9847.5918, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9847.5908203125
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9847.591796875
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9847.591796875
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9847.5908203125
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9847.5908203125
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9847.591796875
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5918, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -9847.5908203125
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9847.5908203125
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9847.5888671875
tensor(9847.5908, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9847.5908203125
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9847.5908203125
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -9847.5888671875
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9847.5888671875
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -9847.5888671875
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9847.5908203125
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9847.5908203125
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5908, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9847.5888671875
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5889, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -9847.58984375
tensor(9847.5889, grad_fn=<NegBackward0>) tensor(9847.5898, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.6517e-01, 3.4831e-02],
        [9.9924e-01, 7.5579e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9910, 0.0090], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1336, 0.2491],
         [0.5343, 0.2101]],

        [[0.7255, 0.1754],
         [0.7220, 0.5755]],

        [[0.6116, 0.1198],
         [0.6534, 0.6594]],

        [[0.7018, 0.1743],
         [0.5092, 0.5077]],

        [[0.6584, 0.2322],
         [0.6684, 0.6970]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: 0.001525737546010555
Average Adjusted Rand Index: 0.0004854192201740228
[0.001525737546010555, 0.001525737546010555] [0.0004854192201740228, 0.0004854192201740228] [9847.5908203125, 9847.58984375]
-------------------------------------
This iteration is 78
True Objective function: Loss = -9962.45659900153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24171.525390625
inf tensor(24171.5254, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9832.7001953125
tensor(24171.5254, grad_fn=<NegBackward0>) tensor(9832.7002, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9831.9208984375
tensor(9832.7002, grad_fn=<NegBackward0>) tensor(9831.9209, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9831.623046875
tensor(9831.9209, grad_fn=<NegBackward0>) tensor(9831.6230, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9831.4033203125
tensor(9831.6230, grad_fn=<NegBackward0>) tensor(9831.4033, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9831.2919921875
tensor(9831.4033, grad_fn=<NegBackward0>) tensor(9831.2920, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9831.2353515625
tensor(9831.2920, grad_fn=<NegBackward0>) tensor(9831.2354, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9831.1943359375
tensor(9831.2354, grad_fn=<NegBackward0>) tensor(9831.1943, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9831.16015625
tensor(9831.1943, grad_fn=<NegBackward0>) tensor(9831.1602, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9831.130859375
tensor(9831.1602, grad_fn=<NegBackward0>) tensor(9831.1309, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9831.1064453125
tensor(9831.1309, grad_fn=<NegBackward0>) tensor(9831.1064, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9831.0869140625
tensor(9831.1064, grad_fn=<NegBackward0>) tensor(9831.0869, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9831.0693359375
tensor(9831.0869, grad_fn=<NegBackward0>) tensor(9831.0693, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9831.0556640625
tensor(9831.0693, grad_fn=<NegBackward0>) tensor(9831.0557, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9831.0419921875
tensor(9831.0557, grad_fn=<NegBackward0>) tensor(9831.0420, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9831.0322265625
tensor(9831.0420, grad_fn=<NegBackward0>) tensor(9831.0322, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9831.0244140625
tensor(9831.0322, grad_fn=<NegBackward0>) tensor(9831.0244, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9831.0185546875
tensor(9831.0244, grad_fn=<NegBackward0>) tensor(9831.0186, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9831.01171875
tensor(9831.0186, grad_fn=<NegBackward0>) tensor(9831.0117, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9831.0068359375
tensor(9831.0117, grad_fn=<NegBackward0>) tensor(9831.0068, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9831.0029296875
tensor(9831.0068, grad_fn=<NegBackward0>) tensor(9831.0029, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9831.0
tensor(9831.0029, grad_fn=<NegBackward0>) tensor(9831., grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9830.9990234375
tensor(9831., grad_fn=<NegBackward0>) tensor(9830.9990, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9830.99609375
tensor(9830.9990, grad_fn=<NegBackward0>) tensor(9830.9961, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9830.994140625
tensor(9830.9961, grad_fn=<NegBackward0>) tensor(9830.9941, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9830.9912109375
tensor(9830.9941, grad_fn=<NegBackward0>) tensor(9830.9912, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9830.98828125
tensor(9830.9912, grad_fn=<NegBackward0>) tensor(9830.9883, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9830.98828125
tensor(9830.9883, grad_fn=<NegBackward0>) tensor(9830.9883, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9830.986328125
tensor(9830.9883, grad_fn=<NegBackward0>) tensor(9830.9863, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9830.984375
tensor(9830.9863, grad_fn=<NegBackward0>) tensor(9830.9844, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9830.9833984375
tensor(9830.9844, grad_fn=<NegBackward0>) tensor(9830.9834, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9830.98046875
tensor(9830.9834, grad_fn=<NegBackward0>) tensor(9830.9805, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9830.978515625
tensor(9830.9805, grad_fn=<NegBackward0>) tensor(9830.9785, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9830.9755859375
tensor(9830.9785, grad_fn=<NegBackward0>) tensor(9830.9756, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9830.966796875
tensor(9830.9756, grad_fn=<NegBackward0>) tensor(9830.9668, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9830.9501953125
tensor(9830.9668, grad_fn=<NegBackward0>) tensor(9830.9502, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9830.91015625
tensor(9830.9502, grad_fn=<NegBackward0>) tensor(9830.9102, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9830.8525390625
tensor(9830.9102, grad_fn=<NegBackward0>) tensor(9830.8525, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9830.830078125
tensor(9830.8525, grad_fn=<NegBackward0>) tensor(9830.8301, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9830.8203125
tensor(9830.8301, grad_fn=<NegBackward0>) tensor(9830.8203, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9830.81640625
tensor(9830.8203, grad_fn=<NegBackward0>) tensor(9830.8164, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9830.814453125
tensor(9830.8164, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9830.814453125
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9830.814453125
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9830.8125
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9830.8125
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9830.8115234375
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9830.8125
tensor(9830.8115, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9830.8125
tensor(9830.8115, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -9830.810546875
tensor(9830.8115, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9830.8115234375
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9830.8115234375
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9830.8115234375
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -9830.8115234375
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -9830.810546875
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9830.8095703125
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9830.80859375
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9830.8095703125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9830.8125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9830.8125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9830.8076171875
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9830.8095703125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -9830.8076171875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9830.8095703125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9830.822265625
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8223, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9830.8671875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8672, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[3.2635e-04, 9.9967e-01],
        [2.9809e-02, 9.7019e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2097, 0.2455],
         [0.5102, 0.1337]],

        [[0.6405, 0.2114],
         [0.6872, 0.7093]],

        [[0.6432, 0.1729],
         [0.7232, 0.6664]],

        [[0.6254, 0.1270],
         [0.6663, 0.5861]],

        [[0.7235, 0.1662],
         [0.5147, 0.6938]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000281772120727798
Average Adjusted Rand Index: -0.0009013459011305159
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20585.994140625
inf tensor(20585.9941, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9832.5224609375
tensor(20585.9941, grad_fn=<NegBackward0>) tensor(9832.5225, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9832.0576171875
tensor(9832.5225, grad_fn=<NegBackward0>) tensor(9832.0576, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9831.8974609375
tensor(9832.0576, grad_fn=<NegBackward0>) tensor(9831.8975, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9831.7197265625
tensor(9831.8975, grad_fn=<NegBackward0>) tensor(9831.7197, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9831.5361328125
tensor(9831.7197, grad_fn=<NegBackward0>) tensor(9831.5361, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9831.3720703125
tensor(9831.5361, grad_fn=<NegBackward0>) tensor(9831.3721, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9831.2333984375
tensor(9831.3721, grad_fn=<NegBackward0>) tensor(9831.2334, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9831.07421875
tensor(9831.2334, grad_fn=<NegBackward0>) tensor(9831.0742, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9830.9404296875
tensor(9831.0742, grad_fn=<NegBackward0>) tensor(9830.9404, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9830.873046875
tensor(9830.9404, grad_fn=<NegBackward0>) tensor(9830.8730, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9830.845703125
tensor(9830.8730, grad_fn=<NegBackward0>) tensor(9830.8457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9830.833984375
tensor(9830.8457, grad_fn=<NegBackward0>) tensor(9830.8340, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9830.830078125
tensor(9830.8340, grad_fn=<NegBackward0>) tensor(9830.8301, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9830.826171875
tensor(9830.8301, grad_fn=<NegBackward0>) tensor(9830.8262, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9830.8232421875
tensor(9830.8262, grad_fn=<NegBackward0>) tensor(9830.8232, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9830.8212890625
tensor(9830.8232, grad_fn=<NegBackward0>) tensor(9830.8213, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9830.8193359375
tensor(9830.8213, grad_fn=<NegBackward0>) tensor(9830.8193, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9830.818359375
tensor(9830.8193, grad_fn=<NegBackward0>) tensor(9830.8184, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9830.818359375
tensor(9830.8184, grad_fn=<NegBackward0>) tensor(9830.8184, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9830.8173828125
tensor(9830.8184, grad_fn=<NegBackward0>) tensor(9830.8174, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9830.8173828125
tensor(9830.8174, grad_fn=<NegBackward0>) tensor(9830.8174, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9830.8154296875
tensor(9830.8174, grad_fn=<NegBackward0>) tensor(9830.8154, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9830.8154296875
tensor(9830.8154, grad_fn=<NegBackward0>) tensor(9830.8154, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9830.814453125
tensor(9830.8154, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9830.814453125
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9830.814453125
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9830.8134765625
tensor(9830.8145, grad_fn=<NegBackward0>) tensor(9830.8135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9830.8134765625
tensor(9830.8135, grad_fn=<NegBackward0>) tensor(9830.8135, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9830.814453125
tensor(9830.8135, grad_fn=<NegBackward0>) tensor(9830.8145, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9830.8125
tensor(9830.8135, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9830.8125
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9830.8125
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9830.8134765625
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8135, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -9830.8134765625
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8135, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -9830.8125
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9830.8125
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9830.8115234375
tensor(9830.8125, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9830.8115234375
tensor(9830.8115, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9830.810546875
tensor(9830.8115, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9830.8125
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8125, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9830.810546875
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9830.8115234375
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9830.810546875
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9830.810546875
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9830.810546875
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9830.8095703125
tensor(9830.8105, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9830.8095703125
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9830.810546875
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8105, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9830.80859375
tensor(9830.8096, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9830.8095703125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9830.8095703125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9830.8095703125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9830.8095703125
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9830.80859375
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9830.8076171875
tensor(9830.8086, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9830.8076171875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9830.8076171875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9830.8115234375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8115, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9830.8095703125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9830.8095703125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -9830.8076171875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9830.8076171875
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8076, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9830.8095703125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8096, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9830.8173828125
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8174, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -9830.80859375
tensor(9830.8076, grad_fn=<NegBackward0>) tensor(9830.8086, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[9.7022e-01, 2.9775e-02],
        [9.9973e-01, 2.6994e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1337, 0.2455],
         [0.6128, 0.2097]],

        [[0.7059, 0.2114],
         [0.6856, 0.5855]],

        [[0.5519, 0.1729],
         [0.5425, 0.5413]],

        [[0.6300, 0.1270],
         [0.5967, 0.6449]],

        [[0.6782, 0.1662],
         [0.7164, 0.7173]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000281772120727798
Average Adjusted Rand Index: -0.0009013459011305159
[-0.000281772120727798, -0.000281772120727798] [-0.0009013459011305159, -0.0009013459011305159] [9830.80859375, 9830.80859375]
-------------------------------------
This iteration is 79
True Objective function: Loss = -9940.813187980853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21003.796875
inf tensor(21003.7969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9786.017578125
tensor(21003.7969, grad_fn=<NegBackward0>) tensor(9786.0176, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9785.5830078125
tensor(9786.0176, grad_fn=<NegBackward0>) tensor(9785.5830, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9785.21484375
tensor(9785.5830, grad_fn=<NegBackward0>) tensor(9785.2148, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9784.8271484375
tensor(9785.2148, grad_fn=<NegBackward0>) tensor(9784.8271, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9784.6025390625
tensor(9784.8271, grad_fn=<NegBackward0>) tensor(9784.6025, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9784.4833984375
tensor(9784.6025, grad_fn=<NegBackward0>) tensor(9784.4834, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9784.4033203125
tensor(9784.4834, grad_fn=<NegBackward0>) tensor(9784.4033, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9784.34765625
tensor(9784.4033, grad_fn=<NegBackward0>) tensor(9784.3477, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9784.3095703125
tensor(9784.3477, grad_fn=<NegBackward0>) tensor(9784.3096, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9784.2802734375
tensor(9784.3096, grad_fn=<NegBackward0>) tensor(9784.2803, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9784.2578125
tensor(9784.2803, grad_fn=<NegBackward0>) tensor(9784.2578, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9784.234375
tensor(9784.2578, grad_fn=<NegBackward0>) tensor(9784.2344, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9784.2119140625
tensor(9784.2344, grad_fn=<NegBackward0>) tensor(9784.2119, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9784.1884765625
tensor(9784.2119, grad_fn=<NegBackward0>) tensor(9784.1885, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9784.1650390625
tensor(9784.1885, grad_fn=<NegBackward0>) tensor(9784.1650, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9784.142578125
tensor(9784.1650, grad_fn=<NegBackward0>) tensor(9784.1426, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9784.1181640625
tensor(9784.1426, grad_fn=<NegBackward0>) tensor(9784.1182, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9784.09765625
tensor(9784.1182, grad_fn=<NegBackward0>) tensor(9784.0977, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9784.0771484375
tensor(9784.0977, grad_fn=<NegBackward0>) tensor(9784.0771, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9784.0537109375
tensor(9784.0771, grad_fn=<NegBackward0>) tensor(9784.0537, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9784.0341796875
tensor(9784.0537, grad_fn=<NegBackward0>) tensor(9784.0342, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9784.0087890625
tensor(9784.0342, grad_fn=<NegBackward0>) tensor(9784.0088, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9783.9755859375
tensor(9784.0088, grad_fn=<NegBackward0>) tensor(9783.9756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9783.923828125
tensor(9783.9756, grad_fn=<NegBackward0>) tensor(9783.9238, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9783.8271484375
tensor(9783.9238, grad_fn=<NegBackward0>) tensor(9783.8271, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9783.6708984375
tensor(9783.8271, grad_fn=<NegBackward0>) tensor(9783.6709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9783.5302734375
tensor(9783.6709, grad_fn=<NegBackward0>) tensor(9783.5303, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9783.453125
tensor(9783.5303, grad_fn=<NegBackward0>) tensor(9783.4531, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9783.4150390625
tensor(9783.4531, grad_fn=<NegBackward0>) tensor(9783.4150, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9783.3955078125
tensor(9783.4150, grad_fn=<NegBackward0>) tensor(9783.3955, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9783.3857421875
tensor(9783.3955, grad_fn=<NegBackward0>) tensor(9783.3857, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9783.3798828125
tensor(9783.3857, grad_fn=<NegBackward0>) tensor(9783.3799, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9783.3740234375
tensor(9783.3799, grad_fn=<NegBackward0>) tensor(9783.3740, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9783.37109375
tensor(9783.3740, grad_fn=<NegBackward0>) tensor(9783.3711, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9783.369140625
tensor(9783.3711, grad_fn=<NegBackward0>) tensor(9783.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9783.3681640625
tensor(9783.3691, grad_fn=<NegBackward0>) tensor(9783.3682, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9783.3671875
tensor(9783.3682, grad_fn=<NegBackward0>) tensor(9783.3672, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9783.365234375
tensor(9783.3672, grad_fn=<NegBackward0>) tensor(9783.3652, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9783.3642578125
tensor(9783.3652, grad_fn=<NegBackward0>) tensor(9783.3643, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9783.36328125
tensor(9783.3643, grad_fn=<NegBackward0>) tensor(9783.3633, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9783.3642578125
tensor(9783.3633, grad_fn=<NegBackward0>) tensor(9783.3643, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9783.3623046875
tensor(9783.3633, grad_fn=<NegBackward0>) tensor(9783.3623, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9783.3623046875
tensor(9783.3623, grad_fn=<NegBackward0>) tensor(9783.3623, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9783.3623046875
tensor(9783.3623, grad_fn=<NegBackward0>) tensor(9783.3623, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9783.3623046875
tensor(9783.3623, grad_fn=<NegBackward0>) tensor(9783.3623, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9783.361328125
tensor(9783.3623, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9783.361328125
tensor(9783.3613, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9783.361328125
tensor(9783.3613, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9783.3623046875
tensor(9783.3613, grad_fn=<NegBackward0>) tensor(9783.3623, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9783.361328125
tensor(9783.3613, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9783.3603515625
tensor(9783.3613, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -9783.3603515625
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9783.3603515625
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9783.3603515625
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9783.361328125
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9783.359375
tensor(9783.3604, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9783.359375
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3594, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -9783.3603515625
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3604, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -9783.361328125
tensor(9783.3594, grad_fn=<NegBackward0>) tensor(9783.3613, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.0378, 0.9622],
        [0.0355, 0.9645]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9871, 0.0129], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1395, 0.1384],
         [0.5242, 0.1292]],

        [[0.6006, 0.2066],
         [0.5437, 0.6365]],

        [[0.6687, 0.1669],
         [0.6875, 0.7260]],

        [[0.5465, 0.1967],
         [0.5608, 0.6774]],

        [[0.7169, 0.1998],
         [0.5035, 0.6000]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0016636090245058127
Average Adjusted Rand Index: -0.0008085897718818593
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20757.9296875
inf tensor(20757.9297, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9786.01953125
tensor(20757.9297, grad_fn=<NegBackward0>) tensor(9786.0195, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9784.9736328125
tensor(9786.0195, grad_fn=<NegBackward0>) tensor(9784.9736, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9784.568359375
tensor(9784.9736, grad_fn=<NegBackward0>) tensor(9784.5684, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9784.248046875
tensor(9784.5684, grad_fn=<NegBackward0>) tensor(9784.2480, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9783.3046875
tensor(9784.2480, grad_fn=<NegBackward0>) tensor(9783.3047, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9781.0634765625
tensor(9783.3047, grad_fn=<NegBackward0>) tensor(9781.0635, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9780.2939453125
tensor(9781.0635, grad_fn=<NegBackward0>) tensor(9780.2939, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9779.9892578125
tensor(9780.2939, grad_fn=<NegBackward0>) tensor(9779.9893, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9779.8310546875
tensor(9779.9893, grad_fn=<NegBackward0>) tensor(9779.8311, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9779.7255859375
tensor(9779.8311, grad_fn=<NegBackward0>) tensor(9779.7256, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9779.6484375
tensor(9779.7256, grad_fn=<NegBackward0>) tensor(9779.6484, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9779.583984375
tensor(9779.6484, grad_fn=<NegBackward0>) tensor(9779.5840, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9779.5283203125
tensor(9779.5840, grad_fn=<NegBackward0>) tensor(9779.5283, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9779.4755859375
tensor(9779.5283, grad_fn=<NegBackward0>) tensor(9779.4756, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9779.431640625
tensor(9779.4756, grad_fn=<NegBackward0>) tensor(9779.4316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9779.390625
tensor(9779.4316, grad_fn=<NegBackward0>) tensor(9779.3906, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9779.3525390625
tensor(9779.3906, grad_fn=<NegBackward0>) tensor(9779.3525, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9779.3193359375
tensor(9779.3525, grad_fn=<NegBackward0>) tensor(9779.3193, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9779.2900390625
tensor(9779.3193, grad_fn=<NegBackward0>) tensor(9779.2900, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9779.265625
tensor(9779.2900, grad_fn=<NegBackward0>) tensor(9779.2656, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9779.2431640625
tensor(9779.2656, grad_fn=<NegBackward0>) tensor(9779.2432, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9779.22265625
tensor(9779.2432, grad_fn=<NegBackward0>) tensor(9779.2227, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9779.205078125
tensor(9779.2227, grad_fn=<NegBackward0>) tensor(9779.2051, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9779.19140625
tensor(9779.2051, grad_fn=<NegBackward0>) tensor(9779.1914, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9779.177734375
tensor(9779.1914, grad_fn=<NegBackward0>) tensor(9779.1777, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9779.1669921875
tensor(9779.1777, grad_fn=<NegBackward0>) tensor(9779.1670, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9779.1572265625
tensor(9779.1670, grad_fn=<NegBackward0>) tensor(9779.1572, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9779.1484375
tensor(9779.1572, grad_fn=<NegBackward0>) tensor(9779.1484, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9779.1435546875
tensor(9779.1484, grad_fn=<NegBackward0>) tensor(9779.1436, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9779.13671875
tensor(9779.1436, grad_fn=<NegBackward0>) tensor(9779.1367, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9779.130859375
tensor(9779.1367, grad_fn=<NegBackward0>) tensor(9779.1309, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9779.126953125
tensor(9779.1309, grad_fn=<NegBackward0>) tensor(9779.1270, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9779.123046875
tensor(9779.1270, grad_fn=<NegBackward0>) tensor(9779.1230, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9779.1201171875
tensor(9779.1230, grad_fn=<NegBackward0>) tensor(9779.1201, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9779.1171875
tensor(9779.1201, grad_fn=<NegBackward0>) tensor(9779.1172, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9779.1142578125
tensor(9779.1172, grad_fn=<NegBackward0>) tensor(9779.1143, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9779.11328125
tensor(9779.1143, grad_fn=<NegBackward0>) tensor(9779.1133, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9779.111328125
tensor(9779.1133, grad_fn=<NegBackward0>) tensor(9779.1113, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9779.109375
tensor(9779.1113, grad_fn=<NegBackward0>) tensor(9779.1094, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9779.109375
tensor(9779.1094, grad_fn=<NegBackward0>) tensor(9779.1094, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9779.1083984375
tensor(9779.1094, grad_fn=<NegBackward0>) tensor(9779.1084, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9779.10546875
tensor(9779.1084, grad_fn=<NegBackward0>) tensor(9779.1055, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9779.107421875
tensor(9779.1055, grad_fn=<NegBackward0>) tensor(9779.1074, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9779.10546875
tensor(9779.1055, grad_fn=<NegBackward0>) tensor(9779.1055, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9779.1044921875
tensor(9779.1055, grad_fn=<NegBackward0>) tensor(9779.1045, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9779.103515625
tensor(9779.1045, grad_fn=<NegBackward0>) tensor(9779.1035, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9779.1044921875
tensor(9779.1035, grad_fn=<NegBackward0>) tensor(9779.1045, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9779.103515625
tensor(9779.1035, grad_fn=<NegBackward0>) tensor(9779.1035, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9779.1025390625
tensor(9779.1035, grad_fn=<NegBackward0>) tensor(9779.1025, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9779.1044921875
tensor(9779.1025, grad_fn=<NegBackward0>) tensor(9779.1045, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9779.103515625
tensor(9779.1025, grad_fn=<NegBackward0>) tensor(9779.1035, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -9779.103515625
tensor(9779.1025, grad_fn=<NegBackward0>) tensor(9779.1035, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -9779.1025390625
tensor(9779.1025, grad_fn=<NegBackward0>) tensor(9779.1025, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9779.1015625
tensor(9779.1025, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9779.1025390625
tensor(9779.1016, grad_fn=<NegBackward0>) tensor(9779.1025, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9779.1015625
tensor(9779.1016, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9779.1025390625
tensor(9779.1016, grad_fn=<NegBackward0>) tensor(9779.1025, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9779.1015625
tensor(9779.1016, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9779.1005859375
tensor(9779.1016, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9779.1015625
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9779.1005859375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.1006, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9779.099609375
tensor(9779.1006, grad_fn=<NegBackward0>) tensor(9779.0996, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9779.1015625
tensor(9779.0996, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9779.1015625
tensor(9779.0996, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9779.1015625
tensor(9779.0996, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -9779.1015625
tensor(9779.0996, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -9779.1015625
tensor(9779.0996, grad_fn=<NegBackward0>) tensor(9779.1016, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.9883, 0.0117],
        [0.5004, 0.4996]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9990e-01, 9.6271e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1348, 0.1305],
         [0.6581, 0.6130]],

        [[0.5391, 0.2436],
         [0.5590, 0.6439]],

        [[0.5193, 0.0848],
         [0.6919, 0.6116]],

        [[0.6771, 0.0384],
         [0.5031, 0.7046]],

        [[0.7136, 0.2261],
         [0.6129, 0.7163]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: 0.0009319421978709173
Average Adjusted Rand Index: -0.00029325268069361305
[-0.0016636090245058127, 0.0009319421978709173] [-0.0008085897718818593, -0.00029325268069361305] [9783.361328125, 9779.1015625]
-------------------------------------
This iteration is 80
True Objective function: Loss = -10078.025409900833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20977.69140625
inf tensor(20977.6914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9952.3994140625
tensor(20977.6914, grad_fn=<NegBackward0>) tensor(9952.3994, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9951.8955078125
tensor(9952.3994, grad_fn=<NegBackward0>) tensor(9951.8955, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9951.533203125
tensor(9951.8955, grad_fn=<NegBackward0>) tensor(9951.5332, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9950.2666015625
tensor(9951.5332, grad_fn=<NegBackward0>) tensor(9950.2666, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9949.580078125
tensor(9950.2666, grad_fn=<NegBackward0>) tensor(9949.5801, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9949.2890625
tensor(9949.5801, grad_fn=<NegBackward0>) tensor(9949.2891, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9949.0361328125
tensor(9949.2891, grad_fn=<NegBackward0>) tensor(9949.0361, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9948.755859375
tensor(9949.0361, grad_fn=<NegBackward0>) tensor(9948.7559, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9948.509765625
tensor(9948.7559, grad_fn=<NegBackward0>) tensor(9948.5098, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9948.27734375
tensor(9948.5098, grad_fn=<NegBackward0>) tensor(9948.2773, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9948.0009765625
tensor(9948.2773, grad_fn=<NegBackward0>) tensor(9948.0010, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9947.68359375
tensor(9948.0010, grad_fn=<NegBackward0>) tensor(9947.6836, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9947.3505859375
tensor(9947.6836, grad_fn=<NegBackward0>) tensor(9947.3506, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9947.0546875
tensor(9947.3506, grad_fn=<NegBackward0>) tensor(9947.0547, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9946.8291015625
tensor(9947.0547, grad_fn=<NegBackward0>) tensor(9946.8291, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9946.6630859375
tensor(9946.8291, grad_fn=<NegBackward0>) tensor(9946.6631, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9946.5458984375
tensor(9946.6631, grad_fn=<NegBackward0>) tensor(9946.5459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9946.462890625
tensor(9946.5459, grad_fn=<NegBackward0>) tensor(9946.4629, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9946.3818359375
tensor(9946.4629, grad_fn=<NegBackward0>) tensor(9946.3818, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9944.6787109375
tensor(9946.3818, grad_fn=<NegBackward0>) tensor(9944.6787, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9943.533203125
tensor(9944.6787, grad_fn=<NegBackward0>) tensor(9943.5332, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9941.8603515625
tensor(9943.5332, grad_fn=<NegBackward0>) tensor(9941.8604, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9941.7216796875
tensor(9941.8604, grad_fn=<NegBackward0>) tensor(9941.7217, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9941.6630859375
tensor(9941.7217, grad_fn=<NegBackward0>) tensor(9941.6631, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9941.62109375
tensor(9941.6631, grad_fn=<NegBackward0>) tensor(9941.6211, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9941.5732421875
tensor(9941.6211, grad_fn=<NegBackward0>) tensor(9941.5732, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9941.546875
tensor(9941.5732, grad_fn=<NegBackward0>) tensor(9941.5469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9941.529296875
tensor(9941.5469, grad_fn=<NegBackward0>) tensor(9941.5293, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9941.5185546875
tensor(9941.5293, grad_fn=<NegBackward0>) tensor(9941.5186, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9941.505859375
tensor(9941.5186, grad_fn=<NegBackward0>) tensor(9941.5059, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9941.49609375
tensor(9941.5059, grad_fn=<NegBackward0>) tensor(9941.4961, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9941.4892578125
tensor(9941.4961, grad_fn=<NegBackward0>) tensor(9941.4893, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9941.4833984375
tensor(9941.4893, grad_fn=<NegBackward0>) tensor(9941.4834, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9941.4775390625
tensor(9941.4834, grad_fn=<NegBackward0>) tensor(9941.4775, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9941.47265625
tensor(9941.4775, grad_fn=<NegBackward0>) tensor(9941.4727, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9941.466796875
tensor(9941.4727, grad_fn=<NegBackward0>) tensor(9941.4668, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9941.4619140625
tensor(9941.4668, grad_fn=<NegBackward0>) tensor(9941.4619, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9941.4541015625
tensor(9941.4619, grad_fn=<NegBackward0>) tensor(9941.4541, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9941.451171875
tensor(9941.4541, grad_fn=<NegBackward0>) tensor(9941.4512, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9941.447265625
tensor(9941.4512, grad_fn=<NegBackward0>) tensor(9941.4473, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9941.4423828125
tensor(9941.4473, grad_fn=<NegBackward0>) tensor(9941.4424, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9941.4404296875
tensor(9941.4424, grad_fn=<NegBackward0>) tensor(9941.4404, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9941.439453125
tensor(9941.4404, grad_fn=<NegBackward0>) tensor(9941.4395, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9941.4345703125
tensor(9941.4395, grad_fn=<NegBackward0>) tensor(9941.4346, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9941.43359375
tensor(9941.4346, grad_fn=<NegBackward0>) tensor(9941.4336, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9941.4248046875
tensor(9941.4336, grad_fn=<NegBackward0>) tensor(9941.4248, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9941.41796875
tensor(9941.4248, grad_fn=<NegBackward0>) tensor(9941.4180, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9941.41796875
tensor(9941.4180, grad_fn=<NegBackward0>) tensor(9941.4180, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9941.4140625
tensor(9941.4180, grad_fn=<NegBackward0>) tensor(9941.4141, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9941.408203125
tensor(9941.4141, grad_fn=<NegBackward0>) tensor(9941.4082, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9941.376953125
tensor(9941.4082, grad_fn=<NegBackward0>) tensor(9941.3770, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9941.3720703125
tensor(9941.3770, grad_fn=<NegBackward0>) tensor(9941.3721, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9941.3701171875
tensor(9941.3721, grad_fn=<NegBackward0>) tensor(9941.3701, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9941.3671875
tensor(9941.3701, grad_fn=<NegBackward0>) tensor(9941.3672, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9941.3642578125
tensor(9941.3672, grad_fn=<NegBackward0>) tensor(9941.3643, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9941.3623046875
tensor(9941.3643, grad_fn=<NegBackward0>) tensor(9941.3623, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9941.36328125
tensor(9941.3623, grad_fn=<NegBackward0>) tensor(9941.3633, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9941.3623046875
tensor(9941.3623, grad_fn=<NegBackward0>) tensor(9941.3623, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9941.3603515625
tensor(9941.3623, grad_fn=<NegBackward0>) tensor(9941.3604, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9941.359375
tensor(9941.3604, grad_fn=<NegBackward0>) tensor(9941.3594, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9941.357421875
tensor(9941.3594, grad_fn=<NegBackward0>) tensor(9941.3574, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9941.353515625
tensor(9941.3574, grad_fn=<NegBackward0>) tensor(9941.3535, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9941.3505859375
tensor(9941.3535, grad_fn=<NegBackward0>) tensor(9941.3506, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9941.349609375
tensor(9941.3506, grad_fn=<NegBackward0>) tensor(9941.3496, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9941.3486328125
tensor(9941.3496, grad_fn=<NegBackward0>) tensor(9941.3486, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9941.3486328125
tensor(9941.3486, grad_fn=<NegBackward0>) tensor(9941.3486, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9941.34765625
tensor(9941.3486, grad_fn=<NegBackward0>) tensor(9941.3477, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9941.34765625
tensor(9941.3477, grad_fn=<NegBackward0>) tensor(9941.3477, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9941.3466796875
tensor(9941.3477, grad_fn=<NegBackward0>) tensor(9941.3467, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9941.345703125
tensor(9941.3467, grad_fn=<NegBackward0>) tensor(9941.3457, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9941.345703125
tensor(9941.3457, grad_fn=<NegBackward0>) tensor(9941.3457, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9941.34375
tensor(9941.3457, grad_fn=<NegBackward0>) tensor(9941.3438, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9941.3447265625
tensor(9941.3438, grad_fn=<NegBackward0>) tensor(9941.3447, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9941.3447265625
tensor(9941.3438, grad_fn=<NegBackward0>) tensor(9941.3447, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9941.341796875
tensor(9941.3438, grad_fn=<NegBackward0>) tensor(9941.3418, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9941.3427734375
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3428, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9941.341796875
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3418, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9941.341796875
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3418, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9941.3427734375
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3428, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9941.341796875
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3418, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9941.341796875
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3418, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9941.3447265625
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3447, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9941.349609375
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3496, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9941.3427734375
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3428, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9941.3408203125
tensor(9941.3418, grad_fn=<NegBackward0>) tensor(9941.3408, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9941.3388671875
tensor(9941.3408, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9941.33984375
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3398, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9941.3388671875
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9941.3388671875
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9941.3388671875
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9941.3427734375
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3428, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9941.337890625
tensor(9941.3389, grad_fn=<NegBackward0>) tensor(9941.3379, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9941.4052734375
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.4053, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9941.337890625
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3379, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9941.3388671875
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9941.33984375
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3398, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -9941.3388671875
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3389, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -9941.337890625
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3379, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9941.337890625
tensor(9941.3379, grad_fn=<NegBackward0>) tensor(9941.3379, grad_fn=<NegBackward0>)
pi: tensor([[9.8525e-01, 1.4750e-02],
        [5.6757e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9238, 0.0762], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1289, 0.1933],
         [0.5499, 0.1516]],

        [[0.7260, 0.1780],
         [0.6864, 0.5391]],

        [[0.5295, 0.1920],
         [0.7266, 0.6761]],

        [[0.7053, 0.1617],
         [0.7037, 0.6243]],

        [[0.5634, 0.1714],
         [0.5985, 0.5912]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.006132953674296353
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.05027851070067429
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0531930439865556
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01802990325417766
Global Adjusted Rand Index: 0.024282021522701057
Average Adjusted Rand Index: 0.02572988033545191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20478.01953125
inf tensor(20478.0195, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9952.06640625
tensor(20478.0195, grad_fn=<NegBackward0>) tensor(9952.0664, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9951.0810546875
tensor(9952.0664, grad_fn=<NegBackward0>) tensor(9951.0811, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9950.0693359375
tensor(9951.0811, grad_fn=<NegBackward0>) tensor(9950.0693, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9949.5166015625
tensor(9950.0693, grad_fn=<NegBackward0>) tensor(9949.5166, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9949.2255859375
tensor(9949.5166, grad_fn=<NegBackward0>) tensor(9949.2256, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9948.9833984375
tensor(9949.2256, grad_fn=<NegBackward0>) tensor(9948.9834, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9948.7109375
tensor(9948.9834, grad_fn=<NegBackward0>) tensor(9948.7109, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9948.412109375
tensor(9948.7109, grad_fn=<NegBackward0>) tensor(9948.4121, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9948.08203125
tensor(9948.4121, grad_fn=<NegBackward0>) tensor(9948.0820, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9947.708984375
tensor(9948.0820, grad_fn=<NegBackward0>) tensor(9947.7090, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9947.3525390625
tensor(9947.7090, grad_fn=<NegBackward0>) tensor(9947.3525, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9947.068359375
tensor(9947.3525, grad_fn=<NegBackward0>) tensor(9947.0684, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9946.875
tensor(9947.0684, grad_fn=<NegBackward0>) tensor(9946.8750, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9946.734375
tensor(9946.8750, grad_fn=<NegBackward0>) tensor(9946.7344, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9946.626953125
tensor(9946.7344, grad_fn=<NegBackward0>) tensor(9946.6270, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9946.5458984375
tensor(9946.6270, grad_fn=<NegBackward0>) tensor(9946.5459, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9946.4833984375
tensor(9946.5459, grad_fn=<NegBackward0>) tensor(9946.4834, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9946.31640625
tensor(9946.4834, grad_fn=<NegBackward0>) tensor(9946.3164, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9944.146484375
tensor(9946.3164, grad_fn=<NegBackward0>) tensor(9944.1465, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9943.5693359375
tensor(9944.1465, grad_fn=<NegBackward0>) tensor(9943.5693, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9941.982421875
tensor(9943.5693, grad_fn=<NegBackward0>) tensor(9941.9824, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9941.8134765625
tensor(9941.9824, grad_fn=<NegBackward0>) tensor(9941.8135, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9941.7529296875
tensor(9941.8135, grad_fn=<NegBackward0>) tensor(9941.7529, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9941.720703125
tensor(9941.7529, grad_fn=<NegBackward0>) tensor(9941.7207, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9941.7001953125
tensor(9941.7207, grad_fn=<NegBackward0>) tensor(9941.7002, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9941.6865234375
tensor(9941.7002, grad_fn=<NegBackward0>) tensor(9941.6865, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9941.67578125
tensor(9941.6865, grad_fn=<NegBackward0>) tensor(9941.6758, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9941.6640625
tensor(9941.6758, grad_fn=<NegBackward0>) tensor(9941.6641, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9941.65234375
tensor(9941.6641, grad_fn=<NegBackward0>) tensor(9941.6523, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9941.640625
tensor(9941.6523, grad_fn=<NegBackward0>) tensor(9941.6406, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9941.6337890625
tensor(9941.6406, grad_fn=<NegBackward0>) tensor(9941.6338, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9941.62890625
tensor(9941.6338, grad_fn=<NegBackward0>) tensor(9941.6289, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9941.6259765625
tensor(9941.6289, grad_fn=<NegBackward0>) tensor(9941.6260, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9941.6220703125
tensor(9941.6260, grad_fn=<NegBackward0>) tensor(9941.6221, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9941.6181640625
tensor(9941.6221, grad_fn=<NegBackward0>) tensor(9941.6182, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9941.61328125
tensor(9941.6182, grad_fn=<NegBackward0>) tensor(9941.6133, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9941.6083984375
tensor(9941.6133, grad_fn=<NegBackward0>) tensor(9941.6084, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9941.587890625
tensor(9941.6084, grad_fn=<NegBackward0>) tensor(9941.5879, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9941.58203125
tensor(9941.5879, grad_fn=<NegBackward0>) tensor(9941.5820, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9941.5771484375
tensor(9941.5820, grad_fn=<NegBackward0>) tensor(9941.5771, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9941.576171875
tensor(9941.5771, grad_fn=<NegBackward0>) tensor(9941.5762, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9941.5732421875
tensor(9941.5762, grad_fn=<NegBackward0>) tensor(9941.5732, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9941.5693359375
tensor(9941.5732, grad_fn=<NegBackward0>) tensor(9941.5693, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9941.5400390625
tensor(9941.5693, grad_fn=<NegBackward0>) tensor(9941.5400, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9941.5341796875
tensor(9941.5400, grad_fn=<NegBackward0>) tensor(9941.5342, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9941.5302734375
tensor(9941.5342, grad_fn=<NegBackward0>) tensor(9941.5303, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9941.5263671875
tensor(9941.5303, grad_fn=<NegBackward0>) tensor(9941.5264, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9941.521484375
tensor(9941.5264, grad_fn=<NegBackward0>) tensor(9941.5215, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9941.5146484375
tensor(9941.5215, grad_fn=<NegBackward0>) tensor(9941.5146, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9941.5126953125
tensor(9941.5146, grad_fn=<NegBackward0>) tensor(9941.5127, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9941.509765625
tensor(9941.5127, grad_fn=<NegBackward0>) tensor(9941.5098, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9941.498046875
tensor(9941.5098, grad_fn=<NegBackward0>) tensor(9941.4980, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9941.49609375
tensor(9941.4980, grad_fn=<NegBackward0>) tensor(9941.4961, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9941.4951171875
tensor(9941.4961, grad_fn=<NegBackward0>) tensor(9941.4951, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9941.4931640625
tensor(9941.4951, grad_fn=<NegBackward0>) tensor(9941.4932, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9941.494140625
tensor(9941.4932, grad_fn=<NegBackward0>) tensor(9941.4941, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9941.4921875
tensor(9941.4932, grad_fn=<NegBackward0>) tensor(9941.4922, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9941.4912109375
tensor(9941.4922, grad_fn=<NegBackward0>) tensor(9941.4912, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9941.4921875
tensor(9941.4912, grad_fn=<NegBackward0>) tensor(9941.4922, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9941.490234375
tensor(9941.4912, grad_fn=<NegBackward0>) tensor(9941.4902, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9941.48828125
tensor(9941.4902, grad_fn=<NegBackward0>) tensor(9941.4883, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9941.4892578125
tensor(9941.4883, grad_fn=<NegBackward0>) tensor(9941.4893, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9941.4853515625
tensor(9941.4883, grad_fn=<NegBackward0>) tensor(9941.4854, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9941.482421875
tensor(9941.4854, grad_fn=<NegBackward0>) tensor(9941.4824, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9941.48046875
tensor(9941.4824, grad_fn=<NegBackward0>) tensor(9941.4805, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9941.470703125
tensor(9941.4805, grad_fn=<NegBackward0>) tensor(9941.4707, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9941.46875
tensor(9941.4707, grad_fn=<NegBackward0>) tensor(9941.4688, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9941.470703125
tensor(9941.4688, grad_fn=<NegBackward0>) tensor(9941.4707, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9941.4658203125
tensor(9941.4688, grad_fn=<NegBackward0>) tensor(9941.4658, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9941.46484375
tensor(9941.4658, grad_fn=<NegBackward0>) tensor(9941.4648, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9941.4638671875
tensor(9941.4648, grad_fn=<NegBackward0>) tensor(9941.4639, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9941.4638671875
tensor(9941.4639, grad_fn=<NegBackward0>) tensor(9941.4639, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9941.46484375
tensor(9941.4639, grad_fn=<NegBackward0>) tensor(9941.4648, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9941.4619140625
tensor(9941.4639, grad_fn=<NegBackward0>) tensor(9941.4619, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9941.4619140625
tensor(9941.4619, grad_fn=<NegBackward0>) tensor(9941.4619, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9941.4619140625
tensor(9941.4619, grad_fn=<NegBackward0>) tensor(9941.4619, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9941.4599609375
tensor(9941.4619, grad_fn=<NegBackward0>) tensor(9941.4600, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9941.45703125
tensor(9941.4600, grad_fn=<NegBackward0>) tensor(9941.4570, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9941.462890625
tensor(9941.4570, grad_fn=<NegBackward0>) tensor(9941.4629, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9941.455078125
tensor(9941.4570, grad_fn=<NegBackward0>) tensor(9941.4551, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9941.5126953125
tensor(9941.4551, grad_fn=<NegBackward0>) tensor(9941.5127, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9941.4423828125
tensor(9941.4551, grad_fn=<NegBackward0>) tensor(9941.4424, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9941.4423828125
tensor(9941.4424, grad_fn=<NegBackward0>) tensor(9941.4424, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9941.4482421875
tensor(9941.4424, grad_fn=<NegBackward0>) tensor(9941.4482, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9941.4267578125
tensor(9941.4424, grad_fn=<NegBackward0>) tensor(9941.4268, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9941.56640625
tensor(9941.4268, grad_fn=<NegBackward0>) tensor(9941.5664, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9941.4208984375
tensor(9941.4268, grad_fn=<NegBackward0>) tensor(9941.4209, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9941.4208984375
tensor(9941.4209, grad_fn=<NegBackward0>) tensor(9941.4209, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9941.4248046875
tensor(9941.4209, grad_fn=<NegBackward0>) tensor(9941.4248, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9941.4208984375
tensor(9941.4209, grad_fn=<NegBackward0>) tensor(9941.4209, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9941.419921875
tensor(9941.4209, grad_fn=<NegBackward0>) tensor(9941.4199, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9941.4189453125
tensor(9941.4199, grad_fn=<NegBackward0>) tensor(9941.4189, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -9941.419921875
tensor(9941.4189, grad_fn=<NegBackward0>) tensor(9941.4199, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -9941.4189453125
tensor(9941.4189, grad_fn=<NegBackward0>) tensor(9941.4189, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9941.41796875
tensor(9941.4189, grad_fn=<NegBackward0>) tensor(9941.4180, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9941.390625
tensor(9941.4180, grad_fn=<NegBackward0>) tensor(9941.3906, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9941.4013671875
tensor(9941.3906, grad_fn=<NegBackward0>) tensor(9941.4014, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9941.3671875
tensor(9941.3906, grad_fn=<NegBackward0>) tensor(9941.3672, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9941.5048828125
tensor(9941.3672, grad_fn=<NegBackward0>) tensor(9941.5049, grad_fn=<NegBackward0>)
1
pi: tensor([[9.8558e-01, 1.4418e-02],
        [6.3674e-05, 9.9994e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9246, 0.0754], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1290, 0.1936],
         [0.5639, 0.1512]],

        [[0.5082, 0.1781],
         [0.6591, 0.6489]],

        [[0.6930, 0.1924],
         [0.6941, 0.6597]],

        [[0.6205, 0.1619],
         [0.7288, 0.5109]],

        [[0.6881, 0.1718],
         [0.7004, 0.7265]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0028334070515487393
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.05027851070067429
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0531930439865556
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01802990325417766
Global Adjusted Rand Index: 0.023022572691729196
Average Adjusted Rand Index: 0.025069971010902387
[0.024282021522701057, 0.023022572691729196] [0.02572988033545191, 0.025069971010902387] [9941.3359375, 9941.36328125]
-------------------------------------
This iteration is 81
True Objective function: Loss = -10509.247758293353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23988.65625
inf tensor(23988.6562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10437.8876953125
tensor(23988.6562, grad_fn=<NegBackward0>) tensor(10437.8877, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10435.9267578125
tensor(10437.8877, grad_fn=<NegBackward0>) tensor(10435.9268, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10433.173828125
tensor(10435.9268, grad_fn=<NegBackward0>) tensor(10433.1738, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10431.8994140625
tensor(10433.1738, grad_fn=<NegBackward0>) tensor(10431.8994, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10430.8349609375
tensor(10431.8994, grad_fn=<NegBackward0>) tensor(10430.8350, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10429.55859375
tensor(10430.8350, grad_fn=<NegBackward0>) tensor(10429.5586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10428.251953125
tensor(10429.5586, grad_fn=<NegBackward0>) tensor(10428.2520, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10426.748046875
tensor(10428.2520, grad_fn=<NegBackward0>) tensor(10426.7480, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10423.8857421875
tensor(10426.7480, grad_fn=<NegBackward0>) tensor(10423.8857, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10408.6689453125
tensor(10423.8857, grad_fn=<NegBackward0>) tensor(10408.6689, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10401.2021484375
tensor(10408.6689, grad_fn=<NegBackward0>) tensor(10401.2021, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10400.7626953125
tensor(10401.2021, grad_fn=<NegBackward0>) tensor(10400.7627, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10400.001953125
tensor(10400.7627, grad_fn=<NegBackward0>) tensor(10400.0020, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10399.5400390625
tensor(10400.0020, grad_fn=<NegBackward0>) tensor(10399.5400, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10399.3828125
tensor(10399.5400, grad_fn=<NegBackward0>) tensor(10399.3828, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10399.34375
tensor(10399.3828, grad_fn=<NegBackward0>) tensor(10399.3438, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10399.3291015625
tensor(10399.3438, grad_fn=<NegBackward0>) tensor(10399.3291, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10399.3017578125
tensor(10399.3291, grad_fn=<NegBackward0>) tensor(10399.3018, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10399.2919921875
tensor(10399.3018, grad_fn=<NegBackward0>) tensor(10399.2920, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10399.287109375
tensor(10399.2920, grad_fn=<NegBackward0>) tensor(10399.2871, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10399.283203125
tensor(10399.2871, grad_fn=<NegBackward0>) tensor(10399.2832, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10399.279296875
tensor(10399.2832, grad_fn=<NegBackward0>) tensor(10399.2793, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10399.2607421875
tensor(10399.2793, grad_fn=<NegBackward0>) tensor(10399.2607, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10399.2353515625
tensor(10399.2607, grad_fn=<NegBackward0>) tensor(10399.2354, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10399.2333984375
tensor(10399.2354, grad_fn=<NegBackward0>) tensor(10399.2334, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10399.2255859375
tensor(10399.2334, grad_fn=<NegBackward0>) tensor(10399.2256, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10399.2041015625
tensor(10399.2256, grad_fn=<NegBackward0>) tensor(10399.2041, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10399.201171875
tensor(10399.2041, grad_fn=<NegBackward0>) tensor(10399.2012, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10399.1962890625
tensor(10399.2012, grad_fn=<NegBackward0>) tensor(10399.1963, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10399.19140625
tensor(10399.1963, grad_fn=<NegBackward0>) tensor(10399.1914, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10399.1943359375
tensor(10399.1914, grad_fn=<NegBackward0>) tensor(10399.1943, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10399.19140625
tensor(10399.1914, grad_fn=<NegBackward0>) tensor(10399.1914, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10399.1904296875
tensor(10399.1914, grad_fn=<NegBackward0>) tensor(10399.1904, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10399.1904296875
tensor(10399.1904, grad_fn=<NegBackward0>) tensor(10399.1904, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10399.1884765625
tensor(10399.1904, grad_fn=<NegBackward0>) tensor(10399.1885, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10399.189453125
tensor(10399.1885, grad_fn=<NegBackward0>) tensor(10399.1895, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10399.189453125
tensor(10399.1885, grad_fn=<NegBackward0>) tensor(10399.1895, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -10399.1884765625
tensor(10399.1885, grad_fn=<NegBackward0>) tensor(10399.1885, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10399.1875
tensor(10399.1885, grad_fn=<NegBackward0>) tensor(10399.1875, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10399.1806640625
tensor(10399.1875, grad_fn=<NegBackward0>) tensor(10399.1807, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10399.1611328125
tensor(10399.1807, grad_fn=<NegBackward0>) tensor(10399.1611, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10399.15625
tensor(10399.1611, grad_fn=<NegBackward0>) tensor(10399.1562, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10399.154296875
tensor(10399.1562, grad_fn=<NegBackward0>) tensor(10399.1543, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10399.1513671875
tensor(10399.1543, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10399.150390625
tensor(10399.1514, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -10399.1552734375
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1553, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10399.1533203125
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1533, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10399.1494140625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1494, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10399.150390625
tensor(10399.1494, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10399.1513671875
tensor(10399.1494, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -10399.158203125
tensor(10399.1494, grad_fn=<NegBackward0>) tensor(10399.1582, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -10399.150390625
tensor(10399.1494, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -10399.150390625
tensor(10399.1494, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.7772, 0.2228],
        [0.0857, 0.9143]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3385, 0.6615], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2829, 0.1222],
         [0.5226, 0.1390]],

        [[0.5505, 0.1200],
         [0.6740, 0.5042]],

        [[0.6508, 0.1686],
         [0.7114, 0.6697]],

        [[0.6761, 0.1392],
         [0.7233, 0.6296]],

        [[0.5748, 0.1127],
         [0.5125, 0.7188]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.2230321472712959
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 29
Adjusted Rand Index: 0.16668015735539332
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.03887382981743634
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.03043064800978128
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.10505724895900947
Global Adjusted Rand Index: 0.08973080175511237
Average Adjusted Rand Index: 0.09726527435560874
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22761.72265625
inf tensor(22761.7227, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10437.658203125
tensor(22761.7227, grad_fn=<NegBackward0>) tensor(10437.6582, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10436.4091796875
tensor(10437.6582, grad_fn=<NegBackward0>) tensor(10436.4092, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10433.0078125
tensor(10436.4092, grad_fn=<NegBackward0>) tensor(10433.0078, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10431.2392578125
tensor(10433.0078, grad_fn=<NegBackward0>) tensor(10431.2393, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10428.5654296875
tensor(10431.2393, grad_fn=<NegBackward0>) tensor(10428.5654, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10426.4599609375
tensor(10428.5654, grad_fn=<NegBackward0>) tensor(10426.4600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10423.4462890625
tensor(10426.4600, grad_fn=<NegBackward0>) tensor(10423.4463, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10407.5966796875
tensor(10423.4463, grad_fn=<NegBackward0>) tensor(10407.5967, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10399.9287109375
tensor(10407.5967, grad_fn=<NegBackward0>) tensor(10399.9287, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10399.4638671875
tensor(10399.9287, grad_fn=<NegBackward0>) tensor(10399.4639, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10399.3828125
tensor(10399.4639, grad_fn=<NegBackward0>) tensor(10399.3828, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10399.3486328125
tensor(10399.3828, grad_fn=<NegBackward0>) tensor(10399.3486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10399.3232421875
tensor(10399.3486, grad_fn=<NegBackward0>) tensor(10399.3232, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10399.224609375
tensor(10399.3232, grad_fn=<NegBackward0>) tensor(10399.2246, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10399.2177734375
tensor(10399.2246, grad_fn=<NegBackward0>) tensor(10399.2178, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10399.2138671875
tensor(10399.2178, grad_fn=<NegBackward0>) tensor(10399.2139, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10399.208984375
tensor(10399.2139, grad_fn=<NegBackward0>) tensor(10399.2090, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10399.20703125
tensor(10399.2090, grad_fn=<NegBackward0>) tensor(10399.2070, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10399.205078125
tensor(10399.2070, grad_fn=<NegBackward0>) tensor(10399.2051, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10399.201171875
tensor(10399.2051, grad_fn=<NegBackward0>) tensor(10399.2012, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10399.19921875
tensor(10399.2012, grad_fn=<NegBackward0>) tensor(10399.1992, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10399.1982421875
tensor(10399.1992, grad_fn=<NegBackward0>) tensor(10399.1982, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10399.1962890625
tensor(10399.1982, grad_fn=<NegBackward0>) tensor(10399.1963, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10399.185546875
tensor(10399.1963, grad_fn=<NegBackward0>) tensor(10399.1855, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10399.15234375
tensor(10399.1855, grad_fn=<NegBackward0>) tensor(10399.1523, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10399.15234375
tensor(10399.1523, grad_fn=<NegBackward0>) tensor(10399.1523, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10399.15234375
tensor(10399.1523, grad_fn=<NegBackward0>) tensor(10399.1523, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10399.1513671875
tensor(10399.1523, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10399.1513671875
tensor(10399.1514, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10399.15234375
tensor(10399.1514, grad_fn=<NegBackward0>) tensor(10399.1523, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10399.1513671875
tensor(10399.1514, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10399.150390625
tensor(10399.1514, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -10399.150390625
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1504, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10399.1552734375
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1553, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -10399.15234375
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1523, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -10399.1513671875
tensor(10399.1504, grad_fn=<NegBackward0>) tensor(10399.1514, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.7772, 0.2228],
        [0.0858, 0.9142]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3378, 0.6622], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2829, 0.1222],
         [0.6329, 0.1390]],

        [[0.5102, 0.1200],
         [0.6552, 0.6877]],

        [[0.6771, 0.1686],
         [0.7184, 0.6355]],

        [[0.6181, 0.1392],
         [0.6101, 0.7044]],

        [[0.5547, 0.1127],
         [0.7000, 0.5058]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.2230321472712959
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 29
Adjusted Rand Index: 0.16668015735539332
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.03887382981743634
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.03043064800978128
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 33
Adjusted Rand Index: 0.10505724895900947
Global Adjusted Rand Index: 0.08973080175511237
Average Adjusted Rand Index: 0.09726527435560874
[0.08973080175511237, 0.08973080175511237] [0.09726527435560874, 0.09726527435560874] [10399.150390625, 10399.1513671875]
-------------------------------------
This iteration is 82
True Objective function: Loss = -10234.165624424195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21821.7578125
inf tensor(21821.7578, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10097.4072265625
tensor(21821.7578, grad_fn=<NegBackward0>) tensor(10097.4072, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10095.2783203125
tensor(10097.4072, grad_fn=<NegBackward0>) tensor(10095.2783, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10094.095703125
tensor(10095.2783, grad_fn=<NegBackward0>) tensor(10094.0957, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10093.28125
tensor(10094.0957, grad_fn=<NegBackward0>) tensor(10093.2812, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10092.8251953125
tensor(10093.2812, grad_fn=<NegBackward0>) tensor(10092.8252, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10092.490234375
tensor(10092.8252, grad_fn=<NegBackward0>) tensor(10092.4902, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10092.2216796875
tensor(10092.4902, grad_fn=<NegBackward0>) tensor(10092.2217, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10091.990234375
tensor(10092.2217, grad_fn=<NegBackward0>) tensor(10091.9902, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10091.80859375
tensor(10091.9902, grad_fn=<NegBackward0>) tensor(10091.8086, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10091.673828125
tensor(10091.8086, grad_fn=<NegBackward0>) tensor(10091.6738, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10091.5771484375
tensor(10091.6738, grad_fn=<NegBackward0>) tensor(10091.5771, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10091.494140625
tensor(10091.5771, grad_fn=<NegBackward0>) tensor(10091.4941, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10091.4208984375
tensor(10091.4941, grad_fn=<NegBackward0>) tensor(10091.4209, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10091.34765625
tensor(10091.4209, grad_fn=<NegBackward0>) tensor(10091.3477, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10091.265625
tensor(10091.3477, grad_fn=<NegBackward0>) tensor(10091.2656, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10091.17578125
tensor(10091.2656, grad_fn=<NegBackward0>) tensor(10091.1758, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10091.0869140625
tensor(10091.1758, grad_fn=<NegBackward0>) tensor(10091.0869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10091.0009765625
tensor(10091.0869, grad_fn=<NegBackward0>) tensor(10091.0010, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10090.9287109375
tensor(10091.0010, grad_fn=<NegBackward0>) tensor(10090.9287, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10090.85546875
tensor(10090.9287, grad_fn=<NegBackward0>) tensor(10090.8555, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10090.78125
tensor(10090.8555, grad_fn=<NegBackward0>) tensor(10090.7812, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10090.6953125
tensor(10090.7812, grad_fn=<NegBackward0>) tensor(10090.6953, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10090.5908203125
tensor(10090.6953, grad_fn=<NegBackward0>) tensor(10090.5908, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10090.4140625
tensor(10090.5908, grad_fn=<NegBackward0>) tensor(10090.4141, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10090.2490234375
tensor(10090.4141, grad_fn=<NegBackward0>) tensor(10090.2490, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10090.1708984375
tensor(10090.2490, grad_fn=<NegBackward0>) tensor(10090.1709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10090.1396484375
tensor(10090.1709, grad_fn=<NegBackward0>) tensor(10090.1396, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10090.1240234375
tensor(10090.1396, grad_fn=<NegBackward0>) tensor(10090.1240, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10090.1142578125
tensor(10090.1240, grad_fn=<NegBackward0>) tensor(10090.1143, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10090.10546875
tensor(10090.1143, grad_fn=<NegBackward0>) tensor(10090.1055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10090.099609375
tensor(10090.1055, grad_fn=<NegBackward0>) tensor(10090.0996, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10090.095703125
tensor(10090.0996, grad_fn=<NegBackward0>) tensor(10090.0957, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10090.0927734375
tensor(10090.0957, grad_fn=<NegBackward0>) tensor(10090.0928, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10090.0888671875
tensor(10090.0928, grad_fn=<NegBackward0>) tensor(10090.0889, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10090.0869140625
tensor(10090.0889, grad_fn=<NegBackward0>) tensor(10090.0869, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10090.083984375
tensor(10090.0869, grad_fn=<NegBackward0>) tensor(10090.0840, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10090.0810546875
tensor(10090.0840, grad_fn=<NegBackward0>) tensor(10090.0811, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10090.080078125
tensor(10090.0811, grad_fn=<NegBackward0>) tensor(10090.0801, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10090.0791015625
tensor(10090.0801, grad_fn=<NegBackward0>) tensor(10090.0791, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10090.078125
tensor(10090.0791, grad_fn=<NegBackward0>) tensor(10090.0781, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10090.076171875
tensor(10090.0781, grad_fn=<NegBackward0>) tensor(10090.0762, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10090.076171875
tensor(10090.0762, grad_fn=<NegBackward0>) tensor(10090.0762, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10090.0771484375
tensor(10090.0762, grad_fn=<NegBackward0>) tensor(10090.0771, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10090.0751953125
tensor(10090.0762, grad_fn=<NegBackward0>) tensor(10090.0752, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10090.07421875
tensor(10090.0752, grad_fn=<NegBackward0>) tensor(10090.0742, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10090.0732421875
tensor(10090.0742, grad_fn=<NegBackward0>) tensor(10090.0732, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10090.0732421875
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0732, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10090.0732421875
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0732, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10090.0732421875
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0732, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10090.07421875
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0742, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10090.0703125
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10090.0712890625
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10090.0712890625
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10090.0712890625
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10090.0703125
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10090.0712890625
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10090.0703125
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10090.0703125
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10090.07421875
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0742, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10090.0693359375
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10090.0712890625
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10090.072265625
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0723, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10090.083984375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0840, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10090.068359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10090.0703125
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10090.068359375
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10090.0712890625
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0713, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -10090.0673828125
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0674, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10090.0693359375
tensor(10090.0674, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -10090.068359375
tensor(10090.0674, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -10090.0703125
tensor(10090.0674, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -10090.0693359375
tensor(10090.0674, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -10090.0693359375
tensor(10090.0674, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.9016, 0.0984],
        [0.3238, 0.6762]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([7.5634e-05, 9.9992e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1593, 0.1376],
         [0.5840, 0.1299]],

        [[0.5461, 0.1427],
         [0.6955, 0.5982]],

        [[0.6043, 0.1426],
         [0.6593, 0.6770]],

        [[0.6202, 0.1481],
         [0.5996, 0.5695]],

        [[0.7012, 0.1354],
         [0.6674, 0.7235]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01851330918808678
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.04805143666507984
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.036947811396319974
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.05006128636862138
Global Adjusted Rand Index: 0.029038544305198928
Average Adjusted Rand Index: 0.0307147687236216
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21291.03125
inf tensor(21291.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10095.6904296875
tensor(21291.0312, grad_fn=<NegBackward0>) tensor(10095.6904, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10094.4208984375
tensor(10095.6904, grad_fn=<NegBackward0>) tensor(10094.4209, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10093.353515625
tensor(10094.4209, grad_fn=<NegBackward0>) tensor(10093.3535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10092.6220703125
tensor(10093.3535, grad_fn=<NegBackward0>) tensor(10092.6221, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10092.228515625
tensor(10092.6221, grad_fn=<NegBackward0>) tensor(10092.2285, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10091.9814453125
tensor(10092.2285, grad_fn=<NegBackward0>) tensor(10091.9814, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10091.802734375
tensor(10091.9814, grad_fn=<NegBackward0>) tensor(10091.8027, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10091.6826171875
tensor(10091.8027, grad_fn=<NegBackward0>) tensor(10091.6826, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10091.59765625
tensor(10091.6826, grad_fn=<NegBackward0>) tensor(10091.5977, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10091.52734375
tensor(10091.5977, grad_fn=<NegBackward0>) tensor(10091.5273, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10091.466796875
tensor(10091.5273, grad_fn=<NegBackward0>) tensor(10091.4668, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10091.412109375
tensor(10091.4668, grad_fn=<NegBackward0>) tensor(10091.4121, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10091.3623046875
tensor(10091.4121, grad_fn=<NegBackward0>) tensor(10091.3623, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10091.3115234375
tensor(10091.3623, grad_fn=<NegBackward0>) tensor(10091.3115, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10091.2568359375
tensor(10091.3115, grad_fn=<NegBackward0>) tensor(10091.2568, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10091.205078125
tensor(10091.2568, grad_fn=<NegBackward0>) tensor(10091.2051, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10091.14453125
tensor(10091.2051, grad_fn=<NegBackward0>) tensor(10091.1445, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10091.0751953125
tensor(10091.1445, grad_fn=<NegBackward0>) tensor(10091.0752, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10091.0068359375
tensor(10091.0752, grad_fn=<NegBackward0>) tensor(10091.0068, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10090.9384765625
tensor(10091.0068, grad_fn=<NegBackward0>) tensor(10090.9385, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10090.8720703125
tensor(10090.9385, grad_fn=<NegBackward0>) tensor(10090.8721, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10090.80078125
tensor(10090.8721, grad_fn=<NegBackward0>) tensor(10090.8008, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10090.7265625
tensor(10090.8008, grad_fn=<NegBackward0>) tensor(10090.7266, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10090.6337890625
tensor(10090.7266, grad_fn=<NegBackward0>) tensor(10090.6338, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10090.5068359375
tensor(10090.6338, grad_fn=<NegBackward0>) tensor(10090.5068, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10090.353515625
tensor(10090.5068, grad_fn=<NegBackward0>) tensor(10090.3535, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10090.216796875
tensor(10090.3535, grad_fn=<NegBackward0>) tensor(10090.2168, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10090.1396484375
tensor(10090.2168, grad_fn=<NegBackward0>) tensor(10090.1396, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10090.107421875
tensor(10090.1396, grad_fn=<NegBackward0>) tensor(10090.1074, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10090.091796875
tensor(10090.1074, grad_fn=<NegBackward0>) tensor(10090.0918, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10090.0830078125
tensor(10090.0918, grad_fn=<NegBackward0>) tensor(10090.0830, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10090.0791015625
tensor(10090.0830, grad_fn=<NegBackward0>) tensor(10090.0791, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10090.078125
tensor(10090.0791, grad_fn=<NegBackward0>) tensor(10090.0781, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10090.0751953125
tensor(10090.0781, grad_fn=<NegBackward0>) tensor(10090.0752, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10090.0732421875
tensor(10090.0752, grad_fn=<NegBackward0>) tensor(10090.0732, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10090.072265625
tensor(10090.0732, grad_fn=<NegBackward0>) tensor(10090.0723, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10090.072265625
tensor(10090.0723, grad_fn=<NegBackward0>) tensor(10090.0723, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10090.0703125
tensor(10090.0723, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10090.0703125
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10090.0703125
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0703, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10090.0693359375
tensor(10090.0703, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10090.0693359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0693, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10090.068359375
tensor(10090.0693, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10090.068359375
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10090.068359375
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10090.068359375
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0684, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10090.06640625
tensor(10090.0684, grad_fn=<NegBackward0>) tensor(10090.0664, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10090.0673828125
tensor(10090.0664, grad_fn=<NegBackward0>) tensor(10090.0674, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -10090.06640625
tensor(10090.0664, grad_fn=<NegBackward0>) tensor(10090.0664, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10090.0634765625
tensor(10090.0664, grad_fn=<NegBackward0>) tensor(10090.0635, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10090.0615234375
tensor(10090.0635, grad_fn=<NegBackward0>) tensor(10090.0615, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10090.044921875
tensor(10090.0615, grad_fn=<NegBackward0>) tensor(10090.0449, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10089.9189453125
tensor(10090.0449, grad_fn=<NegBackward0>) tensor(10089.9189, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10089.9091796875
tensor(10089.9189, grad_fn=<NegBackward0>) tensor(10089.9092, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10089.90625
tensor(10089.9092, grad_fn=<NegBackward0>) tensor(10089.9062, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10089.9013671875
tensor(10089.9062, grad_fn=<NegBackward0>) tensor(10089.9014, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10089.9052734375
tensor(10089.9014, grad_fn=<NegBackward0>) tensor(10089.9053, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -10089.8974609375
tensor(10089.9014, grad_fn=<NegBackward0>) tensor(10089.8975, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10089.9091796875
tensor(10089.8975, grad_fn=<NegBackward0>) tensor(10089.9092, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10089.8955078125
tensor(10089.8975, grad_fn=<NegBackward0>) tensor(10089.8955, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10089.896484375
tensor(10089.8955, grad_fn=<NegBackward0>) tensor(10089.8965, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10089.89453125
tensor(10089.8955, grad_fn=<NegBackward0>) tensor(10089.8945, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10089.892578125
tensor(10089.8945, grad_fn=<NegBackward0>) tensor(10089.8926, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10089.892578125
tensor(10089.8926, grad_fn=<NegBackward0>) tensor(10089.8926, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10089.8916015625
tensor(10089.8926, grad_fn=<NegBackward0>) tensor(10089.8916, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10089.8916015625
tensor(10089.8916, grad_fn=<NegBackward0>) tensor(10089.8916, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10089.890625
tensor(10089.8916, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10089.890625
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10089.890625
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10089.890625
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10089.890625
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10089.8916015625
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8916, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10089.8896484375
tensor(10089.8906, grad_fn=<NegBackward0>) tensor(10089.8896, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10089.892578125
tensor(10089.8896, grad_fn=<NegBackward0>) tensor(10089.8926, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10089.8896484375
tensor(10089.8896, grad_fn=<NegBackward0>) tensor(10089.8896, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10089.8974609375
tensor(10089.8896, grad_fn=<NegBackward0>) tensor(10089.8975, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10089.888671875
tensor(10089.8896, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10090.0107421875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10090.0107, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10089.8896484375
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8896, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10089.8935546875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8936, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -10089.8935546875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8936, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -10089.9150390625
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.9150, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10089.890625
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -10089.89453125
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8945, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -10089.9482421875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.9482, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -10089.890625
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8906, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10089.888671875
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.8887, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10089.9384765625
tensor(10089.8887, grad_fn=<NegBackward0>) tensor(10089.9385, grad_fn=<NegBackward0>)
1
pi: tensor([[0.9990, 0.0010],
        [0.2850, 0.7150]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0171, 0.9829], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1612, 0.1970],
         [0.5900, 0.1274]],

        [[0.6914, 0.1455],
         [0.6164, 0.5413]],

        [[0.5894, 0.1437],
         [0.7273, 0.6991]],

        [[0.7200, 0.1460],
         [0.6775, 0.5027]],

        [[0.6627, 0.1292],
         [0.5334, 0.6174]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.012100586122140291
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.03873515192507652
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.020755256732954187
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.03232049545190633
Global Adjusted Rand Index: 0.022383263206745423
Average Adjusted Rand Index: 0.02094562609560603
[0.029038544305198928, 0.022383263206745423] [0.0307147687236216, 0.02094562609560603] [10090.0693359375, 10089.95703125]
-------------------------------------
This iteration is 83
True Objective function: Loss = -9858.87847400153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23345.21484375
inf tensor(23345.2148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9733.6025390625
tensor(23345.2148, grad_fn=<NegBackward0>) tensor(9733.6025, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9732.125
tensor(9733.6025, grad_fn=<NegBackward0>) tensor(9732.1250, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9731.8173828125
tensor(9732.1250, grad_fn=<NegBackward0>) tensor(9731.8174, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9731.6728515625
tensor(9731.8174, grad_fn=<NegBackward0>) tensor(9731.6729, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9731.57421875
tensor(9731.6729, grad_fn=<NegBackward0>) tensor(9731.5742, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9731.49609375
tensor(9731.5742, grad_fn=<NegBackward0>) tensor(9731.4961, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9731.443359375
tensor(9731.4961, grad_fn=<NegBackward0>) tensor(9731.4434, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9731.4052734375
tensor(9731.4434, grad_fn=<NegBackward0>) tensor(9731.4053, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9731.37109375
tensor(9731.4053, grad_fn=<NegBackward0>) tensor(9731.3711, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9731.3408203125
tensor(9731.3711, grad_fn=<NegBackward0>) tensor(9731.3408, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9731.3125
tensor(9731.3408, grad_fn=<NegBackward0>) tensor(9731.3125, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9731.2880859375
tensor(9731.3125, grad_fn=<NegBackward0>) tensor(9731.2881, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9731.2666015625
tensor(9731.2881, grad_fn=<NegBackward0>) tensor(9731.2666, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9731.248046875
tensor(9731.2666, grad_fn=<NegBackward0>) tensor(9731.2480, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9731.232421875
tensor(9731.2480, grad_fn=<NegBackward0>) tensor(9731.2324, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9731.2197265625
tensor(9731.2324, grad_fn=<NegBackward0>) tensor(9731.2197, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9731.2060546875
tensor(9731.2197, grad_fn=<NegBackward0>) tensor(9731.2061, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9731.193359375
tensor(9731.2061, grad_fn=<NegBackward0>) tensor(9731.1934, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9731.18359375
tensor(9731.1934, grad_fn=<NegBackward0>) tensor(9731.1836, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9731.1728515625
tensor(9731.1836, grad_fn=<NegBackward0>) tensor(9731.1729, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9731.1591796875
tensor(9731.1729, grad_fn=<NegBackward0>) tensor(9731.1592, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9731.150390625
tensor(9731.1592, grad_fn=<NegBackward0>) tensor(9731.1504, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9731.1376953125
tensor(9731.1504, grad_fn=<NegBackward0>) tensor(9731.1377, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9731.12890625
tensor(9731.1377, grad_fn=<NegBackward0>) tensor(9731.1289, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9731.1171875
tensor(9731.1289, grad_fn=<NegBackward0>) tensor(9731.1172, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9731.107421875
tensor(9731.1172, grad_fn=<NegBackward0>) tensor(9731.1074, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9731.095703125
tensor(9731.1074, grad_fn=<NegBackward0>) tensor(9731.0957, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9731.083984375
tensor(9731.0957, grad_fn=<NegBackward0>) tensor(9731.0840, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9731.072265625
tensor(9731.0840, grad_fn=<NegBackward0>) tensor(9731.0723, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9731.0615234375
tensor(9731.0723, grad_fn=<NegBackward0>) tensor(9731.0615, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9731.048828125
tensor(9731.0615, grad_fn=<NegBackward0>) tensor(9731.0488, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9731.037109375
tensor(9731.0488, grad_fn=<NegBackward0>) tensor(9731.0371, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9731.0234375
tensor(9731.0371, grad_fn=<NegBackward0>) tensor(9731.0234, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9731.005859375
tensor(9731.0234, grad_fn=<NegBackward0>) tensor(9731.0059, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9730.9892578125
tensor(9731.0059, grad_fn=<NegBackward0>) tensor(9730.9893, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9730.9677734375
tensor(9730.9893, grad_fn=<NegBackward0>) tensor(9730.9678, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9730.9462890625
tensor(9730.9678, grad_fn=<NegBackward0>) tensor(9730.9463, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9730.923828125
tensor(9730.9463, grad_fn=<NegBackward0>) tensor(9730.9238, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9730.90234375
tensor(9730.9238, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9730.8837890625
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.8838, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9730.8681640625
tensor(9730.8838, grad_fn=<NegBackward0>) tensor(9730.8682, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9730.8544921875
tensor(9730.8682, grad_fn=<NegBackward0>) tensor(9730.8545, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9730.841796875
tensor(9730.8545, grad_fn=<NegBackward0>) tensor(9730.8418, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9730.8291015625
tensor(9730.8418, grad_fn=<NegBackward0>) tensor(9730.8291, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9730.814453125
tensor(9730.8291, grad_fn=<NegBackward0>) tensor(9730.8145, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9730.796875
tensor(9730.8145, grad_fn=<NegBackward0>) tensor(9730.7969, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9730.775390625
tensor(9730.7969, grad_fn=<NegBackward0>) tensor(9730.7754, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9730.7431640625
tensor(9730.7754, grad_fn=<NegBackward0>) tensor(9730.7432, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9730.6982421875
tensor(9730.7432, grad_fn=<NegBackward0>) tensor(9730.6982, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9730.65234375
tensor(9730.6982, grad_fn=<NegBackward0>) tensor(9730.6523, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9730.6357421875
tensor(9730.6523, grad_fn=<NegBackward0>) tensor(9730.6357, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9730.6318359375
tensor(9730.6357, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9730.6318359375
tensor(9730.6318, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9730.6318359375
tensor(9730.6318, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9730.6318359375
tensor(9730.6318, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9730.6318359375
tensor(9730.6318, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9730.630859375
tensor(9730.6318, grad_fn=<NegBackward0>) tensor(9730.6309, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9730.6318359375
tensor(9730.6309, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9730.6318359375
tensor(9730.6309, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9730.6328125
tensor(9730.6309, grad_fn=<NegBackward0>) tensor(9730.6328, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9730.6318359375
tensor(9730.6309, grad_fn=<NegBackward0>) tensor(9730.6318, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -9730.6328125
tensor(9730.6309, grad_fn=<NegBackward0>) tensor(9730.6328, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.2524, 0.7476],
        [0.9792, 0.0208]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9729, 0.0271], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1304, 0.1786],
         [0.5294, 0.1377]],

        [[0.5415, 0.1300],
         [0.5007, 0.5739]],

        [[0.7009, 0.1271],
         [0.6254, 0.5201]],

        [[0.6981, 0.1382],
         [0.5054, 0.5339]],

        [[0.6955, 0.1384],
         [0.6652, 0.7045]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017728041133385733
Average Adjusted Rand Index: 0.00034275671414060895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24137.078125
inf tensor(24137.0781, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9733.4482421875
tensor(24137.0781, grad_fn=<NegBackward0>) tensor(9733.4482, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9732.28125
tensor(9733.4482, grad_fn=<NegBackward0>) tensor(9732.2812, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9731.98828125
tensor(9732.2812, grad_fn=<NegBackward0>) tensor(9731.9883, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9731.8447265625
tensor(9731.9883, grad_fn=<NegBackward0>) tensor(9731.8447, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9731.759765625
tensor(9731.8447, grad_fn=<NegBackward0>) tensor(9731.7598, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9731.705078125
tensor(9731.7598, grad_fn=<NegBackward0>) tensor(9731.7051, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9731.66796875
tensor(9731.7051, grad_fn=<NegBackward0>) tensor(9731.6680, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9731.6435546875
tensor(9731.6680, grad_fn=<NegBackward0>) tensor(9731.6436, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9731.6201171875
tensor(9731.6436, grad_fn=<NegBackward0>) tensor(9731.6201, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9731.6015625
tensor(9731.6201, grad_fn=<NegBackward0>) tensor(9731.6016, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9731.5859375
tensor(9731.6016, grad_fn=<NegBackward0>) tensor(9731.5859, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9731.5673828125
tensor(9731.5859, grad_fn=<NegBackward0>) tensor(9731.5674, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9731.548828125
tensor(9731.5674, grad_fn=<NegBackward0>) tensor(9731.5488, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9731.529296875
tensor(9731.5488, grad_fn=<NegBackward0>) tensor(9731.5293, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9731.5068359375
tensor(9731.5293, grad_fn=<NegBackward0>) tensor(9731.5068, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9731.4814453125
tensor(9731.5068, grad_fn=<NegBackward0>) tensor(9731.4814, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9731.4501953125
tensor(9731.4814, grad_fn=<NegBackward0>) tensor(9731.4502, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9731.416015625
tensor(9731.4502, grad_fn=<NegBackward0>) tensor(9731.4160, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9731.3896484375
tensor(9731.4160, grad_fn=<NegBackward0>) tensor(9731.3896, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9731.3701171875
tensor(9731.3896, grad_fn=<NegBackward0>) tensor(9731.3701, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9731.359375
tensor(9731.3701, grad_fn=<NegBackward0>) tensor(9731.3594, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9731.3486328125
tensor(9731.3594, grad_fn=<NegBackward0>) tensor(9731.3486, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9731.3388671875
tensor(9731.3486, grad_fn=<NegBackward0>) tensor(9731.3389, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9731.3310546875
tensor(9731.3389, grad_fn=<NegBackward0>) tensor(9731.3311, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9731.3203125
tensor(9731.3311, grad_fn=<NegBackward0>) tensor(9731.3203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9731.30078125
tensor(9731.3203, grad_fn=<NegBackward0>) tensor(9731.3008, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9731.265625
tensor(9731.3008, grad_fn=<NegBackward0>) tensor(9731.2656, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9731.2109375
tensor(9731.2656, grad_fn=<NegBackward0>) tensor(9731.2109, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9731.1357421875
tensor(9731.2109, grad_fn=<NegBackward0>) tensor(9731.1357, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9731.05859375
tensor(9731.1357, grad_fn=<NegBackward0>) tensor(9731.0586, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9731.0048828125
tensor(9731.0586, grad_fn=<NegBackward0>) tensor(9731.0049, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9730.9716796875
tensor(9731.0049, grad_fn=<NegBackward0>) tensor(9730.9717, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9730.9521484375
tensor(9730.9717, grad_fn=<NegBackward0>) tensor(9730.9521, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9730.9384765625
tensor(9730.9521, grad_fn=<NegBackward0>) tensor(9730.9385, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9730.9296875
tensor(9730.9385, grad_fn=<NegBackward0>) tensor(9730.9297, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9730.923828125
tensor(9730.9297, grad_fn=<NegBackward0>) tensor(9730.9238, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9730.91796875
tensor(9730.9238, grad_fn=<NegBackward0>) tensor(9730.9180, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9730.916015625
tensor(9730.9180, grad_fn=<NegBackward0>) tensor(9730.9160, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9730.9130859375
tensor(9730.9160, grad_fn=<NegBackward0>) tensor(9730.9131, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9730.912109375
tensor(9730.9131, grad_fn=<NegBackward0>) tensor(9730.9121, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9730.91015625
tensor(9730.9121, grad_fn=<NegBackward0>) tensor(9730.9102, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9730.9091796875
tensor(9730.9102, grad_fn=<NegBackward0>) tensor(9730.9092, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9730.9091796875
tensor(9730.9092, grad_fn=<NegBackward0>) tensor(9730.9092, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9730.908203125
tensor(9730.9092, grad_fn=<NegBackward0>) tensor(9730.9082, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9730.908203125
tensor(9730.9082, grad_fn=<NegBackward0>) tensor(9730.9082, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9730.908203125
tensor(9730.9082, grad_fn=<NegBackward0>) tensor(9730.9082, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9730.90625
tensor(9730.9082, grad_fn=<NegBackward0>) tensor(9730.9062, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9730.90625
tensor(9730.9062, grad_fn=<NegBackward0>) tensor(9730.9062, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9730.9052734375
tensor(9730.9062, grad_fn=<NegBackward0>) tensor(9730.9053, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9730.9052734375
tensor(9730.9053, grad_fn=<NegBackward0>) tensor(9730.9053, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9730.904296875
tensor(9730.9053, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9730.904296875
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9730.904296875
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9730.904296875
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9730.9052734375
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9053, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9730.904296875
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9730.9033203125
tensor(9730.9043, grad_fn=<NegBackward0>) tensor(9730.9033, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9730.904296875
tensor(9730.9033, grad_fn=<NegBackward0>) tensor(9730.9043, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9730.90234375
tensor(9730.9033, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9730.9033203125
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9033, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9730.90234375
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9730.90234375
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9730.90234375
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9730.90234375
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9730.9013671875
tensor(9730.9023, grad_fn=<NegBackward0>) tensor(9730.9014, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9730.90234375
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9730.9013671875
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9014, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9730.90234375
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -9730.90234375
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -9730.9033203125
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9033, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -9730.900390625
tensor(9730.9014, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9730.90234375
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9730.9013671875
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9014, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9730.9013671875
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9014, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9730.9013671875
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9014, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9730.900390625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9730.90234375
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9023, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9730.982421875
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.9824, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9730.8994140625
tensor(9730.9004, grad_fn=<NegBackward0>) tensor(9730.8994, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9730.9599609375
tensor(9730.8994, grad_fn=<NegBackward0>) tensor(9730.9600, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -9730.900390625
tensor(9730.8994, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -9730.8984375
tensor(9730.8994, grad_fn=<NegBackward0>) tensor(9730.8984, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9730.900390625
tensor(9730.8984, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9730.900390625
tensor(9730.8984, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -9731.0322265625
tensor(9730.8984, grad_fn=<NegBackward0>) tensor(9731.0322, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -9730.8994140625
tensor(9730.8984, grad_fn=<NegBackward0>) tensor(9730.8994, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -9730.900390625
tensor(9730.8984, grad_fn=<NegBackward0>) tensor(9730.9004, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[9.9995e-01, 5.2865e-05],
        [8.4638e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0193, 0.9807], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1701, 0.2037],
         [0.7276, 0.1327]],

        [[0.7110, 0.1683],
         [0.5803, 0.6869]],

        [[0.5704, 0.1188],
         [0.6346, 0.7233]],

        [[0.5455, 0.1166],
         [0.5053, 0.6517]],

        [[0.5654, 0.1723],
         [0.7169, 0.6927]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0004446725451460072
Average Adjusted Rand Index: 0.0006434748902678098
[-0.0017728041133385733, -0.0004446725451460072] [0.00034275671414060895, 0.0006434748902678098] [9730.6328125, 9730.900390625]
-------------------------------------
This iteration is 84
True Objective function: Loss = -9745.812231942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22827.94921875
inf tensor(22827.9492, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9594.0439453125
tensor(22827.9492, grad_fn=<NegBackward0>) tensor(9594.0439, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9592.6357421875
tensor(9594.0439, grad_fn=<NegBackward0>) tensor(9592.6357, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9592.255859375
tensor(9592.6357, grad_fn=<NegBackward0>) tensor(9592.2559, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9592.0400390625
tensor(9592.2559, grad_fn=<NegBackward0>) tensor(9592.0400, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9591.83984375
tensor(9592.0400, grad_fn=<NegBackward0>) tensor(9591.8398, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9591.5771484375
tensor(9591.8398, grad_fn=<NegBackward0>) tensor(9591.5771, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9591.255859375
tensor(9591.5771, grad_fn=<NegBackward0>) tensor(9591.2559, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9591.021484375
tensor(9591.2559, grad_fn=<NegBackward0>) tensor(9591.0215, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9590.8701171875
tensor(9591.0215, grad_fn=<NegBackward0>) tensor(9590.8701, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9590.7705078125
tensor(9590.8701, grad_fn=<NegBackward0>) tensor(9590.7705, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9590.701171875
tensor(9590.7705, grad_fn=<NegBackward0>) tensor(9590.7012, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9590.64453125
tensor(9590.7012, grad_fn=<NegBackward0>) tensor(9590.6445, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9590.5986328125
tensor(9590.6445, grad_fn=<NegBackward0>) tensor(9590.5986, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9590.5615234375
tensor(9590.5986, grad_fn=<NegBackward0>) tensor(9590.5615, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9590.5263671875
tensor(9590.5615, grad_fn=<NegBackward0>) tensor(9590.5264, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9590.4970703125
tensor(9590.5264, grad_fn=<NegBackward0>) tensor(9590.4971, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9590.46875
tensor(9590.4971, grad_fn=<NegBackward0>) tensor(9590.4688, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9590.443359375
tensor(9590.4688, grad_fn=<NegBackward0>) tensor(9590.4434, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9590.4189453125
tensor(9590.4434, grad_fn=<NegBackward0>) tensor(9590.4189, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9590.39453125
tensor(9590.4189, grad_fn=<NegBackward0>) tensor(9590.3945, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9590.373046875
tensor(9590.3945, grad_fn=<NegBackward0>) tensor(9590.3730, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9590.353515625
tensor(9590.3730, grad_fn=<NegBackward0>) tensor(9590.3535, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9590.3349609375
tensor(9590.3535, grad_fn=<NegBackward0>) tensor(9590.3350, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9590.3173828125
tensor(9590.3350, grad_fn=<NegBackward0>) tensor(9590.3174, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9590.302734375
tensor(9590.3174, grad_fn=<NegBackward0>) tensor(9590.3027, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9590.2900390625
tensor(9590.3027, grad_fn=<NegBackward0>) tensor(9590.2900, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9590.2783203125
tensor(9590.2900, grad_fn=<NegBackward0>) tensor(9590.2783, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9590.265625
tensor(9590.2783, grad_fn=<NegBackward0>) tensor(9590.2656, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9590.2568359375
tensor(9590.2656, grad_fn=<NegBackward0>) tensor(9590.2568, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9590.2451171875
tensor(9590.2568, grad_fn=<NegBackward0>) tensor(9590.2451, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9590.234375
tensor(9590.2451, grad_fn=<NegBackward0>) tensor(9590.2344, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9590.2216796875
tensor(9590.2344, grad_fn=<NegBackward0>) tensor(9590.2217, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9590.2080078125
tensor(9590.2217, grad_fn=<NegBackward0>) tensor(9590.2080, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9590.1884765625
tensor(9590.2080, grad_fn=<NegBackward0>) tensor(9590.1885, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9590.16796875
tensor(9590.1885, grad_fn=<NegBackward0>) tensor(9590.1680, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9590.1416015625
tensor(9590.1680, grad_fn=<NegBackward0>) tensor(9590.1416, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9590.109375
tensor(9590.1416, grad_fn=<NegBackward0>) tensor(9590.1094, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9590.06640625
tensor(9590.1094, grad_fn=<NegBackward0>) tensor(9590.0664, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9590.00390625
tensor(9590.0664, grad_fn=<NegBackward0>) tensor(9590.0039, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9589.89453125
tensor(9590.0039, grad_fn=<NegBackward0>) tensor(9589.8945, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9589.5537109375
tensor(9589.8945, grad_fn=<NegBackward0>) tensor(9589.5537, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9586.9990234375
tensor(9589.5537, grad_fn=<NegBackward0>) tensor(9586.9990, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9583.1318359375
tensor(9586.9990, grad_fn=<NegBackward0>) tensor(9583.1318, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9582.62109375
tensor(9583.1318, grad_fn=<NegBackward0>) tensor(9582.6211, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9582.484375
tensor(9582.6211, grad_fn=<NegBackward0>) tensor(9582.4844, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9582.4296875
tensor(9582.4844, grad_fn=<NegBackward0>) tensor(9582.4297, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9582.3994140625
tensor(9582.4297, grad_fn=<NegBackward0>) tensor(9582.3994, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9582.3818359375
tensor(9582.3994, grad_fn=<NegBackward0>) tensor(9582.3818, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9582.3681640625
tensor(9582.3818, grad_fn=<NegBackward0>) tensor(9582.3682, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9582.3603515625
tensor(9582.3682, grad_fn=<NegBackward0>) tensor(9582.3604, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9582.3515625
tensor(9582.3604, grad_fn=<NegBackward0>) tensor(9582.3516, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9582.34765625
tensor(9582.3516, grad_fn=<NegBackward0>) tensor(9582.3477, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9582.3427734375
tensor(9582.3477, grad_fn=<NegBackward0>) tensor(9582.3428, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9582.3388671875
tensor(9582.3428, grad_fn=<NegBackward0>) tensor(9582.3389, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9582.3359375
tensor(9582.3389, grad_fn=<NegBackward0>) tensor(9582.3359, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9582.333984375
tensor(9582.3359, grad_fn=<NegBackward0>) tensor(9582.3340, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9582.3310546875
tensor(9582.3340, grad_fn=<NegBackward0>) tensor(9582.3311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9582.328125
tensor(9582.3311, grad_fn=<NegBackward0>) tensor(9582.3281, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9582.328125
tensor(9582.3281, grad_fn=<NegBackward0>) tensor(9582.3281, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9582.326171875
tensor(9582.3281, grad_fn=<NegBackward0>) tensor(9582.3262, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9582.328125
tensor(9582.3262, grad_fn=<NegBackward0>) tensor(9582.3281, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9582.3232421875
tensor(9582.3262, grad_fn=<NegBackward0>) tensor(9582.3232, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9582.3212890625
tensor(9582.3232, grad_fn=<NegBackward0>) tensor(9582.3213, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9582.3203125
tensor(9582.3213, grad_fn=<NegBackward0>) tensor(9582.3203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9582.3193359375
tensor(9582.3203, grad_fn=<NegBackward0>) tensor(9582.3193, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9582.3193359375
tensor(9582.3193, grad_fn=<NegBackward0>) tensor(9582.3193, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9582.3173828125
tensor(9582.3193, grad_fn=<NegBackward0>) tensor(9582.3174, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9582.31640625
tensor(9582.3174, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9582.31640625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9582.31640625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9582.31640625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9582.3212890625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3213, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9582.37890625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3789, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9582.314453125
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3145, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9582.3154296875
tensor(9582.3145, grad_fn=<NegBackward0>) tensor(9582.3154, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9582.3134765625
tensor(9582.3145, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9582.3134765625
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9582.3134765625
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9582.455078125
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.4551, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9582.3125
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3125, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9582.3115234375
tensor(9582.3125, grad_fn=<NegBackward0>) tensor(9582.3115, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9582.3115234375
tensor(9582.3115, grad_fn=<NegBackward0>) tensor(9582.3115, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9582.3115234375
tensor(9582.3115, grad_fn=<NegBackward0>) tensor(9582.3115, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9582.310546875
tensor(9582.3115, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9582.3095703125
tensor(9582.3105, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9582.47265625
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.4727, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9582.310546875
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9582.3095703125
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9582.3095703125
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9582.3076171875
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9582.3095703125
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9582.3095703125
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -9582.30859375
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -9582.3076171875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9582.310546875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9582.306640625
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3066, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9582.556640625
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.5566, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9582.30859375
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9582.3076171875
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9976e-01, 2.3733e-04],
        [1.9709e-03, 9.9803e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7452, 0.2548], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1482, 0.1230],
         [0.6463, 0.1134]],

        [[0.6573, 0.1042],
         [0.6347, 0.6564]],

        [[0.5661, 0.1018],
         [0.7011, 0.5857]],

        [[0.6009, 0.1023],
         [0.6775, 0.6953]],

        [[0.5535, 0.1092],
         [0.6333, 0.6047]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.014305020330360988
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.004811011391094236
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.02012700161587535
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.008307325406289123
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 35
Adjusted Rand Index: 0.08236841963735249
Global Adjusted Rand Index: 0.011538396657543349
Average Adjusted Rand Index: 0.015014412825096699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26014.529296875
inf tensor(26014.5293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9594.423828125
tensor(26014.5293, grad_fn=<NegBackward0>) tensor(9594.4238, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9592.1533203125
tensor(9594.4238, grad_fn=<NegBackward0>) tensor(9592.1533, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9591.6328125
tensor(9592.1533, grad_fn=<NegBackward0>) tensor(9591.6328, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9591.3603515625
tensor(9591.6328, grad_fn=<NegBackward0>) tensor(9591.3604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9591.1982421875
tensor(9591.3604, grad_fn=<NegBackward0>) tensor(9591.1982, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9591.0859375
tensor(9591.1982, grad_fn=<NegBackward0>) tensor(9591.0859, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9591.0009765625
tensor(9591.0859, grad_fn=<NegBackward0>) tensor(9591.0010, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9590.931640625
tensor(9591.0010, grad_fn=<NegBackward0>) tensor(9590.9316, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9590.8779296875
tensor(9590.9316, grad_fn=<NegBackward0>) tensor(9590.8779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9590.83203125
tensor(9590.8779, grad_fn=<NegBackward0>) tensor(9590.8320, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9590.787109375
tensor(9590.8320, grad_fn=<NegBackward0>) tensor(9590.7871, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9590.7451171875
tensor(9590.7871, grad_fn=<NegBackward0>) tensor(9590.7451, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9590.7041015625
tensor(9590.7451, grad_fn=<NegBackward0>) tensor(9590.7041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9590.6650390625
tensor(9590.7041, grad_fn=<NegBackward0>) tensor(9590.6650, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9590.6279296875
tensor(9590.6650, grad_fn=<NegBackward0>) tensor(9590.6279, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9590.59375
tensor(9590.6279, grad_fn=<NegBackward0>) tensor(9590.5938, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9590.5595703125
tensor(9590.5938, grad_fn=<NegBackward0>) tensor(9590.5596, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9590.5263671875
tensor(9590.5596, grad_fn=<NegBackward0>) tensor(9590.5264, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9590.4951171875
tensor(9590.5264, grad_fn=<NegBackward0>) tensor(9590.4951, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9590.462890625
tensor(9590.4951, grad_fn=<NegBackward0>) tensor(9590.4629, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9590.43359375
tensor(9590.4629, grad_fn=<NegBackward0>) tensor(9590.4336, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9590.404296875
tensor(9590.4336, grad_fn=<NegBackward0>) tensor(9590.4043, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9590.376953125
tensor(9590.4043, grad_fn=<NegBackward0>) tensor(9590.3770, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9590.3515625
tensor(9590.3770, grad_fn=<NegBackward0>) tensor(9590.3516, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9590.3291015625
tensor(9590.3516, grad_fn=<NegBackward0>) tensor(9590.3291, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9590.3095703125
tensor(9590.3291, grad_fn=<NegBackward0>) tensor(9590.3096, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9590.29296875
tensor(9590.3096, grad_fn=<NegBackward0>) tensor(9590.2930, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9590.2783203125
tensor(9590.2930, grad_fn=<NegBackward0>) tensor(9590.2783, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9590.2666015625
tensor(9590.2783, grad_fn=<NegBackward0>) tensor(9590.2666, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9590.2548828125
tensor(9590.2666, grad_fn=<NegBackward0>) tensor(9590.2549, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9590.2431640625
tensor(9590.2549, grad_fn=<NegBackward0>) tensor(9590.2432, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9590.228515625
tensor(9590.2432, grad_fn=<NegBackward0>) tensor(9590.2285, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9590.2138671875
tensor(9590.2285, grad_fn=<NegBackward0>) tensor(9590.2139, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9590.193359375
tensor(9590.2139, grad_fn=<NegBackward0>) tensor(9590.1934, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9590.1689453125
tensor(9590.1934, grad_fn=<NegBackward0>) tensor(9590.1689, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9590.1376953125
tensor(9590.1689, grad_fn=<NegBackward0>) tensor(9590.1377, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9590.095703125
tensor(9590.1377, grad_fn=<NegBackward0>) tensor(9590.0957, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9590.029296875
tensor(9590.0957, grad_fn=<NegBackward0>) tensor(9590.0293, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9589.861328125
tensor(9590.0293, grad_fn=<NegBackward0>) tensor(9589.8613, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9587.5869140625
tensor(9589.8613, grad_fn=<NegBackward0>) tensor(9587.5869, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9582.8515625
tensor(9587.5869, grad_fn=<NegBackward0>) tensor(9582.8516, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9582.4814453125
tensor(9582.8516, grad_fn=<NegBackward0>) tensor(9582.4814, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9582.4072265625
tensor(9582.4814, grad_fn=<NegBackward0>) tensor(9582.4072, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9582.3779296875
tensor(9582.4072, grad_fn=<NegBackward0>) tensor(9582.3779, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9582.3623046875
tensor(9582.3779, grad_fn=<NegBackward0>) tensor(9582.3623, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9582.3505859375
tensor(9582.3623, grad_fn=<NegBackward0>) tensor(9582.3506, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9582.34375
tensor(9582.3506, grad_fn=<NegBackward0>) tensor(9582.3438, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9582.337890625
tensor(9582.3438, grad_fn=<NegBackward0>) tensor(9582.3379, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9582.3349609375
tensor(9582.3379, grad_fn=<NegBackward0>) tensor(9582.3350, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9582.33203125
tensor(9582.3350, grad_fn=<NegBackward0>) tensor(9582.3320, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9582.330078125
tensor(9582.3320, grad_fn=<NegBackward0>) tensor(9582.3301, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9582.3271484375
tensor(9582.3301, grad_fn=<NegBackward0>) tensor(9582.3271, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9582.3271484375
tensor(9582.3271, grad_fn=<NegBackward0>) tensor(9582.3271, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9582.322265625
tensor(9582.3271, grad_fn=<NegBackward0>) tensor(9582.3223, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9582.3212890625
tensor(9582.3223, grad_fn=<NegBackward0>) tensor(9582.3213, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9582.3212890625
tensor(9582.3213, grad_fn=<NegBackward0>) tensor(9582.3213, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9582.3193359375
tensor(9582.3213, grad_fn=<NegBackward0>) tensor(9582.3193, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9582.3193359375
tensor(9582.3193, grad_fn=<NegBackward0>) tensor(9582.3193, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9582.31640625
tensor(9582.3193, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9582.31640625
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3164, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9582.3154296875
tensor(9582.3164, grad_fn=<NegBackward0>) tensor(9582.3154, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9582.314453125
tensor(9582.3154, grad_fn=<NegBackward0>) tensor(9582.3145, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9582.314453125
tensor(9582.3145, grad_fn=<NegBackward0>) tensor(9582.3145, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9582.3134765625
tensor(9582.3145, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9582.3134765625
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9582.3134765625
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3135, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9582.3125
tensor(9582.3135, grad_fn=<NegBackward0>) tensor(9582.3125, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9582.3115234375
tensor(9582.3125, grad_fn=<NegBackward0>) tensor(9582.3115, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9582.3232421875
tensor(9582.3115, grad_fn=<NegBackward0>) tensor(9582.3232, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9582.310546875
tensor(9582.3115, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9582.3193359375
tensor(9582.3105, grad_fn=<NegBackward0>) tensor(9582.3193, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9582.388671875
tensor(9582.3105, grad_fn=<NegBackward0>) tensor(9582.3887, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9582.310546875
tensor(9582.3105, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9582.3095703125
tensor(9582.3105, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9582.44140625
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.4414, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -9582.3095703125
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9582.3154296875
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3154, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9582.3095703125
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9582.30859375
tensor(9582.3096, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9582.30859375
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9582.30859375
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9582.310546875
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9582.30859375
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9582.30859375
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9582.3076171875
tensor(9582.3086, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9582.3076171875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9582.30859375
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3086, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9582.3076171875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9582.3076171875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9582.3076171875
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9582.306640625
tensor(9582.3076, grad_fn=<NegBackward0>) tensor(9582.3066, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9582.3095703125
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3096, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9582.3076171875
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -9582.310546875
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3105, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -9582.3076171875
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.3076, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -9582.498046875
tensor(9582.3066, grad_fn=<NegBackward0>) tensor(9582.4980, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[9.9982e-01, 1.7639e-04],
        [1.4085e-03, 9.9859e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7505, 0.2495], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1463, 0.1228],
         [0.6464, 0.1134]],

        [[0.5642, 0.1041],
         [0.5814, 0.7161]],

        [[0.7195, 0.1016],
         [0.7193, 0.6873]],

        [[0.6808, 0.1023],
         [0.5736, 0.7106]],

        [[0.6945, 0.1090],
         [0.5708, 0.6588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.013568821208657498
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.0010786032088445463
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.02801811420787158
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.0019820868897340287
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 35
Adjusted Rand Index: 0.08236841963735249
Global Adjusted Rand Index: 0.01648372154646118
Average Adjusted Rand Index: 0.0187514045075976
[0.011538396657543349, 0.01648372154646118] [0.015014412825096699, 0.0187514045075976] [9582.30859375, 9582.498046875]
-------------------------------------
This iteration is 85
True Objective function: Loss = -9932.67360111485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26203.24609375
inf tensor(26203.2461, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9797.2685546875
tensor(26203.2461, grad_fn=<NegBackward0>) tensor(9797.2686, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9796.0283203125
tensor(9797.2686, grad_fn=<NegBackward0>) tensor(9796.0283, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9795.6103515625
tensor(9796.0283, grad_fn=<NegBackward0>) tensor(9795.6104, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9795.3828125
tensor(9795.6104, grad_fn=<NegBackward0>) tensor(9795.3828, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9795.2177734375
tensor(9795.3828, grad_fn=<NegBackward0>) tensor(9795.2178, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9795.05859375
tensor(9795.2178, grad_fn=<NegBackward0>) tensor(9795.0586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9794.8984375
tensor(9795.0586, grad_fn=<NegBackward0>) tensor(9794.8984, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9794.7392578125
tensor(9794.8984, grad_fn=<NegBackward0>) tensor(9794.7393, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9794.5712890625
tensor(9794.7393, grad_fn=<NegBackward0>) tensor(9794.5713, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9794.41015625
tensor(9794.5713, grad_fn=<NegBackward0>) tensor(9794.4102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9794.265625
tensor(9794.4102, grad_fn=<NegBackward0>) tensor(9794.2656, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9794.1484375
tensor(9794.2656, grad_fn=<NegBackward0>) tensor(9794.1484, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9794.0634765625
tensor(9794.1484, grad_fn=<NegBackward0>) tensor(9794.0635, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9794.001953125
tensor(9794.0635, grad_fn=<NegBackward0>) tensor(9794.0020, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9793.9599609375
tensor(9794.0020, grad_fn=<NegBackward0>) tensor(9793.9600, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9793.9296875
tensor(9793.9600, grad_fn=<NegBackward0>) tensor(9793.9297, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9793.9033203125
tensor(9793.9297, grad_fn=<NegBackward0>) tensor(9793.9033, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9793.8818359375
tensor(9793.9033, grad_fn=<NegBackward0>) tensor(9793.8818, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9793.861328125
tensor(9793.8818, grad_fn=<NegBackward0>) tensor(9793.8613, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9793.841796875
tensor(9793.8613, grad_fn=<NegBackward0>) tensor(9793.8418, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9793.82421875
tensor(9793.8418, grad_fn=<NegBackward0>) tensor(9793.8242, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9793.8076171875
tensor(9793.8242, grad_fn=<NegBackward0>) tensor(9793.8076, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9793.7939453125
tensor(9793.8076, grad_fn=<NegBackward0>) tensor(9793.7939, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9793.787109375
tensor(9793.7939, grad_fn=<NegBackward0>) tensor(9793.7871, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9793.779296875
tensor(9793.7871, grad_fn=<NegBackward0>) tensor(9793.7793, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9793.77734375
tensor(9793.7793, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9793.7744140625
tensor(9793.7773, grad_fn=<NegBackward0>) tensor(9793.7744, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9793.7744140625
tensor(9793.7744, grad_fn=<NegBackward0>) tensor(9793.7744, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9793.7734375
tensor(9793.7744, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9793.7734375
tensor(9793.7734, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9793.7724609375
tensor(9793.7734, grad_fn=<NegBackward0>) tensor(9793.7725, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9793.7734375
tensor(9793.7725, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9793.7734375
tensor(9793.7725, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9793.7734375
tensor(9793.7725, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -9793.7744140625
tensor(9793.7725, grad_fn=<NegBackward0>) tensor(9793.7744, grad_fn=<NegBackward0>)
4
Iteration 3600: Loss = -9793.7734375
tensor(9793.7725, grad_fn=<NegBackward0>) tensor(9793.7734, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3600 due to no improvement.
pi: tensor([[0.0493, 0.9507],
        [0.0960, 0.9040]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3347, 0.6653], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1741, 0.1481],
         [0.5786, 0.1296]],

        [[0.6192, 0.1323],
         [0.6510, 0.6739]],

        [[0.6396, 0.1329],
         [0.5864, 0.6042]],

        [[0.5667, 0.1560],
         [0.5409, 0.7189]],

        [[0.6876, 0.1696],
         [0.5155, 0.6288]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.02397039968412133
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.0035491149357322884
Average Adjusted Rand Index: 0.0042014014487986254
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26627.349609375
inf tensor(26627.3496, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9798.1533203125
tensor(26627.3496, grad_fn=<NegBackward0>) tensor(9798.1533, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9796.6279296875
tensor(9798.1533, grad_fn=<NegBackward0>) tensor(9796.6279, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9796.2080078125
tensor(9796.6279, grad_fn=<NegBackward0>) tensor(9796.2080, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9796.001953125
tensor(9796.2080, grad_fn=<NegBackward0>) tensor(9796.0020, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9795.873046875
tensor(9796.0020, grad_fn=<NegBackward0>) tensor(9795.8730, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9795.7763671875
tensor(9795.8730, grad_fn=<NegBackward0>) tensor(9795.7764, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9795.6943359375
tensor(9795.7764, grad_fn=<NegBackward0>) tensor(9795.6943, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9795.619140625
tensor(9795.6943, grad_fn=<NegBackward0>) tensor(9795.6191, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9795.546875
tensor(9795.6191, grad_fn=<NegBackward0>) tensor(9795.5469, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9795.482421875
tensor(9795.5469, grad_fn=<NegBackward0>) tensor(9795.4824, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9795.421875
tensor(9795.4824, grad_fn=<NegBackward0>) tensor(9795.4219, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9795.36328125
tensor(9795.4219, grad_fn=<NegBackward0>) tensor(9795.3633, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9795.3134765625
tensor(9795.3633, grad_fn=<NegBackward0>) tensor(9795.3135, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9795.2666015625
tensor(9795.3135, grad_fn=<NegBackward0>) tensor(9795.2666, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9795.220703125
tensor(9795.2666, grad_fn=<NegBackward0>) tensor(9795.2207, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9795.1669921875
tensor(9795.2207, grad_fn=<NegBackward0>) tensor(9795.1670, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9795.0947265625
tensor(9795.1670, grad_fn=<NegBackward0>) tensor(9795.0947, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9794.978515625
tensor(9795.0947, grad_fn=<NegBackward0>) tensor(9794.9785, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9794.7578125
tensor(9794.9785, grad_fn=<NegBackward0>) tensor(9794.7578, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9794.26953125
tensor(9794.7578, grad_fn=<NegBackward0>) tensor(9794.2695, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9794.0126953125
tensor(9794.2695, grad_fn=<NegBackward0>) tensor(9794.0127, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9793.9306640625
tensor(9794.0127, grad_fn=<NegBackward0>) tensor(9793.9307, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9793.8798828125
tensor(9793.9307, grad_fn=<NegBackward0>) tensor(9793.8799, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9793.8388671875
tensor(9793.8799, grad_fn=<NegBackward0>) tensor(9793.8389, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9793.80859375
tensor(9793.8389, grad_fn=<NegBackward0>) tensor(9793.8086, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9793.79296875
tensor(9793.8086, grad_fn=<NegBackward0>) tensor(9793.7930, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9793.783203125
tensor(9793.7930, grad_fn=<NegBackward0>) tensor(9793.7832, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9793.779296875
tensor(9793.7832, grad_fn=<NegBackward0>) tensor(9793.7793, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9793.7783203125
tensor(9793.7793, grad_fn=<NegBackward0>) tensor(9793.7783, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9793.77734375
tensor(9793.7783, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9793.77734375
tensor(9793.7773, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9793.77734375
tensor(9793.7773, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9793.77734375
tensor(9793.7773, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9793.7763671875
tensor(9793.7773, grad_fn=<NegBackward0>) tensor(9793.7764, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9793.7763671875
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7764, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9793.77734375
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -9793.77734375
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -9793.77734375
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -9793.77734375
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -9793.77734375
tensor(9793.7764, grad_fn=<NegBackward0>) tensor(9793.7773, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4000 due to no improvement.
pi: tensor([[0.9032, 0.0968],
        [0.9897, 0.0103]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6629, 0.3371], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1298, 0.1480],
         [0.6146, 0.1737]],

        [[0.5655, 0.1311],
         [0.7185, 0.5179]],

        [[0.6633, 0.1325],
         [0.5411, 0.6015]],

        [[0.6542, 0.1558],
         [0.7127, 0.7202]],

        [[0.5559, 0.1701],
         [0.6538, 0.6628]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.02397039968412133
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.0035491149357322884
Average Adjusted Rand Index: 0.0042014014487986254
[0.0035491149357322884, 0.0035491149357322884] [0.0042014014487986254, 0.0042014014487986254] [9793.7734375, 9793.77734375]
-------------------------------------
This iteration is 86
True Objective function: Loss = -10172.301448707021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23786.904296875
inf tensor(23786.9043, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10086.73828125
tensor(23786.9043, grad_fn=<NegBackward0>) tensor(10086.7383, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10084.69140625
tensor(10086.7383, grad_fn=<NegBackward0>) tensor(10084.6914, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10084.1689453125
tensor(10084.6914, grad_fn=<NegBackward0>) tensor(10084.1689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10083.7919921875
tensor(10084.1689, grad_fn=<NegBackward0>) tensor(10083.7920, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10083.2783203125
tensor(10083.7920, grad_fn=<NegBackward0>) tensor(10083.2783, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10082.8349609375
tensor(10083.2783, grad_fn=<NegBackward0>) tensor(10082.8350, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10082.5810546875
tensor(10082.8350, grad_fn=<NegBackward0>) tensor(10082.5811, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10082.41015625
tensor(10082.5811, grad_fn=<NegBackward0>) tensor(10082.4102, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10082.2802734375
tensor(10082.4102, grad_fn=<NegBackward0>) tensor(10082.2803, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10082.169921875
tensor(10082.2803, grad_fn=<NegBackward0>) tensor(10082.1699, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10082.078125
tensor(10082.1699, grad_fn=<NegBackward0>) tensor(10082.0781, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10082.0029296875
tensor(10082.0781, grad_fn=<NegBackward0>) tensor(10082.0029, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10081.93359375
tensor(10082.0029, grad_fn=<NegBackward0>) tensor(10081.9336, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10081.8681640625
tensor(10081.9336, grad_fn=<NegBackward0>) tensor(10081.8682, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10081.802734375
tensor(10081.8682, grad_fn=<NegBackward0>) tensor(10081.8027, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10081.740234375
tensor(10081.8027, grad_fn=<NegBackward0>) tensor(10081.7402, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10081.67578125
tensor(10081.7402, grad_fn=<NegBackward0>) tensor(10081.6758, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10081.6123046875
tensor(10081.6758, grad_fn=<NegBackward0>) tensor(10081.6123, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10081.548828125
tensor(10081.6123, grad_fn=<NegBackward0>) tensor(10081.5488, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10081.4794921875
tensor(10081.5488, grad_fn=<NegBackward0>) tensor(10081.4795, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10081.404296875
tensor(10081.4795, grad_fn=<NegBackward0>) tensor(10081.4043, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10081.314453125
tensor(10081.4043, grad_fn=<NegBackward0>) tensor(10081.3145, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10081.2001953125
tensor(10081.3145, grad_fn=<NegBackward0>) tensor(10081.2002, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10081.056640625
tensor(10081.2002, grad_fn=<NegBackward0>) tensor(10081.0566, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10080.8681640625
tensor(10081.0566, grad_fn=<NegBackward0>) tensor(10080.8682, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10080.6220703125
tensor(10080.8682, grad_fn=<NegBackward0>) tensor(10080.6221, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10080.3134765625
tensor(10080.6221, grad_fn=<NegBackward0>) tensor(10080.3135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10079.955078125
tensor(10080.3135, grad_fn=<NegBackward0>) tensor(10079.9551, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10079.609375
tensor(10079.9551, grad_fn=<NegBackward0>) tensor(10079.6094, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10079.34765625
tensor(10079.6094, grad_fn=<NegBackward0>) tensor(10079.3477, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10079.2021484375
tensor(10079.3477, grad_fn=<NegBackward0>) tensor(10079.2021, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10079.1396484375
tensor(10079.2021, grad_fn=<NegBackward0>) tensor(10079.1396, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10079.1142578125
tensor(10079.1396, grad_fn=<NegBackward0>) tensor(10079.1143, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10079.1064453125
tensor(10079.1143, grad_fn=<NegBackward0>) tensor(10079.1064, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10079.103515625
tensor(10079.1064, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10079.103515625
tensor(10079.1035, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10079.1025390625
tensor(10079.1035, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10079.103515625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -10079.1015625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1016, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10079.11328125
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1133, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -10079.1025390625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -10079.103515625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -10079.103515625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -10079.10546875
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1055, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4700 due to no improvement.
pi: tensor([[0.5388, 0.4612],
        [0.4807, 0.5193]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6118, 0.3882], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1132, 0.1454],
         [0.5730, 0.1798]],

        [[0.6829, 0.1408],
         [0.6073, 0.5756]],

        [[0.5821, 0.1391],
         [0.5135, 0.5373]],

        [[0.6449, 0.1340],
         [0.6889, 0.5522]],

        [[0.5646, 0.1438],
         [0.5375, 0.5604]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0492410037338377
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.1854205028301152
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 73
Adjusted Rand Index: 0.20361764243493694
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.04837209302325581
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.08074805415311274
Global Adjusted Rand Index: 0.11114391058888198
Average Adjusted Rand Index: 0.11347985923505169
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21969.720703125
inf tensor(21969.7207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10085.673828125
tensor(21969.7207, grad_fn=<NegBackward0>) tensor(10085.6738, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10084.4072265625
tensor(10085.6738, grad_fn=<NegBackward0>) tensor(10084.4072, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10084.0595703125
tensor(10084.4072, grad_fn=<NegBackward0>) tensor(10084.0596, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10083.8154296875
tensor(10084.0596, grad_fn=<NegBackward0>) tensor(10083.8154, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10083.59765625
tensor(10083.8154, grad_fn=<NegBackward0>) tensor(10083.5977, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10083.3896484375
tensor(10083.5977, grad_fn=<NegBackward0>) tensor(10083.3896, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10083.2314453125
tensor(10083.3896, grad_fn=<NegBackward0>) tensor(10083.2314, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10083.1357421875
tensor(10083.2314, grad_fn=<NegBackward0>) tensor(10083.1357, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10083.060546875
tensor(10083.1357, grad_fn=<NegBackward0>) tensor(10083.0605, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10082.99609375
tensor(10083.0605, grad_fn=<NegBackward0>) tensor(10082.9961, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10082.94140625
tensor(10082.9961, grad_fn=<NegBackward0>) tensor(10082.9414, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10082.890625
tensor(10082.9414, grad_fn=<NegBackward0>) tensor(10082.8906, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10082.84765625
tensor(10082.8906, grad_fn=<NegBackward0>) tensor(10082.8477, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10082.8076171875
tensor(10082.8477, grad_fn=<NegBackward0>) tensor(10082.8076, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10082.7578125
tensor(10082.8076, grad_fn=<NegBackward0>) tensor(10082.7578, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10082.5361328125
tensor(10082.7578, grad_fn=<NegBackward0>) tensor(10082.5361, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10081.5908203125
tensor(10082.5361, grad_fn=<NegBackward0>) tensor(10081.5908, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10081.53515625
tensor(10081.5908, grad_fn=<NegBackward0>) tensor(10081.5352, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10081.5048828125
tensor(10081.5352, grad_fn=<NegBackward0>) tensor(10081.5049, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10081.482421875
tensor(10081.5049, grad_fn=<NegBackward0>) tensor(10081.4824, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10081.4580078125
tensor(10081.4824, grad_fn=<NegBackward0>) tensor(10081.4580, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10081.4169921875
tensor(10081.4580, grad_fn=<NegBackward0>) tensor(10081.4170, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10081.291015625
tensor(10081.4170, grad_fn=<NegBackward0>) tensor(10081.2910, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10080.935546875
tensor(10081.2910, grad_fn=<NegBackward0>) tensor(10080.9355, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10080.3671875
tensor(10080.9355, grad_fn=<NegBackward0>) tensor(10080.3672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10079.7626953125
tensor(10080.3672, grad_fn=<NegBackward0>) tensor(10079.7627, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10079.32421875
tensor(10079.7627, grad_fn=<NegBackward0>) tensor(10079.3242, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10079.1435546875
tensor(10079.3242, grad_fn=<NegBackward0>) tensor(10079.1436, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10079.1103515625
tensor(10079.1436, grad_fn=<NegBackward0>) tensor(10079.1104, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10079.1044921875
tensor(10079.1104, grad_fn=<NegBackward0>) tensor(10079.1045, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10079.1083984375
tensor(10079.1045, grad_fn=<NegBackward0>) tensor(10079.1084, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -10079.103515625
tensor(10079.1045, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10079.103515625
tensor(10079.1035, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10079.1025390625
tensor(10079.1035, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10079.103515625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10079.111328125
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1113, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10079.1025390625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10079.1015625
tensor(10079.1025, grad_fn=<NegBackward0>) tensor(10079.1016, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10079.1044921875
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1045, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10079.1103515625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1104, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -10079.1025390625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1025, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -10079.103515625
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1035, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -10079.1064453125
tensor(10079.1016, grad_fn=<NegBackward0>) tensor(10079.1064, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4600 due to no improvement.
pi: tensor([[0.5389, 0.4611],
        [0.4815, 0.5185]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6132, 0.3868], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1132, 0.1454],
         [0.5384, 0.1798]],

        [[0.6865, 0.1408],
         [0.6520, 0.5001]],

        [[0.7232, 0.1391],
         [0.6309, 0.5472]],

        [[0.6757, 0.1340],
         [0.6999, 0.6454]],

        [[0.5974, 0.1438],
         [0.5613, 0.6478]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.0492410037338377
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.1854205028301152
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 73
Adjusted Rand Index: 0.20361764243493694
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.03916433108546391
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.08074805415311274
Global Adjusted Rand Index: 0.10846835658177652
Average Adjusted Rand Index: 0.11163830684749329
[0.11114391058888198, 0.10846835658177652] [0.11347985923505169, 0.11163830684749329] [10079.10546875, 10079.1064453125]
-------------------------------------
This iteration is 87
True Objective function: Loss = -10261.975256308191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21474.732421875
inf tensor(21474.7324, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10170.724609375
tensor(21474.7324, grad_fn=<NegBackward0>) tensor(10170.7246, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10167.85546875
tensor(10170.7246, grad_fn=<NegBackward0>) tensor(10167.8555, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10166.490234375
tensor(10167.8555, grad_fn=<NegBackward0>) tensor(10166.4902, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10165.9189453125
tensor(10166.4902, grad_fn=<NegBackward0>) tensor(10165.9189, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10165.5556640625
tensor(10165.9189, grad_fn=<NegBackward0>) tensor(10165.5557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10165.3017578125
tensor(10165.5557, grad_fn=<NegBackward0>) tensor(10165.3018, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10165.0947265625
tensor(10165.3018, grad_fn=<NegBackward0>) tensor(10165.0947, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10164.900390625
tensor(10165.0947, grad_fn=<NegBackward0>) tensor(10164.9004, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10164.7373046875
tensor(10164.9004, grad_fn=<NegBackward0>) tensor(10164.7373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10164.58203125
tensor(10164.7373, grad_fn=<NegBackward0>) tensor(10164.5820, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10164.3720703125
tensor(10164.5820, grad_fn=<NegBackward0>) tensor(10164.3721, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10163.8984375
tensor(10164.3721, grad_fn=<NegBackward0>) tensor(10163.8984, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10162.513671875
tensor(10163.8984, grad_fn=<NegBackward0>) tensor(10162.5137, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10158.26953125
tensor(10162.5137, grad_fn=<NegBackward0>) tensor(10158.2695, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10142.7265625
tensor(10158.2695, grad_fn=<NegBackward0>) tensor(10142.7266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10137.9423828125
tensor(10142.7266, grad_fn=<NegBackward0>) tensor(10137.9424, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10137.447265625
tensor(10137.9424, grad_fn=<NegBackward0>) tensor(10137.4473, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10137.291015625
tensor(10137.4473, grad_fn=<NegBackward0>) tensor(10137.2910, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10137.2109375
tensor(10137.2910, grad_fn=<NegBackward0>) tensor(10137.2109, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10137.15625
tensor(10137.2109, grad_fn=<NegBackward0>) tensor(10137.1562, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10137.1220703125
tensor(10137.1562, grad_fn=<NegBackward0>) tensor(10137.1221, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10137.09765625
tensor(10137.1221, grad_fn=<NegBackward0>) tensor(10137.0977, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10137.078125
tensor(10137.0977, grad_fn=<NegBackward0>) tensor(10137.0781, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10137.0634765625
tensor(10137.0781, grad_fn=<NegBackward0>) tensor(10137.0635, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10137.0517578125
tensor(10137.0635, grad_fn=<NegBackward0>) tensor(10137.0518, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10137.041015625
tensor(10137.0518, grad_fn=<NegBackward0>) tensor(10137.0410, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10137.0341796875
tensor(10137.0410, grad_fn=<NegBackward0>) tensor(10137.0342, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10137.0283203125
tensor(10137.0342, grad_fn=<NegBackward0>) tensor(10137.0283, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10137.0224609375
tensor(10137.0283, grad_fn=<NegBackward0>) tensor(10137.0225, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10137.017578125
tensor(10137.0225, grad_fn=<NegBackward0>) tensor(10137.0176, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10137.0126953125
tensor(10137.0176, grad_fn=<NegBackward0>) tensor(10137.0127, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10137.0087890625
tensor(10137.0127, grad_fn=<NegBackward0>) tensor(10137.0088, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10137.005859375
tensor(10137.0088, grad_fn=<NegBackward0>) tensor(10137.0059, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10137.00390625
tensor(10137.0059, grad_fn=<NegBackward0>) tensor(10137.0039, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10136.9990234375
tensor(10137.0039, grad_fn=<NegBackward0>) tensor(10136.9990, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10137.005859375
tensor(10136.9990, grad_fn=<NegBackward0>) tensor(10137.0059, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -10136.99609375
tensor(10136.9990, grad_fn=<NegBackward0>) tensor(10136.9961, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10136.9921875
tensor(10136.9961, grad_fn=<NegBackward0>) tensor(10136.9922, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10136.990234375
tensor(10136.9922, grad_fn=<NegBackward0>) tensor(10136.9902, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10136.9892578125
tensor(10136.9902, grad_fn=<NegBackward0>) tensor(10136.9893, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10136.98828125
tensor(10136.9893, grad_fn=<NegBackward0>) tensor(10136.9883, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10136.986328125
tensor(10136.9883, grad_fn=<NegBackward0>) tensor(10136.9863, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10136.986328125
tensor(10136.9863, grad_fn=<NegBackward0>) tensor(10136.9863, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10136.984375
tensor(10136.9863, grad_fn=<NegBackward0>) tensor(10136.9844, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10136.9833984375
tensor(10136.9844, grad_fn=<NegBackward0>) tensor(10136.9834, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10136.982421875
tensor(10136.9834, grad_fn=<NegBackward0>) tensor(10136.9824, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10136.98046875
tensor(10136.9824, grad_fn=<NegBackward0>) tensor(10136.9805, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10136.982421875
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9824, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -10136.98046875
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9805, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10136.98046875
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9805, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10136.9814453125
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9814, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10136.9775390625
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9775, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10136.9765625
tensor(10136.9775, grad_fn=<NegBackward0>) tensor(10136.9766, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10136.9755859375
tensor(10136.9766, grad_fn=<NegBackward0>) tensor(10136.9756, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10136.974609375
tensor(10136.9756, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10136.9765625
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9766, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10136.9775390625
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9775, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -10136.974609375
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10136.974609375
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10136.974609375
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10136.974609375
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10136.97265625
tensor(10136.9746, grad_fn=<NegBackward0>) tensor(10136.9727, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10136.974609375
tensor(10136.9727, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -10136.9716796875
tensor(10136.9727, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10136.97265625
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9727, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10136.9716796875
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10136.9716796875
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10136.9697265625
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10136.970703125
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -10136.9990234375
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9990, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -10136.970703125
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -10136.970703125
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10136.9833984375
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9834, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10136.9794921875
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9795, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -10136.9853515625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9854, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10136.9677734375
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -10136.96875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -10136.98046875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9805, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -10136.96875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10137.0078125
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10137.0078, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10136.96875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10136.966796875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -10136.96875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -10136.970703125
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -10136.982421875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9824, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[9.9999e-01, 6.3822e-06],
        [2.6526e-01, 7.3474e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4263, 0.5737], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1417, 0.0976],
         [0.5298, 0.2298]],

        [[0.5663, 0.1087],
         [0.5790, 0.5539]],

        [[0.7102, 0.1295],
         [0.6215, 0.6955]],

        [[0.6526, 0.1421],
         [0.5046, 0.7020]],

        [[0.5245, 0.1121],
         [0.5906, 0.5450]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.669076647074352
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 85
Adjusted Rand Index: 0.48484848484848486
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.10421810475601015
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.059154975056103586
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0007177784756179624
Global Adjusted Rand Index: 0.19122811961413033
Average Adjusted Rand Index: 0.26360319804211374
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24992.828125
inf tensor(24992.8281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10157.4443359375
tensor(24992.8281, grad_fn=<NegBackward0>) tensor(10157.4443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10152.154296875
tensor(10157.4443, grad_fn=<NegBackward0>) tensor(10152.1543, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10151.78515625
tensor(10152.1543, grad_fn=<NegBackward0>) tensor(10151.7852, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10151.654296875
tensor(10151.7852, grad_fn=<NegBackward0>) tensor(10151.6543, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10151.4375
tensor(10151.6543, grad_fn=<NegBackward0>) tensor(10151.4375, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10151.2255859375
tensor(10151.4375, grad_fn=<NegBackward0>) tensor(10151.2256, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10147.4931640625
tensor(10151.2256, grad_fn=<NegBackward0>) tensor(10147.4932, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10137.4072265625
tensor(10147.4932, grad_fn=<NegBackward0>) tensor(10137.4072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10137.1298828125
tensor(10137.4072, grad_fn=<NegBackward0>) tensor(10137.1299, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10137.07421875
tensor(10137.1299, grad_fn=<NegBackward0>) tensor(10137.0742, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10137.0341796875
tensor(10137.0742, grad_fn=<NegBackward0>) tensor(10137.0342, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10137.0205078125
tensor(10137.0342, grad_fn=<NegBackward0>) tensor(10137.0205, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10137.01171875
tensor(10137.0205, grad_fn=<NegBackward0>) tensor(10137.0117, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10137.0029296875
tensor(10137.0117, grad_fn=<NegBackward0>) tensor(10137.0029, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10136.998046875
tensor(10137.0029, grad_fn=<NegBackward0>) tensor(10136.9980, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10136.9921875
tensor(10136.9980, grad_fn=<NegBackward0>) tensor(10136.9922, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10136.986328125
tensor(10136.9922, grad_fn=<NegBackward0>) tensor(10136.9863, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10136.984375
tensor(10136.9863, grad_fn=<NegBackward0>) tensor(10136.9844, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10136.982421875
tensor(10136.9844, grad_fn=<NegBackward0>) tensor(10136.9824, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10136.98046875
tensor(10136.9824, grad_fn=<NegBackward0>) tensor(10136.9805, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10136.9794921875
tensor(10136.9805, grad_fn=<NegBackward0>) tensor(10136.9795, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10136.9794921875
tensor(10136.9795, grad_fn=<NegBackward0>) tensor(10136.9795, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10136.9775390625
tensor(10136.9795, grad_fn=<NegBackward0>) tensor(10136.9775, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10136.9765625
tensor(10136.9775, grad_fn=<NegBackward0>) tensor(10136.9766, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10136.9765625
tensor(10136.9766, grad_fn=<NegBackward0>) tensor(10136.9766, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10136.9833984375
tensor(10136.9766, grad_fn=<NegBackward0>) tensor(10136.9834, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -10136.9736328125
tensor(10136.9766, grad_fn=<NegBackward0>) tensor(10136.9736, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10136.974609375
tensor(10136.9736, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -10136.9736328125
tensor(10136.9736, grad_fn=<NegBackward0>) tensor(10136.9736, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10136.9736328125
tensor(10136.9736, grad_fn=<NegBackward0>) tensor(10136.9736, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10136.97265625
tensor(10136.9736, grad_fn=<NegBackward0>) tensor(10136.9727, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10136.97265625
tensor(10136.9727, grad_fn=<NegBackward0>) tensor(10136.9727, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10136.9716796875
tensor(10136.9727, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10136.9716796875
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10136.9716796875
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9717, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10136.970703125
tensor(10136.9717, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10136.970703125
tensor(10136.9707, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10136.9697265625
tensor(10136.9707, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10136.970703125
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10136.9697265625
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10136.9755859375
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9756, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10136.96875
tensor(10136.9697, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10136.96875
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10136.96875
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10136.96875
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10136.970703125
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9707, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10136.96875
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10136.96875
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10136.9697265625
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10136.9677734375
tensor(10136.9688, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10136.9697265625
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10136.9697265625
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9697, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10136.9677734375
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10136.966796875
tensor(10136.9678, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10136.974609375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9746, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -10136.9755859375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9756, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10136.986328125
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9863, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10136.96875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10136.966796875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9668, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -10136.96875
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9688, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -10136.9677734375
tensor(10136.9668, grad_fn=<NegBackward0>) tensor(10136.9678, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[7.3518e-01, 2.6482e-01],
        [4.0049e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5763, 0.4237], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2293, 0.0976],
         [0.5532, 0.1414]],

        [[0.6158, 0.1088],
         [0.5303, 0.5750]],

        [[0.6036, 0.1296],
         [0.5523, 0.5533]],

        [[0.7251, 0.1423],
         [0.6413, 0.6090]],

        [[0.6111, 0.1122],
         [0.7269, 0.6968]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.669076647074352
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 15
Adjusted Rand Index: 0.48484848484848486
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 33
Adjusted Rand Index: 0.10421810475601015
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.059154975056103586
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0007177784756179624
Global Adjusted Rand Index: 0.19122811961413033
Average Adjusted Rand Index: 0.26360319804211374
[0.19122811961413033, 0.19122811961413033] [0.26360319804211374, 0.26360319804211374] [10136.982421875, 10136.9677734375]
-------------------------------------
This iteration is 88
True Objective function: Loss = -10145.149433338333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22519.251953125
inf tensor(22519.2520, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10046.572265625
tensor(22519.2520, grad_fn=<NegBackward0>) tensor(10046.5723, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10045.6259765625
tensor(10046.5723, grad_fn=<NegBackward0>) tensor(10045.6260, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10043.6650390625
tensor(10045.6260, grad_fn=<NegBackward0>) tensor(10043.6650, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10041.8544921875
tensor(10043.6650, grad_fn=<NegBackward0>) tensor(10041.8545, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10040.9404296875
tensor(10041.8545, grad_fn=<NegBackward0>) tensor(10040.9404, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10039.9541015625
tensor(10040.9404, grad_fn=<NegBackward0>) tensor(10039.9541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10038.7255859375
tensor(10039.9541, grad_fn=<NegBackward0>) tensor(10038.7256, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10037.89453125
tensor(10038.7256, grad_fn=<NegBackward0>) tensor(10037.8945, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10037.3232421875
tensor(10037.8945, grad_fn=<NegBackward0>) tensor(10037.3232, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10037.033203125
tensor(10037.3232, grad_fn=<NegBackward0>) tensor(10037.0332, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10036.791015625
tensor(10037.0332, grad_fn=<NegBackward0>) tensor(10036.7910, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10036.568359375
tensor(10036.7910, grad_fn=<NegBackward0>) tensor(10036.5684, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10036.4140625
tensor(10036.5684, grad_fn=<NegBackward0>) tensor(10036.4141, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10036.3154296875
tensor(10036.4141, grad_fn=<NegBackward0>) tensor(10036.3154, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10036.251953125
tensor(10036.3154, grad_fn=<NegBackward0>) tensor(10036.2520, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10036.2099609375
tensor(10036.2520, grad_fn=<NegBackward0>) tensor(10036.2100, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10036.18359375
tensor(10036.2100, grad_fn=<NegBackward0>) tensor(10036.1836, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10036.1650390625
tensor(10036.1836, grad_fn=<NegBackward0>) tensor(10036.1650, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10036.1513671875
tensor(10036.1650, grad_fn=<NegBackward0>) tensor(10036.1514, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10036.142578125
tensor(10036.1514, grad_fn=<NegBackward0>) tensor(10036.1426, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10036.134765625
tensor(10036.1426, grad_fn=<NegBackward0>) tensor(10036.1348, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10036.12890625
tensor(10036.1348, grad_fn=<NegBackward0>) tensor(10036.1289, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10036.125
tensor(10036.1289, grad_fn=<NegBackward0>) tensor(10036.1250, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10036.1220703125
tensor(10036.1250, grad_fn=<NegBackward0>) tensor(10036.1221, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10036.1201171875
tensor(10036.1221, grad_fn=<NegBackward0>) tensor(10036.1201, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10036.1171875
tensor(10036.1201, grad_fn=<NegBackward0>) tensor(10036.1172, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10036.115234375
tensor(10036.1172, grad_fn=<NegBackward0>) tensor(10036.1152, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10036.1123046875
tensor(10036.1152, grad_fn=<NegBackward0>) tensor(10036.1123, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10036.111328125
tensor(10036.1123, grad_fn=<NegBackward0>) tensor(10036.1113, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10036.1103515625
tensor(10036.1113, grad_fn=<NegBackward0>) tensor(10036.1104, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10036.109375
tensor(10036.1104, grad_fn=<NegBackward0>) tensor(10036.1094, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10036.107421875
tensor(10036.1094, grad_fn=<NegBackward0>) tensor(10036.1074, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10036.1083984375
tensor(10036.1074, grad_fn=<NegBackward0>) tensor(10036.1084, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -10036.107421875
tensor(10036.1074, grad_fn=<NegBackward0>) tensor(10036.1074, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10036.1064453125
tensor(10036.1074, grad_fn=<NegBackward0>) tensor(10036.1064, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10036.1064453125
tensor(10036.1064, grad_fn=<NegBackward0>) tensor(10036.1064, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10036.10546875
tensor(10036.1064, grad_fn=<NegBackward0>) tensor(10036.1055, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10036.1044921875
tensor(10036.1055, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10036.1044921875
tensor(10036.1045, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10036.1044921875
tensor(10036.1045, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10036.103515625
tensor(10036.1045, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10036.1044921875
tensor(10036.1035, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10036.1025390625
tensor(10036.1035, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10036.103515625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -10036.103515625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -10036.103515625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -10036.1025390625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10036.1025390625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10036.1025390625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10036.1025390625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10036.1025390625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10036.103515625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10036.103515625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10036.1015625
tensor(10036.1025, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -10036.1015625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.9945, 0.0055],
        [0.7618, 0.2382]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6886, 0.3114], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1339, 0.1599],
         [0.6876, 0.1794]],

        [[0.6821, 0.2221],
         [0.6953, 0.6454]],

        [[0.6052, 0.1352],
         [0.5933, 0.6695]],

        [[0.5433, 0.2640],
         [0.5387, 0.6593]],

        [[0.5899, 0.2499],
         [0.7025, 0.7086]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.01570657266520905
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.01150857271232653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.007291217680123475
Average Adjusted Rand Index: 0.006710265918934087
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26157.77734375
inf tensor(26157.7773, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10046.0947265625
tensor(26157.7773, grad_fn=<NegBackward0>) tensor(10046.0947, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10044.77734375
tensor(10046.0947, grad_fn=<NegBackward0>) tensor(10044.7773, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10043.787109375
tensor(10044.7773, grad_fn=<NegBackward0>) tensor(10043.7871, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10042.6015625
tensor(10043.7871, grad_fn=<NegBackward0>) tensor(10042.6016, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10041.7802734375
tensor(10042.6016, grad_fn=<NegBackward0>) tensor(10041.7803, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10039.9189453125
tensor(10041.7803, grad_fn=<NegBackward0>) tensor(10039.9189, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10039.3134765625
tensor(10039.9189, grad_fn=<NegBackward0>) tensor(10039.3135, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10038.9072265625
tensor(10039.3135, grad_fn=<NegBackward0>) tensor(10038.9072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10038.3876953125
tensor(10038.9072, grad_fn=<NegBackward0>) tensor(10038.3877, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10037.62890625
tensor(10038.3877, grad_fn=<NegBackward0>) tensor(10037.6289, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10037.306640625
tensor(10037.6289, grad_fn=<NegBackward0>) tensor(10037.3066, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10036.6826171875
tensor(10037.3066, grad_fn=<NegBackward0>) tensor(10036.6826, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10036.44140625
tensor(10036.6826, grad_fn=<NegBackward0>) tensor(10036.4414, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10036.341796875
tensor(10036.4414, grad_fn=<NegBackward0>) tensor(10036.3418, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10036.267578125
tensor(10036.3418, grad_fn=<NegBackward0>) tensor(10036.2676, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10036.1435546875
tensor(10036.2676, grad_fn=<NegBackward0>) tensor(10036.1436, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10036.1240234375
tensor(10036.1436, grad_fn=<NegBackward0>) tensor(10036.1240, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10036.1171875
tensor(10036.1240, grad_fn=<NegBackward0>) tensor(10036.1172, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10036.1142578125
tensor(10036.1172, grad_fn=<NegBackward0>) tensor(10036.1143, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10036.1103515625
tensor(10036.1143, grad_fn=<NegBackward0>) tensor(10036.1104, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10036.109375
tensor(10036.1104, grad_fn=<NegBackward0>) tensor(10036.1094, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10036.107421875
tensor(10036.1094, grad_fn=<NegBackward0>) tensor(10036.1074, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10036.107421875
tensor(10036.1074, grad_fn=<NegBackward0>) tensor(10036.1074, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10036.1064453125
tensor(10036.1074, grad_fn=<NegBackward0>) tensor(10036.1064, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10036.10546875
tensor(10036.1064, grad_fn=<NegBackward0>) tensor(10036.1055, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10036.1044921875
tensor(10036.1055, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10036.1044921875
tensor(10036.1045, grad_fn=<NegBackward0>) tensor(10036.1045, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10036.103515625
tensor(10036.1045, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10036.1015625
tensor(10036.1035, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -10036.103515625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
3
Iteration 3300: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
4
Iteration 3400: Loss = -10036.1015625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10036.1015625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10036.1015625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10036.103515625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1035, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -10036.1015625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1016, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10036.1064453125
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1064, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -10036.1025390625
tensor(10036.1016, grad_fn=<NegBackward0>) tensor(10036.1025, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.2382, 0.7618],
        [0.0055, 0.9945]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3114, 0.6886], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1794, 0.1599],
         [0.5931, 0.1339]],

        [[0.5505, 0.2221],
         [0.6505, 0.6766]],

        [[0.5795, 0.1352],
         [0.6591, 0.7163]],

        [[0.6862, 0.2640],
         [0.5819, 0.6515]],

        [[0.6432, 0.2499],
         [0.6496, 0.6807]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.01570657266520905
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.01150857271232653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.007291217680123475
Average Adjusted Rand Index: 0.006710265918934087
[0.007291217680123475, 0.007291217680123475] [0.006710265918934087, 0.006710265918934087] [10036.1025390625, 10036.1025390625]
-------------------------------------
This iteration is 89
True Objective function: Loss = -10001.661841317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19752.44140625
inf tensor(19752.4414, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9861.9443359375
tensor(19752.4414, grad_fn=<NegBackward0>) tensor(9861.9443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9861.3154296875
tensor(9861.9443, grad_fn=<NegBackward0>) tensor(9861.3154, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9860.7763671875
tensor(9861.3154, grad_fn=<NegBackward0>) tensor(9860.7764, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9860.234375
tensor(9860.7764, grad_fn=<NegBackward0>) tensor(9860.2344, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9859.521484375
tensor(9860.2344, grad_fn=<NegBackward0>) tensor(9859.5215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9859.1181640625
tensor(9859.5215, grad_fn=<NegBackward0>) tensor(9859.1182, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9858.8427734375
tensor(9859.1182, grad_fn=<NegBackward0>) tensor(9858.8428, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9858.7431640625
tensor(9858.8428, grad_fn=<NegBackward0>) tensor(9858.7432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9858.6943359375
tensor(9858.7432, grad_fn=<NegBackward0>) tensor(9858.6943, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9858.6572265625
tensor(9858.6943, grad_fn=<NegBackward0>) tensor(9858.6572, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9858.625
tensor(9858.6572, grad_fn=<NegBackward0>) tensor(9858.6250, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9858.5947265625
tensor(9858.6250, grad_fn=<NegBackward0>) tensor(9858.5947, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9858.5615234375
tensor(9858.5947, grad_fn=<NegBackward0>) tensor(9858.5615, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9858.525390625
tensor(9858.5615, grad_fn=<NegBackward0>) tensor(9858.5254, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9858.4794921875
tensor(9858.5254, grad_fn=<NegBackward0>) tensor(9858.4795, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9858.4150390625
tensor(9858.4795, grad_fn=<NegBackward0>) tensor(9858.4150, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9858.3369140625
tensor(9858.4150, grad_fn=<NegBackward0>) tensor(9858.3369, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9858.2724609375
tensor(9858.3369, grad_fn=<NegBackward0>) tensor(9858.2725, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9858.2392578125
tensor(9858.2725, grad_fn=<NegBackward0>) tensor(9858.2393, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9858.2255859375
tensor(9858.2393, grad_fn=<NegBackward0>) tensor(9858.2256, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9858.2216796875
tensor(9858.2256, grad_fn=<NegBackward0>) tensor(9858.2217, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9858.2216796875
tensor(9858.2217, grad_fn=<NegBackward0>) tensor(9858.2217, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9858.2197265625
tensor(9858.2217, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9858.2197265625
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9858.21875
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9858.2197265625
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -9858.220703125
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -9858.2197265625
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
3
Iteration 2900: Loss = -9858.21875
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9858.21875
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9858.2177734375
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9858.2197265625
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9858.21875
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -9858.21875
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -9858.21875
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
4
Iteration 3600: Loss = -9858.220703125
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3600 due to no improvement.
pi: tensor([[0.9234, 0.0766],
        [0.9965, 0.0035]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9841, 0.0159], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1354, 0.1998],
         [0.5479, 0.1499]],

        [[0.5284, 0.1856],
         [0.7047, 0.5813]],

        [[0.5135, 0.1498],
         [0.5646, 0.6518]],

        [[0.6063, 0.0854],
         [0.6886, 0.6910]],

        [[0.6538, 0.1362],
         [0.7047, 0.6971]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 69
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008570565949531054
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20824.70703125
inf tensor(20824.7070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9861.759765625
tensor(20824.7070, grad_fn=<NegBackward0>) tensor(9861.7598, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9861.232421875
tensor(9861.7598, grad_fn=<NegBackward0>) tensor(9861.2324, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9860.7890625
tensor(9861.2324, grad_fn=<NegBackward0>) tensor(9860.7891, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9860.15625
tensor(9860.7891, grad_fn=<NegBackward0>) tensor(9860.1562, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9859.6708984375
tensor(9860.1562, grad_fn=<NegBackward0>) tensor(9859.6709, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9859.4345703125
tensor(9859.6709, grad_fn=<NegBackward0>) tensor(9859.4346, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9859.150390625
tensor(9859.4346, grad_fn=<NegBackward0>) tensor(9859.1504, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9858.828125
tensor(9859.1504, grad_fn=<NegBackward0>) tensor(9858.8281, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9858.67578125
tensor(9858.8281, grad_fn=<NegBackward0>) tensor(9858.6758, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9858.611328125
tensor(9858.6758, grad_fn=<NegBackward0>) tensor(9858.6113, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9858.5634765625
tensor(9858.6113, grad_fn=<NegBackward0>) tensor(9858.5635, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9858.517578125
tensor(9858.5635, grad_fn=<NegBackward0>) tensor(9858.5176, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9858.4619140625
tensor(9858.5176, grad_fn=<NegBackward0>) tensor(9858.4619, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9858.390625
tensor(9858.4619, grad_fn=<NegBackward0>) tensor(9858.3906, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9858.322265625
tensor(9858.3906, grad_fn=<NegBackward0>) tensor(9858.3223, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9858.2734375
tensor(9858.3223, grad_fn=<NegBackward0>) tensor(9858.2734, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9858.2470703125
tensor(9858.2734, grad_fn=<NegBackward0>) tensor(9858.2471, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9858.2373046875
tensor(9858.2471, grad_fn=<NegBackward0>) tensor(9858.2373, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9858.2314453125
tensor(9858.2373, grad_fn=<NegBackward0>) tensor(9858.2314, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9858.228515625
tensor(9858.2314, grad_fn=<NegBackward0>) tensor(9858.2285, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9858.2255859375
tensor(9858.2285, grad_fn=<NegBackward0>) tensor(9858.2256, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9858.224609375
tensor(9858.2256, grad_fn=<NegBackward0>) tensor(9858.2246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9858.2236328125
tensor(9858.2246, grad_fn=<NegBackward0>) tensor(9858.2236, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9858.22265625
tensor(9858.2236, grad_fn=<NegBackward0>) tensor(9858.2227, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9858.220703125
tensor(9858.2227, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9858.2216796875
tensor(9858.2207, grad_fn=<NegBackward0>) tensor(9858.2217, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -9858.220703125
tensor(9858.2207, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9858.2197265625
tensor(9858.2207, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9858.220703125
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -9858.220703125
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -9858.2216796875
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2217, grad_fn=<NegBackward0>)
3
Iteration 3200: Loss = -9858.2197265625
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9858.21875
tensor(9858.2197, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9858.2197265625
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9858.21875
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9858.21875
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9858.2197265625
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9858.2197265625
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -9858.21875
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9858.2177734375
tensor(9858.2188, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9858.2177734375
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9858.2197265625
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2197, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9858.216796875
tensor(9858.2178, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9858.216796875
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9858.2177734375
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9858.2177734375
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -9858.216796875
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9858.21875
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2188, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9858.2177734375
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -9858.2158203125
tensor(9858.2168, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9858.216796875
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9858.2177734375
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9858.2177734375
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -9858.2158203125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9858.2177734375
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9858.216796875
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9858.216796875
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -9858.220703125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2207, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -9858.2158203125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9858.2158203125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9858.216796875
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9858.2158203125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9858.23828125
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2383, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9858.21484375
tensor(9858.2158, grad_fn=<NegBackward0>) tensor(9858.2148, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9858.2177734375
tensor(9858.2148, grad_fn=<NegBackward0>) tensor(9858.2178, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9858.216796875
tensor(9858.2148, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9858.2158203125
tensor(9858.2148, grad_fn=<NegBackward0>) tensor(9858.2158, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -9858.2353515625
tensor(9858.2148, grad_fn=<NegBackward0>) tensor(9858.2354, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -9858.216796875
tensor(9858.2148, grad_fn=<NegBackward0>) tensor(9858.2168, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[9.2331e-01, 7.6687e-02],
        [9.9906e-01, 9.4460e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9839, 0.0161], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1354, 0.1997],
         [0.5294, 0.1503]],

        [[0.6972, 0.1857],
         [0.5486, 0.5141]],

        [[0.5326, 0.1500],
         [0.6308, 0.5044]],

        [[0.6484, 0.0854],
         [0.5564, 0.7050]],

        [[0.6512, 0.1364],
         [0.5407, 0.5346]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 69
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008570565949531054
Average Adjusted Rand Index: 0.0
[-0.0008570565949531054, -0.0008570565949531054] [0.0, 0.0] [9858.220703125, 9858.216796875]
-------------------------------------
This iteration is 90
True Objective function: Loss = -9872.899494909829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22987.48046875
inf tensor(22987.4805, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9761.0625
tensor(22987.4805, grad_fn=<NegBackward0>) tensor(9761.0625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9760.14453125
tensor(9761.0625, grad_fn=<NegBackward0>) tensor(9760.1445, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9759.8994140625
tensor(9760.1445, grad_fn=<NegBackward0>) tensor(9759.8994, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9759.7421875
tensor(9759.8994, grad_fn=<NegBackward0>) tensor(9759.7422, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9759.615234375
tensor(9759.7422, grad_fn=<NegBackward0>) tensor(9759.6152, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9759.5029296875
tensor(9759.6152, grad_fn=<NegBackward0>) tensor(9759.5029, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9759.408203125
tensor(9759.5029, grad_fn=<NegBackward0>) tensor(9759.4082, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9759.33203125
tensor(9759.4082, grad_fn=<NegBackward0>) tensor(9759.3320, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9759.2685546875
tensor(9759.3320, grad_fn=<NegBackward0>) tensor(9759.2686, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9759.2158203125
tensor(9759.2686, grad_fn=<NegBackward0>) tensor(9759.2158, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9759.16796875
tensor(9759.2158, grad_fn=<NegBackward0>) tensor(9759.1680, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9759.125
tensor(9759.1680, grad_fn=<NegBackward0>) tensor(9759.1250, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9759.0888671875
tensor(9759.1250, grad_fn=<NegBackward0>) tensor(9759.0889, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9759.056640625
tensor(9759.0889, grad_fn=<NegBackward0>) tensor(9759.0566, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9759.029296875
tensor(9759.0566, grad_fn=<NegBackward0>) tensor(9759.0293, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9759.0087890625
tensor(9759.0293, grad_fn=<NegBackward0>) tensor(9759.0088, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9758.990234375
tensor(9759.0088, grad_fn=<NegBackward0>) tensor(9758.9902, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9758.9755859375
tensor(9758.9902, grad_fn=<NegBackward0>) tensor(9758.9756, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9758.9619140625
tensor(9758.9756, grad_fn=<NegBackward0>) tensor(9758.9619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9758.951171875
tensor(9758.9619, grad_fn=<NegBackward0>) tensor(9758.9512, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9758.939453125
tensor(9758.9512, grad_fn=<NegBackward0>) tensor(9758.9395, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9758.9287109375
tensor(9758.9395, grad_fn=<NegBackward0>) tensor(9758.9287, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9758.9189453125
tensor(9758.9287, grad_fn=<NegBackward0>) tensor(9758.9189, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9758.91015625
tensor(9758.9189, grad_fn=<NegBackward0>) tensor(9758.9102, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9758.900390625
tensor(9758.9102, grad_fn=<NegBackward0>) tensor(9758.9004, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9758.8876953125
tensor(9758.9004, grad_fn=<NegBackward0>) tensor(9758.8877, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9758.876953125
tensor(9758.8877, grad_fn=<NegBackward0>) tensor(9758.8770, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9758.8623046875
tensor(9758.8770, grad_fn=<NegBackward0>) tensor(9758.8623, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9758.83984375
tensor(9758.8623, grad_fn=<NegBackward0>) tensor(9758.8398, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9758.7744140625
tensor(9758.8398, grad_fn=<NegBackward0>) tensor(9758.7744, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9758.5498046875
tensor(9758.7744, grad_fn=<NegBackward0>) tensor(9758.5498, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9758.341796875
tensor(9758.5498, grad_fn=<NegBackward0>) tensor(9758.3418, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9758.240234375
tensor(9758.3418, grad_fn=<NegBackward0>) tensor(9758.2402, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9758.1806640625
tensor(9758.2402, grad_fn=<NegBackward0>) tensor(9758.1807, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9758.13671875
tensor(9758.1807, grad_fn=<NegBackward0>) tensor(9758.1367, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9758.1044921875
tensor(9758.1367, grad_fn=<NegBackward0>) tensor(9758.1045, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9758.078125
tensor(9758.1045, grad_fn=<NegBackward0>) tensor(9758.0781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9758.0537109375
tensor(9758.0781, grad_fn=<NegBackward0>) tensor(9758.0537, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9758.0341796875
tensor(9758.0537, grad_fn=<NegBackward0>) tensor(9758.0342, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9758.017578125
tensor(9758.0342, grad_fn=<NegBackward0>) tensor(9758.0176, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9758.0029296875
tensor(9758.0176, grad_fn=<NegBackward0>) tensor(9758.0029, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9757.990234375
tensor(9758.0029, grad_fn=<NegBackward0>) tensor(9757.9902, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9757.9794921875
tensor(9757.9902, grad_fn=<NegBackward0>) tensor(9757.9795, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9757.97265625
tensor(9757.9795, grad_fn=<NegBackward0>) tensor(9757.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9757.9658203125
tensor(9757.9727, grad_fn=<NegBackward0>) tensor(9757.9658, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9757.9580078125
tensor(9757.9658, grad_fn=<NegBackward0>) tensor(9757.9580, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9757.9541015625
tensor(9757.9580, grad_fn=<NegBackward0>) tensor(9757.9541, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9757.94921875
tensor(9757.9541, grad_fn=<NegBackward0>) tensor(9757.9492, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9757.9443359375
tensor(9757.9492, grad_fn=<NegBackward0>) tensor(9757.9443, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9757.94140625
tensor(9757.9443, grad_fn=<NegBackward0>) tensor(9757.9414, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9757.9375
tensor(9757.9414, grad_fn=<NegBackward0>) tensor(9757.9375, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9757.935546875
tensor(9757.9375, grad_fn=<NegBackward0>) tensor(9757.9355, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9757.931640625
tensor(9757.9355, grad_fn=<NegBackward0>) tensor(9757.9316, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9757.9296875
tensor(9757.9316, grad_fn=<NegBackward0>) tensor(9757.9297, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9757.9287109375
tensor(9757.9297, grad_fn=<NegBackward0>) tensor(9757.9287, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9757.92578125
tensor(9757.9287, grad_fn=<NegBackward0>) tensor(9757.9258, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9757.9248046875
tensor(9757.9258, grad_fn=<NegBackward0>) tensor(9757.9248, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9757.9228515625
tensor(9757.9248, grad_fn=<NegBackward0>) tensor(9757.9229, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9757.9208984375
tensor(9757.9229, grad_fn=<NegBackward0>) tensor(9757.9209, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9757.919921875
tensor(9757.9209, grad_fn=<NegBackward0>) tensor(9757.9199, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9757.91796875
tensor(9757.9199, grad_fn=<NegBackward0>) tensor(9757.9180, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9757.9169921875
tensor(9757.9180, grad_fn=<NegBackward0>) tensor(9757.9170, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9757.9140625
tensor(9757.9170, grad_fn=<NegBackward0>) tensor(9757.9141, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9757.9130859375
tensor(9757.9141, grad_fn=<NegBackward0>) tensor(9757.9131, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9757.9140625
tensor(9757.9131, grad_fn=<NegBackward0>) tensor(9757.9141, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9757.912109375
tensor(9757.9131, grad_fn=<NegBackward0>) tensor(9757.9121, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9757.9111328125
tensor(9757.9121, grad_fn=<NegBackward0>) tensor(9757.9111, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9757.9091796875
tensor(9757.9111, grad_fn=<NegBackward0>) tensor(9757.9092, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9757.908203125
tensor(9757.9092, grad_fn=<NegBackward0>) tensor(9757.9082, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9757.908203125
tensor(9757.9082, grad_fn=<NegBackward0>) tensor(9757.9082, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9757.9072265625
tensor(9757.9082, grad_fn=<NegBackward0>) tensor(9757.9072, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9757.90625
tensor(9757.9072, grad_fn=<NegBackward0>) tensor(9757.9062, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9757.9052734375
tensor(9757.9062, grad_fn=<NegBackward0>) tensor(9757.9053, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9757.9033203125
tensor(9757.9053, grad_fn=<NegBackward0>) tensor(9757.9033, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9757.9033203125
tensor(9757.9033, grad_fn=<NegBackward0>) tensor(9757.9033, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9757.9013671875
tensor(9757.9033, grad_fn=<NegBackward0>) tensor(9757.9014, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9757.9013671875
tensor(9757.9014, grad_fn=<NegBackward0>) tensor(9757.9014, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9757.900390625
tensor(9757.9014, grad_fn=<NegBackward0>) tensor(9757.9004, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9757.9013671875
tensor(9757.9004, grad_fn=<NegBackward0>) tensor(9757.9014, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9757.8994140625
tensor(9757.9004, grad_fn=<NegBackward0>) tensor(9757.8994, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9757.8994140625
tensor(9757.8994, grad_fn=<NegBackward0>) tensor(9757.8994, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9757.8974609375
tensor(9757.8994, grad_fn=<NegBackward0>) tensor(9757.8975, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9757.8974609375
tensor(9757.8975, grad_fn=<NegBackward0>) tensor(9757.8975, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9757.896484375
tensor(9757.8975, grad_fn=<NegBackward0>) tensor(9757.8965, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9757.9111328125
tensor(9757.8965, grad_fn=<NegBackward0>) tensor(9757.9111, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -9757.896484375
tensor(9757.8965, grad_fn=<NegBackward0>) tensor(9757.8965, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9757.9873046875
tensor(9757.8965, grad_fn=<NegBackward0>) tensor(9757.9873, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9757.89453125
tensor(9757.8965, grad_fn=<NegBackward0>) tensor(9757.8945, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9757.8935546875
tensor(9757.8945, grad_fn=<NegBackward0>) tensor(9757.8936, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9757.8955078125
tensor(9757.8936, grad_fn=<NegBackward0>) tensor(9757.8955, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9757.8935546875
tensor(9757.8936, grad_fn=<NegBackward0>) tensor(9757.8936, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9757.8955078125
tensor(9757.8936, grad_fn=<NegBackward0>) tensor(9757.8955, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9757.892578125
tensor(9757.8936, grad_fn=<NegBackward0>) tensor(9757.8926, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9757.8916015625
tensor(9757.8926, grad_fn=<NegBackward0>) tensor(9757.8916, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9757.8916015625
tensor(9757.8916, grad_fn=<NegBackward0>) tensor(9757.8916, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9757.890625
tensor(9757.8916, grad_fn=<NegBackward0>) tensor(9757.8906, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9757.8994140625
tensor(9757.8906, grad_fn=<NegBackward0>) tensor(9757.8994, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9757.8896484375
tensor(9757.8906, grad_fn=<NegBackward0>) tensor(9757.8896, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9757.900390625
tensor(9757.8896, grad_fn=<NegBackward0>) tensor(9757.9004, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9999e-01, 9.0916e-06],
        [8.7911e-04, 9.9912e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9897, 0.0103], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1337, 0.1118],
         [0.6582, 0.0447]],

        [[0.7309, 0.1711],
         [0.6230, 0.5491]],

        [[0.7309, 0.2560],
         [0.5941, 0.6815]],

        [[0.5108, 0.1597],
         [0.6751, 0.7099]],

        [[0.7292, 0.1012],
         [0.7291, 0.5597]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0022526126173215573
Average Adjusted Rand Index: 0.002654529644347866
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24907.904296875
inf tensor(24907.9043, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9760.12890625
tensor(24907.9043, grad_fn=<NegBackward0>) tensor(9760.1289, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9759.615234375
tensor(9760.1289, grad_fn=<NegBackward0>) tensor(9759.6152, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9759.525390625
tensor(9759.6152, grad_fn=<NegBackward0>) tensor(9759.5254, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9759.451171875
tensor(9759.5254, grad_fn=<NegBackward0>) tensor(9759.4512, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9759.373046875
tensor(9759.4512, grad_fn=<NegBackward0>) tensor(9759.3730, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9759.298828125
tensor(9759.3730, grad_fn=<NegBackward0>) tensor(9759.2988, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9759.232421875
tensor(9759.2988, grad_fn=<NegBackward0>) tensor(9759.2324, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9759.1689453125
tensor(9759.2324, grad_fn=<NegBackward0>) tensor(9759.1689, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9759.1044921875
tensor(9759.1689, grad_fn=<NegBackward0>) tensor(9759.1045, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9759.048828125
tensor(9759.1045, grad_fn=<NegBackward0>) tensor(9759.0488, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9759.0087890625
tensor(9759.0488, grad_fn=<NegBackward0>) tensor(9759.0088, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9758.982421875
tensor(9759.0088, grad_fn=<NegBackward0>) tensor(9758.9824, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9758.966796875
tensor(9758.9824, grad_fn=<NegBackward0>) tensor(9758.9668, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9758.9580078125
tensor(9758.9668, grad_fn=<NegBackward0>) tensor(9758.9580, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9758.9501953125
tensor(9758.9580, grad_fn=<NegBackward0>) tensor(9758.9502, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9758.9423828125
tensor(9758.9502, grad_fn=<NegBackward0>) tensor(9758.9424, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9758.9345703125
tensor(9758.9424, grad_fn=<NegBackward0>) tensor(9758.9346, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9758.919921875
tensor(9758.9346, grad_fn=<NegBackward0>) tensor(9758.9199, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9758.8818359375
tensor(9758.9199, grad_fn=<NegBackward0>) tensor(9758.8818, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9758.740234375
tensor(9758.8818, grad_fn=<NegBackward0>) tensor(9758.7402, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9758.4267578125
tensor(9758.7402, grad_fn=<NegBackward0>) tensor(9758.4268, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9758.1572265625
tensor(9758.4268, grad_fn=<NegBackward0>) tensor(9758.1572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9758.0625
tensor(9758.1572, grad_fn=<NegBackward0>) tensor(9758.0625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9758.0234375
tensor(9758.0625, grad_fn=<NegBackward0>) tensor(9758.0234, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9758.0
tensor(9758.0234, grad_fn=<NegBackward0>) tensor(9758., grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9757.984375
tensor(9758., grad_fn=<NegBackward0>) tensor(9757.9844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9757.97265625
tensor(9757.9844, grad_fn=<NegBackward0>) tensor(9757.9727, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9757.9638671875
tensor(9757.9727, grad_fn=<NegBackward0>) tensor(9757.9639, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9757.95703125
tensor(9757.9639, grad_fn=<NegBackward0>) tensor(9757.9570, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9757.9521484375
tensor(9757.9570, grad_fn=<NegBackward0>) tensor(9757.9521, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9757.94921875
tensor(9757.9521, grad_fn=<NegBackward0>) tensor(9757.9492, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9757.9453125
tensor(9757.9492, grad_fn=<NegBackward0>) tensor(9757.9453, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9757.94140625
tensor(9757.9453, grad_fn=<NegBackward0>) tensor(9757.9414, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9757.939453125
tensor(9757.9414, grad_fn=<NegBackward0>) tensor(9757.9395, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9757.9365234375
tensor(9757.9395, grad_fn=<NegBackward0>) tensor(9757.9365, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9757.93359375
tensor(9757.9365, grad_fn=<NegBackward0>) tensor(9757.9336, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9757.9326171875
tensor(9757.9336, grad_fn=<NegBackward0>) tensor(9757.9326, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9757.9306640625
tensor(9757.9326, grad_fn=<NegBackward0>) tensor(9757.9307, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9757.9306640625
tensor(9757.9307, grad_fn=<NegBackward0>) tensor(9757.9307, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9757.9267578125
tensor(9757.9307, grad_fn=<NegBackward0>) tensor(9757.9268, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9757.9248046875
tensor(9757.9268, grad_fn=<NegBackward0>) tensor(9757.9248, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9757.9228515625
tensor(9757.9248, grad_fn=<NegBackward0>) tensor(9757.9229, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9757.9228515625
tensor(9757.9229, grad_fn=<NegBackward0>) tensor(9757.9229, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9757.921875
tensor(9757.9229, grad_fn=<NegBackward0>) tensor(9757.9219, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9757.9208984375
tensor(9757.9219, grad_fn=<NegBackward0>) tensor(9757.9209, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9757.919921875
tensor(9757.9209, grad_fn=<NegBackward0>) tensor(9757.9199, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9757.919921875
tensor(9757.9199, grad_fn=<NegBackward0>) tensor(9757.9199, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9757.9169921875
tensor(9757.9199, grad_fn=<NegBackward0>) tensor(9757.9170, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9757.9169921875
tensor(9757.9170, grad_fn=<NegBackward0>) tensor(9757.9170, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9757.916015625
tensor(9757.9170, grad_fn=<NegBackward0>) tensor(9757.9160, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9757.9150390625
tensor(9757.9160, grad_fn=<NegBackward0>) tensor(9757.9150, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9757.9150390625
tensor(9757.9150, grad_fn=<NegBackward0>) tensor(9757.9150, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9757.9130859375
tensor(9757.9150, grad_fn=<NegBackward0>) tensor(9757.9131, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9757.9130859375
tensor(9757.9131, grad_fn=<NegBackward0>) tensor(9757.9131, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9757.9111328125
tensor(9757.9131, grad_fn=<NegBackward0>) tensor(9757.9111, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9757.912109375
tensor(9757.9111, grad_fn=<NegBackward0>) tensor(9757.9121, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9757.912109375
tensor(9757.9111, grad_fn=<NegBackward0>) tensor(9757.9121, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9757.9091796875
tensor(9757.9111, grad_fn=<NegBackward0>) tensor(9757.9092, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9757.91015625
tensor(9757.9092, grad_fn=<NegBackward0>) tensor(9757.9102, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9757.91015625
tensor(9757.9092, grad_fn=<NegBackward0>) tensor(9757.9102, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -9757.908203125
tensor(9757.9092, grad_fn=<NegBackward0>) tensor(9757.9082, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9757.908203125
tensor(9757.9082, grad_fn=<NegBackward0>) tensor(9757.9082, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9757.908203125
tensor(9757.9082, grad_fn=<NegBackward0>) tensor(9757.9082, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9757.9072265625
tensor(9757.9082, grad_fn=<NegBackward0>) tensor(9757.9072, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9757.90625
tensor(9757.9072, grad_fn=<NegBackward0>) tensor(9757.9062, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9757.904296875
tensor(9757.9062, grad_fn=<NegBackward0>) tensor(9757.9043, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9757.9052734375
tensor(9757.9043, grad_fn=<NegBackward0>) tensor(9757.9053, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9757.9052734375
tensor(9757.9043, grad_fn=<NegBackward0>) tensor(9757.9053, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9757.904296875
tensor(9757.9043, grad_fn=<NegBackward0>) tensor(9757.9043, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9757.90234375
tensor(9757.9043, grad_fn=<NegBackward0>) tensor(9757.9023, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9757.9013671875
tensor(9757.9023, grad_fn=<NegBackward0>) tensor(9757.9014, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9757.9013671875
tensor(9757.9014, grad_fn=<NegBackward0>) tensor(9757.9014, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9757.90234375
tensor(9757.9014, grad_fn=<NegBackward0>) tensor(9757.9023, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9757.900390625
tensor(9757.9014, grad_fn=<NegBackward0>) tensor(9757.9004, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9757.900390625
tensor(9757.9004, grad_fn=<NegBackward0>) tensor(9757.9004, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9757.8994140625
tensor(9757.9004, grad_fn=<NegBackward0>) tensor(9757.8994, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9757.8984375
tensor(9757.8994, grad_fn=<NegBackward0>) tensor(9757.8984, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9757.8994140625
tensor(9757.8984, grad_fn=<NegBackward0>) tensor(9757.8994, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9757.8984375
tensor(9757.8984, grad_fn=<NegBackward0>) tensor(9757.8984, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9757.8974609375
tensor(9757.8984, grad_fn=<NegBackward0>) tensor(9757.8975, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9757.896484375
tensor(9757.8975, grad_fn=<NegBackward0>) tensor(9757.8965, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9757.8955078125
tensor(9757.8965, grad_fn=<NegBackward0>) tensor(9757.8955, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9757.896484375
tensor(9757.8955, grad_fn=<NegBackward0>) tensor(9757.8965, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -9757.89453125
tensor(9757.8955, grad_fn=<NegBackward0>) tensor(9757.8945, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9757.89453125
tensor(9757.8945, grad_fn=<NegBackward0>) tensor(9757.8945, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9757.8935546875
tensor(9757.8945, grad_fn=<NegBackward0>) tensor(9757.8936, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9757.892578125
tensor(9757.8936, grad_fn=<NegBackward0>) tensor(9757.8926, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9757.91015625
tensor(9757.8926, grad_fn=<NegBackward0>) tensor(9757.9102, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -9757.892578125
tensor(9757.8926, grad_fn=<NegBackward0>) tensor(9757.8926, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9757.8935546875
tensor(9757.8926, grad_fn=<NegBackward0>) tensor(9757.8936, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9757.8916015625
tensor(9757.8926, grad_fn=<NegBackward0>) tensor(9757.8916, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -9757.892578125
tensor(9757.8916, grad_fn=<NegBackward0>) tensor(9757.8926, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -9757.8916015625
tensor(9757.8916, grad_fn=<NegBackward0>) tensor(9757.8916, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9757.8896484375
tensor(9757.8916, grad_fn=<NegBackward0>) tensor(9757.8896, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9757.8916015625
tensor(9757.8896, grad_fn=<NegBackward0>) tensor(9757.8916, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -9757.8896484375
tensor(9757.8896, grad_fn=<NegBackward0>) tensor(9757.8896, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9757.9189453125
tensor(9757.8896, grad_fn=<NegBackward0>) tensor(9757.9189, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9757.8876953125
tensor(9757.8896, grad_fn=<NegBackward0>) tensor(9757.8877, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9757.888671875
tensor(9757.8877, grad_fn=<NegBackward0>) tensor(9757.8887, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9999e-01, 7.7882e-06],
        [4.7787e-04, 9.9952e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9897, 0.0103], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1337, 0.1118],
         [0.6366, 0.0466]],

        [[0.5498, 0.1711],
         [0.6237, 0.6221]],

        [[0.5871, 0.2558],
         [0.5593, 0.5787]],

        [[0.5460, 0.1596],
         [0.7069, 0.6554]],

        [[0.7102, 0.1012],
         [0.5017, 0.5005]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0022526126173215573
Average Adjusted Rand Index: 0.002654529644347866
[0.0022526126173215573, 0.0022526126173215573] [0.002654529644347866, 0.002654529644347866] [9757.8916015625, 9757.888671875]
-------------------------------------
This iteration is 91
True Objective function: Loss = -10336.826859855853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22597.5234375
inf tensor(22597.5234, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10216.3779296875
tensor(22597.5234, grad_fn=<NegBackward0>) tensor(10216.3779, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10214.388671875
tensor(10216.3779, grad_fn=<NegBackward0>) tensor(10214.3887, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10213.8603515625
tensor(10214.3887, grad_fn=<NegBackward0>) tensor(10213.8604, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10213.59765625
tensor(10213.8604, grad_fn=<NegBackward0>) tensor(10213.5977, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10213.310546875
tensor(10213.5977, grad_fn=<NegBackward0>) tensor(10213.3105, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10213.017578125
tensor(10213.3105, grad_fn=<NegBackward0>) tensor(10213.0176, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10212.93359375
tensor(10213.0176, grad_fn=<NegBackward0>) tensor(10212.9336, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10212.876953125
tensor(10212.9336, grad_fn=<NegBackward0>) tensor(10212.8770, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10212.8310546875
tensor(10212.8770, grad_fn=<NegBackward0>) tensor(10212.8311, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10212.7861328125
tensor(10212.8311, grad_fn=<NegBackward0>) tensor(10212.7861, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10212.744140625
tensor(10212.7861, grad_fn=<NegBackward0>) tensor(10212.7441, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10212.7021484375
tensor(10212.7441, grad_fn=<NegBackward0>) tensor(10212.7021, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10212.6630859375
tensor(10212.7021, grad_fn=<NegBackward0>) tensor(10212.6631, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10212.626953125
tensor(10212.6631, grad_fn=<NegBackward0>) tensor(10212.6270, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10212.5927734375
tensor(10212.6270, grad_fn=<NegBackward0>) tensor(10212.5928, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10212.560546875
tensor(10212.5928, grad_fn=<NegBackward0>) tensor(10212.5605, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10212.5263671875
tensor(10212.5605, grad_fn=<NegBackward0>) tensor(10212.5264, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10212.4931640625
tensor(10212.5264, grad_fn=<NegBackward0>) tensor(10212.4932, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10212.455078125
tensor(10212.4932, grad_fn=<NegBackward0>) tensor(10212.4551, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10212.4140625
tensor(10212.4551, grad_fn=<NegBackward0>) tensor(10212.4141, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10212.3662109375
tensor(10212.4141, grad_fn=<NegBackward0>) tensor(10212.3662, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10212.30859375
tensor(10212.3662, grad_fn=<NegBackward0>) tensor(10212.3086, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10212.251953125
tensor(10212.3086, grad_fn=<NegBackward0>) tensor(10212.2520, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10212.19921875
tensor(10212.2520, grad_fn=<NegBackward0>) tensor(10212.1992, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10212.1552734375
tensor(10212.1992, grad_fn=<NegBackward0>) tensor(10212.1553, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10212.1171875
tensor(10212.1553, grad_fn=<NegBackward0>) tensor(10212.1172, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10212.0888671875
tensor(10212.1172, grad_fn=<NegBackward0>) tensor(10212.0889, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10212.0595703125
tensor(10212.0889, grad_fn=<NegBackward0>) tensor(10212.0596, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10212.0361328125
tensor(10212.0596, grad_fn=<NegBackward0>) tensor(10212.0361, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10212.0146484375
tensor(10212.0361, grad_fn=<NegBackward0>) tensor(10212.0146, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10211.9970703125
tensor(10212.0146, grad_fn=<NegBackward0>) tensor(10211.9971, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10211.984375
tensor(10211.9971, grad_fn=<NegBackward0>) tensor(10211.9844, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10211.9697265625
tensor(10211.9844, grad_fn=<NegBackward0>) tensor(10211.9697, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10211.9599609375
tensor(10211.9697, grad_fn=<NegBackward0>) tensor(10211.9600, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10211.951171875
tensor(10211.9600, grad_fn=<NegBackward0>) tensor(10211.9512, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10211.943359375
tensor(10211.9512, grad_fn=<NegBackward0>) tensor(10211.9434, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10211.9365234375
tensor(10211.9434, grad_fn=<NegBackward0>) tensor(10211.9365, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10211.9326171875
tensor(10211.9365, grad_fn=<NegBackward0>) tensor(10211.9326, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10211.92578125
tensor(10211.9326, grad_fn=<NegBackward0>) tensor(10211.9258, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10211.9228515625
tensor(10211.9258, grad_fn=<NegBackward0>) tensor(10211.9229, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10211.919921875
tensor(10211.9229, grad_fn=<NegBackward0>) tensor(10211.9199, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10211.9208984375
tensor(10211.9199, grad_fn=<NegBackward0>) tensor(10211.9209, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -10211.9150390625
tensor(10211.9199, grad_fn=<NegBackward0>) tensor(10211.9150, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10211.9130859375
tensor(10211.9150, grad_fn=<NegBackward0>) tensor(10211.9131, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10211.9111328125
tensor(10211.9131, grad_fn=<NegBackward0>) tensor(10211.9111, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10211.91015625
tensor(10211.9111, grad_fn=<NegBackward0>) tensor(10211.9102, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10211.91015625
tensor(10211.9102, grad_fn=<NegBackward0>) tensor(10211.9102, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10211.9091796875
tensor(10211.9102, grad_fn=<NegBackward0>) tensor(10211.9092, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10211.912109375
tensor(10211.9092, grad_fn=<NegBackward0>) tensor(10211.9121, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10211.90625
tensor(10211.9092, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10211.9072265625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -10211.9072265625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -10211.90625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10211.908203125
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9082, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -10211.90625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10211.9072265625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -10211.90625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10211.90625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10211.90625
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10211.9052734375
tensor(10211.9062, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -10211.9091796875
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9092, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10211.9130859375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9131, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -10212.1064453125
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10212.1064, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.0199, 0.9801],
        [0.0843, 0.9157]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2672, 0.7328], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1646, 0.1537],
         [0.5918, 0.1433]],

        [[0.6905, 0.1654],
         [0.6260, 0.5448]],

        [[0.5469, 0.1260],
         [0.5652, 0.5309]],

        [[0.5441, 0.1410],
         [0.7307, 0.5688]],

        [[0.5809, 0.1680],
         [0.6344, 0.5684]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22485.576171875
inf tensor(22485.5762, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10214.8818359375
tensor(22485.5762, grad_fn=<NegBackward0>) tensor(10214.8818, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10213.7177734375
tensor(10214.8818, grad_fn=<NegBackward0>) tensor(10213.7178, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10213.4150390625
tensor(10213.7178, grad_fn=<NegBackward0>) tensor(10213.4150, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10213.0859375
tensor(10213.4150, grad_fn=<NegBackward0>) tensor(10213.0859, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10212.857421875
tensor(10213.0859, grad_fn=<NegBackward0>) tensor(10212.8574, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10212.7919921875
tensor(10212.8574, grad_fn=<NegBackward0>) tensor(10212.7920, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10212.7421875
tensor(10212.7920, grad_fn=<NegBackward0>) tensor(10212.7422, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10212.69921875
tensor(10212.7422, grad_fn=<NegBackward0>) tensor(10212.6992, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10212.6591796875
tensor(10212.6992, grad_fn=<NegBackward0>) tensor(10212.6592, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10212.6240234375
tensor(10212.6592, grad_fn=<NegBackward0>) tensor(10212.6240, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10212.59375
tensor(10212.6240, grad_fn=<NegBackward0>) tensor(10212.5938, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10212.5693359375
tensor(10212.5938, grad_fn=<NegBackward0>) tensor(10212.5693, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10212.546875
tensor(10212.5693, grad_fn=<NegBackward0>) tensor(10212.5469, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10212.5263671875
tensor(10212.5469, grad_fn=<NegBackward0>) tensor(10212.5264, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10212.5087890625
tensor(10212.5264, grad_fn=<NegBackward0>) tensor(10212.5088, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10212.490234375
tensor(10212.5088, grad_fn=<NegBackward0>) tensor(10212.4902, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10212.470703125
tensor(10212.4902, grad_fn=<NegBackward0>) tensor(10212.4707, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10212.447265625
tensor(10212.4707, grad_fn=<NegBackward0>) tensor(10212.4473, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10212.41796875
tensor(10212.4473, grad_fn=<NegBackward0>) tensor(10212.4180, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10212.376953125
tensor(10212.4180, grad_fn=<NegBackward0>) tensor(10212.3770, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10212.3212890625
tensor(10212.3770, grad_fn=<NegBackward0>) tensor(10212.3213, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10212.2470703125
tensor(10212.3213, grad_fn=<NegBackward0>) tensor(10212.2471, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10212.16796875
tensor(10212.2471, grad_fn=<NegBackward0>) tensor(10212.1680, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10212.1103515625
tensor(10212.1680, grad_fn=<NegBackward0>) tensor(10212.1104, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10212.0712890625
tensor(10212.1104, grad_fn=<NegBackward0>) tensor(10212.0713, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10212.04296875
tensor(10212.0713, grad_fn=<NegBackward0>) tensor(10212.0430, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10212.0244140625
tensor(10212.0430, grad_fn=<NegBackward0>) tensor(10212.0244, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10212.0107421875
tensor(10212.0244, grad_fn=<NegBackward0>) tensor(10212.0107, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10211.9990234375
tensor(10212.0107, grad_fn=<NegBackward0>) tensor(10211.9990, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10211.9873046875
tensor(10211.9990, grad_fn=<NegBackward0>) tensor(10211.9873, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10211.9775390625
tensor(10211.9873, grad_fn=<NegBackward0>) tensor(10211.9775, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10211.96875
tensor(10211.9775, grad_fn=<NegBackward0>) tensor(10211.9688, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10211.9599609375
tensor(10211.9688, grad_fn=<NegBackward0>) tensor(10211.9600, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10211.951171875
tensor(10211.9600, grad_fn=<NegBackward0>) tensor(10211.9512, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10211.9443359375
tensor(10211.9512, grad_fn=<NegBackward0>) tensor(10211.9443, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10211.9384765625
tensor(10211.9443, grad_fn=<NegBackward0>) tensor(10211.9385, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10211.931640625
tensor(10211.9385, grad_fn=<NegBackward0>) tensor(10211.9316, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10211.92578125
tensor(10211.9316, grad_fn=<NegBackward0>) tensor(10211.9258, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10211.921875
tensor(10211.9258, grad_fn=<NegBackward0>) tensor(10211.9219, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10211.9189453125
tensor(10211.9219, grad_fn=<NegBackward0>) tensor(10211.9189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10211.9150390625
tensor(10211.9189, grad_fn=<NegBackward0>) tensor(10211.9150, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10211.9140625
tensor(10211.9150, grad_fn=<NegBackward0>) tensor(10211.9141, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10211.9130859375
tensor(10211.9141, grad_fn=<NegBackward0>) tensor(10211.9131, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10211.9091796875
tensor(10211.9131, grad_fn=<NegBackward0>) tensor(10211.9092, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10211.9091796875
tensor(10211.9092, grad_fn=<NegBackward0>) tensor(10211.9092, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10211.908203125
tensor(10211.9092, grad_fn=<NegBackward0>) tensor(10211.9082, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10211.9072265625
tensor(10211.9082, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10211.9052734375
tensor(10211.9072, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10211.908203125
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9082, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10211.90625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -10211.908203125
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9082, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -10211.9052734375
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10211.9072265625
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9072, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -10211.904296875
tensor(10211.9053, grad_fn=<NegBackward0>) tensor(10211.9043, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10211.9052734375
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -10211.90625
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -10211.9052734375
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -10211.904296875
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9043, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10211.904296875
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9043, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10211.9052734375
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -10211.90625
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9062, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -10211.9140625
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9141, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -10211.9052734375
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9053, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -10211.91015625
tensor(10211.9043, grad_fn=<NegBackward0>) tensor(10211.9102, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.0138, 0.9862],
        [0.0854, 0.9146]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2619, 0.7381], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1650, 0.1542],
         [0.5206, 0.1420]],

        [[0.7001, 0.1660],
         [0.6558, 0.6530]],

        [[0.6348, 0.1256],
         [0.6264, 0.5073]],

        [[0.5434, 0.1407],
         [0.5875, 0.6393]],

        [[0.5012, 0.1684],
         [0.5670, 0.5500]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [10212.1064453125, 10211.91015625]
-------------------------------------
This iteration is 92
True Objective function: Loss = -10059.18639904651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23317.32421875
inf tensor(23317.3242, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9941.0703125
tensor(23317.3242, grad_fn=<NegBackward0>) tensor(9941.0703, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9939.609375
tensor(9941.0703, grad_fn=<NegBackward0>) tensor(9939.6094, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9939.2705078125
tensor(9939.6094, grad_fn=<NegBackward0>) tensor(9939.2705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9939.1484375
tensor(9939.2705, grad_fn=<NegBackward0>) tensor(9939.1484, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9939.0869140625
tensor(9939.1484, grad_fn=<NegBackward0>) tensor(9939.0869, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9939.041015625
tensor(9939.0869, grad_fn=<NegBackward0>) tensor(9939.0410, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9939.0068359375
tensor(9939.0410, grad_fn=<NegBackward0>) tensor(9939.0068, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9938.974609375
tensor(9939.0068, grad_fn=<NegBackward0>) tensor(9938.9746, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9938.9462890625
tensor(9938.9746, grad_fn=<NegBackward0>) tensor(9938.9463, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9938.91796875
tensor(9938.9463, grad_fn=<NegBackward0>) tensor(9938.9180, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9938.8935546875
tensor(9938.9180, grad_fn=<NegBackward0>) tensor(9938.8936, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9938.8662109375
tensor(9938.8936, grad_fn=<NegBackward0>) tensor(9938.8662, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9938.837890625
tensor(9938.8662, grad_fn=<NegBackward0>) tensor(9938.8379, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9938.8095703125
tensor(9938.8379, grad_fn=<NegBackward0>) tensor(9938.8096, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9938.779296875
tensor(9938.8096, grad_fn=<NegBackward0>) tensor(9938.7793, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9938.7451171875
tensor(9938.7793, grad_fn=<NegBackward0>) tensor(9938.7451, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9938.69921875
tensor(9938.7451, grad_fn=<NegBackward0>) tensor(9938.6992, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9938.634765625
tensor(9938.6992, grad_fn=<NegBackward0>) tensor(9938.6348, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9938.5263671875
tensor(9938.6348, grad_fn=<NegBackward0>) tensor(9938.5264, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9938.3583984375
tensor(9938.5264, grad_fn=<NegBackward0>) tensor(9938.3584, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9938.1923828125
tensor(9938.3584, grad_fn=<NegBackward0>) tensor(9938.1924, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9938.076171875
tensor(9938.1924, grad_fn=<NegBackward0>) tensor(9938.0762, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9938.005859375
tensor(9938.0762, grad_fn=<NegBackward0>) tensor(9938.0059, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9937.962890625
tensor(9938.0059, grad_fn=<NegBackward0>) tensor(9937.9629, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9937.931640625
tensor(9937.9629, grad_fn=<NegBackward0>) tensor(9937.9316, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9937.9072265625
tensor(9937.9316, grad_fn=<NegBackward0>) tensor(9937.9072, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9937.88671875
tensor(9937.9072, grad_fn=<NegBackward0>) tensor(9937.8867, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9937.8681640625
tensor(9937.8867, grad_fn=<NegBackward0>) tensor(9937.8682, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9937.8486328125
tensor(9937.8682, grad_fn=<NegBackward0>) tensor(9937.8486, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9937.830078125
tensor(9937.8486, grad_fn=<NegBackward0>) tensor(9937.8301, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9937.8095703125
tensor(9937.8301, grad_fn=<NegBackward0>) tensor(9937.8096, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9937.7880859375
tensor(9937.8096, grad_fn=<NegBackward0>) tensor(9937.7881, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9937.7587890625
tensor(9937.7881, grad_fn=<NegBackward0>) tensor(9937.7588, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9937.7294921875
tensor(9937.7588, grad_fn=<NegBackward0>) tensor(9937.7295, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9937.6943359375
tensor(9937.7295, grad_fn=<NegBackward0>) tensor(9937.6943, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9937.6572265625
tensor(9937.6943, grad_fn=<NegBackward0>) tensor(9937.6572, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9937.6162109375
tensor(9937.6572, grad_fn=<NegBackward0>) tensor(9937.6162, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9937.5703125
tensor(9937.6162, grad_fn=<NegBackward0>) tensor(9937.5703, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9937.525390625
tensor(9937.5703, grad_fn=<NegBackward0>) tensor(9937.5254, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9937.4765625
tensor(9937.5254, grad_fn=<NegBackward0>) tensor(9937.4766, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9937.4267578125
tensor(9937.4766, grad_fn=<NegBackward0>) tensor(9937.4268, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9937.3759765625
tensor(9937.4268, grad_fn=<NegBackward0>) tensor(9937.3760, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9937.3173828125
tensor(9937.3760, grad_fn=<NegBackward0>) tensor(9937.3174, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9937.25
tensor(9937.3174, grad_fn=<NegBackward0>) tensor(9937.2500, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9937.1826171875
tensor(9937.2500, grad_fn=<NegBackward0>) tensor(9937.1826, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9937.1328125
tensor(9937.1826, grad_fn=<NegBackward0>) tensor(9937.1328, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9937.099609375
tensor(9937.1328, grad_fn=<NegBackward0>) tensor(9937.0996, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9937.078125
tensor(9937.0996, grad_fn=<NegBackward0>) tensor(9937.0781, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9937.0615234375
tensor(9937.0781, grad_fn=<NegBackward0>) tensor(9937.0615, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9937.1181640625
tensor(9937.0615, grad_fn=<NegBackward0>) tensor(9937.1182, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9937.041015625
tensor(9937.0615, grad_fn=<NegBackward0>) tensor(9937.0410, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9937.033203125
tensor(9937.0410, grad_fn=<NegBackward0>) tensor(9937.0332, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9937.0283203125
tensor(9937.0332, grad_fn=<NegBackward0>) tensor(9937.0283, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9937.0224609375
tensor(9937.0283, grad_fn=<NegBackward0>) tensor(9937.0225, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9937.0205078125
tensor(9937.0225, grad_fn=<NegBackward0>) tensor(9937.0205, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9937.0234375
tensor(9937.0205, grad_fn=<NegBackward0>) tensor(9937.0234, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9937.015625
tensor(9937.0205, grad_fn=<NegBackward0>) tensor(9937.0156, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9937.013671875
tensor(9937.0156, grad_fn=<NegBackward0>) tensor(9937.0137, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9937.01171875
tensor(9937.0137, grad_fn=<NegBackward0>) tensor(9937.0117, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9937.013671875
tensor(9937.0117, grad_fn=<NegBackward0>) tensor(9937.0137, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9937.0087890625
tensor(9937.0117, grad_fn=<NegBackward0>) tensor(9937.0088, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9937.01953125
tensor(9937.0088, grad_fn=<NegBackward0>) tensor(9937.0195, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9937.0087890625
tensor(9937.0088, grad_fn=<NegBackward0>) tensor(9937.0088, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9937.005859375
tensor(9937.0088, grad_fn=<NegBackward0>) tensor(9937.0059, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9937.0068359375
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0068, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9937.005859375
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0059, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9937.017578125
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0176, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9937.005859375
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0059, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9937.0078125
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0078, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9937.00390625
tensor(9937.0059, grad_fn=<NegBackward0>) tensor(9937.0039, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9937.0048828125
tensor(9937.0039, grad_fn=<NegBackward0>) tensor(9937.0049, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9937.0048828125
tensor(9937.0039, grad_fn=<NegBackward0>) tensor(9937.0049, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9937.00390625
tensor(9937.0039, grad_fn=<NegBackward0>) tensor(9937.0039, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9937.0126953125
tensor(9937.0039, grad_fn=<NegBackward0>) tensor(9937.0127, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9937.0029296875
tensor(9937.0039, grad_fn=<NegBackward0>) tensor(9937.0029, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9937.009765625
tensor(9937.0029, grad_fn=<NegBackward0>) tensor(9937.0098, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9937.013671875
tensor(9937.0029, grad_fn=<NegBackward0>) tensor(9937.0137, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -9937.0947265625
tensor(9937.0029, grad_fn=<NegBackward0>) tensor(9937.0947, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -9937.0
tensor(9937.0029, grad_fn=<NegBackward0>) tensor(9937., grad_fn=<NegBackward0>)
Iteration 8000: Loss = -9937.0
tensor(9937., grad_fn=<NegBackward0>) tensor(9937., grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9937.0107421875
tensor(9937., grad_fn=<NegBackward0>) tensor(9937.0107, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9936.4892578125
tensor(9937., grad_fn=<NegBackward0>) tensor(9936.4893, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9935.875
tensor(9936.4893, grad_fn=<NegBackward0>) tensor(9935.8750, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9935.4658203125
tensor(9935.8750, grad_fn=<NegBackward0>) tensor(9935.4658, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9935.4482421875
tensor(9935.4658, grad_fn=<NegBackward0>) tensor(9935.4482, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9935.662109375
tensor(9935.4482, grad_fn=<NegBackward0>) tensor(9935.6621, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9935.4462890625
tensor(9935.4482, grad_fn=<NegBackward0>) tensor(9935.4463, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -9935.4443359375
tensor(9935.4463, grad_fn=<NegBackward0>) tensor(9935.4443, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9935.4443359375
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4443, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9935.4462890625
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4463, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9935.4453125
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4453, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -9935.4453125
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4453, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -9935.7177734375
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.7178, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -9935.4443359375
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4443, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -9935.4443359375
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4443, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9935.4697265625
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4697, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -9935.4453125
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4453, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -9935.443359375
tensor(9935.4443, grad_fn=<NegBackward0>) tensor(9935.4434, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -9935.4453125
tensor(9935.4434, grad_fn=<NegBackward0>) tensor(9935.4453, grad_fn=<NegBackward0>)
1
pi: tensor([[0.9611, 0.0389],
        [0.9584, 0.0416]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0120, 0.9880], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1429, 0.0704],
         [0.6349, 0.1322]],

        [[0.6813, 0.0737],
         [0.6973, 0.5703]],

        [[0.6261, 0.1500],
         [0.6699, 0.6917]],

        [[0.5186, 0.1198],
         [0.6449, 0.6949]],

        [[0.5054, 0.0786],
         [0.6900, 0.6224]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: -0.0008326733275004926
Average Adjusted Rand Index: 0.003122888692553136
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21898.51171875
inf tensor(21898.5117, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9941.416015625
tensor(21898.5117, grad_fn=<NegBackward0>) tensor(9941.4160, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9939.7119140625
tensor(9941.4160, grad_fn=<NegBackward0>) tensor(9939.7119, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9939.2607421875
tensor(9939.7119, grad_fn=<NegBackward0>) tensor(9939.2607, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9939.08984375
tensor(9939.2607, grad_fn=<NegBackward0>) tensor(9939.0898, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9939.0078125
tensor(9939.0898, grad_fn=<NegBackward0>) tensor(9939.0078, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9938.9599609375
tensor(9939.0078, grad_fn=<NegBackward0>) tensor(9938.9600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9938.923828125
tensor(9938.9600, grad_fn=<NegBackward0>) tensor(9938.9238, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9938.89453125
tensor(9938.9238, grad_fn=<NegBackward0>) tensor(9938.8945, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9938.869140625
tensor(9938.8945, grad_fn=<NegBackward0>) tensor(9938.8691, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9938.841796875
tensor(9938.8691, grad_fn=<NegBackward0>) tensor(9938.8418, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9938.81640625
tensor(9938.8418, grad_fn=<NegBackward0>) tensor(9938.8164, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9938.79296875
tensor(9938.8164, grad_fn=<NegBackward0>) tensor(9938.7930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9938.763671875
tensor(9938.7930, grad_fn=<NegBackward0>) tensor(9938.7637, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9938.732421875
tensor(9938.7637, grad_fn=<NegBackward0>) tensor(9938.7324, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9938.6962890625
tensor(9938.7324, grad_fn=<NegBackward0>) tensor(9938.6963, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9938.6533203125
tensor(9938.6963, grad_fn=<NegBackward0>) tensor(9938.6533, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9938.6005859375
tensor(9938.6533, grad_fn=<NegBackward0>) tensor(9938.6006, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9938.533203125
tensor(9938.6006, grad_fn=<NegBackward0>) tensor(9938.5332, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9938.4521484375
tensor(9938.5332, grad_fn=<NegBackward0>) tensor(9938.4521, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9938.3583984375
tensor(9938.4521, grad_fn=<NegBackward0>) tensor(9938.3584, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9938.2548828125
tensor(9938.3584, grad_fn=<NegBackward0>) tensor(9938.2549, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9938.1474609375
tensor(9938.2549, grad_fn=<NegBackward0>) tensor(9938.1475, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9938.0380859375
tensor(9938.1475, grad_fn=<NegBackward0>) tensor(9938.0381, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9937.9228515625
tensor(9938.0381, grad_fn=<NegBackward0>) tensor(9937.9229, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9937.8017578125
tensor(9937.9229, grad_fn=<NegBackward0>) tensor(9937.8018, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9937.669921875
tensor(9937.8018, grad_fn=<NegBackward0>) tensor(9937.6699, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9937.54296875
tensor(9937.6699, grad_fn=<NegBackward0>) tensor(9937.5430, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9937.4208984375
tensor(9937.5430, grad_fn=<NegBackward0>) tensor(9937.4209, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9937.3134765625
tensor(9937.4209, grad_fn=<NegBackward0>) tensor(9937.3135, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9937.23828125
tensor(9937.3135, grad_fn=<NegBackward0>) tensor(9937.2383, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9937.189453125
tensor(9937.2383, grad_fn=<NegBackward0>) tensor(9937.1895, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9937.15625
tensor(9937.1895, grad_fn=<NegBackward0>) tensor(9937.1562, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9937.1328125
tensor(9937.1562, grad_fn=<NegBackward0>) tensor(9937.1328, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9937.1142578125
tensor(9937.1328, grad_fn=<NegBackward0>) tensor(9937.1143, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9937.0986328125
tensor(9937.1143, grad_fn=<NegBackward0>) tensor(9937.0986, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9937.0859375
tensor(9937.0986, grad_fn=<NegBackward0>) tensor(9937.0859, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9937.078125
tensor(9937.0859, grad_fn=<NegBackward0>) tensor(9937.0781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9937.0703125
tensor(9937.0781, grad_fn=<NegBackward0>) tensor(9937.0703, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9937.064453125
tensor(9937.0703, grad_fn=<NegBackward0>) tensor(9937.0645, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9937.05859375
tensor(9937.0645, grad_fn=<NegBackward0>) tensor(9937.0586, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9937.0546875
tensor(9937.0586, grad_fn=<NegBackward0>) tensor(9937.0547, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9937.05078125
tensor(9937.0547, grad_fn=<NegBackward0>) tensor(9937.0508, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9937.0498046875
tensor(9937.0508, grad_fn=<NegBackward0>) tensor(9937.0498, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9937.0458984375
tensor(9937.0498, grad_fn=<NegBackward0>) tensor(9937.0459, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9937.0478515625
tensor(9937.0459, grad_fn=<NegBackward0>) tensor(9937.0479, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9937.041015625
tensor(9937.0459, grad_fn=<NegBackward0>) tensor(9937.0410, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9937.041015625
tensor(9937.0410, grad_fn=<NegBackward0>) tensor(9937.0410, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9937.0390625
tensor(9937.0410, grad_fn=<NegBackward0>) tensor(9937.0391, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9937.0380859375
tensor(9937.0391, grad_fn=<NegBackward0>) tensor(9937.0381, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9937.044921875
tensor(9937.0381, grad_fn=<NegBackward0>) tensor(9937.0449, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9937.0361328125
tensor(9937.0381, grad_fn=<NegBackward0>) tensor(9937.0361, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9937.0341796875
tensor(9937.0361, grad_fn=<NegBackward0>) tensor(9937.0342, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9937.033203125
tensor(9937.0342, grad_fn=<NegBackward0>) tensor(9937.0332, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9937.04296875
tensor(9937.0332, grad_fn=<NegBackward0>) tensor(9937.0430, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9937.033203125
tensor(9937.0332, grad_fn=<NegBackward0>) tensor(9937.0332, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9937.0498046875
tensor(9937.0332, grad_fn=<NegBackward0>) tensor(9937.0498, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9937.03125
tensor(9937.0332, grad_fn=<NegBackward0>) tensor(9937.0312, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9937.0302734375
tensor(9937.0312, grad_fn=<NegBackward0>) tensor(9937.0303, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9937.0302734375
tensor(9937.0303, grad_fn=<NegBackward0>) tensor(9937.0303, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9937.0302734375
tensor(9937.0303, grad_fn=<NegBackward0>) tensor(9937.0303, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9937.0302734375
tensor(9937.0303, grad_fn=<NegBackward0>) tensor(9937.0303, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9937.029296875
tensor(9937.0303, grad_fn=<NegBackward0>) tensor(9937.0293, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9937.025390625
tensor(9937.0293, grad_fn=<NegBackward0>) tensor(9937.0254, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9936.8369140625
tensor(9937.0254, grad_fn=<NegBackward0>) tensor(9936.8369, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9936.671875
tensor(9936.8369, grad_fn=<NegBackward0>) tensor(9936.6719, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9935.5029296875
tensor(9936.6719, grad_fn=<NegBackward0>) tensor(9935.5029, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9935.4970703125
tensor(9935.5029, grad_fn=<NegBackward0>) tensor(9935.4971, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9935.4951171875
tensor(9935.4971, grad_fn=<NegBackward0>) tensor(9935.4951, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9935.49609375
tensor(9935.4951, grad_fn=<NegBackward0>) tensor(9935.4961, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9935.4951171875
tensor(9935.4951, grad_fn=<NegBackward0>) tensor(9935.4951, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9935.494140625
tensor(9935.4951, grad_fn=<NegBackward0>) tensor(9935.4941, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9935.494140625
tensor(9935.4941, grad_fn=<NegBackward0>) tensor(9935.4941, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9935.494140625
tensor(9935.4941, grad_fn=<NegBackward0>) tensor(9935.4941, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -9935.4951171875
tensor(9935.4941, grad_fn=<NegBackward0>) tensor(9935.4951, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -9935.494140625
tensor(9935.4941, grad_fn=<NegBackward0>) tensor(9935.4941, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9935.4931640625
tensor(9935.4941, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9935.509765625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.5098, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9935.49609375
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4961, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9935.4931640625
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -9935.4970703125
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4971, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -9935.4921875
tensor(9935.4932, grad_fn=<NegBackward0>) tensor(9935.4922, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9935.4921875
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4922, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9935.4921875
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4922, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -9935.4931640625
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -9935.5234375
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.5234, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -9935.4931640625
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -9935.4931640625
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4932, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -9935.494140625
tensor(9935.4922, grad_fn=<NegBackward0>) tensor(9935.4941, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.0415, 0.9585],
        [0.0403, 0.9597]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9970e-01, 2.9762e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1307, 0.1191],
         [0.5869, 0.1430]],

        [[0.6807, 0.0738],
         [0.6562, 0.5287]],

        [[0.5639, 0.1495],
         [0.6190, 0.7139]],

        [[0.5535, 0.1198],
         [0.5885, 0.6785]],

        [[0.7152, 0.0791],
         [0.6450, 0.5185]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: -0.0008326733275004926
Average Adjusted Rand Index: 0.003122888692553136
[-0.0008326733275004926, -0.0008326733275004926] [0.003122888692553136, 0.003122888692553136] [9935.443359375, 9935.494140625]
-------------------------------------
This iteration is 93
True Objective function: Loss = -9929.482338531672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24467.93359375
inf tensor(24467.9336, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9811.8017578125
tensor(24467.9336, grad_fn=<NegBackward0>) tensor(9811.8018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9810.4150390625
tensor(9811.8018, grad_fn=<NegBackward0>) tensor(9810.4150, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9810.1220703125
tensor(9810.4150, grad_fn=<NegBackward0>) tensor(9810.1221, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9809.9892578125
tensor(9810.1221, grad_fn=<NegBackward0>) tensor(9809.9893, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9809.896484375
tensor(9809.9893, grad_fn=<NegBackward0>) tensor(9809.8965, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9809.8076171875
tensor(9809.8965, grad_fn=<NegBackward0>) tensor(9809.8076, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9809.7060546875
tensor(9809.8076, grad_fn=<NegBackward0>) tensor(9809.7061, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9809.58984375
tensor(9809.7061, grad_fn=<NegBackward0>) tensor(9809.5898, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9809.44140625
tensor(9809.5898, grad_fn=<NegBackward0>) tensor(9809.4414, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9809.2578125
tensor(9809.4414, grad_fn=<NegBackward0>) tensor(9809.2578, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9809.052734375
tensor(9809.2578, grad_fn=<NegBackward0>) tensor(9809.0527, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9808.841796875
tensor(9809.0527, grad_fn=<NegBackward0>) tensor(9808.8418, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9808.619140625
tensor(9808.8418, grad_fn=<NegBackward0>) tensor(9808.6191, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9808.3818359375
tensor(9808.6191, grad_fn=<NegBackward0>) tensor(9808.3818, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9808.146484375
tensor(9808.3818, grad_fn=<NegBackward0>) tensor(9808.1465, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9807.9228515625
tensor(9808.1465, grad_fn=<NegBackward0>) tensor(9807.9229, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9807.6474609375
tensor(9807.9229, grad_fn=<NegBackward0>) tensor(9807.6475, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9806.91015625
tensor(9807.6475, grad_fn=<NegBackward0>) tensor(9806.9102, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9805.9013671875
tensor(9806.9102, grad_fn=<NegBackward0>) tensor(9805.9014, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9805.357421875
tensor(9805.9014, grad_fn=<NegBackward0>) tensor(9805.3574, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9805.205078125
tensor(9805.3574, grad_fn=<NegBackward0>) tensor(9805.2051, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9805.216796875
tensor(9805.2051, grad_fn=<NegBackward0>) tensor(9805.2168, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -9805.1767578125
tensor(9805.2051, grad_fn=<NegBackward0>) tensor(9805.1768, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9805.162109375
tensor(9805.1768, grad_fn=<NegBackward0>) tensor(9805.1621, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9805.138671875
tensor(9805.1621, grad_fn=<NegBackward0>) tensor(9805.1387, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9805.1005859375
tensor(9805.1387, grad_fn=<NegBackward0>) tensor(9805.1006, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9805.0341796875
tensor(9805.1006, grad_fn=<NegBackward0>) tensor(9805.0342, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9804.927734375
tensor(9805.0342, grad_fn=<NegBackward0>) tensor(9804.9277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9804.802734375
tensor(9804.9277, grad_fn=<NegBackward0>) tensor(9804.8027, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9804.6875
tensor(9804.8027, grad_fn=<NegBackward0>) tensor(9804.6875, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9804.5830078125
tensor(9804.6875, grad_fn=<NegBackward0>) tensor(9804.5830, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9804.4736328125
tensor(9804.5830, grad_fn=<NegBackward0>) tensor(9804.4736, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9804.33984375
tensor(9804.4736, grad_fn=<NegBackward0>) tensor(9804.3398, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9804.212890625
tensor(9804.3398, grad_fn=<NegBackward0>) tensor(9804.2129, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9804.158203125
tensor(9804.2129, grad_fn=<NegBackward0>) tensor(9804.1582, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9804.1455078125
tensor(9804.1582, grad_fn=<NegBackward0>) tensor(9804.1455, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9804.1435546875
tensor(9804.1455, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9804.1435546875
tensor(9804.1436, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9804.14453125
tensor(9804.1436, grad_fn=<NegBackward0>) tensor(9804.1445, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -9804.142578125
tensor(9804.1436, grad_fn=<NegBackward0>) tensor(9804.1426, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9804.1435546875
tensor(9804.1426, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9804.1435546875
tensor(9804.1426, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -9804.1435546875
tensor(9804.1426, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -9804.1533203125
tensor(9804.1426, grad_fn=<NegBackward0>) tensor(9804.1533, grad_fn=<NegBackward0>)
4
Iteration 4500: Loss = -9804.1435546875
tensor(9804.1426, grad_fn=<NegBackward0>) tensor(9804.1436, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4500 due to no improvement.
pi: tensor([[0.2479, 0.7521],
        [0.2777, 0.7223]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1107, 0.8893], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0925, 0.0997],
         [0.6229, 0.1524]],

        [[0.6760, 0.1170],
         [0.7017, 0.6763]],

        [[0.7154, 0.1177],
         [0.6377, 0.6335]],

        [[0.6501, 0.1276],
         [0.5860, 0.6600]],

        [[0.5747, 0.1037],
         [0.5843, 0.6089]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.012148896880163282
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0037655832998208607
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.02704564545834377
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.061307918532710164
Global Adjusted Rand Index: 0.007497006537263478
Average Adjusted Rand Index: 0.01805444622134998
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23351.46875
inf tensor(23351.4688, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9811.2177734375
tensor(23351.4688, grad_fn=<NegBackward0>) tensor(9811.2178, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9810.1943359375
tensor(9811.2178, grad_fn=<NegBackward0>) tensor(9810.1943, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9809.841796875
tensor(9810.1943, grad_fn=<NegBackward0>) tensor(9809.8418, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9809.404296875
tensor(9809.8418, grad_fn=<NegBackward0>) tensor(9809.4043, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9808.98046875
tensor(9809.4043, grad_fn=<NegBackward0>) tensor(9808.9805, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9808.3359375
tensor(9808.9805, grad_fn=<NegBackward0>) tensor(9808.3359, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9807.849609375
tensor(9808.3359, grad_fn=<NegBackward0>) tensor(9807.8496, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9807.5126953125
tensor(9807.8496, grad_fn=<NegBackward0>) tensor(9807.5127, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9807.24609375
tensor(9807.5127, grad_fn=<NegBackward0>) tensor(9807.2461, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9807.05078125
tensor(9807.2461, grad_fn=<NegBackward0>) tensor(9807.0508, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9806.908203125
tensor(9807.0508, grad_fn=<NegBackward0>) tensor(9806.9082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9806.7919921875
tensor(9806.9082, grad_fn=<NegBackward0>) tensor(9806.7920, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9806.673828125
tensor(9806.7920, grad_fn=<NegBackward0>) tensor(9806.6738, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9806.5693359375
tensor(9806.6738, grad_fn=<NegBackward0>) tensor(9806.5693, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9806.501953125
tensor(9806.5693, grad_fn=<NegBackward0>) tensor(9806.5020, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9806.4609375
tensor(9806.5020, grad_fn=<NegBackward0>) tensor(9806.4609, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9806.4345703125
tensor(9806.4609, grad_fn=<NegBackward0>) tensor(9806.4346, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9806.4189453125
tensor(9806.4346, grad_fn=<NegBackward0>) tensor(9806.4189, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9806.4111328125
tensor(9806.4189, grad_fn=<NegBackward0>) tensor(9806.4111, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9806.4052734375
tensor(9806.4111, grad_fn=<NegBackward0>) tensor(9806.4053, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9806.400390625
tensor(9806.4053, grad_fn=<NegBackward0>) tensor(9806.4004, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9806.396484375
tensor(9806.4004, grad_fn=<NegBackward0>) tensor(9806.3965, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9806.3955078125
tensor(9806.3965, grad_fn=<NegBackward0>) tensor(9806.3955, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9806.3935546875
tensor(9806.3955, grad_fn=<NegBackward0>) tensor(9806.3936, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9806.390625
tensor(9806.3936, grad_fn=<NegBackward0>) tensor(9806.3906, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9806.388671875
tensor(9806.3906, grad_fn=<NegBackward0>) tensor(9806.3887, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9806.38671875
tensor(9806.3887, grad_fn=<NegBackward0>) tensor(9806.3867, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9806.3857421875
tensor(9806.3867, grad_fn=<NegBackward0>) tensor(9806.3857, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9806.384765625
tensor(9806.3857, grad_fn=<NegBackward0>) tensor(9806.3848, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9806.3837890625
tensor(9806.3848, grad_fn=<NegBackward0>) tensor(9806.3838, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9806.3828125
tensor(9806.3838, grad_fn=<NegBackward0>) tensor(9806.3828, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9806.3828125
tensor(9806.3828, grad_fn=<NegBackward0>) tensor(9806.3828, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9806.3818359375
tensor(9806.3828, grad_fn=<NegBackward0>) tensor(9806.3818, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9806.3828125
tensor(9806.3818, grad_fn=<NegBackward0>) tensor(9806.3828, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9806.380859375
tensor(9806.3818, grad_fn=<NegBackward0>) tensor(9806.3809, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9806.380859375
tensor(9806.3809, grad_fn=<NegBackward0>) tensor(9806.3809, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9806.380859375
tensor(9806.3809, grad_fn=<NegBackward0>) tensor(9806.3809, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9806.3798828125
tensor(9806.3809, grad_fn=<NegBackward0>) tensor(9806.3799, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9806.37890625
tensor(9806.3799, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9806.3798828125
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3799, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9806.37890625
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9806.37890625
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9806.37890625
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9806.37890625
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9806.3779296875
tensor(9806.3789, grad_fn=<NegBackward0>) tensor(9806.3779, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9806.3779296875
tensor(9806.3779, grad_fn=<NegBackward0>) tensor(9806.3779, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9806.37890625
tensor(9806.3779, grad_fn=<NegBackward0>) tensor(9806.3789, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9806.373046875
tensor(9806.3779, grad_fn=<NegBackward0>) tensor(9806.3730, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9805.9853515625
tensor(9806.3730, grad_fn=<NegBackward0>) tensor(9805.9854, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9805.9833984375
tensor(9805.9854, grad_fn=<NegBackward0>) tensor(9805.9834, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9805.982421875
tensor(9805.9834, grad_fn=<NegBackward0>) tensor(9805.9824, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9805.9833984375
tensor(9805.9824, grad_fn=<NegBackward0>) tensor(9805.9834, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9805.9814453125
tensor(9805.9824, grad_fn=<NegBackward0>) tensor(9805.9814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9805.9833984375
tensor(9805.9814, grad_fn=<NegBackward0>) tensor(9805.9834, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9805.982421875
tensor(9805.9814, grad_fn=<NegBackward0>) tensor(9805.9824, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -9805.984375
tensor(9805.9814, grad_fn=<NegBackward0>) tensor(9805.9844, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -9805.982421875
tensor(9805.9814, grad_fn=<NegBackward0>) tensor(9805.9824, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -9805.982421875
tensor(9805.9814, grad_fn=<NegBackward0>) tensor(9805.9824, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.9395, 0.0605],
        [0.9591, 0.0409]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0657, 0.9343], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1348, 0.0902],
         [0.6574, 0.1489]],

        [[0.6481, 0.1139],
         [0.6979, 0.5191]],

        [[0.7124, 0.1214],
         [0.7207, 0.5144]],

        [[0.5461, 0.1770],
         [0.6168, 0.6133]],

        [[0.5253, 0.0794],
         [0.6315, 0.6228]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001191810578917815
Average Adjusted Rand Index: 0.0
[0.007497006537263478, -0.001191810578917815] [0.01805444622134998, 0.0] [9804.1435546875, 9805.982421875]
-------------------------------------
This iteration is 94
True Objective function: Loss = -10017.225543641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19767.181640625
inf tensor(19767.1816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9926.37890625
tensor(19767.1816, grad_fn=<NegBackward0>) tensor(9926.3789, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9925.822265625
tensor(9926.3789, grad_fn=<NegBackward0>) tensor(9925.8223, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9925.0810546875
tensor(9925.8223, grad_fn=<NegBackward0>) tensor(9925.0811, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9923.9326171875
tensor(9925.0811, grad_fn=<NegBackward0>) tensor(9923.9326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9923.583984375
tensor(9923.9326, grad_fn=<NegBackward0>) tensor(9923.5840, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9923.4384765625
tensor(9923.5840, grad_fn=<NegBackward0>) tensor(9923.4385, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9923.3486328125
tensor(9923.4385, grad_fn=<NegBackward0>) tensor(9923.3486, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9923.1416015625
tensor(9923.3486, grad_fn=<NegBackward0>) tensor(9923.1416, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9921.6083984375
tensor(9923.1416, grad_fn=<NegBackward0>) tensor(9921.6084, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9920.9892578125
tensor(9921.6084, grad_fn=<NegBackward0>) tensor(9920.9893, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9920.765625
tensor(9920.9893, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9920.640625
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.6406, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9920.5810546875
tensor(9920.6406, grad_fn=<NegBackward0>) tensor(9920.5811, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9920.544921875
tensor(9920.5811, grad_fn=<NegBackward0>) tensor(9920.5449, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9920.51953125
tensor(9920.5449, grad_fn=<NegBackward0>) tensor(9920.5195, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9920.501953125
tensor(9920.5195, grad_fn=<NegBackward0>) tensor(9920.5020, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9920.4892578125
tensor(9920.5020, grad_fn=<NegBackward0>) tensor(9920.4893, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9920.4775390625
tensor(9920.4893, grad_fn=<NegBackward0>) tensor(9920.4775, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9920.4677734375
tensor(9920.4775, grad_fn=<NegBackward0>) tensor(9920.4678, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9920.4619140625
tensor(9920.4678, grad_fn=<NegBackward0>) tensor(9920.4619, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9920.4541015625
tensor(9920.4619, grad_fn=<NegBackward0>) tensor(9920.4541, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9920.44921875
tensor(9920.4541, grad_fn=<NegBackward0>) tensor(9920.4492, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9920.4453125
tensor(9920.4492, grad_fn=<NegBackward0>) tensor(9920.4453, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9920.44140625
tensor(9920.4453, grad_fn=<NegBackward0>) tensor(9920.4414, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9920.4375
tensor(9920.4414, grad_fn=<NegBackward0>) tensor(9920.4375, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9920.43359375
tensor(9920.4375, grad_fn=<NegBackward0>) tensor(9920.4336, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9920.4306640625
tensor(9920.4336, grad_fn=<NegBackward0>) tensor(9920.4307, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9920.427734375
tensor(9920.4307, grad_fn=<NegBackward0>) tensor(9920.4277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9920.4248046875
tensor(9920.4277, grad_fn=<NegBackward0>) tensor(9920.4248, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9920.423828125
tensor(9920.4248, grad_fn=<NegBackward0>) tensor(9920.4238, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9920.419921875
tensor(9920.4238, grad_fn=<NegBackward0>) tensor(9920.4199, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9920.4189453125
tensor(9920.4199, grad_fn=<NegBackward0>) tensor(9920.4189, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9920.4150390625
tensor(9920.4189, grad_fn=<NegBackward0>) tensor(9920.4150, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9920.4140625
tensor(9920.4150, grad_fn=<NegBackward0>) tensor(9920.4141, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9920.4130859375
tensor(9920.4141, grad_fn=<NegBackward0>) tensor(9920.4131, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9920.412109375
tensor(9920.4131, grad_fn=<NegBackward0>) tensor(9920.4121, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9920.4091796875
tensor(9920.4121, grad_fn=<NegBackward0>) tensor(9920.4092, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9920.4091796875
tensor(9920.4092, grad_fn=<NegBackward0>) tensor(9920.4092, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9920.4072265625
tensor(9920.4092, grad_fn=<NegBackward0>) tensor(9920.4072, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9920.4072265625
tensor(9920.4072, grad_fn=<NegBackward0>) tensor(9920.4072, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9920.40625
tensor(9920.4072, grad_fn=<NegBackward0>) tensor(9920.4062, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9920.4052734375
tensor(9920.4062, grad_fn=<NegBackward0>) tensor(9920.4053, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9920.4052734375
tensor(9920.4053, grad_fn=<NegBackward0>) tensor(9920.4053, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9920.4033203125
tensor(9920.4053, grad_fn=<NegBackward0>) tensor(9920.4033, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9920.404296875
tensor(9920.4033, grad_fn=<NegBackward0>) tensor(9920.4043, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9920.4033203125
tensor(9920.4033, grad_fn=<NegBackward0>) tensor(9920.4033, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9920.4033203125
tensor(9920.4033, grad_fn=<NegBackward0>) tensor(9920.4033, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9920.40234375
tensor(9920.4033, grad_fn=<NegBackward0>) tensor(9920.4023, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9920.40234375
tensor(9920.4023, grad_fn=<NegBackward0>) tensor(9920.4023, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9920.40234375
tensor(9920.4023, grad_fn=<NegBackward0>) tensor(9920.4023, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9920.4013671875
tensor(9920.4023, grad_fn=<NegBackward0>) tensor(9920.4014, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9920.40234375
tensor(9920.4014, grad_fn=<NegBackward0>) tensor(9920.4023, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9920.4013671875
tensor(9920.4014, grad_fn=<NegBackward0>) tensor(9920.4014, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9920.400390625
tensor(9920.4014, grad_fn=<NegBackward0>) tensor(9920.4004, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9920.400390625
tensor(9920.4004, grad_fn=<NegBackward0>) tensor(9920.4004, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9920.3994140625
tensor(9920.4004, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9920.400390625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.4004, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9920.3994140625
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9920.3984375
tensor(9920.3994, grad_fn=<NegBackward0>) tensor(9920.3984, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9920.3994140625
tensor(9920.3984, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9920.3974609375
tensor(9920.3984, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9920.3984375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3984, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9920.3984375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3984, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9920.3984375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3984, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9920.3994140625
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9920.3974609375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9920.3994140625
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3994, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9920.4189453125
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.4189, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9920.396484375
tensor(9920.3975, grad_fn=<NegBackward0>) tensor(9920.3965, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9920.3984375
tensor(9920.3965, grad_fn=<NegBackward0>) tensor(9920.3984, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9920.40234375
tensor(9920.3965, grad_fn=<NegBackward0>) tensor(9920.4023, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -9920.3955078125
tensor(9920.3965, grad_fn=<NegBackward0>) tensor(9920.3955, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9920.5107421875
tensor(9920.3955, grad_fn=<NegBackward0>) tensor(9920.5107, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -9920.3974609375
tensor(9920.3955, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -9920.3974609375
tensor(9920.3955, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -9920.3974609375
tensor(9920.3955, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -9920.3974609375
tensor(9920.3955, grad_fn=<NegBackward0>) tensor(9920.3975, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[7.0096e-01, 2.9904e-01],
        [6.7601e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0712, 0.9288], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1393, 0.1696],
         [0.5942, 0.1341]],

        [[0.7143, 0.1856],
         [0.6199, 0.5869]],

        [[0.5180, 0.2020],
         [0.6306, 0.6616]],

        [[0.5738, 0.2400],
         [0.7090, 0.7166]],

        [[0.7196, 0.1097],
         [0.7202, 0.7094]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.011530202595462608
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.0038486510285712307
Average Adjusted Rand Index: 0.004053420398619875
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24454.1484375
inf tensor(24454.1484, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9927.958984375
tensor(24454.1484, grad_fn=<NegBackward0>) tensor(9927.9590, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9926.3486328125
tensor(9927.9590, grad_fn=<NegBackward0>) tensor(9926.3486, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9925.857421875
tensor(9926.3486, grad_fn=<NegBackward0>) tensor(9925.8574, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9925.5830078125
tensor(9925.8574, grad_fn=<NegBackward0>) tensor(9925.5830, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9925.421875
tensor(9925.5830, grad_fn=<NegBackward0>) tensor(9925.4219, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9925.30859375
tensor(9925.4219, grad_fn=<NegBackward0>) tensor(9925.3086, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9925.21484375
tensor(9925.3086, grad_fn=<NegBackward0>) tensor(9925.2148, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9925.1259765625
tensor(9925.2148, grad_fn=<NegBackward0>) tensor(9925.1260, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9925.0361328125
tensor(9925.1260, grad_fn=<NegBackward0>) tensor(9925.0361, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9924.9296875
tensor(9925.0361, grad_fn=<NegBackward0>) tensor(9924.9297, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9924.7646484375
tensor(9924.9297, grad_fn=<NegBackward0>) tensor(9924.7646, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9924.568359375
tensor(9924.7646, grad_fn=<NegBackward0>) tensor(9924.5684, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9924.47265625
tensor(9924.5684, grad_fn=<NegBackward0>) tensor(9924.4727, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9924.3857421875
tensor(9924.4727, grad_fn=<NegBackward0>) tensor(9924.3857, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9924.3076171875
tensor(9924.3857, grad_fn=<NegBackward0>) tensor(9924.3076, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9924.2412109375
tensor(9924.3076, grad_fn=<NegBackward0>) tensor(9924.2412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9924.1875
tensor(9924.2412, grad_fn=<NegBackward0>) tensor(9924.1875, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9924.1484375
tensor(9924.1875, grad_fn=<NegBackward0>) tensor(9924.1484, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9924.1201171875
tensor(9924.1484, grad_fn=<NegBackward0>) tensor(9924.1201, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9924.10546875
tensor(9924.1201, grad_fn=<NegBackward0>) tensor(9924.1055, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9924.095703125
tensor(9924.1055, grad_fn=<NegBackward0>) tensor(9924.0957, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9924.0888671875
tensor(9924.0957, grad_fn=<NegBackward0>) tensor(9924.0889, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9924.083984375
tensor(9924.0889, grad_fn=<NegBackward0>) tensor(9924.0840, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9924.080078125
tensor(9924.0840, grad_fn=<NegBackward0>) tensor(9924.0801, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9924.0791015625
tensor(9924.0801, grad_fn=<NegBackward0>) tensor(9924.0791, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9924.076171875
tensor(9924.0791, grad_fn=<NegBackward0>) tensor(9924.0762, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9924.0732421875
tensor(9924.0762, grad_fn=<NegBackward0>) tensor(9924.0732, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9924.0732421875
tensor(9924.0732, grad_fn=<NegBackward0>) tensor(9924.0732, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9924.0712890625
tensor(9924.0732, grad_fn=<NegBackward0>) tensor(9924.0713, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9924.0712890625
tensor(9924.0713, grad_fn=<NegBackward0>) tensor(9924.0713, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9924.0703125
tensor(9924.0713, grad_fn=<NegBackward0>) tensor(9924.0703, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9924.0693359375
tensor(9924.0703, grad_fn=<NegBackward0>) tensor(9924.0693, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9924.0693359375
tensor(9924.0693, grad_fn=<NegBackward0>) tensor(9924.0693, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9924.072265625
tensor(9924.0693, grad_fn=<NegBackward0>) tensor(9924.0723, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9924.0673828125
tensor(9924.0693, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9924.0673828125
tensor(9924.0674, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9924.0673828125
tensor(9924.0674, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9924.0673828125
tensor(9924.0674, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9924.0673828125
tensor(9924.0674, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9924.06640625
tensor(9924.0674, grad_fn=<NegBackward0>) tensor(9924.0664, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9924.0673828125
tensor(9924.0664, grad_fn=<NegBackward0>) tensor(9924.0674, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9924.0654296875
tensor(9924.0664, grad_fn=<NegBackward0>) tensor(9924.0654, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9924.06640625
tensor(9924.0654, grad_fn=<NegBackward0>) tensor(9924.0664, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9924.064453125
tensor(9924.0654, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9924.064453125
tensor(9924.0645, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9924.0634765625
tensor(9924.0645, grad_fn=<NegBackward0>) tensor(9924.0635, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9924.064453125
tensor(9924.0635, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9924.0634765625
tensor(9924.0635, grad_fn=<NegBackward0>) tensor(9924.0635, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9924.0634765625
tensor(9924.0635, grad_fn=<NegBackward0>) tensor(9924.0635, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9924.064453125
tensor(9924.0635, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9924.0625
tensor(9924.0635, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9924.0625
tensor(9924.0625, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9924.0625
tensor(9924.0625, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9924.0634765625
tensor(9924.0625, grad_fn=<NegBackward0>) tensor(9924.0635, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9924.0615234375
tensor(9924.0625, grad_fn=<NegBackward0>) tensor(9924.0615, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9924.064453125
tensor(9924.0615, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9924.0625
tensor(9924.0615, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -9924.064453125
tensor(9924.0615, grad_fn=<NegBackward0>) tensor(9924.0645, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -9924.0625
tensor(9924.0615, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -9924.0625
tensor(9924.0615, grad_fn=<NegBackward0>) tensor(9924.0625, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9519, 0.0481],
        [0.9985, 0.0015]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9174, 0.0826], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1365, 0.1562],
         [0.5883, 0.1725]],

        [[0.7209, 0.1680],
         [0.7019, 0.6461]],

        [[0.6564, 0.1112],
         [0.6166, 0.5188]],

        [[0.6663, 0.1945],
         [0.6917, 0.7001]],

        [[0.6394, 0.1061],
         [0.6615, 0.6986]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: -0.0005926784880256398
[0.0038486510285712307, 0.0] [0.004053420398619875, -0.0005926784880256398] [9920.3974609375, 9924.0625]
-------------------------------------
This iteration is 95
True Objective function: Loss = -10119.642474257842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22528.703125
inf tensor(22528.7031, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10019.712890625
tensor(22528.7031, grad_fn=<NegBackward0>) tensor(10019.7129, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10018.236328125
tensor(10019.7129, grad_fn=<NegBackward0>) tensor(10018.2363, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10017.849609375
tensor(10018.2363, grad_fn=<NegBackward0>) tensor(10017.8496, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10017.6953125
tensor(10017.8496, grad_fn=<NegBackward0>) tensor(10017.6953, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10017.6162109375
tensor(10017.6953, grad_fn=<NegBackward0>) tensor(10017.6162, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10017.56640625
tensor(10017.6162, grad_fn=<NegBackward0>) tensor(10017.5664, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10017.53125
tensor(10017.5664, grad_fn=<NegBackward0>) tensor(10017.5312, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10017.50390625
tensor(10017.5312, grad_fn=<NegBackward0>) tensor(10017.5039, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10017.4833984375
tensor(10017.5039, grad_fn=<NegBackward0>) tensor(10017.4834, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10017.4658203125
tensor(10017.4834, grad_fn=<NegBackward0>) tensor(10017.4658, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10017.451171875
tensor(10017.4658, grad_fn=<NegBackward0>) tensor(10017.4512, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10017.4384765625
tensor(10017.4512, grad_fn=<NegBackward0>) tensor(10017.4385, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10017.427734375
tensor(10017.4385, grad_fn=<NegBackward0>) tensor(10017.4277, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10017.4189453125
tensor(10017.4277, grad_fn=<NegBackward0>) tensor(10017.4189, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10017.4111328125
tensor(10017.4189, grad_fn=<NegBackward0>) tensor(10017.4111, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10017.4052734375
tensor(10017.4111, grad_fn=<NegBackward0>) tensor(10017.4053, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10017.3984375
tensor(10017.4053, grad_fn=<NegBackward0>) tensor(10017.3984, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10017.3935546875
tensor(10017.3984, grad_fn=<NegBackward0>) tensor(10017.3936, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10017.3896484375
tensor(10017.3936, grad_fn=<NegBackward0>) tensor(10017.3896, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10017.384765625
tensor(10017.3896, grad_fn=<NegBackward0>) tensor(10017.3848, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10017.3818359375
tensor(10017.3848, grad_fn=<NegBackward0>) tensor(10017.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10017.3779296875
tensor(10017.3818, grad_fn=<NegBackward0>) tensor(10017.3779, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10017.373046875
tensor(10017.3779, grad_fn=<NegBackward0>) tensor(10017.3730, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10017.37109375
tensor(10017.3730, grad_fn=<NegBackward0>) tensor(10017.3711, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10017.3681640625
tensor(10017.3711, grad_fn=<NegBackward0>) tensor(10017.3682, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10017.365234375
tensor(10017.3682, grad_fn=<NegBackward0>) tensor(10017.3652, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10017.3623046875
tensor(10017.3652, grad_fn=<NegBackward0>) tensor(10017.3623, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10017.3603515625
tensor(10017.3623, grad_fn=<NegBackward0>) tensor(10017.3604, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10017.357421875
tensor(10017.3604, grad_fn=<NegBackward0>) tensor(10017.3574, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10017.35546875
tensor(10017.3574, grad_fn=<NegBackward0>) tensor(10017.3555, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10017.3525390625
tensor(10017.3555, grad_fn=<NegBackward0>) tensor(10017.3525, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10017.349609375
tensor(10017.3525, grad_fn=<NegBackward0>) tensor(10017.3496, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10017.3486328125
tensor(10017.3496, grad_fn=<NegBackward0>) tensor(10017.3486, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10017.3466796875
tensor(10017.3486, grad_fn=<NegBackward0>) tensor(10017.3467, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10017.34375
tensor(10017.3467, grad_fn=<NegBackward0>) tensor(10017.3438, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10017.3408203125
tensor(10017.3438, grad_fn=<NegBackward0>) tensor(10017.3408, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10017.3388671875
tensor(10017.3408, grad_fn=<NegBackward0>) tensor(10017.3389, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10017.3359375
tensor(10017.3389, grad_fn=<NegBackward0>) tensor(10017.3359, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10017.3330078125
tensor(10017.3359, grad_fn=<NegBackward0>) tensor(10017.3330, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10017.328125
tensor(10017.3330, grad_fn=<NegBackward0>) tensor(10017.3281, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10017.3251953125
tensor(10017.3281, grad_fn=<NegBackward0>) tensor(10017.3252, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10017.322265625
tensor(10017.3252, grad_fn=<NegBackward0>) tensor(10017.3223, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10017.3193359375
tensor(10017.3223, grad_fn=<NegBackward0>) tensor(10017.3193, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10017.3154296875
tensor(10017.3193, grad_fn=<NegBackward0>) tensor(10017.3154, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10017.3125
tensor(10017.3154, grad_fn=<NegBackward0>) tensor(10017.3125, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10017.3076171875
tensor(10017.3125, grad_fn=<NegBackward0>) tensor(10017.3076, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10017.3037109375
tensor(10017.3076, grad_fn=<NegBackward0>) tensor(10017.3037, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10017.2998046875
tensor(10017.3037, grad_fn=<NegBackward0>) tensor(10017.2998, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10017.2939453125
tensor(10017.2998, grad_fn=<NegBackward0>) tensor(10017.2939, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10017.2890625
tensor(10017.2939, grad_fn=<NegBackward0>) tensor(10017.2891, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10017.2841796875
tensor(10017.2891, grad_fn=<NegBackward0>) tensor(10017.2842, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10017.27734375
tensor(10017.2842, grad_fn=<NegBackward0>) tensor(10017.2773, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10017.2724609375
tensor(10017.2773, grad_fn=<NegBackward0>) tensor(10017.2725, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10017.265625
tensor(10017.2725, grad_fn=<NegBackward0>) tensor(10017.2656, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10017.2587890625
tensor(10017.2656, grad_fn=<NegBackward0>) tensor(10017.2588, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10017.2548828125
tensor(10017.2588, grad_fn=<NegBackward0>) tensor(10017.2549, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10017.25
tensor(10017.2549, grad_fn=<NegBackward0>) tensor(10017.2500, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10017.24609375
tensor(10017.2500, grad_fn=<NegBackward0>) tensor(10017.2461, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10017.2431640625
tensor(10017.2461, grad_fn=<NegBackward0>) tensor(10017.2432, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10017.240234375
tensor(10017.2432, grad_fn=<NegBackward0>) tensor(10017.2402, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10017.23828125
tensor(10017.2402, grad_fn=<NegBackward0>) tensor(10017.2383, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10017.236328125
tensor(10017.2383, grad_fn=<NegBackward0>) tensor(10017.2363, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10017.2353515625
tensor(10017.2363, grad_fn=<NegBackward0>) tensor(10017.2354, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10017.234375
tensor(10017.2354, grad_fn=<NegBackward0>) tensor(10017.2344, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10017.232421875
tensor(10017.2344, grad_fn=<NegBackward0>) tensor(10017.2324, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10017.232421875
tensor(10017.2324, grad_fn=<NegBackward0>) tensor(10017.2324, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10017.2314453125
tensor(10017.2324, grad_fn=<NegBackward0>) tensor(10017.2314, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10017.244140625
tensor(10017.2314, grad_fn=<NegBackward0>) tensor(10017.2441, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -10017.2294921875
tensor(10017.2314, grad_fn=<NegBackward0>) tensor(10017.2295, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10017.2294921875
tensor(10017.2295, grad_fn=<NegBackward0>) tensor(10017.2295, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -10017.228515625
tensor(10017.2295, grad_fn=<NegBackward0>) tensor(10017.2285, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10017.228515625
tensor(10017.2285, grad_fn=<NegBackward0>) tensor(10017.2285, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10017.228515625
tensor(10017.2285, grad_fn=<NegBackward0>) tensor(10017.2285, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10017.2275390625
tensor(10017.2285, grad_fn=<NegBackward0>) tensor(10017.2275, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -10017.2314453125
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2314, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -10017.2275390625
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2275, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -10017.2783203125
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2783, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -10017.2265625
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2266, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10017.23046875
tensor(10017.2266, grad_fn=<NegBackward0>) tensor(10017.2305, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10017.2236328125
tensor(10017.2266, grad_fn=<NegBackward0>) tensor(10017.2236, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10017.2265625
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2266, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -10017.2685546875
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2686, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -10017.2294921875
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2295, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -10017.228515625
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2285, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -10017.2236328125
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2236, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10017.22265625
tensor(10017.2236, grad_fn=<NegBackward0>) tensor(10017.2227, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -10017.21875
tensor(10017.2227, grad_fn=<NegBackward0>) tensor(10017.2188, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -10017.2177734375
tensor(10017.2188, grad_fn=<NegBackward0>) tensor(10017.2178, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10017.2158203125
tensor(10017.2178, grad_fn=<NegBackward0>) tensor(10017.2158, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10017.2216796875
tensor(10017.2158, grad_fn=<NegBackward0>) tensor(10017.2217, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -10017.2236328125
tensor(10017.2158, grad_fn=<NegBackward0>) tensor(10017.2236, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -10017.2216796875
tensor(10017.2158, grad_fn=<NegBackward0>) tensor(10017.2217, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -10017.201171875
tensor(10017.2158, grad_fn=<NegBackward0>) tensor(10017.2012, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10017.1923828125
tensor(10017.2012, grad_fn=<NegBackward0>) tensor(10017.1924, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10017.1865234375
tensor(10017.1924, grad_fn=<NegBackward0>) tensor(10017.1865, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10017.18359375
tensor(10017.1865, grad_fn=<NegBackward0>) tensor(10017.1836, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10017.2158203125
tensor(10017.1836, grad_fn=<NegBackward0>) tensor(10017.2158, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -10017.17578125
tensor(10017.1836, grad_fn=<NegBackward0>) tensor(10017.1758, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10017.1796875
tensor(10017.1758, grad_fn=<NegBackward0>) tensor(10017.1797, grad_fn=<NegBackward0>)
1
pi: tensor([[0.0254, 0.9746],
        [0.7893, 0.2107]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0108, 0.9892], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1418, 0.1556],
         [0.6826, 0.1386]],

        [[0.5504, 0.1405],
         [0.6435, 0.7258]],

        [[0.6093, 0.1359],
         [0.6639, 0.5298]],

        [[0.5188, 0.1407],
         [0.5645, 0.5992]],

        [[0.6053, 0.1422],
         [0.5814, 0.5930]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006443275578054761
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23468.412109375
inf tensor(23468.4121, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10020.373046875
tensor(23468.4121, grad_fn=<NegBackward0>) tensor(10020.3730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10018.3935546875
tensor(10020.3730, grad_fn=<NegBackward0>) tensor(10018.3936, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10017.8720703125
tensor(10018.3936, grad_fn=<NegBackward0>) tensor(10017.8721, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10017.6787109375
tensor(10017.8721, grad_fn=<NegBackward0>) tensor(10017.6787, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10017.58984375
tensor(10017.6787, grad_fn=<NegBackward0>) tensor(10017.5898, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10017.5439453125
tensor(10017.5898, grad_fn=<NegBackward0>) tensor(10017.5439, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10017.5146484375
tensor(10017.5439, grad_fn=<NegBackward0>) tensor(10017.5146, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10017.4921875
tensor(10017.5146, grad_fn=<NegBackward0>) tensor(10017.4922, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10017.4755859375
tensor(10017.4922, grad_fn=<NegBackward0>) tensor(10017.4756, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10017.4619140625
tensor(10017.4756, grad_fn=<NegBackward0>) tensor(10017.4619, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10017.44921875
tensor(10017.4619, grad_fn=<NegBackward0>) tensor(10017.4492, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10017.44140625
tensor(10017.4492, grad_fn=<NegBackward0>) tensor(10017.4414, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10017.431640625
tensor(10017.4414, grad_fn=<NegBackward0>) tensor(10017.4316, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10017.423828125
tensor(10017.4316, grad_fn=<NegBackward0>) tensor(10017.4238, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10017.4169921875
tensor(10017.4238, grad_fn=<NegBackward0>) tensor(10017.4170, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10017.4111328125
tensor(10017.4170, grad_fn=<NegBackward0>) tensor(10017.4111, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10017.4052734375
tensor(10017.4111, grad_fn=<NegBackward0>) tensor(10017.4053, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10017.4013671875
tensor(10017.4053, grad_fn=<NegBackward0>) tensor(10017.4014, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10017.3974609375
tensor(10017.4014, grad_fn=<NegBackward0>) tensor(10017.3975, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10017.392578125
tensor(10017.3975, grad_fn=<NegBackward0>) tensor(10017.3926, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10017.390625
tensor(10017.3926, grad_fn=<NegBackward0>) tensor(10017.3906, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10017.38671875
tensor(10017.3906, grad_fn=<NegBackward0>) tensor(10017.3867, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10017.384765625
tensor(10017.3867, grad_fn=<NegBackward0>) tensor(10017.3848, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10017.380859375
tensor(10017.3848, grad_fn=<NegBackward0>) tensor(10017.3809, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10017.3798828125
tensor(10017.3809, grad_fn=<NegBackward0>) tensor(10017.3799, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10017.3779296875
tensor(10017.3799, grad_fn=<NegBackward0>) tensor(10017.3779, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10017.3759765625
tensor(10017.3779, grad_fn=<NegBackward0>) tensor(10017.3760, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10017.3740234375
tensor(10017.3760, grad_fn=<NegBackward0>) tensor(10017.3740, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10017.3701171875
tensor(10017.3740, grad_fn=<NegBackward0>) tensor(10017.3701, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10017.3681640625
tensor(10017.3701, grad_fn=<NegBackward0>) tensor(10017.3682, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10017.3671875
tensor(10017.3682, grad_fn=<NegBackward0>) tensor(10017.3672, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10017.3662109375
tensor(10017.3672, grad_fn=<NegBackward0>) tensor(10017.3662, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -10017.3642578125
tensor(10017.3662, grad_fn=<NegBackward0>) tensor(10017.3643, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -10017.3623046875
tensor(10017.3643, grad_fn=<NegBackward0>) tensor(10017.3623, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -10017.3603515625
tensor(10017.3623, grad_fn=<NegBackward0>) tensor(10017.3604, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -10017.3583984375
tensor(10017.3604, grad_fn=<NegBackward0>) tensor(10017.3584, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -10017.3564453125
tensor(10017.3584, grad_fn=<NegBackward0>) tensor(10017.3564, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -10017.3544921875
tensor(10017.3564, grad_fn=<NegBackward0>) tensor(10017.3545, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -10017.3525390625
tensor(10017.3545, grad_fn=<NegBackward0>) tensor(10017.3525, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -10017.3505859375
tensor(10017.3525, grad_fn=<NegBackward0>) tensor(10017.3506, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -10017.34765625
tensor(10017.3506, grad_fn=<NegBackward0>) tensor(10017.3477, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -10017.3447265625
tensor(10017.3477, grad_fn=<NegBackward0>) tensor(10017.3447, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -10017.3408203125
tensor(10017.3447, grad_fn=<NegBackward0>) tensor(10017.3408, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -10017.3388671875
tensor(10017.3408, grad_fn=<NegBackward0>) tensor(10017.3389, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -10017.3369140625
tensor(10017.3389, grad_fn=<NegBackward0>) tensor(10017.3369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -10017.3330078125
tensor(10017.3369, grad_fn=<NegBackward0>) tensor(10017.3330, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -10017.328125
tensor(10017.3330, grad_fn=<NegBackward0>) tensor(10017.3281, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -10017.3251953125
tensor(10017.3281, grad_fn=<NegBackward0>) tensor(10017.3252, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -10017.3193359375
tensor(10017.3252, grad_fn=<NegBackward0>) tensor(10017.3193, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -10017.314453125
tensor(10017.3193, grad_fn=<NegBackward0>) tensor(10017.3145, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -10017.3095703125
tensor(10017.3145, grad_fn=<NegBackward0>) tensor(10017.3096, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -10017.3037109375
tensor(10017.3096, grad_fn=<NegBackward0>) tensor(10017.3037, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -10017.298828125
tensor(10017.3037, grad_fn=<NegBackward0>) tensor(10017.2988, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -10017.29296875
tensor(10017.2988, grad_fn=<NegBackward0>) tensor(10017.2930, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -10017.28515625
tensor(10017.2930, grad_fn=<NegBackward0>) tensor(10017.2852, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -10017.27734375
tensor(10017.2852, grad_fn=<NegBackward0>) tensor(10017.2773, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -10017.2685546875
tensor(10017.2773, grad_fn=<NegBackward0>) tensor(10017.2686, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -10017.2626953125
tensor(10017.2686, grad_fn=<NegBackward0>) tensor(10017.2627, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -10017.2568359375
tensor(10017.2627, grad_fn=<NegBackward0>) tensor(10017.2568, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -10017.2509765625
tensor(10017.2568, grad_fn=<NegBackward0>) tensor(10017.2510, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -10017.24609375
tensor(10017.2510, grad_fn=<NegBackward0>) tensor(10017.2461, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -10017.2412109375
tensor(10017.2461, grad_fn=<NegBackward0>) tensor(10017.2412, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -10017.2392578125
tensor(10017.2412, grad_fn=<NegBackward0>) tensor(10017.2393, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -10017.23828125
tensor(10017.2393, grad_fn=<NegBackward0>) tensor(10017.2383, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -10017.2353515625
tensor(10017.2383, grad_fn=<NegBackward0>) tensor(10017.2354, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -10017.2353515625
tensor(10017.2354, grad_fn=<NegBackward0>) tensor(10017.2354, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -10017.2333984375
tensor(10017.2354, grad_fn=<NegBackward0>) tensor(10017.2334, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -10017.2314453125
tensor(10017.2334, grad_fn=<NegBackward0>) tensor(10017.2314, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -10017.2314453125
tensor(10017.2314, grad_fn=<NegBackward0>) tensor(10017.2314, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -10017.232421875
tensor(10017.2314, grad_fn=<NegBackward0>) tensor(10017.2324, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -10017.2294921875
tensor(10017.2314, grad_fn=<NegBackward0>) tensor(10017.2295, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -10017.2294921875
tensor(10017.2295, grad_fn=<NegBackward0>) tensor(10017.2295, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -10017.228515625
tensor(10017.2295, grad_fn=<NegBackward0>) tensor(10017.2285, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -10017.23046875
tensor(10017.2285, grad_fn=<NegBackward0>) tensor(10017.2305, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -10017.2275390625
tensor(10017.2285, grad_fn=<NegBackward0>) tensor(10017.2275, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -10017.23828125
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2383, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -10017.2275390625
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2275, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -10017.2265625
tensor(10017.2275, grad_fn=<NegBackward0>) tensor(10017.2266, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -10017.2275390625
tensor(10017.2266, grad_fn=<NegBackward0>) tensor(10017.2275, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -10017.2255859375
tensor(10017.2266, grad_fn=<NegBackward0>) tensor(10017.2256, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -10017.224609375
tensor(10017.2256, grad_fn=<NegBackward0>) tensor(10017.2246, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -10017.2265625
tensor(10017.2246, grad_fn=<NegBackward0>) tensor(10017.2266, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -10017.2607421875
tensor(10017.2246, grad_fn=<NegBackward0>) tensor(10017.2607, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -10017.22265625
tensor(10017.2246, grad_fn=<NegBackward0>) tensor(10017.2227, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -10017.22265625
tensor(10017.2227, grad_fn=<NegBackward0>) tensor(10017.2227, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -10017.2412109375
tensor(10017.2227, grad_fn=<NegBackward0>) tensor(10017.2412, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -10017.2373046875
tensor(10017.2227, grad_fn=<NegBackward0>) tensor(10017.2373, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -10017.2197265625
tensor(10017.2227, grad_fn=<NegBackward0>) tensor(10017.2197, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -10017.2158203125
tensor(10017.2197, grad_fn=<NegBackward0>) tensor(10017.2158, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -10017.2138671875
tensor(10017.2158, grad_fn=<NegBackward0>) tensor(10017.2139, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -10017.265625
tensor(10017.2139, grad_fn=<NegBackward0>) tensor(10017.2656, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -10017.2080078125
tensor(10017.2139, grad_fn=<NegBackward0>) tensor(10017.2080, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -10017.2021484375
tensor(10017.2080, grad_fn=<NegBackward0>) tensor(10017.2021, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -10017.1962890625
tensor(10017.2021, grad_fn=<NegBackward0>) tensor(10017.1963, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -10017.1904296875
tensor(10017.1963, grad_fn=<NegBackward0>) tensor(10017.1904, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -10017.1845703125
tensor(10017.1904, grad_fn=<NegBackward0>) tensor(10017.1846, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -10017.1796875
tensor(10017.1846, grad_fn=<NegBackward0>) tensor(10017.1797, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -10017.177734375
tensor(10017.1797, grad_fn=<NegBackward0>) tensor(10017.1777, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -10017.17578125
tensor(10017.1777, grad_fn=<NegBackward0>) tensor(10017.1758, grad_fn=<NegBackward0>)
pi: tensor([[0.2235, 0.7765],
        [0.9797, 0.0203]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9901, 0.0099], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1387, 0.1561],
         [0.5150, 0.1418]],

        [[0.6858, 0.1404],
         [0.5556, 0.6600]],

        [[0.6295, 0.1360],
         [0.5338, 0.6609]],

        [[0.6042, 0.1407],
         [0.6234, 0.6008]],

        [[0.5326, 0.1422],
         [0.5463, 0.6555]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006443275578054761
Average Adjusted Rand Index: 0.0
[-0.0006443275578054761, -0.0006443275578054761] [0.0, 0.0] [10017.173828125, 10017.1748046875]
-------------------------------------
This iteration is 96
True Objective function: Loss = -10074.677096888212
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24660.79296875
inf tensor(24660.7930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9933.9169921875
tensor(24660.7930, grad_fn=<NegBackward0>) tensor(9933.9170, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9930.5185546875
tensor(9933.9170, grad_fn=<NegBackward0>) tensor(9930.5186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9926.154296875
tensor(9930.5186, grad_fn=<NegBackward0>) tensor(9926.1543, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9925.52734375
tensor(9926.1543, grad_fn=<NegBackward0>) tensor(9925.5273, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9925.189453125
tensor(9925.5273, grad_fn=<NegBackward0>) tensor(9925.1895, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9925.0703125
tensor(9925.1895, grad_fn=<NegBackward0>) tensor(9925.0703, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9925.017578125
tensor(9925.0703, grad_fn=<NegBackward0>) tensor(9925.0176, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9924.9765625
tensor(9925.0176, grad_fn=<NegBackward0>) tensor(9924.9766, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9924.947265625
tensor(9924.9766, grad_fn=<NegBackward0>) tensor(9924.9473, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9924.923828125
tensor(9924.9473, grad_fn=<NegBackward0>) tensor(9924.9238, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9924.90234375
tensor(9924.9238, grad_fn=<NegBackward0>) tensor(9924.9023, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9924.876953125
tensor(9924.9023, grad_fn=<NegBackward0>) tensor(9924.8770, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9924.841796875
tensor(9924.8770, grad_fn=<NegBackward0>) tensor(9924.8418, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9924.7626953125
tensor(9924.8418, grad_fn=<NegBackward0>) tensor(9924.7627, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9924.51953125
tensor(9924.7627, grad_fn=<NegBackward0>) tensor(9924.5195, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9924.19921875
tensor(9924.5195, grad_fn=<NegBackward0>) tensor(9924.1992, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9924.017578125
tensor(9924.1992, grad_fn=<NegBackward0>) tensor(9924.0176, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9923.9228515625
tensor(9924.0176, grad_fn=<NegBackward0>) tensor(9923.9229, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9923.859375
tensor(9923.9229, grad_fn=<NegBackward0>) tensor(9923.8594, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9923.796875
tensor(9923.8594, grad_fn=<NegBackward0>) tensor(9923.7969, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9923.677734375
tensor(9923.7969, grad_fn=<NegBackward0>) tensor(9923.6777, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9922.052734375
tensor(9923.6777, grad_fn=<NegBackward0>) tensor(9922.0527, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9921.1171875
tensor(9922.0527, grad_fn=<NegBackward0>) tensor(9921.1172, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9920.9345703125
tensor(9921.1172, grad_fn=<NegBackward0>) tensor(9920.9346, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9920.8544921875
tensor(9920.9346, grad_fn=<NegBackward0>) tensor(9920.8545, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9920.8359375
tensor(9920.8545, grad_fn=<NegBackward0>) tensor(9920.8359, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9920.8203125
tensor(9920.8359, grad_fn=<NegBackward0>) tensor(9920.8203, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9920.80859375
tensor(9920.8203, grad_fn=<NegBackward0>) tensor(9920.8086, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9920.8017578125
tensor(9920.8086, grad_fn=<NegBackward0>) tensor(9920.8018, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9920.796875
tensor(9920.8018, grad_fn=<NegBackward0>) tensor(9920.7969, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9920.794921875
tensor(9920.7969, grad_fn=<NegBackward0>) tensor(9920.7949, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9920.7890625
tensor(9920.7949, grad_fn=<NegBackward0>) tensor(9920.7891, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9920.7880859375
tensor(9920.7891, grad_fn=<NegBackward0>) tensor(9920.7881, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9920.7861328125
tensor(9920.7881, grad_fn=<NegBackward0>) tensor(9920.7861, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9920.783203125
tensor(9920.7861, grad_fn=<NegBackward0>) tensor(9920.7832, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9920.7822265625
tensor(9920.7832, grad_fn=<NegBackward0>) tensor(9920.7822, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9920.7841796875
tensor(9920.7822, grad_fn=<NegBackward0>) tensor(9920.7842, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9920.779296875
tensor(9920.7822, grad_fn=<NegBackward0>) tensor(9920.7793, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9920.7783203125
tensor(9920.7793, grad_fn=<NegBackward0>) tensor(9920.7783, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9920.7763671875
tensor(9920.7783, grad_fn=<NegBackward0>) tensor(9920.7764, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9920.77734375
tensor(9920.7764, grad_fn=<NegBackward0>) tensor(9920.7773, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -9920.775390625
tensor(9920.7764, grad_fn=<NegBackward0>) tensor(9920.7754, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9920.7744140625
tensor(9920.7754, grad_fn=<NegBackward0>) tensor(9920.7744, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9920.7734375
tensor(9920.7744, grad_fn=<NegBackward0>) tensor(9920.7734, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9920.7734375
tensor(9920.7734, grad_fn=<NegBackward0>) tensor(9920.7734, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9920.7724609375
tensor(9920.7734, grad_fn=<NegBackward0>) tensor(9920.7725, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9920.771484375
tensor(9920.7725, grad_fn=<NegBackward0>) tensor(9920.7715, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9920.771484375
tensor(9920.7715, grad_fn=<NegBackward0>) tensor(9920.7715, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9920.76953125
tensor(9920.7715, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9920.7705078125
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7705, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9920.76953125
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9920.76953125
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9920.7685546875
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9920.76953125
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9920.7685546875
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9920.771484375
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7715, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -9920.767578125
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9920.7666015625
tensor(9920.7676, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -9920.7744140625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7744, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -9920.7666015625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9920.7666015625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9920.7666015625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9920.7666015625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9920.765625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9920.765625
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9920.7666015625
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9920.765625
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9920.7646484375
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7646, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9920.7666015625
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9920.7646484375
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7646, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -9920.7646484375
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7646, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9920.7646484375
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7646, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9920.765625
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -9920.7646484375
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7646, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -9920.767578125
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -9920.765625
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -9920.7685546875
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -9920.765625
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -9920.7666015625
tensor(9920.7646, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[9.9994e-01, 6.0188e-05],
        [2.5866e-01, 7.4134e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.6091e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1789, 0.1090],
         [0.5360, 0.1262]],

        [[0.6556, 0.1447],
         [0.5103, 0.6344]],

        [[0.6395, 0.1386],
         [0.6676, 0.5073]],

        [[0.6414, 0.1283],
         [0.5296, 0.6656]],

        [[0.5295, 0.0936],
         [0.5013, 0.7102]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0029454462531745824
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 28
Adjusted Rand Index: 0.18540348045081
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 27
Adjusted Rand Index: 0.20262102953255445
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 23
Adjusted Rand Index: 0.28488152695652685
Global Adjusted Rand Index: 0.09544828839335909
Average Adjusted Rand Index: 0.13517029663861319
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23206.6875
inf tensor(23206.6875, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9934.4892578125
tensor(23206.6875, grad_fn=<NegBackward0>) tensor(9934.4893, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9933.1357421875
tensor(9934.4893, grad_fn=<NegBackward0>) tensor(9933.1357, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9931.9794921875
tensor(9933.1357, grad_fn=<NegBackward0>) tensor(9931.9795, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9928.765625
tensor(9931.9795, grad_fn=<NegBackward0>) tensor(9928.7656, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9926.05859375
tensor(9928.7656, grad_fn=<NegBackward0>) tensor(9926.0586, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9925.4931640625
tensor(9926.0586, grad_fn=<NegBackward0>) tensor(9925.4932, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9925.240234375
tensor(9925.4932, grad_fn=<NegBackward0>) tensor(9925.2402, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9925.1220703125
tensor(9925.2402, grad_fn=<NegBackward0>) tensor(9925.1221, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9925.03515625
tensor(9925.1221, grad_fn=<NegBackward0>) tensor(9925.0352, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9924.9892578125
tensor(9925.0352, grad_fn=<NegBackward0>) tensor(9924.9893, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9924.955078125
tensor(9924.9893, grad_fn=<NegBackward0>) tensor(9924.9551, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9924.9267578125
tensor(9924.9551, grad_fn=<NegBackward0>) tensor(9924.9268, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9924.9013671875
tensor(9924.9268, grad_fn=<NegBackward0>) tensor(9924.9014, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9924.8779296875
tensor(9924.9014, grad_fn=<NegBackward0>) tensor(9924.8779, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9924.841796875
tensor(9924.8779, grad_fn=<NegBackward0>) tensor(9924.8418, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9924.744140625
tensor(9924.8418, grad_fn=<NegBackward0>) tensor(9924.7441, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9924.4306640625
tensor(9924.7441, grad_fn=<NegBackward0>) tensor(9924.4307, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9924.1494140625
tensor(9924.4307, grad_fn=<NegBackward0>) tensor(9924.1494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9923.9970703125
tensor(9924.1494, grad_fn=<NegBackward0>) tensor(9923.9971, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9923.9306640625
tensor(9923.9971, grad_fn=<NegBackward0>) tensor(9923.9307, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9923.8544921875
tensor(9923.9307, grad_fn=<NegBackward0>) tensor(9923.8545, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9923.7822265625
tensor(9923.8545, grad_fn=<NegBackward0>) tensor(9923.7822, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9923.5224609375
tensor(9923.7822, grad_fn=<NegBackward0>) tensor(9923.5225, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9921.759765625
tensor(9923.5225, grad_fn=<NegBackward0>) tensor(9921.7598, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9921.0361328125
tensor(9921.7598, grad_fn=<NegBackward0>) tensor(9921.0361, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9920.947265625
tensor(9921.0361, grad_fn=<NegBackward0>) tensor(9920.9473, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9920.9111328125
tensor(9920.9473, grad_fn=<NegBackward0>) tensor(9920.9111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9920.8251953125
tensor(9920.9111, grad_fn=<NegBackward0>) tensor(9920.8252, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9920.810546875
tensor(9920.8252, grad_fn=<NegBackward0>) tensor(9920.8105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9920.8037109375
tensor(9920.8105, grad_fn=<NegBackward0>) tensor(9920.8037, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9920.7978515625
tensor(9920.8037, grad_fn=<NegBackward0>) tensor(9920.7979, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9920.7939453125
tensor(9920.7979, grad_fn=<NegBackward0>) tensor(9920.7939, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9920.7900390625
tensor(9920.7939, grad_fn=<NegBackward0>) tensor(9920.7900, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9920.7880859375
tensor(9920.7900, grad_fn=<NegBackward0>) tensor(9920.7881, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9920.7861328125
tensor(9920.7881, grad_fn=<NegBackward0>) tensor(9920.7861, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9920.783203125
tensor(9920.7861, grad_fn=<NegBackward0>) tensor(9920.7832, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9920.7822265625
tensor(9920.7832, grad_fn=<NegBackward0>) tensor(9920.7822, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9920.7802734375
tensor(9920.7822, grad_fn=<NegBackward0>) tensor(9920.7803, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9920.7802734375
tensor(9920.7803, grad_fn=<NegBackward0>) tensor(9920.7803, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9920.77734375
tensor(9920.7803, grad_fn=<NegBackward0>) tensor(9920.7773, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9920.7763671875
tensor(9920.7773, grad_fn=<NegBackward0>) tensor(9920.7764, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9920.7763671875
tensor(9920.7764, grad_fn=<NegBackward0>) tensor(9920.7764, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9920.775390625
tensor(9920.7764, grad_fn=<NegBackward0>) tensor(9920.7754, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9920.7744140625
tensor(9920.7754, grad_fn=<NegBackward0>) tensor(9920.7744, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9920.775390625
tensor(9920.7744, grad_fn=<NegBackward0>) tensor(9920.7754, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9920.7734375
tensor(9920.7744, grad_fn=<NegBackward0>) tensor(9920.7734, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9920.7724609375
tensor(9920.7734, grad_fn=<NegBackward0>) tensor(9920.7725, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9920.7724609375
tensor(9920.7725, grad_fn=<NegBackward0>) tensor(9920.7725, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9920.771484375
tensor(9920.7725, grad_fn=<NegBackward0>) tensor(9920.7715, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9920.7724609375
tensor(9920.7715, grad_fn=<NegBackward0>) tensor(9920.7725, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -9920.7705078125
tensor(9920.7715, grad_fn=<NegBackward0>) tensor(9920.7705, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9920.7763671875
tensor(9920.7705, grad_fn=<NegBackward0>) tensor(9920.7764, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -9920.7705078125
tensor(9920.7705, grad_fn=<NegBackward0>) tensor(9920.7705, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9920.76953125
tensor(9920.7705, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9920.76953125
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7695, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9920.7685546875
tensor(9920.7695, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9920.771484375
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7715, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9920.7685546875
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9920.7685546875
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7686, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9920.7734375
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7734, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9920.767578125
tensor(9920.7686, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9920.775390625
tensor(9920.7676, grad_fn=<NegBackward0>) tensor(9920.7754, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9920.767578125
tensor(9920.7676, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9920.7666015625
tensor(9920.7676, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9920.767578125
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7676, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9920.7666015625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9920.765625
tensor(9920.7666, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9920.7666015625
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -9920.763671875
tensor(9920.7656, grad_fn=<NegBackward0>) tensor(9920.7637, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9920.7666015625
tensor(9920.7637, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -9920.765625
tensor(9920.7637, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -9920.7666015625
tensor(9920.7637, grad_fn=<NegBackward0>) tensor(9920.7666, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -9920.765625
tensor(9920.7637, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -9920.765625
tensor(9920.7637, grad_fn=<NegBackward0>) tensor(9920.7656, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[7.4085e-01, 2.5915e-01],
        [8.3492e-05, 9.9992e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.0716e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1262, 0.1243],
         [0.6958, 0.1788]],

        [[0.7146, 0.1447],
         [0.6841, 0.6475]],

        [[0.5057, 0.1385],
         [0.5210, 0.6076]],

        [[0.6057, 0.1283],
         [0.7250, 0.5061]],

        [[0.6749, 0.0936],
         [0.6247, 0.6259]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0029454462531745824
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 72
Adjusted Rand Index: 0.18540348045081
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 73
Adjusted Rand Index: 0.20262102953255445
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.28488152695652685
Global Adjusted Rand Index: 0.09544828839335909
Average Adjusted Rand Index: 0.13517029663861319
[0.09544828839335909, 0.09544828839335909] [0.13517029663861319, 0.13517029663861319] [9920.7666015625, 9920.765625]
-------------------------------------
This iteration is 97
True Objective function: Loss = -9963.148251537514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22722.244140625
inf tensor(22722.2441, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9827.7939453125
tensor(22722.2441, grad_fn=<NegBackward0>) tensor(9827.7939, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9826.9541015625
tensor(9827.7939, grad_fn=<NegBackward0>) tensor(9826.9541, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9826.7431640625
tensor(9826.9541, grad_fn=<NegBackward0>) tensor(9826.7432, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9826.619140625
tensor(9826.7432, grad_fn=<NegBackward0>) tensor(9826.6191, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9826.4912109375
tensor(9826.6191, grad_fn=<NegBackward0>) tensor(9826.4912, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9826.3583984375
tensor(9826.4912, grad_fn=<NegBackward0>) tensor(9826.3584, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9826.2666015625
tensor(9826.3584, grad_fn=<NegBackward0>) tensor(9826.2666, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9826.208984375
tensor(9826.2666, grad_fn=<NegBackward0>) tensor(9826.2090, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9826.1728515625
tensor(9826.2090, grad_fn=<NegBackward0>) tensor(9826.1729, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9826.146484375
tensor(9826.1729, grad_fn=<NegBackward0>) tensor(9826.1465, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9826.125
tensor(9826.1465, grad_fn=<NegBackward0>) tensor(9826.1250, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9826.1064453125
tensor(9826.1250, grad_fn=<NegBackward0>) tensor(9826.1064, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9826.0927734375
tensor(9826.1064, grad_fn=<NegBackward0>) tensor(9826.0928, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9826.0791015625
tensor(9826.0928, grad_fn=<NegBackward0>) tensor(9826.0791, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9826.0673828125
tensor(9826.0791, grad_fn=<NegBackward0>) tensor(9826.0674, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9826.056640625
tensor(9826.0674, grad_fn=<NegBackward0>) tensor(9826.0566, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9826.0458984375
tensor(9826.0566, grad_fn=<NegBackward0>) tensor(9826.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9826.037109375
tensor(9826.0459, grad_fn=<NegBackward0>) tensor(9826.0371, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9826.029296875
tensor(9826.0371, grad_fn=<NegBackward0>) tensor(9826.0293, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9826.021484375
tensor(9826.0293, grad_fn=<NegBackward0>) tensor(9826.0215, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9826.013671875
tensor(9826.0215, grad_fn=<NegBackward0>) tensor(9826.0137, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9826.005859375
tensor(9826.0137, grad_fn=<NegBackward0>) tensor(9826.0059, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9826.0
tensor(9826.0059, grad_fn=<NegBackward0>) tensor(9826., grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9825.994140625
tensor(9826., grad_fn=<NegBackward0>) tensor(9825.9941, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9825.9892578125
tensor(9825.9941, grad_fn=<NegBackward0>) tensor(9825.9893, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9825.9833984375
tensor(9825.9893, grad_fn=<NegBackward0>) tensor(9825.9834, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9825.978515625
tensor(9825.9834, grad_fn=<NegBackward0>) tensor(9825.9785, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9825.974609375
tensor(9825.9785, grad_fn=<NegBackward0>) tensor(9825.9746, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9825.970703125
tensor(9825.9746, grad_fn=<NegBackward0>) tensor(9825.9707, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9825.9677734375
tensor(9825.9707, grad_fn=<NegBackward0>) tensor(9825.9678, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9825.96484375
tensor(9825.9678, grad_fn=<NegBackward0>) tensor(9825.9648, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9825.9619140625
tensor(9825.9648, grad_fn=<NegBackward0>) tensor(9825.9619, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9825.9599609375
tensor(9825.9619, grad_fn=<NegBackward0>) tensor(9825.9600, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9825.9580078125
tensor(9825.9600, grad_fn=<NegBackward0>) tensor(9825.9580, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9825.95703125
tensor(9825.9580, grad_fn=<NegBackward0>) tensor(9825.9570, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9825.9541015625
tensor(9825.9570, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9825.9541015625
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9825.9541015625
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9825.9521484375
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9521, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9825.9541015625
tensor(9825.9521, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -9825.9521484375
tensor(9825.9521, grad_fn=<NegBackward0>) tensor(9825.9521, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9825.951171875
tensor(9825.9521, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9825.9521484375
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9521, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -9825.9501953125
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9502, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9825.951171875
tensor(9825.9502, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9825.94921875
tensor(9825.9502, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9825.9482421875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9825.94921875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9825.947265625
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9825.94921875
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9825.9482421875
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -9825.9482421875
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -9825.9482421875
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -9825.947265625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9825.947265625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9825.9482421875
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -9825.9462890625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9825.947265625
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -9825.9462890625
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9825.9462890625
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9825.9462890625
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9825.9462890625
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9825.9453125
tensor(9825.9463, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -9825.953125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9531, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[9.2549e-04, 9.9907e-01],
        [5.7526e-02, 9.4247e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0288, 0.9712], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.1809],
         [0.7230, 0.1323]],

        [[0.5977, 0.1804],
         [0.7174, 0.6433]],

        [[0.5990, 0.1585],
         [0.6720, 0.6384]],

        [[0.6024, 0.1567],
         [0.6383, 0.6291]],

        [[0.5704, 0.1575],
         [0.7217, 0.5387]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004129840720541819
Average Adjusted Rand Index: 0.0001617442499919128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22537.833984375
inf tensor(22537.8340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9828.28515625
tensor(22537.8340, grad_fn=<NegBackward0>) tensor(9828.2852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9827.087890625
tensor(9828.2852, grad_fn=<NegBackward0>) tensor(9827.0879, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9826.81640625
tensor(9827.0879, grad_fn=<NegBackward0>) tensor(9826.8164, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9826.67578125
tensor(9826.8164, grad_fn=<NegBackward0>) tensor(9826.6758, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9826.580078125
tensor(9826.6758, grad_fn=<NegBackward0>) tensor(9826.5801, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9826.5087890625
tensor(9826.5801, grad_fn=<NegBackward0>) tensor(9826.5088, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9826.4541015625
tensor(9826.5088, grad_fn=<NegBackward0>) tensor(9826.4541, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9826.412109375
tensor(9826.4541, grad_fn=<NegBackward0>) tensor(9826.4121, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9826.3779296875
tensor(9826.4121, grad_fn=<NegBackward0>) tensor(9826.3779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9826.3515625
tensor(9826.3779, grad_fn=<NegBackward0>) tensor(9826.3516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9826.3310546875
tensor(9826.3516, grad_fn=<NegBackward0>) tensor(9826.3311, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9826.3134765625
tensor(9826.3311, grad_fn=<NegBackward0>) tensor(9826.3135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9826.298828125
tensor(9826.3135, grad_fn=<NegBackward0>) tensor(9826.2988, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9826.28515625
tensor(9826.2988, grad_fn=<NegBackward0>) tensor(9826.2852, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9826.2724609375
tensor(9826.2852, grad_fn=<NegBackward0>) tensor(9826.2725, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9826.26171875
tensor(9826.2725, grad_fn=<NegBackward0>) tensor(9826.2617, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9826.25
tensor(9826.2617, grad_fn=<NegBackward0>) tensor(9826.2500, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9826.2412109375
tensor(9826.2500, grad_fn=<NegBackward0>) tensor(9826.2412, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9826.232421875
tensor(9826.2412, grad_fn=<NegBackward0>) tensor(9826.2324, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9826.224609375
tensor(9826.2324, grad_fn=<NegBackward0>) tensor(9826.2246, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9826.216796875
tensor(9826.2246, grad_fn=<NegBackward0>) tensor(9826.2168, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9826.2080078125
tensor(9826.2168, grad_fn=<NegBackward0>) tensor(9826.2080, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9826.19921875
tensor(9826.2080, grad_fn=<NegBackward0>) tensor(9826.1992, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9826.1884765625
tensor(9826.1992, grad_fn=<NegBackward0>) tensor(9826.1885, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9826.17578125
tensor(9826.1885, grad_fn=<NegBackward0>) tensor(9826.1758, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9826.1533203125
tensor(9826.1758, grad_fn=<NegBackward0>) tensor(9826.1533, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9826.078125
tensor(9826.1533, grad_fn=<NegBackward0>) tensor(9826.0781, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9826.03125
tensor(9826.0781, grad_fn=<NegBackward0>) tensor(9826.0312, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9826.017578125
tensor(9826.0312, grad_fn=<NegBackward0>) tensor(9826.0176, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9826.0087890625
tensor(9826.0176, grad_fn=<NegBackward0>) tensor(9826.0088, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9826.0009765625
tensor(9826.0088, grad_fn=<NegBackward0>) tensor(9826.0010, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9825.9951171875
tensor(9826.0010, grad_fn=<NegBackward0>) tensor(9825.9951, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9825.9931640625
tensor(9825.9951, grad_fn=<NegBackward0>) tensor(9825.9932, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9825.986328125
tensor(9825.9932, grad_fn=<NegBackward0>) tensor(9825.9863, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9825.9833984375
tensor(9825.9863, grad_fn=<NegBackward0>) tensor(9825.9834, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9825.98046875
tensor(9825.9834, grad_fn=<NegBackward0>) tensor(9825.9805, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9825.9775390625
tensor(9825.9805, grad_fn=<NegBackward0>) tensor(9825.9775, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9825.974609375
tensor(9825.9775, grad_fn=<NegBackward0>) tensor(9825.9746, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9825.9716796875
tensor(9825.9746, grad_fn=<NegBackward0>) tensor(9825.9717, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9825.9697265625
tensor(9825.9717, grad_fn=<NegBackward0>) tensor(9825.9697, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9825.9677734375
tensor(9825.9697, grad_fn=<NegBackward0>) tensor(9825.9678, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9825.9658203125
tensor(9825.9678, grad_fn=<NegBackward0>) tensor(9825.9658, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9825.96484375
tensor(9825.9658, grad_fn=<NegBackward0>) tensor(9825.9648, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9825.962890625
tensor(9825.9648, grad_fn=<NegBackward0>) tensor(9825.9629, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9825.9609375
tensor(9825.9629, grad_fn=<NegBackward0>) tensor(9825.9609, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9825.9599609375
tensor(9825.9609, grad_fn=<NegBackward0>) tensor(9825.9600, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9825.9599609375
tensor(9825.9600, grad_fn=<NegBackward0>) tensor(9825.9600, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9825.958984375
tensor(9825.9600, grad_fn=<NegBackward0>) tensor(9825.9590, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9825.95703125
tensor(9825.9590, grad_fn=<NegBackward0>) tensor(9825.9570, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9825.95703125
tensor(9825.9570, grad_fn=<NegBackward0>) tensor(9825.9570, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9825.955078125
tensor(9825.9570, grad_fn=<NegBackward0>) tensor(9825.9551, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9825.955078125
tensor(9825.9551, grad_fn=<NegBackward0>) tensor(9825.9551, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9825.9541015625
tensor(9825.9551, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9825.955078125
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9551, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -9825.9541015625
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9825.953125
tensor(9825.9541, grad_fn=<NegBackward0>) tensor(9825.9531, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9825.9541015625
tensor(9825.9531, grad_fn=<NegBackward0>) tensor(9825.9541, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9825.9521484375
tensor(9825.9531, grad_fn=<NegBackward0>) tensor(9825.9521, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9825.951171875
tensor(9825.9521, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9825.9521484375
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9521, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -9825.951171875
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9825.951171875
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9825.951171875
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9512, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9825.9501953125
tensor(9825.9512, grad_fn=<NegBackward0>) tensor(9825.9502, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9825.94921875
tensor(9825.9502, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9825.94921875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9825.9501953125
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9502, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -9825.94921875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9825.94921875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9825.94921875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9825.94921875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9825.9482421875
tensor(9825.9492, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9825.94921875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9825.94921875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9492, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -9825.9482421875
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9482, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9825.962890625
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9629, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9825.953125
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9531, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9825.947265625
tensor(9825.9482, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9825.9599609375
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9600, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -9825.947265625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -9825.947265625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -9825.947265625
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -9825.9453125
tensor(9825.9473, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -9825.95703125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9570, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -9825.9580078125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9580, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -9825.947265625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9473, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -9825.9453125
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9453, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -9825.9462890625
tensor(9825.9453, grad_fn=<NegBackward0>) tensor(9825.9463, grad_fn=<NegBackward0>)
3
pi: tensor([[6.5530e-04, 9.9934e-01],
        [5.7430e-02, 9.4257e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0290, 0.9710], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.1808],
         [0.6262, 0.1323]],

        [[0.5955, 0.1802],
         [0.5083, 0.7173]],

        [[0.6152, 0.1585],
         [0.5821, 0.5833]],

        [[0.6146, 0.1567],
         [0.6686, 0.5906]],

        [[0.6090, 0.1575],
         [0.5415, 0.6885]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004129840720541819
Average Adjusted Rand Index: 0.0001617442499919128
[0.004129840720541819, 0.004129840720541819] [0.0001617442499919128, 0.0001617442499919128] [9825.953125, 9825.9453125]
-------------------------------------
This iteration is 98
True Objective function: Loss = -9930.352722528987
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20662.01171875
inf tensor(20662.0117, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9790.181640625
tensor(20662.0117, grad_fn=<NegBackward0>) tensor(9790.1816, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9789.4853515625
tensor(9790.1816, grad_fn=<NegBackward0>) tensor(9789.4854, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9789.2333984375
tensor(9789.4854, grad_fn=<NegBackward0>) tensor(9789.2334, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9789.0458984375
tensor(9789.2334, grad_fn=<NegBackward0>) tensor(9789.0459, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9788.912109375
tensor(9789.0459, grad_fn=<NegBackward0>) tensor(9788.9121, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9788.8330078125
tensor(9788.9121, grad_fn=<NegBackward0>) tensor(9788.8330, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9788.7841796875
tensor(9788.8330, grad_fn=<NegBackward0>) tensor(9788.7842, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9788.74609375
tensor(9788.7842, grad_fn=<NegBackward0>) tensor(9788.7461, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9788.71875
tensor(9788.7461, grad_fn=<NegBackward0>) tensor(9788.7188, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9788.7001953125
tensor(9788.7188, grad_fn=<NegBackward0>) tensor(9788.7002, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9788.685546875
tensor(9788.7002, grad_fn=<NegBackward0>) tensor(9788.6855, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9788.673828125
tensor(9788.6855, grad_fn=<NegBackward0>) tensor(9788.6738, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9788.6640625
tensor(9788.6738, grad_fn=<NegBackward0>) tensor(9788.6641, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9788.658203125
tensor(9788.6641, grad_fn=<NegBackward0>) tensor(9788.6582, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9788.6533203125
tensor(9788.6582, grad_fn=<NegBackward0>) tensor(9788.6533, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9788.650390625
tensor(9788.6533, grad_fn=<NegBackward0>) tensor(9788.6504, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9788.646484375
tensor(9788.6504, grad_fn=<NegBackward0>) tensor(9788.6465, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9788.6435546875
tensor(9788.6465, grad_fn=<NegBackward0>) tensor(9788.6436, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9788.6435546875
tensor(9788.6436, grad_fn=<NegBackward0>) tensor(9788.6436, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9788.6416015625
tensor(9788.6436, grad_fn=<NegBackward0>) tensor(9788.6416, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9788.6416015625
tensor(9788.6416, grad_fn=<NegBackward0>) tensor(9788.6416, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9788.6376953125
tensor(9788.6416, grad_fn=<NegBackward0>) tensor(9788.6377, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9788.638671875
tensor(9788.6377, grad_fn=<NegBackward0>) tensor(9788.6387, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -9788.63671875
tensor(9788.6377, grad_fn=<NegBackward0>) tensor(9788.6367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9788.63671875
tensor(9788.6367, grad_fn=<NegBackward0>) tensor(9788.6367, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9788.6357421875
tensor(9788.6367, grad_fn=<NegBackward0>) tensor(9788.6357, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9788.634765625
tensor(9788.6357, grad_fn=<NegBackward0>) tensor(9788.6348, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9788.6337890625
tensor(9788.6348, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9788.6337890625
tensor(9788.6338, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9788.6328125
tensor(9788.6338, grad_fn=<NegBackward0>) tensor(9788.6328, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9788.6337890625
tensor(9788.6328, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -9788.6328125
tensor(9788.6328, grad_fn=<NegBackward0>) tensor(9788.6328, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9788.6318359375
tensor(9788.6328, grad_fn=<NegBackward0>) tensor(9788.6318, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9788.630859375
tensor(9788.6318, grad_fn=<NegBackward0>) tensor(9788.6309, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9788.6318359375
tensor(9788.6309, grad_fn=<NegBackward0>) tensor(9788.6318, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -9788.6298828125
tensor(9788.6309, grad_fn=<NegBackward0>) tensor(9788.6299, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9788.630859375
tensor(9788.6299, grad_fn=<NegBackward0>) tensor(9788.6309, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -9788.62890625
tensor(9788.6299, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9788.62890625
tensor(9788.6289, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9788.62890625
tensor(9788.6289, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9788.6279296875
tensor(9788.6289, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9788.62890625
tensor(9788.6279, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -9788.62890625
tensor(9788.6279, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -9788.62890625
tensor(9788.6279, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -9788.626953125
tensor(9788.6279, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -9788.62890625
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -9788.6259765625
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -9788.6259765625
tensor(9788.6260, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -9788.6259765625
tensor(9788.6260, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9788.625
tensor(9788.6260, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9788.626953125
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.9848, 0.0152],
        [0.9973, 0.0027]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9783, 0.0217], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1333, 0.1610],
         [0.6454, 0.2135]],

        [[0.7043, 0.2085],
         [0.5591, 0.5188]],

        [[0.7238, 0.1526],
         [0.5343, 0.7182]],

        [[0.6123, 0.2335],
         [0.5983, 0.6677]],

        [[0.5079, 0.1565],
         [0.5593, 0.5330]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22463.990234375
inf tensor(22463.9902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -9791.7158203125
tensor(22463.9902, grad_fn=<NegBackward0>) tensor(9791.7158, grad_fn=<NegBackward0>)
Iteration 200: Loss = -9790.3291015625
tensor(9791.7158, grad_fn=<NegBackward0>) tensor(9790.3291, grad_fn=<NegBackward0>)
Iteration 300: Loss = -9789.828125
tensor(9790.3291, grad_fn=<NegBackward0>) tensor(9789.8281, grad_fn=<NegBackward0>)
Iteration 400: Loss = -9789.6123046875
tensor(9789.8281, grad_fn=<NegBackward0>) tensor(9789.6123, grad_fn=<NegBackward0>)
Iteration 500: Loss = -9789.498046875
tensor(9789.6123, grad_fn=<NegBackward0>) tensor(9789.4980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9789.4228515625
tensor(9789.4980, grad_fn=<NegBackward0>) tensor(9789.4229, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9789.3623046875
tensor(9789.4229, grad_fn=<NegBackward0>) tensor(9789.3623, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9789.3056640625
tensor(9789.3623, grad_fn=<NegBackward0>) tensor(9789.3057, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9789.234375
tensor(9789.3057, grad_fn=<NegBackward0>) tensor(9789.2344, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9789.1083984375
tensor(9789.2344, grad_fn=<NegBackward0>) tensor(9789.1084, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9788.8935546875
tensor(9789.1084, grad_fn=<NegBackward0>) tensor(9788.8936, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9788.7724609375
tensor(9788.8936, grad_fn=<NegBackward0>) tensor(9788.7725, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9788.72265625
tensor(9788.7725, grad_fn=<NegBackward0>) tensor(9788.7227, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9788.693359375
tensor(9788.7227, grad_fn=<NegBackward0>) tensor(9788.6934, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9788.6787109375
tensor(9788.6934, grad_fn=<NegBackward0>) tensor(9788.6787, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9788.666015625
tensor(9788.6787, grad_fn=<NegBackward0>) tensor(9788.6660, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9788.66015625
tensor(9788.6660, grad_fn=<NegBackward0>) tensor(9788.6602, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9788.654296875
tensor(9788.6602, grad_fn=<NegBackward0>) tensor(9788.6543, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9788.650390625
tensor(9788.6543, grad_fn=<NegBackward0>) tensor(9788.6504, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9788.6474609375
tensor(9788.6504, grad_fn=<NegBackward0>) tensor(9788.6475, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9788.646484375
tensor(9788.6475, grad_fn=<NegBackward0>) tensor(9788.6465, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9788.6455078125
tensor(9788.6465, grad_fn=<NegBackward0>) tensor(9788.6455, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9788.64453125
tensor(9788.6455, grad_fn=<NegBackward0>) tensor(9788.6445, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9788.6435546875
tensor(9788.6445, grad_fn=<NegBackward0>) tensor(9788.6436, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9788.6416015625
tensor(9788.6436, grad_fn=<NegBackward0>) tensor(9788.6416, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9788.640625
tensor(9788.6416, grad_fn=<NegBackward0>) tensor(9788.6406, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9788.6396484375
tensor(9788.6406, grad_fn=<NegBackward0>) tensor(9788.6396, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9788.640625
tensor(9788.6396, grad_fn=<NegBackward0>) tensor(9788.6406, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -9788.6396484375
tensor(9788.6396, grad_fn=<NegBackward0>) tensor(9788.6396, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9788.638671875
tensor(9788.6396, grad_fn=<NegBackward0>) tensor(9788.6387, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9788.63671875
tensor(9788.6387, grad_fn=<NegBackward0>) tensor(9788.6367, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9788.6376953125
tensor(9788.6367, grad_fn=<NegBackward0>) tensor(9788.6377, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -9788.6357421875
tensor(9788.6367, grad_fn=<NegBackward0>) tensor(9788.6357, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9788.63671875
tensor(9788.6357, grad_fn=<NegBackward0>) tensor(9788.6367, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -9788.6376953125
tensor(9788.6357, grad_fn=<NegBackward0>) tensor(9788.6377, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -9788.6357421875
tensor(9788.6357, grad_fn=<NegBackward0>) tensor(9788.6357, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9788.6337890625
tensor(9788.6357, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9788.6337890625
tensor(9788.6338, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9788.6337890625
tensor(9788.6338, grad_fn=<NegBackward0>) tensor(9788.6338, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9788.6318359375
tensor(9788.6338, grad_fn=<NegBackward0>) tensor(9788.6318, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9788.630859375
tensor(9788.6318, grad_fn=<NegBackward0>) tensor(9788.6309, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9788.630859375
tensor(9788.6309, grad_fn=<NegBackward0>) tensor(9788.6309, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9788.6298828125
tensor(9788.6309, grad_fn=<NegBackward0>) tensor(9788.6299, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9788.62890625
tensor(9788.6299, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9788.62890625
tensor(9788.6289, grad_fn=<NegBackward0>) tensor(9788.6289, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9788.6279296875
tensor(9788.6289, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9788.626953125
tensor(9788.6279, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9788.6279296875
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6279, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -9788.626953125
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -9788.6259765625
tensor(9788.6270, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9788.626953125
tensor(9788.6260, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -9788.625
tensor(9788.6260, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9788.626953125
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9788.626953125
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6270, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9788.6259765625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -9788.625
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9788.623046875
tensor(9788.6250, grad_fn=<NegBackward0>) tensor(9788.6230, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -9788.625
tensor(9788.6230, grad_fn=<NegBackward0>) tensor(9788.6250, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -9788.6240234375
tensor(9788.6230, grad_fn=<NegBackward0>) tensor(9788.6240, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -9788.6240234375
tensor(9788.6230, grad_fn=<NegBackward0>) tensor(9788.6240, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -9788.6259765625
tensor(9788.6230, grad_fn=<NegBackward0>) tensor(9788.6260, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -9788.6240234375
tensor(9788.6230, grad_fn=<NegBackward0>) tensor(9788.6240, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.9849, 0.0151],
        [0.9984, 0.0016]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9784, 0.0216], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1333, 0.1610],
         [0.6466, 0.2136]],

        [[0.5135, 0.2085],
         [0.6593, 0.6719]],

        [[0.6056, 0.1527],
         [0.6147, 0.7145]],

        [[0.6663, 0.2337],
         [0.6950, 0.6120]],

        [[0.6011, 0.1565],
         [0.5779, 0.6826]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.0003077958928485548
[-0.0004811949056573946, -0.0004811949056573946] [-0.0003077958928485548, -0.0003077958928485548] [9788.6259765625, 9788.6240234375]
-------------------------------------
This iteration is 99
True Objective function: Loss = -10092.412899975014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24675.572265625
inf tensor(24675.5723, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10006.57421875
tensor(24675.5723, grad_fn=<NegBackward0>) tensor(10006.5742, grad_fn=<NegBackward0>)
Iteration 200: Loss = -10003.5673828125
tensor(10006.5742, grad_fn=<NegBackward0>) tensor(10003.5674, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10002.5478515625
tensor(10003.5674, grad_fn=<NegBackward0>) tensor(10002.5479, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10001.6201171875
tensor(10002.5479, grad_fn=<NegBackward0>) tensor(10001.6201, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10000.52734375
tensor(10001.6201, grad_fn=<NegBackward0>) tensor(10000.5273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -9999.2275390625
tensor(10000.5273, grad_fn=<NegBackward0>) tensor(9999.2275, grad_fn=<NegBackward0>)
Iteration 700: Loss = -9996.54296875
tensor(9999.2275, grad_fn=<NegBackward0>) tensor(9996.5430, grad_fn=<NegBackward0>)
Iteration 800: Loss = -9989.78515625
tensor(9996.5430, grad_fn=<NegBackward0>) tensor(9989.7852, grad_fn=<NegBackward0>)
Iteration 900: Loss = -9987.1630859375
tensor(9989.7852, grad_fn=<NegBackward0>) tensor(9987.1631, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -9986.5576171875
tensor(9987.1631, grad_fn=<NegBackward0>) tensor(9986.5576, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -9985.962890625
tensor(9986.5576, grad_fn=<NegBackward0>) tensor(9985.9629, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -9985.712890625
tensor(9985.9629, grad_fn=<NegBackward0>) tensor(9985.7129, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -9985.65625
tensor(9985.7129, grad_fn=<NegBackward0>) tensor(9985.6562, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -9985.61328125
tensor(9985.6562, grad_fn=<NegBackward0>) tensor(9985.6133, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -9985.576171875
tensor(9985.6133, grad_fn=<NegBackward0>) tensor(9985.5762, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -9985.55078125
tensor(9985.5762, grad_fn=<NegBackward0>) tensor(9985.5508, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -9985.529296875
tensor(9985.5508, grad_fn=<NegBackward0>) tensor(9985.5293, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -9985.517578125
tensor(9985.5293, grad_fn=<NegBackward0>) tensor(9985.5176, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -9985.5068359375
tensor(9985.5176, grad_fn=<NegBackward0>) tensor(9985.5068, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -9985.498046875
tensor(9985.5068, grad_fn=<NegBackward0>) tensor(9985.4980, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -9985.4912109375
tensor(9985.4980, grad_fn=<NegBackward0>) tensor(9985.4912, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -9985.484375
tensor(9985.4912, grad_fn=<NegBackward0>) tensor(9985.4844, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -9985.4794921875
tensor(9985.4844, grad_fn=<NegBackward0>) tensor(9985.4795, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -9985.4736328125
tensor(9985.4795, grad_fn=<NegBackward0>) tensor(9985.4736, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -9985.46875
tensor(9985.4736, grad_fn=<NegBackward0>) tensor(9985.4688, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -9985.4580078125
tensor(9985.4688, grad_fn=<NegBackward0>) tensor(9985.4580, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -9985.451171875
tensor(9985.4580, grad_fn=<NegBackward0>) tensor(9985.4512, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -9985.4482421875
tensor(9985.4512, grad_fn=<NegBackward0>) tensor(9985.4482, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -9985.4462890625
tensor(9985.4482, grad_fn=<NegBackward0>) tensor(9985.4463, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -9985.443359375
tensor(9985.4463, grad_fn=<NegBackward0>) tensor(9985.4434, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -9985.44140625
tensor(9985.4434, grad_fn=<NegBackward0>) tensor(9985.4414, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -9985.439453125
tensor(9985.4414, grad_fn=<NegBackward0>) tensor(9985.4395, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9985.4384765625
tensor(9985.4395, grad_fn=<NegBackward0>) tensor(9985.4385, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9985.435546875
tensor(9985.4385, grad_fn=<NegBackward0>) tensor(9985.4355, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9985.4345703125
tensor(9985.4355, grad_fn=<NegBackward0>) tensor(9985.4346, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9985.43359375
tensor(9985.4346, grad_fn=<NegBackward0>) tensor(9985.4336, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9985.4326171875
tensor(9985.4336, grad_fn=<NegBackward0>) tensor(9985.4326, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9985.431640625
tensor(9985.4326, grad_fn=<NegBackward0>) tensor(9985.4316, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9985.4306640625
tensor(9985.4316, grad_fn=<NegBackward0>) tensor(9985.4307, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9985.4296875
tensor(9985.4307, grad_fn=<NegBackward0>) tensor(9985.4297, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9985.4287109375
tensor(9985.4297, grad_fn=<NegBackward0>) tensor(9985.4287, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9985.4267578125
tensor(9985.4287, grad_fn=<NegBackward0>) tensor(9985.4268, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9985.4267578125
tensor(9985.4268, grad_fn=<NegBackward0>) tensor(9985.4268, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9985.42578125
tensor(9985.4268, grad_fn=<NegBackward0>) tensor(9985.4258, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9985.42578125
tensor(9985.4258, grad_fn=<NegBackward0>) tensor(9985.4258, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -9985.4248046875
tensor(9985.4258, grad_fn=<NegBackward0>) tensor(9985.4248, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9985.423828125
tensor(9985.4248, grad_fn=<NegBackward0>) tensor(9985.4238, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -9985.4228515625
tensor(9985.4238, grad_fn=<NegBackward0>) tensor(9985.4229, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9985.423828125
tensor(9985.4229, grad_fn=<NegBackward0>) tensor(9985.4238, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9985.421875
tensor(9985.4229, grad_fn=<NegBackward0>) tensor(9985.4219, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -9985.421875
tensor(9985.4219, grad_fn=<NegBackward0>) tensor(9985.4219, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -9985.4208984375
tensor(9985.4219, grad_fn=<NegBackward0>) tensor(9985.4209, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -9985.4267578125
tensor(9985.4209, grad_fn=<NegBackward0>) tensor(9985.4268, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -9985.4208984375
tensor(9985.4209, grad_fn=<NegBackward0>) tensor(9985.4209, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -9985.435546875
tensor(9985.4209, grad_fn=<NegBackward0>) tensor(9985.4355, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -9985.419921875
tensor(9985.4209, grad_fn=<NegBackward0>) tensor(9985.4199, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -9985.4189453125
tensor(9985.4199, grad_fn=<NegBackward0>) tensor(9985.4189, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -9985.419921875
tensor(9985.4189, grad_fn=<NegBackward0>) tensor(9985.4199, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -9985.439453125
tensor(9985.4189, grad_fn=<NegBackward0>) tensor(9985.4395, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -9985.4189453125
tensor(9985.4189, grad_fn=<NegBackward0>) tensor(9985.4189, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -9985.4189453125
tensor(9985.4189, grad_fn=<NegBackward0>) tensor(9985.4189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -9985.41796875
tensor(9985.4189, grad_fn=<NegBackward0>) tensor(9985.4180, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -9985.4189453125
tensor(9985.4180, grad_fn=<NegBackward0>) tensor(9985.4189, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -9985.41796875
tensor(9985.4180, grad_fn=<NegBackward0>) tensor(9985.4180, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -9985.41796875
tensor(9985.4180, grad_fn=<NegBackward0>) tensor(9985.4180, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -9985.4169921875
tensor(9985.4180, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -9985.4169921875
tensor(9985.4170, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -9985.4169921875
tensor(9985.4170, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -9985.4169921875
tensor(9985.4170, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -9985.4169921875
tensor(9985.4170, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -9985.416015625
tensor(9985.4170, grad_fn=<NegBackward0>) tensor(9985.4160, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -9985.4169921875
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -9985.4169921875
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4170, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -9985.419921875
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4199, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -9985.416015625
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4160, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -9985.42578125
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4258, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -9985.4150390625
tensor(9985.4160, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -9985.42578125
tensor(9985.4150, grad_fn=<NegBackward0>) tensor(9985.4258, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -9985.416015625
tensor(9985.4150, grad_fn=<NegBackward0>) tensor(9985.4160, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -9985.4150390625
tensor(9985.4150, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -9985.4140625
tensor(9985.4150, grad_fn=<NegBackward0>) tensor(9985.4141, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -9985.41796875
tensor(9985.4141, grad_fn=<NegBackward0>) tensor(9985.4180, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -9985.4150390625
tensor(9985.4141, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -9985.4150390625
tensor(9985.4141, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -9985.4150390625
tensor(9985.4141, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -9985.4150390625
tensor(9985.4141, grad_fn=<NegBackward0>) tensor(9985.4150, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[9.9998e-01, 1.6909e-05],
        [2.1011e-01, 7.8989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5445, 0.4555], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1295, 0.1120],
         [0.5848, 0.2287]],

        [[0.7043, 0.1250],
         [0.6749, 0.5810]],

        [[0.6264, 0.1609],
         [0.6167, 0.5511]],

        [[0.6306, 0.1282],
         [0.6503, 0.6522]],

        [[0.6198, 0.1245],
         [0.6623, 0.6601]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.3535225836110792
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.3536483612047995
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.023939606509632682
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.05020054416662757
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
Global Adjusted Rand Index: 0.10739832122469077
Average Adjusted Rand Index: 0.15432282515903387
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24535.833984375
inf tensor(24535.8340, grad_fn=<NegBackward0>)
Iteration 100: Loss = -10007.0
tensor(24535.8340, grad_fn=<NegBackward0>) tensor(10007., grad_fn=<NegBackward0>)
Iteration 200: Loss = -10004.4697265625
tensor(10007., grad_fn=<NegBackward0>) tensor(10004.4697, grad_fn=<NegBackward0>)
Iteration 300: Loss = -10003.916015625
tensor(10004.4697, grad_fn=<NegBackward0>) tensor(10003.9160, grad_fn=<NegBackward0>)
Iteration 400: Loss = -10002.9140625
tensor(10003.9160, grad_fn=<NegBackward0>) tensor(10002.9141, grad_fn=<NegBackward0>)
Iteration 500: Loss = -10001.794921875
tensor(10002.9141, grad_fn=<NegBackward0>) tensor(10001.7949, grad_fn=<NegBackward0>)
Iteration 600: Loss = -10001.1171875
tensor(10001.7949, grad_fn=<NegBackward0>) tensor(10001.1172, grad_fn=<NegBackward0>)
Iteration 700: Loss = -10000.6845703125
tensor(10001.1172, grad_fn=<NegBackward0>) tensor(10000.6846, grad_fn=<NegBackward0>)
Iteration 800: Loss = -10000.42578125
tensor(10000.6846, grad_fn=<NegBackward0>) tensor(10000.4258, grad_fn=<NegBackward0>)
Iteration 900: Loss = -10000.298828125
tensor(10000.4258, grad_fn=<NegBackward0>) tensor(10000.2988, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -10000.23046875
tensor(10000.2988, grad_fn=<NegBackward0>) tensor(10000.2305, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -10000.1865234375
tensor(10000.2305, grad_fn=<NegBackward0>) tensor(10000.1865, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -10000.158203125
tensor(10000.1865, grad_fn=<NegBackward0>) tensor(10000.1582, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -10000.13671875
tensor(10000.1582, grad_fn=<NegBackward0>) tensor(10000.1367, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -10000.12109375
tensor(10000.1367, grad_fn=<NegBackward0>) tensor(10000.1211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -10000.1103515625
tensor(10000.1211, grad_fn=<NegBackward0>) tensor(10000.1104, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -10000.103515625
tensor(10000.1104, grad_fn=<NegBackward0>) tensor(10000.1035, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -10000.0947265625
tensor(10000.1035, grad_fn=<NegBackward0>) tensor(10000.0947, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -10000.0888671875
tensor(10000.0947, grad_fn=<NegBackward0>) tensor(10000.0889, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -10000.0849609375
tensor(10000.0889, grad_fn=<NegBackward0>) tensor(10000.0850, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -10000.0810546875
tensor(10000.0850, grad_fn=<NegBackward0>) tensor(10000.0811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -10000.0751953125
tensor(10000.0811, grad_fn=<NegBackward0>) tensor(10000.0752, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -10000.0732421875
tensor(10000.0752, grad_fn=<NegBackward0>) tensor(10000.0732, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -10000.072265625
tensor(10000.0732, grad_fn=<NegBackward0>) tensor(10000.0723, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -10000.0693359375
tensor(10000.0723, grad_fn=<NegBackward0>) tensor(10000.0693, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -10000.0673828125
tensor(10000.0693, grad_fn=<NegBackward0>) tensor(10000.0674, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -10000.0654296875
tensor(10000.0674, grad_fn=<NegBackward0>) tensor(10000.0654, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -10000.0634765625
tensor(10000.0654, grad_fn=<NegBackward0>) tensor(10000.0635, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -10000.0634765625
tensor(10000.0635, grad_fn=<NegBackward0>) tensor(10000.0635, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -10000.0625
tensor(10000.0635, grad_fn=<NegBackward0>) tensor(10000.0625, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -10000.0615234375
tensor(10000.0625, grad_fn=<NegBackward0>) tensor(10000.0615, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -10000.0556640625
tensor(10000.0615, grad_fn=<NegBackward0>) tensor(10000.0557, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -10000.0380859375
tensor(10000.0557, grad_fn=<NegBackward0>) tensor(10000.0381, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -9999.9931640625
tensor(10000.0381, grad_fn=<NegBackward0>) tensor(9999.9932, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -9999.9716796875
tensor(9999.9932, grad_fn=<NegBackward0>) tensor(9999.9717, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -9999.962890625
tensor(9999.9717, grad_fn=<NegBackward0>) tensor(9999.9629, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -9999.95703125
tensor(9999.9629, grad_fn=<NegBackward0>) tensor(9999.9570, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -9999.9560546875
tensor(9999.9570, grad_fn=<NegBackward0>) tensor(9999.9561, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -9999.955078125
tensor(9999.9561, grad_fn=<NegBackward0>) tensor(9999.9551, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -9999.9541015625
tensor(9999.9551, grad_fn=<NegBackward0>) tensor(9999.9541, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -9999.9541015625
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9541, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -9999.9521484375
tensor(9999.9541, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -9999.9521484375
tensor(9999.9521, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -9999.9521484375
tensor(9999.9521, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -9999.951171875
tensor(9999.9521, grad_fn=<NegBackward0>) tensor(9999.9512, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -9999.9521484375
tensor(9999.9512, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -9999.951171875
tensor(9999.9512, grad_fn=<NegBackward0>) tensor(9999.9512, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -9999.953125
tensor(9999.9512, grad_fn=<NegBackward0>) tensor(9999.9531, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -9999.94921875
tensor(9999.9512, grad_fn=<NegBackward0>) tensor(9999.9492, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -9999.9501953125
tensor(9999.9492, grad_fn=<NegBackward0>) tensor(9999.9502, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -9999.9521484375
tensor(9999.9492, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -9999.9521484375
tensor(9999.9492, grad_fn=<NegBackward0>) tensor(9999.9521, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -9999.951171875
tensor(9999.9492, grad_fn=<NegBackward0>) tensor(9999.9512, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -9999.9501953125
tensor(9999.9492, grad_fn=<NegBackward0>) tensor(9999.9502, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.2621, 0.7379],
        [0.0047, 0.9953]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0481, 0.9519], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2798, 0.2228],
         [0.6756, 0.1379]],

        [[0.5036, 0.0350],
         [0.5735, 0.7202]],

        [[0.6175, 0.2025],
         [0.6248, 0.6219]],

        [[0.5899, 0.2224],
         [0.6939, 0.6747]],

        [[0.6141, 0.1200],
         [0.5503, 0.5061]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004698695290936857
Average Adjusted Rand Index: 0.00024173310452909832
[0.10739832122469077, -0.0004698695290936857] [0.15432282515903387, 0.00024173310452909832] [9985.4150390625, 9999.9501953125]
